% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[a4paper,8pt,english]{sphinxmanual}


\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsfonts}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{longtable}
\usepackage{sphinx}
\usepackage{multirow}
\usepackage{eqparbox}


\addto\captionsenglish{\renewcommand{\figurename}{Fig. }}
\addto\captionsenglish{\renewcommand{\tablename}{Table }}
\SetupFloatingEnvironment{literal-block}{name=Listing }


	% Use some font with UTF-8 support with XeLaTeX
        \usepackage{fontspec}
        \setsansfont{DejaVu Serif}
        \setromanfont{DejaVu Sans}
        \setmonofont{DejaVu Sans Mono}

     \usepackage[margin=0.5in, top=1in, bottom=1in]{geometry}
        \usepackage{ifthen}

        % Put notes in color and let them be inside a table
	\definecolor{NoteColor}{RGB}{204,255,255}
	\definecolor{WarningColor}{RGB}{255,204,204}
	\definecolor{AttentionColor}{RGB}{255,255,204}
	\definecolor{ImportantColor}{RGB}{192,255,204}
	\definecolor{OtherColor}{RGB}{204,204,204}
        \newlength{\mynoticelength}
        \makeatletter\newenvironment{coloredbox}[1]{%
	   \setlength{\fboxrule}{1pt}
	   \setlength{\fboxsep}{7pt}
	   \setlength{\mynoticelength}{\linewidth}
	   \addtolength{\mynoticelength}{-2\fboxsep}
	   \addtolength{\mynoticelength}{-2\fboxrule}
           \begin{lrbox}{\@tempboxa}\begin{minipage}{\mynoticelength}}{\end{minipage}\end{lrbox}%
	   \ifthenelse%
	      {\equal{\py@noticetype}{note}}%
	      {\colorbox{NoteColor}{\usebox{\@tempboxa}}}%
	      {%
	         \ifthenelse%
	         {\equal{\py@noticetype}{warning}}%
	         {\colorbox{WarningColor}{\usebox{\@tempboxa}}}%
		 {%
	            \ifthenelse%
	            {\equal{\py@noticetype}{attention}}%
	            {\colorbox{AttentionColor}{\usebox{\@tempboxa}}}%
		    {%
	               \ifthenelse%
	               {\equal{\py@noticetype}{important}}%
	               {\colorbox{ImportantColor}{\usebox{\@tempboxa}}}%
	               {\colorbox{OtherColor}{\usebox{\@tempboxa}}}%
		    }%
		 }%
	      }%
        }\makeatother

        \makeatletter
        \renewenvironment{notice}[2]{%
          \def\py@noticetype{#1}
          \begin{coloredbox}{#1}
          \bf\it
          \par\strong{#2}
          \csname py@noticestart@#1\endcsname
        }
	{
          \csname py@noticeend@\py@noticetype\endcsname
          \end{coloredbox}
        }
	\makeatother

     

\title{The kernel core API manual}
\date{March 08, 2018}
\release{4.16.0-rc4+}
\author{The kernel development community}
\newcommand{\sphinxlogo}{}
\renewcommand{\releasename}{Release}
\setcounter{tocdepth}{0}
\makeindex

\makeatletter
\def\PYG@reset{\let\PYG@it=\relax \let\PYG@bf=\relax%
    \let\PYG@ul=\relax \let\PYG@tc=\relax%
    \let\PYG@bc=\relax \let\PYG@ff=\relax}
\def\PYG@tok#1{\csname PYG@tok@#1\endcsname}
\def\PYG@toks#1+{\ifx\relax#1\empty\else%
    \PYG@tok{#1}\expandafter\PYG@toks\fi}
\def\PYG@do#1{\PYG@bc{\PYG@tc{\PYG@ul{%
    \PYG@it{\PYG@bf{\PYG@ff{#1}}}}}}}
\def\PYG#1#2{\PYG@reset\PYG@toks#1+\relax+\PYG@do{#2}}

\expandafter\def\csname PYG@tok@gd\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@gu\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@gt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PYG@tok@gs\endcsname{\let\PYG@bf=\textbf}
\expandafter\def\csname PYG@tok@gr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@cm\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@vg\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@vi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@mh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@cs\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\colorbox[rgb]{1.00,0.94,0.94}{\strut ##1}}}
\expandafter\def\csname PYG@tok@ge\endcsname{\let\PYG@it=\textit}
\expandafter\def\csname PYG@tok@vc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@il\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@go\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.20,0.20,0.20}{##1}}}
\expandafter\def\csname PYG@tok@cp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@gi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PYG@tok@gh\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@ni\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.84,0.33,0.22}{##1}}}
\expandafter\def\csname PYG@tok@nl\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.13,0.44}{##1}}}
\expandafter\def\csname PYG@tok@nn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@no\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.38,0.68,0.84}{##1}}}
\expandafter\def\csname PYG@tok@na\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@nb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@nc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@nd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.33,0.33,0.33}{##1}}}
\expandafter\def\csname PYG@tok@ne\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@nf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.49}{##1}}}
\expandafter\def\csname PYG@tok@si\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.44,0.63,0.82}{##1}}}
\expandafter\def\csname PYG@tok@s2\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@nt\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.45}{##1}}}
\expandafter\def\csname PYG@tok@nv\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@s1\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@ch\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@m\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@gp\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@sh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@ow\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@sx\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@bp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@c1\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@o\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PYG@tok@kc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@c\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@mf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@err\endcsname{\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PYG@tok@mb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@ss\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.32,0.47,0.09}{##1}}}
\expandafter\def\csname PYG@tok@sr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.14,0.33,0.53}{##1}}}
\expandafter\def\csname PYG@tok@mo\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@kd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@mi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@kn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@cpf\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@kr\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@s\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@kp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@w\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PYG@tok@kt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.56,0.13,0.00}{##1}}}
\expandafter\def\csname PYG@tok@sc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@sb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@k\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@se\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@sd\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}

\def\PYGZbs{\char`\\}
\def\PYGZus{\char`\_}
\def\PYGZob{\char`\{}
\def\PYGZcb{\char`\}}
\def\PYGZca{\char`\^}
\def\PYGZam{\char`\&}
\def\PYGZlt{\char`\<}
\def\PYGZgt{\char`\>}
\def\PYGZsh{\char`\#}
\def\PYGZpc{\char`\%}
\def\PYGZdl{\char`\$}
\def\PYGZhy{\char`\-}
\def\PYGZsq{\char`\'}
\def\PYGZdq{\char`\"}
\def\PYGZti{\char`\~}
% for compatibility with earlier versions
\def\PYGZat{@}
\def\PYGZlb{[}
\def\PYGZrb{]}
\makeatother

\renewcommand\PYGZsq{\textquotesingle}

\begin{document}

\maketitle
\tableofcontents
\phantomsection\label{core-api/index::doc}


This is the beginning of a manual for core kernel APIs.  The conversion
(and writing!) of documents for this manual is much appreciated!


\chapter{Core utilities}
\label{core-api/index:core-utilities}\label{core-api/index:core-api-documentation}

\section{The Linux Kernel API}
\label{core-api/kernel-api::doc}\label{core-api/kernel-api:the-linux-kernel-api}

\subsection{List Management Functions}
\label{core-api/kernel-api:list-management-functions}\index{list\_add (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_add}\pysiglinewithargsret{void \bfcode{list\_add}}{struct list\_head *\emph{ new}, struct list\_head *\emph{ head}}{}
add a new entry

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct list\_head * new}}] \leavevmode
new entry to be added

\item[{\code{struct list\_head * head}}] \leavevmode
list head to add it after

\end{description}

\textbf{Description}

Insert a new entry after the specified head.
This is good for implementing stacks.
\index{list\_add\_tail (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_add_tail}\pysiglinewithargsret{void \bfcode{list\_add\_tail}}{struct list\_head *\emph{ new}, struct list\_head *\emph{ head}}{}
add a new entry

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct list\_head * new}}] \leavevmode
new entry to be added

\item[{\code{struct list\_head * head}}] \leavevmode
list head to add it before

\end{description}

\textbf{Description}

Insert a new entry before the specified head.
This is useful for implementing queues.
\index{\_\_list\_del\_entry (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__list_del_entry}\pysiglinewithargsret{void \bfcode{\_\_list\_del\_entry}}{struct list\_head *\emph{ entry}}{}
deletes entry from list.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct list\_head * entry}}] \leavevmode
the element to delete from the list.

\end{description}

\textbf{Note}

{\hyperref[core\string-api/kernel\string-api:c.list_empty]{\emph{\code{list\_empty()}}}} on entry does not return true after this, the entry is
in an undefined state.
\index{list\_replace (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_replace}\pysiglinewithargsret{void \bfcode{list\_replace}}{struct list\_head *\emph{ old}, struct list\_head *\emph{ new}}{}
replace old entry by new one

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct list\_head * old}}] \leavevmode
the element to be replaced

\item[{\code{struct list\_head * new}}] \leavevmode
the new element to insert

\end{description}

\textbf{Description}

If \textbf{old} was empty, it will be overwritten.
\index{list\_del\_init (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_del_init}\pysiglinewithargsret{void \bfcode{list\_del\_init}}{struct list\_head *\emph{ entry}}{}
deletes entry from list and reinitialize it.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct list\_head * entry}}] \leavevmode
the element to delete from the list.

\end{description}
\index{list\_move (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_move}\pysiglinewithargsret{void \bfcode{list\_move}}{struct list\_head *\emph{ list}, struct list\_head *\emph{ head}}{}
delete from one list and add as another's head

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct list\_head * list}}] \leavevmode
the entry to move

\item[{\code{struct list\_head * head}}] \leavevmode
the head that will precede our entry

\end{description}
\index{list\_move\_tail (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_move_tail}\pysiglinewithargsret{void \bfcode{list\_move\_tail}}{struct list\_head *\emph{ list}, struct list\_head *\emph{ head}}{}
delete from one list and add as another's tail

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct list\_head * list}}] \leavevmode
the entry to move

\item[{\code{struct list\_head * head}}] \leavevmode
the head that will follow our entry

\end{description}
\index{list\_is\_last (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_is_last}\pysiglinewithargsret{int \bfcode{list\_is\_last}}{const struct list\_head *\emph{ list}, const struct list\_head *\emph{ head}}{}
tests whether \textbf{list} is the last entry in list \textbf{head}

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const struct list\_head * list}}] \leavevmode
the entry to test

\item[{\code{const struct list\_head * head}}] \leavevmode
the head of the list

\end{description}
\index{list\_empty (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_empty}\pysiglinewithargsret{int \bfcode{list\_empty}}{const struct list\_head *\emph{ head}}{}
tests whether a list is empty

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const struct list\_head * head}}] \leavevmode
the list to test.

\end{description}
\index{list\_empty\_careful (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_empty_careful}\pysiglinewithargsret{int \bfcode{list\_empty\_careful}}{const struct list\_head *\emph{ head}}{}
tests whether a list is empty and not being modified

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const struct list\_head * head}}] \leavevmode
the list to test

\end{description}

\textbf{Description}

tests whether a list is empty \_and\_ checks that no other CPU might be
in the process of modifying either member (next or prev)

\textbf{NOTE}

using {\hyperref[core\string-api/kernel\string-api:c.list_empty_careful]{\emph{\code{list\_empty\_careful()}}}} without synchronization
can only be safe if the only activity that can happen
to the list entry is {\hyperref[core\string-api/kernel\string-api:c.list_del_init]{\emph{\code{list\_del\_init()}}}}. Eg. it cannot be used
if another CPU could re-{\hyperref[core\string-api/kernel\string-api:c.list_add]{\emph{\code{list\_add()}}}} it.
\index{list\_rotate\_left (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_rotate_left}\pysiglinewithargsret{void \bfcode{list\_rotate\_left}}{struct list\_head *\emph{ head}}{}
rotate the list to the left

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct list\_head * head}}] \leavevmode
the head of the list

\end{description}
\index{list\_is\_singular (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_is_singular}\pysiglinewithargsret{int \bfcode{list\_is\_singular}}{const struct list\_head *\emph{ head}}{}
tests whether a list has just one entry.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const struct list\_head * head}}] \leavevmode
the list to test.

\end{description}
\index{list\_cut\_position (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_cut_position}\pysiglinewithargsret{void \bfcode{list\_cut\_position}}{struct list\_head *\emph{ list}, struct list\_head *\emph{ head}, struct list\_head *\emph{ entry}}{}
cut a list into two

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct list\_head * list}}] \leavevmode
a new list to add all removed entries

\item[{\code{struct list\_head * head}}] \leavevmode
a list with entries

\item[{\code{struct list\_head * entry}}] \leavevmode
an entry within head, could be the head itself
and if so we won't cut the list

\end{description}

\textbf{Description}

This helper moves the initial part of \textbf{head}, up to and
including \textbf{entry}, from \textbf{head} to \textbf{list}. You should
pass on \textbf{entry} an element you know is on \textbf{head}. \textbf{list}
should be an empty list or a list you do not care about
losing its data.
\index{list\_splice (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_splice}\pysiglinewithargsret{void \bfcode{list\_splice}}{const struct list\_head *\emph{ list}, struct list\_head *\emph{ head}}{}
join two lists, this is designed for stacks

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const struct list\_head * list}}] \leavevmode
the new list to add.

\item[{\code{struct list\_head * head}}] \leavevmode
the place to add it in the first list.

\end{description}
\index{list\_splice\_tail (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_splice_tail}\pysiglinewithargsret{void \bfcode{list\_splice\_tail}}{struct list\_head *\emph{ list}, struct list\_head *\emph{ head}}{}
join two lists, each list being a queue

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct list\_head * list}}] \leavevmode
the new list to add.

\item[{\code{struct list\_head * head}}] \leavevmode
the place to add it in the first list.

\end{description}
\index{list\_splice\_init (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_splice_init}\pysiglinewithargsret{void \bfcode{list\_splice\_init}}{struct list\_head *\emph{ list}, struct list\_head *\emph{ head}}{}
join two lists and reinitialise the emptied list.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct list\_head * list}}] \leavevmode
the new list to add.

\item[{\code{struct list\_head * head}}] \leavevmode
the place to add it in the first list.

\end{description}

\textbf{Description}

The list at \textbf{list} is reinitialised
\index{list\_splice\_tail\_init (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_splice_tail_init}\pysiglinewithargsret{void \bfcode{list\_splice\_tail\_init}}{struct list\_head *\emph{ list}, struct list\_head *\emph{ head}}{}
join two lists and reinitialise the emptied list

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct list\_head * list}}] \leavevmode
the new list to add.

\item[{\code{struct list\_head * head}}] \leavevmode
the place to add it in the first list.

\end{description}

\textbf{Description}

Each of the lists is a queue.
The list at \textbf{list} is reinitialised
\index{list\_entry (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_entry}\pysiglinewithargsret{\bfcode{list\_entry}}{\emph{ptr}, \emph{type}, \emph{member}}{}
get the struct for this entry

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{ptr}}] \leavevmode
the \code{struct list\_head} pointer.

\item[{\code{type}}] \leavevmode
the type of the struct this is embedded in.

\item[{\code{member}}] \leavevmode
the name of the list\_head within the struct.

\end{description}
\index{list\_first\_entry (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_first_entry}\pysiglinewithargsret{\bfcode{list\_first\_entry}}{\emph{ptr}, \emph{type}, \emph{member}}{}
get the first element from a list

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{ptr}}] \leavevmode
the list head to take the element from.

\item[{\code{type}}] \leavevmode
the type of the struct this is embedded in.

\item[{\code{member}}] \leavevmode
the name of the list\_head within the struct.

\end{description}

\textbf{Description}

Note, that list is expected to be not empty.
\index{list\_last\_entry (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_last_entry}\pysiglinewithargsret{\bfcode{list\_last\_entry}}{\emph{ptr}, \emph{type}, \emph{member}}{}
get the last element from a list

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{ptr}}] \leavevmode
the list head to take the element from.

\item[{\code{type}}] \leavevmode
the type of the struct this is embedded in.

\item[{\code{member}}] \leavevmode
the name of the list\_head within the struct.

\end{description}

\textbf{Description}

Note, that list is expected to be not empty.
\index{list\_first\_entry\_or\_null (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_first_entry_or_null}\pysiglinewithargsret{\bfcode{list\_first\_entry\_or\_null}}{\emph{ptr}, \emph{type}, \emph{member}}{}
get the first element from a list

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{ptr}}] \leavevmode
the list head to take the element from.

\item[{\code{type}}] \leavevmode
the type of the struct this is embedded in.

\item[{\code{member}}] \leavevmode
the name of the list\_head within the struct.

\end{description}

\textbf{Description}

Note that if the list is empty, it returns NULL.
\index{list\_next\_entry (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_next_entry}\pysiglinewithargsret{\bfcode{list\_next\_entry}}{\emph{pos}, \emph{member}}{}
get the next element in list

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{pos}}] \leavevmode
the type * to cursor

\item[{\code{member}}] \leavevmode
the name of the list\_head within the struct.

\end{description}
\index{list\_prev\_entry (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_prev_entry}\pysiglinewithargsret{\bfcode{list\_prev\_entry}}{\emph{pos}, \emph{member}}{}
get the prev element in list

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{pos}}] \leavevmode
the type * to cursor

\item[{\code{member}}] \leavevmode
the name of the list\_head within the struct.

\end{description}
\index{list\_for\_each (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_for_each}\pysiglinewithargsret{\bfcode{list\_for\_each}}{\emph{pos}, \emph{head}}{}
iterate over a list

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{pos}}] \leavevmode
the \code{struct list\_head} to use as a loop cursor.

\item[{\code{head}}] \leavevmode
the head for your list.

\end{description}
\index{list\_for\_each\_prev (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_for_each_prev}\pysiglinewithargsret{\bfcode{list\_for\_each\_prev}}{\emph{pos}, \emph{head}}{}
iterate over a list backwards

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{pos}}] \leavevmode
the \code{struct list\_head} to use as a loop cursor.

\item[{\code{head}}] \leavevmode
the head for your list.

\end{description}
\index{list\_for\_each\_safe (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_for_each_safe}\pysiglinewithargsret{\bfcode{list\_for\_each\_safe}}{\emph{pos}, \emph{n}, \emph{head}}{}
iterate over a list safe against removal of list entry

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{pos}}] \leavevmode
the \code{struct list\_head} to use as a loop cursor.

\item[{\code{n}}] \leavevmode
another \code{struct list\_head} to use as temporary storage

\item[{\code{head}}] \leavevmode
the head for your list.

\end{description}
\index{list\_for\_each\_prev\_safe (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_for_each_prev_safe}\pysiglinewithargsret{\bfcode{list\_for\_each\_prev\_safe}}{\emph{pos}, \emph{n}, \emph{head}}{}
iterate over a list backwards safe against removal of list entry

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{pos}}] \leavevmode
the \code{struct list\_head} to use as a loop cursor.

\item[{\code{n}}] \leavevmode
another \code{struct list\_head} to use as temporary storage

\item[{\code{head}}] \leavevmode
the head for your list.

\end{description}
\index{list\_for\_each\_entry (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_for_each_entry}\pysiglinewithargsret{\bfcode{list\_for\_each\_entry}}{\emph{pos}, \emph{head}, \emph{member}}{}
iterate over list of given type

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{pos}}] \leavevmode
the type * to use as a loop cursor.

\item[{\code{head}}] \leavevmode
the head for your list.

\item[{\code{member}}] \leavevmode
the name of the list\_head within the struct.

\end{description}
\index{list\_for\_each\_entry\_reverse (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_for_each_entry_reverse}\pysiglinewithargsret{\bfcode{list\_for\_each\_entry\_reverse}}{\emph{pos}, \emph{head}, \emph{member}}{}
iterate backwards over list of given type.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{pos}}] \leavevmode
the type * to use as a loop cursor.

\item[{\code{head}}] \leavevmode
the head for your list.

\item[{\code{member}}] \leavevmode
the name of the list\_head within the struct.

\end{description}
\index{list\_prepare\_entry (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_prepare_entry}\pysiglinewithargsret{\bfcode{list\_prepare\_entry}}{\emph{pos}, \emph{head}, \emph{member}}{}
prepare a pos entry for use in {\hyperref[core\string-api/kernel\string-api:c.list_for_each_entry_continue]{\emph{\code{list\_for\_each\_entry\_continue()}}}}

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{pos}}] \leavevmode
the type * to use as a start point

\item[{\code{head}}] \leavevmode
the head of the list

\item[{\code{member}}] \leavevmode
the name of the list\_head within the struct.

\end{description}

\textbf{Description}

Prepares a pos entry for use as a start point in {\hyperref[core\string-api/kernel\string-api:c.list_for_each_entry_continue]{\emph{\code{list\_for\_each\_entry\_continue()}}}}.
\index{list\_for\_each\_entry\_continue (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_for_each_entry_continue}\pysiglinewithargsret{\bfcode{list\_for\_each\_entry\_continue}}{\emph{pos}, \emph{head}, \emph{member}}{}
continue iteration over list of given type

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{pos}}] \leavevmode
the type * to use as a loop cursor.

\item[{\code{head}}] \leavevmode
the head for your list.

\item[{\code{member}}] \leavevmode
the name of the list\_head within the struct.

\end{description}

\textbf{Description}

Continue to iterate over list of given type, continuing after
the current position.
\index{list\_for\_each\_entry\_continue\_reverse (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_for_each_entry_continue_reverse}\pysiglinewithargsret{\bfcode{list\_for\_each\_entry\_continue\_reverse}}{\emph{pos}, \emph{head}, \emph{member}}{}
iterate backwards from the given point

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{pos}}] \leavevmode
the type * to use as a loop cursor.

\item[{\code{head}}] \leavevmode
the head for your list.

\item[{\code{member}}] \leavevmode
the name of the list\_head within the struct.

\end{description}

\textbf{Description}

Start to iterate over list of given type backwards, continuing after
the current position.
\index{list\_for\_each\_entry\_from (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_for_each_entry_from}\pysiglinewithargsret{\bfcode{list\_for\_each\_entry\_from}}{\emph{pos}, \emph{head}, \emph{member}}{}
iterate over list of given type from the current point

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{pos}}] \leavevmode
the type * to use as a loop cursor.

\item[{\code{head}}] \leavevmode
the head for your list.

\item[{\code{member}}] \leavevmode
the name of the list\_head within the struct.

\end{description}

\textbf{Description}

Iterate over list of given type, continuing from current position.
\index{list\_for\_each\_entry\_from\_reverse (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_for_each_entry_from_reverse}\pysiglinewithargsret{\bfcode{list\_for\_each\_entry\_from\_reverse}}{\emph{pos}, \emph{head}, \emph{member}}{}
iterate backwards over list of given type from the current point

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{pos}}] \leavevmode
the type * to use as a loop cursor.

\item[{\code{head}}] \leavevmode
the head for your list.

\item[{\code{member}}] \leavevmode
the name of the list\_head within the struct.

\end{description}

\textbf{Description}

Iterate backwards over list of given type, continuing from current position.
\index{list\_for\_each\_entry\_safe (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_for_each_entry_safe}\pysiglinewithargsret{\bfcode{list\_for\_each\_entry\_safe}}{\emph{pos}, \emph{n}, \emph{head}, \emph{member}}{}
iterate over list of given type safe against removal of list entry

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{pos}}] \leavevmode
the type * to use as a loop cursor.

\item[{\code{n}}] \leavevmode
another type * to use as temporary storage

\item[{\code{head}}] \leavevmode
the head for your list.

\item[{\code{member}}] \leavevmode
the name of the list\_head within the struct.

\end{description}
\index{list\_for\_each\_entry\_safe\_continue (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_for_each_entry_safe_continue}\pysiglinewithargsret{\bfcode{list\_for\_each\_entry\_safe\_continue}}{\emph{pos}, \emph{n}, \emph{head}, \emph{member}}{}
continue list iteration safe against removal

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{pos}}] \leavevmode
the type * to use as a loop cursor.

\item[{\code{n}}] \leavevmode
another type * to use as temporary storage

\item[{\code{head}}] \leavevmode
the head for your list.

\item[{\code{member}}] \leavevmode
the name of the list\_head within the struct.

\end{description}

\textbf{Description}

Iterate over list of given type, continuing after current point,
safe against removal of list entry.
\index{list\_for\_each\_entry\_safe\_from (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_for_each_entry_safe_from}\pysiglinewithargsret{\bfcode{list\_for\_each\_entry\_safe\_from}}{\emph{pos}, \emph{n}, \emph{head}, \emph{member}}{}
iterate over list from current point safe against removal

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{pos}}] \leavevmode
the type * to use as a loop cursor.

\item[{\code{n}}] \leavevmode
another type * to use as temporary storage

\item[{\code{head}}] \leavevmode
the head for your list.

\item[{\code{member}}] \leavevmode
the name of the list\_head within the struct.

\end{description}

\textbf{Description}

Iterate over list of given type from current point, safe against
removal of list entry.
\index{list\_for\_each\_entry\_safe\_reverse (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_for_each_entry_safe_reverse}\pysiglinewithargsret{\bfcode{list\_for\_each\_entry\_safe\_reverse}}{\emph{pos}, \emph{n}, \emph{head}, \emph{member}}{}
iterate backwards over list safe against removal

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{pos}}] \leavevmode
the type * to use as a loop cursor.

\item[{\code{n}}] \leavevmode
another type * to use as temporary storage

\item[{\code{head}}] \leavevmode
the head for your list.

\item[{\code{member}}] \leavevmode
the name of the list\_head within the struct.

\end{description}

\textbf{Description}

Iterate backwards over list of given type, safe against removal
of list entry.
\index{list\_safe\_reset\_next (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_safe_reset_next}\pysiglinewithargsret{\bfcode{list\_safe\_reset\_next}}{\emph{pos}, \emph{n}, \emph{member}}{}
reset a stale list\_for\_each\_entry\_safe loop

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{pos}}] \leavevmode
the loop cursor used in the list\_for\_each\_entry\_safe loop

\item[{\code{n}}] \leavevmode
temporary storage used in list\_for\_each\_entry\_safe

\item[{\code{member}}] \leavevmode
the name of the list\_head within the struct.

\end{description}

\textbf{Description}

list\_safe\_reset\_next is not safe to use in general if the list may be
modified concurrently (eg. the lock is dropped in the loop body). An
exception to this is if the cursor element (pos) is pinned in the list,
and list\_safe\_reset\_next is called after re-taking the lock and before
completing the current iteration of the loop body.
\index{hlist\_for\_each\_entry (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.hlist_for_each_entry}\pysiglinewithargsret{\bfcode{hlist\_for\_each\_entry}}{\emph{pos}, \emph{head}, \emph{member}}{}
iterate over list of given type

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{pos}}] \leavevmode
the type * to use as a loop cursor.

\item[{\code{head}}] \leavevmode
the head for your list.

\item[{\code{member}}] \leavevmode
the name of the hlist\_node within the struct.

\end{description}
\index{hlist\_for\_each\_entry\_continue (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.hlist_for_each_entry_continue}\pysiglinewithargsret{\bfcode{hlist\_for\_each\_entry\_continue}}{\emph{pos}, \emph{member}}{}
iterate over a hlist continuing after current point

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{pos}}] \leavevmode
the type * to use as a loop cursor.

\item[{\code{member}}] \leavevmode
the name of the hlist\_node within the struct.

\end{description}
\index{hlist\_for\_each\_entry\_from (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.hlist_for_each_entry_from}\pysiglinewithargsret{\bfcode{hlist\_for\_each\_entry\_from}}{\emph{pos}, \emph{member}}{}
iterate over a hlist continuing from current point

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{pos}}] \leavevmode
the type * to use as a loop cursor.

\item[{\code{member}}] \leavevmode
the name of the hlist\_node within the struct.

\end{description}
\index{hlist\_for\_each\_entry\_safe (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.hlist_for_each_entry_safe}\pysiglinewithargsret{\bfcode{hlist\_for\_each\_entry\_safe}}{\emph{pos}, \emph{n}, \emph{head}, \emph{member}}{}
iterate over list of given type safe against removal of list entry

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{pos}}] \leavevmode
the type * to use as a loop cursor.

\item[{\code{n}}] \leavevmode
another \code{struct hlist\_node} to use as temporary storage

\item[{\code{head}}] \leavevmode
the head for your list.

\item[{\code{member}}] \leavevmode
the name of the hlist\_node within the struct.

\end{description}


\subsection{Basic C Library Functions}
\label{core-api/kernel-api:basic-c-library-functions}
When writing drivers, you cannot in general use routines which are from
the C Library. Some of the functions have been found generally useful
and they are listed below. The behaviour of these functions may vary
slightly from those defined by ANSI, and these deviations are noted in
the text.


\subsubsection{String Conversions}
\label{core-api/kernel-api:string-conversions}\index{simple\_strtoull (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.simple_strtoull}\pysiglinewithargsret{unsigned long long \bfcode{simple\_strtoull}}{const char *\emph{ cp}, char **\emph{ endp}, unsigned int\emph{ base}}{}
convert a string to an unsigned long long

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * cp}}] \leavevmode
The start of the string

\item[{\code{char ** endp}}] \leavevmode
A pointer to the end of the parsed string will be placed here

\item[{\code{unsigned int base}}] \leavevmode
The number base to use

\end{description}

\textbf{Description}

This function is obsolete. Please use kstrtoull instead.
\index{simple\_strtoul (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.simple_strtoul}\pysiglinewithargsret{unsigned long \bfcode{simple\_strtoul}}{const char *\emph{ cp}, char **\emph{ endp}, unsigned int\emph{ base}}{}
convert a string to an unsigned long

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * cp}}] \leavevmode
The start of the string

\item[{\code{char ** endp}}] \leavevmode
A pointer to the end of the parsed string will be placed here

\item[{\code{unsigned int base}}] \leavevmode
The number base to use

\end{description}

\textbf{Description}

This function is obsolete. Please use kstrtoul instead.
\index{simple\_strtol (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.simple_strtol}\pysiglinewithargsret{long \bfcode{simple\_strtol}}{const char *\emph{ cp}, char **\emph{ endp}, unsigned int\emph{ base}}{}
convert a string to a signed long

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * cp}}] \leavevmode
The start of the string

\item[{\code{char ** endp}}] \leavevmode
A pointer to the end of the parsed string will be placed here

\item[{\code{unsigned int base}}] \leavevmode
The number base to use

\end{description}

\textbf{Description}

This function is obsolete. Please use kstrtol instead.
\index{simple\_strtoll (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.simple_strtoll}\pysiglinewithargsret{long long \bfcode{simple\_strtoll}}{const char *\emph{ cp}, char **\emph{ endp}, unsigned int\emph{ base}}{}
convert a string to a signed long long

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * cp}}] \leavevmode
The start of the string

\item[{\code{char ** endp}}] \leavevmode
A pointer to the end of the parsed string will be placed here

\item[{\code{unsigned int base}}] \leavevmode
The number base to use

\end{description}

\textbf{Description}

This function is obsolete. Please use kstrtoll instead.
\index{vsnprintf (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.vsnprintf}\pysiglinewithargsret{int \bfcode{vsnprintf}}{char *\emph{ buf}, size\_t\emph{ size}, const char *\emph{ fmt}, va\_list\emph{ args}}{}
Format a string and place it in a buffer

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{char * buf}}] \leavevmode
The buffer to place the result into

\item[{\code{size\_t size}}] \leavevmode
The size of the buffer, including the trailing null space

\item[{\code{const char * fmt}}] \leavevmode
The format string to use

\item[{\code{va\_list args}}] \leavevmode
Arguments for the format string

\end{description}

\textbf{Description}

This function generally follows C99 vsnprintf, but has some
extensions and a few limitations:
\begin{itemize}
\item {} 
\code{{}`{}`n{}`{}`} is unsupported

\item {} 
\code{{}`{}`p{}`{}`*} is handled by \code{pointer()}

\end{itemize}

See \code{pointer()} or Documentation/core-api/printk-formats.rst for more
extensive description.

\textbf{Please update the documentation in both places when making changes}

The return value is the number of characters which would
be generated for the given input, excluding the trailing
`0', as per ISO C99. If you want to have the exact
number of characters written into \textbf{buf} as return value
(not including the trailing `0'), use {\hyperref[core\string-api/kernel\string-api:c.vscnprintf]{\emph{\code{vscnprintf()}}}}. If the
return is greater than or equal to \textbf{size}, the resulting
string is truncated.

If you're not already dealing with a va\_list consider using {\hyperref[core\string-api/kernel\string-api:c.snprintf]{\emph{\code{snprintf()}}}}.
\index{vscnprintf (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.vscnprintf}\pysiglinewithargsret{int \bfcode{vscnprintf}}{char *\emph{ buf}, size\_t\emph{ size}, const char *\emph{ fmt}, va\_list\emph{ args}}{}
Format a string and place it in a buffer

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{char * buf}}] \leavevmode
The buffer to place the result into

\item[{\code{size\_t size}}] \leavevmode
The size of the buffer, including the trailing null space

\item[{\code{const char * fmt}}] \leavevmode
The format string to use

\item[{\code{va\_list args}}] \leavevmode
Arguments for the format string

\end{description}

\textbf{Description}

The return value is the number of characters which have been written into
the \textbf{buf} not including the trailing `0'. If \textbf{size} is == 0 the function
returns 0.

If you're not already dealing with a va\_list consider using {\hyperref[core\string-api/kernel\string-api:c.scnprintf]{\emph{\code{scnprintf()}}}}.

See the {\hyperref[core\string-api/kernel\string-api:c.vsnprintf]{\emph{\code{vsnprintf()}}}} documentation for format string extensions over C99.
\index{snprintf (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.snprintf}\pysiglinewithargsret{int \bfcode{snprintf}}{char *\emph{ buf}, size\_t\emph{ size}, const char *\emph{ fmt}, ...}{}
Format a string and place it in a buffer

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{char * buf}}] \leavevmode
The buffer to place the result into

\item[{\code{size\_t size}}] \leavevmode
The size of the buffer, including the trailing null space

\item[{\code{const char * fmt}}] \leavevmode
The format string to use

\item[{\code{...}}] \leavevmode
Arguments for the format string

\end{description}

\textbf{Description}

The return value is the number of characters which would be
generated for the given input, excluding the trailing null,
as per ISO C99.  If the return is greater than or equal to
\textbf{size}, the resulting string is truncated.

See the {\hyperref[core\string-api/kernel\string-api:c.vsnprintf]{\emph{\code{vsnprintf()}}}} documentation for format string extensions over C99.
\index{scnprintf (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.scnprintf}\pysiglinewithargsret{int \bfcode{scnprintf}}{char *\emph{ buf}, size\_t\emph{ size}, const char *\emph{ fmt}, ...}{}
Format a string and place it in a buffer

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{char * buf}}] \leavevmode
The buffer to place the result into

\item[{\code{size\_t size}}] \leavevmode
The size of the buffer, including the trailing null space

\item[{\code{const char * fmt}}] \leavevmode
The format string to use

\item[{\code{...}}] \leavevmode
Arguments for the format string

\end{description}

\textbf{Description}

The return value is the number of characters written into \textbf{buf} not including
the trailing `0'. If \textbf{size} is == 0 the function returns 0.
\index{vsprintf (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.vsprintf}\pysiglinewithargsret{int \bfcode{vsprintf}}{char *\emph{ buf}, const char *\emph{ fmt}, va\_list\emph{ args}}{}
Format a string and place it in a buffer

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{char * buf}}] \leavevmode
The buffer to place the result into

\item[{\code{const char * fmt}}] \leavevmode
The format string to use

\item[{\code{va\_list args}}] \leavevmode
Arguments for the format string

\end{description}

\textbf{Description}

The function returns the number of characters written
into \textbf{buf}. Use {\hyperref[core\string-api/kernel\string-api:c.vsnprintf]{\emph{\code{vsnprintf()}}}} or {\hyperref[core\string-api/kernel\string-api:c.vscnprintf]{\emph{\code{vscnprintf()}}}} in order to avoid
buffer overflows.

If you're not already dealing with a va\_list consider using {\hyperref[core\string-api/kernel\string-api:c.sprintf]{\emph{\code{sprintf()}}}}.

See the {\hyperref[core\string-api/kernel\string-api:c.vsnprintf]{\emph{\code{vsnprintf()}}}} documentation for format string extensions over C99.
\index{sprintf (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.sprintf}\pysiglinewithargsret{int \bfcode{sprintf}}{char *\emph{ buf}, const char *\emph{ fmt}, ...}{}
Format a string and place it in a buffer

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{char * buf}}] \leavevmode
The buffer to place the result into

\item[{\code{const char * fmt}}] \leavevmode
The format string to use

\item[{\code{...}}] \leavevmode
Arguments for the format string

\end{description}

\textbf{Description}

The function returns the number of characters written
into \textbf{buf}. Use {\hyperref[core\string-api/kernel\string-api:c.snprintf]{\emph{\code{snprintf()}}}} or {\hyperref[core\string-api/kernel\string-api:c.scnprintf]{\emph{\code{scnprintf()}}}} in order to avoid
buffer overflows.

See the {\hyperref[core\string-api/kernel\string-api:c.vsnprintf]{\emph{\code{vsnprintf()}}}} documentation for format string extensions over C99.
\index{vbin\_printf (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.vbin_printf}\pysiglinewithargsret{int \bfcode{vbin\_printf}}{u32 *\emph{ bin\_buf}, size\_t\emph{ size}, const char *\emph{ fmt}, va\_list\emph{ args}}{}
Parse a format string and place args' binary value in a buffer

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{u32 * bin\_buf}}] \leavevmode
The buffer to place args' binary value

\item[{\code{size\_t size}}] \leavevmode
The size of the buffer(by words(32bits), not characters)

\item[{\code{const char * fmt}}] \leavevmode
The format string to use

\item[{\code{va\_list args}}] \leavevmode
Arguments for the format string

\end{description}

\textbf{Description}

The format follows C99 vsnprintf, except \code{n} is ignored, and its argument
is skipped.

The return value is the number of words(32bits) which would be generated for
the given input.

\textbf{NOTE}

If the return value is greater than \textbf{size}, the resulting bin\_buf is NOT
valid for {\hyperref[core\string-api/kernel\string-api:c.bstr_printf]{\emph{\code{bstr\_printf()}}}}.
\index{bstr\_printf (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.bstr_printf}\pysiglinewithargsret{int \bfcode{bstr\_printf}}{char *\emph{ buf}, size\_t\emph{ size}, const char *\emph{ fmt}, const u32 *\emph{ bin\_buf}}{}
Format a string from binary arguments and place it in a buffer

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{char * buf}}] \leavevmode
The buffer to place the result into

\item[{\code{size\_t size}}] \leavevmode
The size of the buffer, including the trailing null space

\item[{\code{const char * fmt}}] \leavevmode
The format string to use

\item[{\code{const u32 * bin\_buf}}] \leavevmode
Binary arguments for the format string

\end{description}

\textbf{Description}

This function like C99 vsnprintf, but the difference is that vsnprintf gets
arguments from stack, and bstr\_printf gets arguments from \textbf{bin\_buf} which is
a binary buffer that generated by vbin\_printf.
\begin{description}
\item[{The format follows C99 vsnprintf, but has some extensions:}] \leavevmode
see vsnprintf comment for details.

\end{description}

The return value is the number of characters which would
be generated for the given input, excluding the trailing
`0', as per ISO C99. If you want to have the exact
number of characters written into \textbf{buf} as return value
(not including the trailing `0'), use {\hyperref[core\string-api/kernel\string-api:c.vscnprintf]{\emph{\code{vscnprintf()}}}}. If the
return is greater than or equal to \textbf{size}, the resulting
string is truncated.
\index{bprintf (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.bprintf}\pysiglinewithargsret{int \bfcode{bprintf}}{u32 *\emph{ bin\_buf}, size\_t\emph{ size}, const char *\emph{ fmt}, ...}{}
Parse a format string and place args' binary value in a buffer

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{u32 * bin\_buf}}] \leavevmode
The buffer to place args' binary value

\item[{\code{size\_t size}}] \leavevmode
The size of the buffer(by words(32bits), not characters)

\item[{\code{const char * fmt}}] \leavevmode
The format string to use

\item[{\code{...}}] \leavevmode
Arguments for the format string

\end{description}

\textbf{Description}

The function returns the number of words(u32) written
into \textbf{bin\_buf}.
\index{vsscanf (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.vsscanf}\pysiglinewithargsret{int \bfcode{vsscanf}}{const char *\emph{ buf}, const char *\emph{ fmt}, va\_list\emph{ args}}{}
Unformat a buffer into a list of arguments

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * buf}}] \leavevmode
input buffer

\item[{\code{const char * fmt}}] \leavevmode
format of buffer

\item[{\code{va\_list args}}] \leavevmode
arguments

\end{description}
\index{sscanf (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.sscanf}\pysiglinewithargsret{int \bfcode{sscanf}}{const char *\emph{ buf}, const char *\emph{ fmt}, ...}{}
Unformat a buffer into a list of arguments

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * buf}}] \leavevmode
input buffer

\item[{\code{const char * fmt}}] \leavevmode
formatting of buffer

\item[{\code{...}}] \leavevmode
resulting arguments

\end{description}
\index{kstrtol (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kstrtol}\pysiglinewithargsret{int \bfcode{kstrtol}}{const char *\emph{ s}, unsigned int\emph{ base}, long *\emph{ res}}{}
convert a string to a long

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * s}}] \leavevmode
The start of the string. The string must be null-terminated, and may also
include a single newline before its terminating null. The first character
may also be a plus sign or a minus sign.

\item[{\code{unsigned int base}}] \leavevmode
The number base to use. The maximum supported base is 16. If base is
given as 0, then the base of the string is automatically detected with the
conventional semantics - If it begins with 0x the number will be parsed as a
hexadecimal (case insensitive), if it otherwise begins with 0, it will be
parsed as an octal number. Otherwise it will be parsed as a decimal.

\item[{\code{long * res}}] \leavevmode
Where to write the result of the conversion on success.

\end{description}

\textbf{Description}

Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.
Used as a replacement for the obsolete simple\_strtoull. Return code must
be checked.
\index{kstrtoul (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kstrtoul}\pysiglinewithargsret{int \bfcode{kstrtoul}}{const char *\emph{ s}, unsigned int\emph{ base}, unsigned long *\emph{ res}}{}
convert a string to an unsigned long

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * s}}] \leavevmode
The start of the string. The string must be null-terminated, and may also
include a single newline before its terminating null. The first character
may also be a plus sign, but not a minus sign.

\item[{\code{unsigned int base}}] \leavevmode
The number base to use. The maximum supported base is 16. If base is
given as 0, then the base of the string is automatically detected with the
conventional semantics - If it begins with 0x the number will be parsed as a
hexadecimal (case insensitive), if it otherwise begins with 0, it will be
parsed as an octal number. Otherwise it will be parsed as a decimal.

\item[{\code{unsigned long * res}}] \leavevmode
Where to write the result of the conversion on success.

\end{description}

\textbf{Description}

Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.
Used as a replacement for the obsolete simple\_strtoull. Return code must
be checked.
\index{kstrtoull (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kstrtoull}\pysiglinewithargsret{int \bfcode{kstrtoull}}{const char *\emph{ s}, unsigned int\emph{ base}, unsigned long long *\emph{ res}}{}
convert a string to an unsigned long long

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * s}}] \leavevmode
The start of the string. The string must be null-terminated, and may also
include a single newline before its terminating null. The first character
may also be a plus sign, but not a minus sign.

\item[{\code{unsigned int base}}] \leavevmode
The number base to use. The maximum supported base is 16. If base is
given as 0, then the base of the string is automatically detected with the
conventional semantics - If it begins with 0x the number will be parsed as a
hexadecimal (case insensitive), if it otherwise begins with 0, it will be
parsed as an octal number. Otherwise it will be parsed as a decimal.

\item[{\code{unsigned long long * res}}] \leavevmode
Where to write the result of the conversion on success.

\end{description}

\textbf{Description}

Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.
Used as a replacement for the obsolete simple\_strtoull. Return code must
be checked.
\index{kstrtoll (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kstrtoll}\pysiglinewithargsret{int \bfcode{kstrtoll}}{const char *\emph{ s}, unsigned int\emph{ base}, long long *\emph{ res}}{}
convert a string to a long long

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * s}}] \leavevmode
The start of the string. The string must be null-terminated, and may also
include a single newline before its terminating null. The first character
may also be a plus sign or a minus sign.

\item[{\code{unsigned int base}}] \leavevmode
The number base to use. The maximum supported base is 16. If base is
given as 0, then the base of the string is automatically detected with the
conventional semantics - If it begins with 0x the number will be parsed as a
hexadecimal (case insensitive), if it otherwise begins with 0, it will be
parsed as an octal number. Otherwise it will be parsed as a decimal.

\item[{\code{long long * res}}] \leavevmode
Where to write the result of the conversion on success.

\end{description}

\textbf{Description}

Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.
Used as a replacement for the obsolete simple\_strtoull. Return code must
be checked.
\index{kstrtouint (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kstrtouint}\pysiglinewithargsret{int \bfcode{kstrtouint}}{const char *\emph{ s}, unsigned int\emph{ base}, unsigned int *\emph{ res}}{}
convert a string to an unsigned int

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * s}}] \leavevmode
The start of the string. The string must be null-terminated, and may also
include a single newline before its terminating null. The first character
may also be a plus sign, but not a minus sign.

\item[{\code{unsigned int base}}] \leavevmode
The number base to use. The maximum supported base is 16. If base is
given as 0, then the base of the string is automatically detected with the
conventional semantics - If it begins with 0x the number will be parsed as a
hexadecimal (case insensitive), if it otherwise begins with 0, it will be
parsed as an octal number. Otherwise it will be parsed as a decimal.

\item[{\code{unsigned int * res}}] \leavevmode
Where to write the result of the conversion on success.

\end{description}

\textbf{Description}

Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.
Used as a replacement for the obsolete simple\_strtoull. Return code must
be checked.
\index{kstrtoint (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kstrtoint}\pysiglinewithargsret{int \bfcode{kstrtoint}}{const char *\emph{ s}, unsigned int\emph{ base}, int *\emph{ res}}{}
convert a string to an int

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * s}}] \leavevmode
The start of the string. The string must be null-terminated, and may also
include a single newline before its terminating null. The first character
may also be a plus sign or a minus sign.

\item[{\code{unsigned int base}}] \leavevmode
The number base to use. The maximum supported base is 16. If base is
given as 0, then the base of the string is automatically detected with the
conventional semantics - If it begins with 0x the number will be parsed as a
hexadecimal (case insensitive), if it otherwise begins with 0, it will be
parsed as an octal number. Otherwise it will be parsed as a decimal.

\item[{\code{int * res}}] \leavevmode
Where to write the result of the conversion on success.

\end{description}

\textbf{Description}

Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.
Used as a replacement for the obsolete simple\_strtoull. Return code must
be checked.
\index{kstrtobool (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kstrtobool}\pysiglinewithargsret{int \bfcode{kstrtobool}}{const char *\emph{ s}, bool *\emph{ res}}{}
convert common user inputs into boolean values

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * s}}] \leavevmode
input string

\item[{\code{bool * res}}] \leavevmode
result

\end{description}

\textbf{Description}

This routine returns 0 iff the first character is one of `Yy1Nn0', or
{[}oO{]}{[}NnFf{]} for ``on'' and ``off''. Otherwise it will return -EINVAL.  Value
pointed to by res is updated upon finding a match.


\subsubsection{String Manipulation}
\label{core-api/kernel-api:string-manipulation}\index{strncasecmp (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.strncasecmp}\pysiglinewithargsret{int \bfcode{strncasecmp}}{const char *\emph{ s1}, const char *\emph{ s2}, size\_t\emph{ len}}{}
Case insensitive, length-limited string comparison

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * s1}}] \leavevmode
One string

\item[{\code{const char * s2}}] \leavevmode
The other string

\item[{\code{size\_t len}}] \leavevmode
the maximum number of characters to compare

\end{description}
\index{strcpy (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.strcpy}\pysiglinewithargsret{char * \bfcode{strcpy}}{char *\emph{ dest}, const char *\emph{ src}}{}
Copy a \code{NUL} terminated string

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{char * dest}}] \leavevmode
Where to copy the string to

\item[{\code{const char * src}}] \leavevmode
Where to copy the string from

\end{description}
\index{strncpy (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.strncpy}\pysiglinewithargsret{char * \bfcode{strncpy}}{char *\emph{ dest}, const char *\emph{ src}, size\_t\emph{ count}}{}
Copy a length-limited, C-string

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{char * dest}}] \leavevmode
Where to copy the string to

\item[{\code{const char * src}}] \leavevmode
Where to copy the string from

\item[{\code{size\_t count}}] \leavevmode
The maximum number of bytes to copy

\end{description}

\textbf{Description}

The result is not \code{NUL-terminated} if the source exceeds
\textbf{count} bytes.

In the case where the length of \textbf{src} is less than  that  of
count, the remainder of \textbf{dest} will be padded with \code{NUL}.
\index{strlcpy (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.strlcpy}\pysiglinewithargsret{size\_t \bfcode{strlcpy}}{char *\emph{ dest}, const char *\emph{ src}, size\_t\emph{ size}}{}
Copy a C-string into a sized buffer

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{char * dest}}] \leavevmode
Where to copy the string to

\item[{\code{const char * src}}] \leavevmode
Where to copy the string from

\item[{\code{size\_t size}}] \leavevmode
size of destination buffer

\end{description}

\textbf{Description}

Compatible with \code{*BSD}: the result is always a valid
NUL-terminated string that fits in the buffer (unless,
of course, the buffer size is zero). It does not pad
out the result like {\hyperref[core\string-api/kernel\string-api:c.strncpy]{\emph{\code{strncpy()}}}} does.
\index{strscpy (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.strscpy}\pysiglinewithargsret{ssize\_t \bfcode{strscpy}}{char *\emph{ dest}, const char *\emph{ src}, size\_t\emph{ count}}{}
Copy a C-string into a sized buffer

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{char * dest}}] \leavevmode
Where to copy the string to

\item[{\code{const char * src}}] \leavevmode
Where to copy the string from

\item[{\code{size\_t count}}] \leavevmode
Size of destination buffer

\end{description}

\textbf{Description}

Copy the string, or as much of it as fits, into the dest buffer.
The routine returns the number of characters copied (not including
the trailing NUL) or -E2BIG if the destination buffer wasn't big enough.
The behavior is undefined if the string buffers overlap.
The destination buffer is always NUL terminated, unless it's zero-sized.

Preferred to {\hyperref[core\string-api/kernel\string-api:c.strlcpy]{\emph{\code{strlcpy()}}}} since the API doesn't require reading memory
from the src string beyond the specified ``count'' bytes, and since
the return value is easier to error-check than {\hyperref[core\string-api/kernel\string-api:c.strlcpy]{\emph{\code{strlcpy()}}}}`s.
In addition, the implementation is robust to the string changing out
from underneath it, unlike the current {\hyperref[core\string-api/kernel\string-api:c.strlcpy]{\emph{\code{strlcpy()}}}} implementation.

Preferred to {\hyperref[core\string-api/kernel\string-api:c.strncpy]{\emph{\code{strncpy()}}}} since it always returns a valid string, and
doesn't unnecessarily force the tail of the destination buffer to be
zeroed.  If the zeroing is desired, it's likely cleaner to use {\hyperref[core\string-api/kernel\string-api:c.strscpy]{\emph{\code{strscpy()}}}}
with an overflow test, then just {\hyperref[core\string-api/kernel\string-api:c.memset]{\emph{\code{memset()}}}} the tail of the dest buffer.
\index{strcat (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.strcat}\pysiglinewithargsret{char * \bfcode{strcat}}{char *\emph{ dest}, const char *\emph{ src}}{}
Append one \code{NUL-terminated} string to another

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{char * dest}}] \leavevmode
The string to be appended to

\item[{\code{const char * src}}] \leavevmode
The string to append to it

\end{description}
\index{strncat (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.strncat}\pysiglinewithargsret{char * \bfcode{strncat}}{char *\emph{ dest}, const char *\emph{ src}, size\_t\emph{ count}}{}
Append a length-limited, C-string to another

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{char * dest}}] \leavevmode
The string to be appended to

\item[{\code{const char * src}}] \leavevmode
The string to append to it

\item[{\code{size\_t count}}] \leavevmode
The maximum numbers of bytes to copy

\end{description}

\textbf{Description}

Note that in contrast to {\hyperref[core\string-api/kernel\string-api:c.strncpy]{\emph{\code{strncpy()}}}}, {\hyperref[core\string-api/kernel\string-api:c.strncat]{\emph{\code{strncat()}}}} ensures the result is
terminated.
\index{strlcat (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.strlcat}\pysiglinewithargsret{size\_t \bfcode{strlcat}}{char *\emph{ dest}, const char *\emph{ src}, size\_t\emph{ count}}{}
Append a length-limited, C-string to another

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{char * dest}}] \leavevmode
The string to be appended to

\item[{\code{const char * src}}] \leavevmode
The string to append to it

\item[{\code{size\_t count}}] \leavevmode
The size of the destination buffer.

\end{description}
\index{strcmp (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.strcmp}\pysiglinewithargsret{int \bfcode{strcmp}}{const char *\emph{ cs}, const char *\emph{ ct}}{}
Compare two strings

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * cs}}] \leavevmode
One string

\item[{\code{const char * ct}}] \leavevmode
Another string

\end{description}
\index{strncmp (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.strncmp}\pysiglinewithargsret{int \bfcode{strncmp}}{const char *\emph{ cs}, const char *\emph{ ct}, size\_t\emph{ count}}{}
Compare two length-limited strings

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * cs}}] \leavevmode
One string

\item[{\code{const char * ct}}] \leavevmode
Another string

\item[{\code{size\_t count}}] \leavevmode
The maximum number of bytes to compare

\end{description}
\index{strchr (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.strchr}\pysiglinewithargsret{char * \bfcode{strchr}}{const char *\emph{ s}, int\emph{ c}}{}
Find the first occurrence of a character in a string

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * s}}] \leavevmode
The string to be searched

\item[{\code{int c}}] \leavevmode
The character to search for

\end{description}
\index{strchrnul (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.strchrnul}\pysiglinewithargsret{char * \bfcode{strchrnul}}{const char *\emph{ s}, int\emph{ c}}{}
Find and return a character in a string, or end of string

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * s}}] \leavevmode
The string to be searched

\item[{\code{int c}}] \leavevmode
The character to search for

\end{description}

\textbf{Description}

Returns pointer to first occurrence of `c' in s. If c is not found, then
return a pointer to the null byte at the end of s.
\index{strrchr (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.strrchr}\pysiglinewithargsret{char * \bfcode{strrchr}}{const char *\emph{ s}, int\emph{ c}}{}
Find the last occurrence of a character in a string

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * s}}] \leavevmode
The string to be searched

\item[{\code{int c}}] \leavevmode
The character to search for

\end{description}
\index{strnchr (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.strnchr}\pysiglinewithargsret{char * \bfcode{strnchr}}{const char *\emph{ s}, size\_t\emph{ count}, int\emph{ c}}{}
Find a character in a length limited string

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * s}}] \leavevmode
The string to be searched

\item[{\code{size\_t count}}] \leavevmode
The number of characters to be searched

\item[{\code{int c}}] \leavevmode
The character to search for

\end{description}
\index{skip\_spaces (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.skip_spaces}\pysiglinewithargsret{char * \bfcode{skip\_spaces}}{const char *\emph{ str}}{}
Removes leading whitespace from \textbf{str}.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * str}}] \leavevmode
The string to be stripped.

\end{description}

\textbf{Description}

Returns a pointer to the first non-whitespace character in \textbf{str}.
\index{strim (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.strim}\pysiglinewithargsret{char * \bfcode{strim}}{char *\emph{ s}}{}
Removes leading and trailing whitespace from \textbf{s}.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{char * s}}] \leavevmode
The string to be stripped.

\end{description}

\textbf{Description}

Note that the first trailing whitespace is replaced with a \code{NUL-terminator}
in the given string \textbf{s}. Returns a pointer to the first non-whitespace
character in \textbf{s}.
\index{strlen (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.strlen}\pysiglinewithargsret{size\_t \bfcode{strlen}}{const char *\emph{ s}}{}
Find the length of a string

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * s}}] \leavevmode
The string to be sized

\end{description}
\index{strnlen (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.strnlen}\pysiglinewithargsret{size\_t \bfcode{strnlen}}{const char *\emph{ s}, size\_t\emph{ count}}{}
Find the length of a length-limited string

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * s}}] \leavevmode
The string to be sized

\item[{\code{size\_t count}}] \leavevmode
The maximum number of bytes to search

\end{description}
\index{strspn (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.strspn}\pysiglinewithargsret{size\_t \bfcode{strspn}}{const char *\emph{ s}, const char *\emph{ accept}}{}
Calculate the length of the initial substring of \textbf{s} which only contain letters in \textbf{accept}

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * s}}] \leavevmode
The string to be searched

\item[{\code{const char * accept}}] \leavevmode
The string to search for

\end{description}
\index{strcspn (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.strcspn}\pysiglinewithargsret{size\_t \bfcode{strcspn}}{const char *\emph{ s}, const char *\emph{ reject}}{}
Calculate the length of the initial substring of \textbf{s} which does not contain letters in \textbf{reject}

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * s}}] \leavevmode
The string to be searched

\item[{\code{const char * reject}}] \leavevmode
The string to avoid

\end{description}
\index{strpbrk (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.strpbrk}\pysiglinewithargsret{char * \bfcode{strpbrk}}{const char *\emph{ cs}, const char *\emph{ ct}}{}
Find the first occurrence of a set of characters

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * cs}}] \leavevmode
The string to be searched

\item[{\code{const char * ct}}] \leavevmode
The characters to search for

\end{description}
\index{strsep (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.strsep}\pysiglinewithargsret{char * \bfcode{strsep}}{char **\emph{ s}, const char *\emph{ ct}}{}
Split a string into tokens

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{char ** s}}] \leavevmode
The string to be searched

\item[{\code{const char * ct}}] \leavevmode
The characters to search for

\end{description}

\textbf{Description}

{\hyperref[core\string-api/kernel\string-api:c.strsep]{\emph{\code{strsep()}}}} updates \textbf{s} to point after the token, ready for the next call.

It returns empty tokens, too, behaving exactly like the libc function
of that name. In fact, it was stolen from glibc2 and de-fancy-fied.
Same semantics, slimmer shape. ;)
\index{sysfs\_streq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.sysfs_streq}\pysiglinewithargsret{bool \bfcode{sysfs\_streq}}{const char *\emph{ s1}, const char *\emph{ s2}}{}
return true if strings are equal, modulo trailing newline

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * s1}}] \leavevmode
one string

\item[{\code{const char * s2}}] \leavevmode
another string

\end{description}

\textbf{Description}

This routine returns true iff two strings are equal, treating both
NUL and newline-then-NUL as equivalent string terminations.  It's
geared for use with sysfs input strings, which generally terminate
with newlines but are compared against values without newlines.
\index{match\_string (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.match_string}\pysiglinewithargsret{int \bfcode{match\_string}}{const char *const *\emph{ array}, size\_t\emph{ n}, const char *\emph{ string}}{}
matches given string in an array

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char *const * array}}] \leavevmode
array of strings

\item[{\code{size\_t n}}] \leavevmode
number of strings in the array or -1 for NULL terminated arrays

\item[{\code{const char * string}}] \leavevmode
string to match with

\end{description}

\textbf{Return}

index of a \textbf{string} in the \textbf{array} if matches, or \code{-EINVAL} otherwise.
\index{\_\_sysfs\_match\_string (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__sysfs_match_string}\pysiglinewithargsret{int \bfcode{\_\_sysfs\_match\_string}}{const char *const *\emph{ array}, size\_t\emph{ n}, const char *\emph{ str}}{}
matches given string in an array

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char *const * array}}] \leavevmode
array of strings

\item[{\code{size\_t n}}] \leavevmode
number of strings in the array or -1 for NULL terminated arrays

\item[{\code{const char * str}}] \leavevmode
string to match with

\end{description}

\textbf{Description}

Returns index of \textbf{str} in the \textbf{array} or -EINVAL, just like {\hyperref[core\string-api/kernel\string-api:c.match_string]{\emph{\code{match\_string()}}}}.
Uses sysfs\_streq instead of strcmp for matching.
\index{memset (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.memset}\pysiglinewithargsret{void * \bfcode{memset}}{void *\emph{ s}, int\emph{ c}, size\_t\emph{ count}}{}
Fill a region of memory with the given value

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void * s}}] \leavevmode
Pointer to the start of the area.

\item[{\code{int c}}] \leavevmode
The byte to fill the area with

\item[{\code{size\_t count}}] \leavevmode
The size of the area.

\end{description}

\textbf{Description}

Do not use {\hyperref[core\string-api/kernel\string-api:c.memset]{\emph{\code{memset()}}}} to access IO space, use \code{memset\_io()} instead.
\index{memzero\_explicit (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.memzero_explicit}\pysiglinewithargsret{void \bfcode{memzero\_explicit}}{void *\emph{ s}, size\_t\emph{ count}}{}
Fill a region of memory (e.g. sensitive keying data) with 0s.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void * s}}] \leavevmode
Pointer to the start of the area.

\item[{\code{size\_t count}}] \leavevmode
The size of the area.

\end{description}

\textbf{Note}

usually using {\hyperref[core\string-api/kernel\string-api:c.memset]{\emph{\code{memset()}}}} is just fine (!), but in cases
where clearing out \_local\_ data at the end of a scope is
necessary, {\hyperref[core\string-api/kernel\string-api:c.memzero_explicit]{\emph{\code{memzero\_explicit()}}}} should be used instead in
order to prevent the compiler from optimising away zeroing.

{\hyperref[core\string-api/kernel\string-api:c.memzero_explicit]{\emph{\code{memzero\_explicit()}}}} doesn't need an arch-specific version as
it just invokes the one of {\hyperref[core\string-api/kernel\string-api:c.memset]{\emph{\code{memset()}}}} implicitly.
\index{memset16 (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.memset16}\pysiglinewithargsret{void * \bfcode{memset16}}{uint16\_t *\emph{ s}, uint16\_t\emph{ v}, size\_t\emph{ count}}{}
Fill a memory area with a uint16\_t

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{uint16\_t * s}}] \leavevmode
Pointer to the start of the area.

\item[{\code{uint16\_t v}}] \leavevmode
The value to fill the area with

\item[{\code{size\_t count}}] \leavevmode
The number of values to store

\end{description}

\textbf{Description}

Differs from {\hyperref[core\string-api/kernel\string-api:c.memset]{\emph{\code{memset()}}}} in that it fills with a uint16\_t instead
of a byte.  Remember that \textbf{count} is the number of uint16\_ts to
store, not the number of bytes.
\index{memset32 (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.memset32}\pysiglinewithargsret{void * \bfcode{memset32}}{uint32\_t *\emph{ s}, uint32\_t\emph{ v}, size\_t\emph{ count}}{}
Fill a memory area with a uint32\_t

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{uint32\_t * s}}] \leavevmode
Pointer to the start of the area.

\item[{\code{uint32\_t v}}] \leavevmode
The value to fill the area with

\item[{\code{size\_t count}}] \leavevmode
The number of values to store

\end{description}

\textbf{Description}

Differs from {\hyperref[core\string-api/kernel\string-api:c.memset]{\emph{\code{memset()}}}} in that it fills with a uint32\_t instead
of a byte.  Remember that \textbf{count} is the number of uint32\_ts to
store, not the number of bytes.
\index{memset64 (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.memset64}\pysiglinewithargsret{void * \bfcode{memset64}}{uint64\_t *\emph{ s}, uint64\_t\emph{ v}, size\_t\emph{ count}}{}
Fill a memory area with a uint64\_t

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{uint64\_t * s}}] \leavevmode
Pointer to the start of the area.

\item[{\code{uint64\_t v}}] \leavevmode
The value to fill the area with

\item[{\code{size\_t count}}] \leavevmode
The number of values to store

\end{description}

\textbf{Description}

Differs from {\hyperref[core\string-api/kernel\string-api:c.memset]{\emph{\code{memset()}}}} in that it fills with a uint64\_t instead
of a byte.  Remember that \textbf{count} is the number of uint64\_ts to
store, not the number of bytes.
\index{memcpy (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.memcpy}\pysiglinewithargsret{void * \bfcode{memcpy}}{void *\emph{ dest}, const void *\emph{ src}, size\_t\emph{ count}}{}
Copy one area of memory to another

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void * dest}}] \leavevmode
Where to copy to

\item[{\code{const void * src}}] \leavevmode
Where to copy from

\item[{\code{size\_t count}}] \leavevmode
The size of the area.

\end{description}

\textbf{Description}

You should not use this function to access IO space, use \code{memcpy\_toio()}
or \code{memcpy\_fromio()} instead.
\index{memmove (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.memmove}\pysiglinewithargsret{void * \bfcode{memmove}}{void *\emph{ dest}, const void *\emph{ src}, size\_t\emph{ count}}{}
Copy one area of memory to another

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void * dest}}] \leavevmode
Where to copy to

\item[{\code{const void * src}}] \leavevmode
Where to copy from

\item[{\code{size\_t count}}] \leavevmode
The size of the area.

\end{description}

\textbf{Description}

Unlike {\hyperref[core\string-api/kernel\string-api:c.memcpy]{\emph{\code{memcpy()}}}}, {\hyperref[core\string-api/kernel\string-api:c.memmove]{\emph{\code{memmove()}}}} copes with overlapping areas.
\index{memcmp (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.memcmp}\pysiglinewithargsret{\_\_visible int \bfcode{memcmp}}{const void *\emph{ cs}, const void *\emph{ ct}, size\_t\emph{ count}}{}
Compare two areas of memory

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const void * cs}}] \leavevmode
One area of memory

\item[{\code{const void * ct}}] \leavevmode
Another area of memory

\item[{\code{size\_t count}}] \leavevmode
The size of the area.

\end{description}
\index{memscan (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.memscan}\pysiglinewithargsret{void * \bfcode{memscan}}{void *\emph{ addr}, int\emph{ c}, size\_t\emph{ size}}{}
Find a character in an area of memory.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void * addr}}] \leavevmode
The memory area

\item[{\code{int c}}] \leavevmode
The byte to search for

\item[{\code{size\_t size}}] \leavevmode
The size of the area.

\end{description}

\textbf{Description}

returns the address of the first occurrence of \textbf{c}, or 1 byte past
the area if \textbf{c} is not found
\index{strstr (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.strstr}\pysiglinewithargsret{char * \bfcode{strstr}}{const char *\emph{ s1}, const char *\emph{ s2}}{}
Find the first substring in a \code{NUL} terminated string

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * s1}}] \leavevmode
The string to be searched

\item[{\code{const char * s2}}] \leavevmode
The string to search for

\end{description}
\index{strnstr (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.strnstr}\pysiglinewithargsret{char * \bfcode{strnstr}}{const char *\emph{ s1}, const char *\emph{ s2}, size\_t\emph{ len}}{}
Find the first substring in a length-limited string

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * s1}}] \leavevmode
The string to be searched

\item[{\code{const char * s2}}] \leavevmode
The string to search for

\item[{\code{size\_t len}}] \leavevmode
the maximum number of characters to search

\end{description}
\index{memchr (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.memchr}\pysiglinewithargsret{void * \bfcode{memchr}}{const void *\emph{ s}, int\emph{ c}, size\_t\emph{ n}}{}
Find a character in an area of memory.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const void * s}}] \leavevmode
The memory area

\item[{\code{int c}}] \leavevmode
The byte to search for

\item[{\code{size\_t n}}] \leavevmode
The size of the area.

\end{description}

\textbf{Description}

returns the address of the first occurrence of \textbf{c}, or \code{NULL}
if \textbf{c} is not found
\index{memchr\_inv (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.memchr_inv}\pysiglinewithargsret{void * \bfcode{memchr\_inv}}{const void *\emph{ start}, int\emph{ c}, size\_t\emph{ bytes}}{}
Find an unmatching character in an area of memory.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const void * start}}] \leavevmode
The memory area

\item[{\code{int c}}] \leavevmode
Find a character other than c

\item[{\code{size\_t bytes}}] \leavevmode
The size of the area.

\end{description}

\textbf{Description}

returns the address of the first character other than \textbf{c}, or \code{NULL}
if the whole buffer contains just \textbf{c}.
\index{strreplace (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.strreplace}\pysiglinewithargsret{char * \bfcode{strreplace}}{char *\emph{ s}, char\emph{ old}, char\emph{ new}}{}
Replace all occurrences of character in string.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{char * s}}] \leavevmode
The string to operate on.

\item[{\code{char old}}] \leavevmode
The character being replaced.

\item[{\code{char new}}] \leavevmode
The character \textbf{old} is replaced with.

\end{description}

\textbf{Description}

Returns pointer to the nul byte at the end of \textbf{s}.


\subsubsection{Bit Operations}
\label{core-api/kernel-api:bit-operations}\index{set\_bit (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.set_bit}\pysiglinewithargsret{void \bfcode{set\_bit}}{long\emph{ nr}, volatile unsigned long *\emph{ addr}}{}
Atomically set a bit in memory

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{long nr}}] \leavevmode
the bit to set

\item[{\code{volatile unsigned long * addr}}] \leavevmode
the address to start counting from

\end{description}

\textbf{Description}

This function is atomic and may not be reordered.  See {\hyperref[core\string-api/kernel\string-api:c.__set_bit]{\emph{\code{\_\_set\_bit()}}}}
if you do not require the atomic guarantees.

\textbf{Note}

there are no guarantees that this function will not be reordered
on non x86 architectures, so if you are writing portable code,
make sure not to rely on its reordering guarantees.

Note that \textbf{nr} may be almost arbitrarily large; this function is not
restricted to acting on a single-word quantity.
\index{\_\_set\_bit (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__set_bit}\pysiglinewithargsret{void \bfcode{\_\_set\_bit}}{long\emph{ nr}, volatile unsigned long *\emph{ addr}}{}
Set a bit in memory

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{long nr}}] \leavevmode
the bit to set

\item[{\code{volatile unsigned long * addr}}] \leavevmode
the address to start counting from

\end{description}

\textbf{Description}

Unlike {\hyperref[core\string-api/kernel\string-api:c.set_bit]{\emph{\code{set\_bit()}}}}, this function is non-atomic and may be reordered.
If it's called on the same region of memory simultaneously, the effect
may be that only one operation succeeds.
\index{clear\_bit (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.clear_bit}\pysiglinewithargsret{void \bfcode{clear\_bit}}{long\emph{ nr}, volatile unsigned long *\emph{ addr}}{}
Clears a bit in memory

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{long nr}}] \leavevmode
Bit to clear

\item[{\code{volatile unsigned long * addr}}] \leavevmode
Address to start counting from

\end{description}

\textbf{Description}

{\hyperref[core\string-api/kernel\string-api:c.clear_bit]{\emph{\code{clear\_bit()}}}} is atomic and may not be reordered.  However, it does
not contain a memory barrier, so if it is used for locking purposes,
you should call \code{smp\_mb\_\_before\_atomic()} and/or \code{smp\_mb\_\_after\_atomic()}
in order to ensure changes are visible on other processors.
\index{\_\_change\_bit (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__change_bit}\pysiglinewithargsret{void \bfcode{\_\_change\_bit}}{long\emph{ nr}, volatile unsigned long *\emph{ addr}}{}
Toggle a bit in memory

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{long nr}}] \leavevmode
the bit to change

\item[{\code{volatile unsigned long * addr}}] \leavevmode
the address to start counting from

\end{description}

\textbf{Description}

Unlike {\hyperref[core\string-api/kernel\string-api:c.change_bit]{\emph{\code{change\_bit()}}}}, this function is non-atomic and may be reordered.
If it's called on the same region of memory simultaneously, the effect
may be that only one operation succeeds.
\index{change\_bit (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.change_bit}\pysiglinewithargsret{void \bfcode{change\_bit}}{long\emph{ nr}, volatile unsigned long *\emph{ addr}}{}
Toggle a bit in memory

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{long nr}}] \leavevmode
Bit to change

\item[{\code{volatile unsigned long * addr}}] \leavevmode
Address to start counting from

\end{description}

\textbf{Description}

{\hyperref[core\string-api/kernel\string-api:c.change_bit]{\emph{\code{change\_bit()}}}} is atomic and may not be reordered.
Note that \textbf{nr} may be almost arbitrarily large; this function is not
restricted to acting on a single-word quantity.
\index{test\_and\_set\_bit (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.test_and_set_bit}\pysiglinewithargsret{bool \bfcode{test\_and\_set\_bit}}{long\emph{ nr}, volatile unsigned long *\emph{ addr}}{}
Set a bit and return its old value

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{long nr}}] \leavevmode
Bit to set

\item[{\code{volatile unsigned long * addr}}] \leavevmode
Address to count from

\end{description}

\textbf{Description}

This operation is atomic and cannot be reordered.
It also implies a memory barrier.
\index{test\_and\_set\_bit\_lock (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.test_and_set_bit_lock}\pysiglinewithargsret{bool \bfcode{test\_and\_set\_bit\_lock}}{long\emph{ nr}, volatile unsigned long *\emph{ addr}}{}
Set a bit and return its old value for lock

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{long nr}}] \leavevmode
Bit to set

\item[{\code{volatile unsigned long * addr}}] \leavevmode
Address to count from

\end{description}

\textbf{Description}

This is the same as test\_and\_set\_bit on x86.
\index{\_\_test\_and\_set\_bit (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__test_and_set_bit}\pysiglinewithargsret{bool \bfcode{\_\_test\_and\_set\_bit}}{long\emph{ nr}, volatile unsigned long *\emph{ addr}}{}
Set a bit and return its old value

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{long nr}}] \leavevmode
Bit to set

\item[{\code{volatile unsigned long * addr}}] \leavevmode
Address to count from

\end{description}

\textbf{Description}

This operation is non-atomic and can be reordered.
If two examples of this operation race, one can appear to succeed
but actually fail.  You must protect multiple accesses with a lock.
\index{test\_and\_clear\_bit (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.test_and_clear_bit}\pysiglinewithargsret{bool \bfcode{test\_and\_clear\_bit}}{long\emph{ nr}, volatile unsigned long *\emph{ addr}}{}
Clear a bit and return its old value

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{long nr}}] \leavevmode
Bit to clear

\item[{\code{volatile unsigned long * addr}}] \leavevmode
Address to count from

\end{description}

\textbf{Description}

This operation is atomic and cannot be reordered.
It also implies a memory barrier.
\index{\_\_test\_and\_clear\_bit (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__test_and_clear_bit}\pysiglinewithargsret{bool \bfcode{\_\_test\_and\_clear\_bit}}{long\emph{ nr}, volatile unsigned long *\emph{ addr}}{}
Clear a bit and return its old value

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{long nr}}] \leavevmode
Bit to clear

\item[{\code{volatile unsigned long * addr}}] \leavevmode
Address to count from

\end{description}

\textbf{Description}

This operation is non-atomic and can be reordered.
If two examples of this operation race, one can appear to succeed
but actually fail.  You must protect multiple accesses with a lock.

\textbf{Note}

the operation is performed atomically with respect to
the local CPU, but not other CPUs. Portable code should not
rely on this behaviour.
KVM relies on this behaviour on x86 for modifying memory that is also
accessed from a hypervisor on the same CPU if running in a VM: don't change
this without also updating arch/x86/kernel/kvm.c
\index{test\_and\_change\_bit (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.test_and_change_bit}\pysiglinewithargsret{bool \bfcode{test\_and\_change\_bit}}{long\emph{ nr}, volatile unsigned long *\emph{ addr}}{}
Change a bit and return its old value

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{long nr}}] \leavevmode
Bit to change

\item[{\code{volatile unsigned long * addr}}] \leavevmode
Address to count from

\end{description}

\textbf{Description}

This operation is atomic and cannot be reordered.
It also implies a memory barrier.
\index{test\_bit (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.test_bit}\pysiglinewithargsret{bool \bfcode{test\_bit}}{int\emph{ nr}, const volatile unsigned long *\emph{ addr}}{}
Determine whether a bit is set

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{int nr}}] \leavevmode
bit number to test

\item[{\code{const volatile unsigned long * addr}}] \leavevmode
Address to start counting from

\end{description}
\index{\_\_ffs (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__ffs}\pysiglinewithargsret{unsigned long \bfcode{\_\_ffs}}{unsigned long\emph{ word}}{}
find first set bit in word

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned long word}}] \leavevmode
The word to search

\end{description}

\textbf{Description}

Undefined if no bit exists, so code should check against 0 first.
\index{ffz (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.ffz}\pysiglinewithargsret{unsigned long \bfcode{ffz}}{unsigned long\emph{ word}}{}
find first zero bit in word

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned long word}}] \leavevmode
The word to search

\end{description}

\textbf{Description}

Undefined if no zero exists, so code should check against \textasciitilde{}0UL first.
\index{ffs (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.ffs}\pysiglinewithargsret{int \bfcode{ffs}}{int\emph{ x}}{}
find first set bit in word

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{int x}}] \leavevmode
the word to search

\end{description}

\textbf{Description}

This is defined the same way as the libc and compiler builtin ffs
routines, therefore differs in spirit from the other bitops.

ffs(value) returns 0 if value is 0 or the position of the first
set bit if value is nonzero. The first (least significant) bit
is at position 1.
\index{fls (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.fls}\pysiglinewithargsret{int \bfcode{fls}}{int\emph{ x}}{}
find last set bit in word

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{int x}}] \leavevmode
the word to search

\end{description}

\textbf{Description}

This is defined in a similar way as the libc and compiler builtin
ffs, but returns the position of the most significant set bit.

fls(value) returns 0 if value is 0 or the position of the last
set bit if value is nonzero. The last (most significant) bit is
at position 32.
\index{fls64 (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.fls64}\pysiglinewithargsret{int \bfcode{fls64}}{\_\_u64\emph{ x}}{}
find last set bit in a 64-bit word

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{\_\_u64 x}}] \leavevmode
the word to search

\end{description}

\textbf{Description}

This is defined in a similar way as the libc and compiler builtin
ffsll, but returns the position of the most significant set bit.

fls64(value) returns 0 if value is 0 or the position of the last
set bit if value is nonzero. The last (most significant) bit is
at position 64.


\subsection{Basic Kernel Library Functions}
\label{core-api/kernel-api:basic-kernel-library-functions}
The Linux kernel provides more basic utility functions.


\subsubsection{Bitmap Operations}
\label{core-api/kernel-api:bitmap-operations}
bitmaps provide an array of bits, implemented using an an
array of unsigned longs.  The number of valid bits in a
given bitmap does \_not\_ need to be an exact multiple of
BITS\_PER\_LONG.

The possible unused bits in the last, partially used word
of a bitmap are `don't care'.  The implementation makes
no particular effort to keep them zero.  It ensures that
their value will not affect the results of any operation.
The bitmap operations that return Boolean (bitmap\_empty,
for example) or scalar (bitmap\_weight, for example) results
carefully filter out these unused bits from impacting their
results.

These operations actually hold to a slightly stronger rule:
if you don't input any bitmaps to these ops that have some
unused bits set, then they won't output any set unused bits
in output bitmaps.

The byte ordering of bitmaps is more natural on little
endian architectures.  See the big-endian headers
include/asm-ppc64/bitops.h and include/asm-s390/bitops.h
for the best explanations of this ordering.

The DECLARE\_BITMAP(name,bits) macro, in linux/types.h, can be used
to declare an array named `name' of just enough unsigned longs to
contain all bit positions from 0 to `bits' - 1.

The available bitmap operations and their rough meaning in the
case that the bitmap is a single unsigned long are thus:

Note that nbits should be always a compile time evaluable constant.
Otherwise many inlines will generate horrible code.

\begin{Verbatim}[commandchars=\\\{\}]
bitmap\PYGZus{}zero(dst, nbits)                     *dst = 0UL
bitmap\PYGZus{}fill(dst, nbits)                     *dst = \PYGZti{}0UL
bitmap\PYGZus{}copy(dst, src, nbits)                *dst = *src
bitmap\PYGZus{}and(dst, src1, src2, nbits)          *dst = *src1 \PYGZam{} *src2
bitmap\PYGZus{}or(dst, src1, src2, nbits)           *dst = *src1 \textbar{} *src2
bitmap\PYGZus{}xor(dst, src1, src2, nbits)          *dst = *src1 \PYGZca{} *src2
bitmap\PYGZus{}andnot(dst, src1, src2, nbits)       *dst = *src1 \PYGZam{} \PYGZti{}(*src2)
bitmap\PYGZus{}complement(dst, src, nbits)          *dst = \PYGZti{}(*src)
bitmap\PYGZus{}equal(src1, src2, nbits)             Are *src1 and *src2 equal?
bitmap\PYGZus{}intersects(src1, src2, nbits)        Do *src1 and *src2 overlap?
bitmap\PYGZus{}subset(src1, src2, nbits)            Is *src1 a subset of *src2?
bitmap\PYGZus{}empty(src, nbits)                    Are all bits zero in *src?
bitmap\PYGZus{}full(src, nbits)                     Are all bits set in *src?
bitmap\PYGZus{}weight(src, nbits)                   Hamming Weight: number set bits
bitmap\PYGZus{}set(dst, pos, nbits)                 Set specified bit area
bitmap\PYGZus{}clear(dst, pos, nbits)               Clear specified bit area
bitmap\PYGZus{}find\PYGZus{}next\PYGZus{}zero\PYGZus{}area(buf, len, pos, n, mask)  Find bit free area
bitmap\PYGZus{}find\PYGZus{}next\PYGZus{}zero\PYGZus{}area\PYGZus{}off(buf, len, pos, n, mask)  as above
bitmap\PYGZus{}shift\PYGZus{}right(dst, src, n, nbits)      *dst = *src \PYGZgt{}\PYGZgt{} n
bitmap\PYGZus{}shift\PYGZus{}left(dst, src, n, nbits)       *dst = *src \PYGZlt{}\PYGZlt{} n
bitmap\PYGZus{}remap(dst, src, old, new, nbits)     *dst = map(old, new)(src)
bitmap\PYGZus{}bitremap(oldbit, old, new, nbits)    newbit = map(old, new)(oldbit)
bitmap\PYGZus{}onto(dst, orig, relmap, nbits)       *dst = orig relative to relmap
bitmap\PYGZus{}fold(dst, orig, sz, nbits)           dst bits = orig bits mod sz
bitmap\PYGZus{}parse(buf, buflen, dst, nbits)       Parse bitmap dst from kernel buf
bitmap\PYGZus{}parse\PYGZus{}user(ubuf, ulen, dst, nbits)   Parse bitmap dst from user buf
bitmap\PYGZus{}parselist(buf, dst, nbits)           Parse bitmap dst from kernel buf
bitmap\PYGZus{}parselist\PYGZus{}user(buf, dst, nbits)      Parse bitmap dst from user buf
bitmap\PYGZus{}find\PYGZus{}free\PYGZus{}region(bitmap, bits, order)  Find and allocate bit region
bitmap\PYGZus{}release\PYGZus{}region(bitmap, pos, order)   Free specified bit region
bitmap\PYGZus{}allocate\PYGZus{}region(bitmap, pos, order)  Allocate specified bit region
bitmap\PYGZus{}from\PYGZus{}arr32(dst, buf, nbits)          Copy nbits from u32[] buf to dst
bitmap\PYGZus{}to\PYGZus{}arr32(buf, src, nbits)            Copy nbits from buf to u32[] dst
\end{Verbatim}

Note, \code{bitmap\_zero()} and \code{bitmap\_fill()} operate over the region of
unsigned longs, that is, bits behind bitmap till the unsigned long
boundary will be zeroed or filled as well. Consider to use
\code{bitmap\_clear()} or \code{bitmap\_set()} to make explicit zeroing or filling
respectively.

Also the following operations in asm/bitops.h apply to bitmaps.:

\begin{Verbatim}[commandchars=\\\{\}]
set\PYGZus{}bit(bit, addr)                  *addr \textbar{}= bit
clear\PYGZus{}bit(bit, addr)                *addr \PYGZam{}= \PYGZti{}bit
change\PYGZus{}bit(bit, addr)               *addr \PYGZca{}= bit
test\PYGZus{}bit(bit, addr)                 Is bit set in *addr?
test\PYGZus{}and\PYGZus{}set\PYGZus{}bit(bit, addr)         Set bit and return old value
test\PYGZus{}and\PYGZus{}clear\PYGZus{}bit(bit, addr)       Clear bit and return old value
test\PYGZus{}and\PYGZus{}change\PYGZus{}bit(bit, addr)      Change bit and return old value
find\PYGZus{}first\PYGZus{}zero\PYGZus{}bit(addr, nbits)    Position first zero bit in *addr
find\PYGZus{}first\PYGZus{}bit(addr, nbits)         Position first set bit in *addr
find\PYGZus{}next\PYGZus{}zero\PYGZus{}bit(addr, nbits, bit)
                                    Position next zero bit in *addr \PYGZgt{}= bit
find\PYGZus{}next\PYGZus{}bit(addr, nbits, bit)     Position next set bit in *addr \PYGZgt{}= bit
find\PYGZus{}next\PYGZus{}and\PYGZus{}bit(addr1, addr2, nbits, bit)
                                    Same as find\PYGZus{}next\PYGZus{}bit, but in
                                    (*addr1 \PYGZam{} *addr2)
\end{Verbatim}
\index{\_\_bitmap\_shift\_right (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__bitmap_shift_right}\pysiglinewithargsret{void \bfcode{\_\_bitmap\_shift\_right}}{unsigned long *\emph{ dst}, const unsigned long *\emph{ src}, unsigned\emph{ shift}, unsigned\emph{ nbits}}{}
logical right shift of the bits in a bitmap

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned long * dst}}] \leavevmode
destination bitmap

\item[{\code{const unsigned long * src}}] \leavevmode
source bitmap

\item[{\code{unsigned shift}}] \leavevmode
shift by this many bits

\item[{\code{unsigned nbits}}] \leavevmode
bitmap size, in bits

\end{description}

\textbf{Description}

Shifting right (dividing) means moving bits in the MS -\textgreater{} LS bit
direction.  Zeros are fed into the vacated MS positions and the
LS bits shifted off the bottom are lost.
\index{\_\_bitmap\_shift\_left (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__bitmap_shift_left}\pysiglinewithargsret{void \bfcode{\_\_bitmap\_shift\_left}}{unsigned long *\emph{ dst}, const unsigned long *\emph{ src}, unsigned int\emph{ shift}, unsigned int\emph{ nbits}}{}
logical left shift of the bits in a bitmap

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned long * dst}}] \leavevmode
destination bitmap

\item[{\code{const unsigned long * src}}] \leavevmode
source bitmap

\item[{\code{unsigned int shift}}] \leavevmode
shift by this many bits

\item[{\code{unsigned int nbits}}] \leavevmode
bitmap size, in bits

\end{description}

\textbf{Description}

Shifting left (multiplying) means moving bits in the LS -\textgreater{} MS
direction.  Zeros are fed into the vacated LS bit positions
and those MS bits shifted off the top are lost.
\index{bitmap\_find\_next\_zero\_area\_off (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.bitmap_find_next_zero_area_off}\pysiglinewithargsret{unsigned long \bfcode{bitmap\_find\_next\_zero\_area\_off}}{unsigned long *\emph{ map}, unsigned long\emph{ size}, unsigned long\emph{ start}, unsigned int\emph{ nr}, unsigned long\emph{ align\_mask}, unsigned long\emph{ align\_offset}}{}
find a contiguous aligned zero area

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned long * map}}] \leavevmode
The address to base the search on

\item[{\code{unsigned long size}}] \leavevmode
The bitmap size in bits

\item[{\code{unsigned long start}}] \leavevmode
The bitnumber to start searching at

\item[{\code{unsigned int nr}}] \leavevmode
The number of zeroed bits we're looking for

\item[{\code{unsigned long align\_mask}}] \leavevmode
Alignment mask for zero area

\item[{\code{unsigned long align\_offset}}] \leavevmode
Alignment offset for zero area.

\end{description}

\textbf{Description}

The \textbf{align\_mask} should be one less than a power of 2; the effect is that
the bit offset of all zero areas this function finds plus \textbf{align\_offset}
is multiple of that power of 2.
\index{\_\_bitmap\_parse (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__bitmap_parse}\pysiglinewithargsret{int \bfcode{\_\_bitmap\_parse}}{const char *\emph{ buf}, unsigned int\emph{ buflen}, int\emph{ is\_user}, unsigned long *\emph{ maskp}, int\emph{ nmaskbits}}{}
convert an ASCII hex string into a bitmap.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * buf}}] \leavevmode
pointer to buffer containing string.

\item[{\code{unsigned int buflen}}] \leavevmode
buffer size in bytes.  If string is smaller than this
then it must be terminated with a 0.

\item[{\code{int is\_user}}] \leavevmode
location of buffer, 0 indicates kernel space

\item[{\code{unsigned long * maskp}}] \leavevmode
pointer to bitmap array that will contain result.

\item[{\code{int nmaskbits}}] \leavevmode
size of bitmap, in bits.

\end{description}

\textbf{Description}

Commas group hex digits into chunks.  Each chunk defines exactly 32
bits of the resultant bitmask.  No chunk may specify a value larger
than 32 bits (\code{-EOVERFLOW}), and if a chunk specifies a smaller value
then leading 0-bits are prepended.  \code{-EINVAL} is returned for illegal
characters and for grouping errors such as ``1,,5'', '',44'', '','' and ``''.
Leading and trailing whitespace accepted, but not embedded whitespace.
\index{bitmap\_parse\_user (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.bitmap_parse_user}\pysiglinewithargsret{int \bfcode{bitmap\_parse\_user}}{const char \_\_user *\emph{ ubuf}, unsigned int\emph{ ulen}, unsigned long *\emph{ maskp}, int\emph{ nmaskbits}}{}
convert an ASCII hex string in a user buffer into a bitmap

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char \_\_user * ubuf}}] \leavevmode
pointer to user buffer containing string.

\item[{\code{unsigned int ulen}}] \leavevmode
buffer size in bytes.  If string is smaller than this
then it must be terminated with a 0.

\item[{\code{unsigned long * maskp}}] \leavevmode
pointer to bitmap array that will contain result.

\item[{\code{int nmaskbits}}] \leavevmode
size of bitmap, in bits.

\end{description}

\textbf{Description}

Wrapper for {\hyperref[core\string-api/kernel\string-api:c.__bitmap_parse]{\emph{\code{\_\_bitmap\_parse()}}}}, providing it with user buffer.

We cannot have this as an inline function in bitmap.h because it needs
linux/uaccess.h to get the {\hyperref[core\string-api/kernel\string-api:c.access_ok]{\emph{\code{access\_ok()}}}} declaration and this causes
cyclic dependencies.
\index{bitmap\_print\_to\_pagebuf (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.bitmap_print_to_pagebuf}\pysiglinewithargsret{int \bfcode{bitmap\_print\_to\_pagebuf}}{bool\emph{ list}, char *\emph{ buf}, const unsigned long *\emph{ maskp}, int\emph{ nmaskbits}}{}
convert bitmap to list or hex format ASCII string

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{bool list}}] \leavevmode
indicates whether the bitmap must be list

\item[{\code{char * buf}}] \leavevmode
page aligned buffer into which string is placed

\item[{\code{const unsigned long * maskp}}] \leavevmode
pointer to bitmap to convert

\item[{\code{int nmaskbits}}] \leavevmode
size of bitmap, in bits

\end{description}

\textbf{Description}

Output format is a comma-separated list of decimal numbers and
ranges if list is specified or hex digits grouped into comma-separated
sets of 8 digits/set. Returns the number of characters written to buf.

It is assumed that \textbf{buf} is a pointer into a PAGE\_SIZE area and that
sufficient storage remains at \textbf{buf} to accommodate the
{\hyperref[core\string-api/kernel\string-api:c.bitmap_print_to_pagebuf]{\emph{\code{bitmap\_print\_to\_pagebuf()}}}} output.
\index{bitmap\_parselist\_user (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.bitmap_parselist_user}\pysiglinewithargsret{int \bfcode{bitmap\_parselist\_user}}{const char \_\_user *\emph{ ubuf}, unsigned int\emph{ ulen}, unsigned long *\emph{ maskp}, int\emph{ nmaskbits}}{}
\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char \_\_user * ubuf}}] \leavevmode
pointer to user buffer containing string.

\item[{\code{unsigned int ulen}}] \leavevmode
buffer size in bytes.  If string is smaller than this
then it must be terminated with a 0.

\item[{\code{unsigned long * maskp}}] \leavevmode
pointer to bitmap array that will contain result.

\item[{\code{int nmaskbits}}] \leavevmode
size of bitmap, in bits.

\end{description}

\textbf{Description}

Wrapper for \code{bitmap\_parselist()}, providing it with user buffer.

We cannot have this as an inline function in bitmap.h because it needs
linux/uaccess.h to get the {\hyperref[core\string-api/kernel\string-api:c.access_ok]{\emph{\code{access\_ok()}}}} declaration and this causes
cyclic dependencies.
\index{bitmap\_remap (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.bitmap_remap}\pysiglinewithargsret{void \bfcode{bitmap\_remap}}{unsigned long *\emph{ dst}, const unsigned long *\emph{ src}, const unsigned long *\emph{ old}, const unsigned long *\emph{ new}, unsigned int\emph{ nbits}}{}
Apply map defined by a pair of bitmaps to another bitmap

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned long * dst}}] \leavevmode
remapped result

\item[{\code{const unsigned long * src}}] \leavevmode
subset to be remapped

\item[{\code{const unsigned long * old}}] \leavevmode
defines domain of map

\item[{\code{const unsigned long * new}}] \leavevmode
defines range of map

\item[{\code{unsigned int nbits}}] \leavevmode
number of bits in each of these bitmaps

\end{description}

\textbf{Description}

Let \textbf{old} and \textbf{new} define a mapping of bit positions, such that
whatever position is held by the n-th set bit in \textbf{old} is mapped
to the n-th set bit in \textbf{new}.  In the more general case, allowing
for the possibility that the weight `w' of \textbf{new} is less than the
weight of \textbf{old}, map the position of the n-th set bit in \textbf{old} to
the position of the m-th set bit in \textbf{new}, where m == n \% w.

If either of the \textbf{old} and \textbf{new} bitmaps are empty, or if \textbf{src} and
\textbf{dst} point to the same location, then this routine copies \textbf{src}
to \textbf{dst}.

The positions of unset bits in \textbf{old} are mapped to themselves
(the identify map).

Apply the above specified mapping to \textbf{src}, placing the result in
\textbf{dst}, clearing any bits previously set in \textbf{dst}.

For example, lets say that \textbf{old} has bits 4 through 7 set, and
\textbf{new} has bits 12 through 15 set.  This defines the mapping of bit
position 4 to 12, 5 to 13, 6 to 14 and 7 to 15, and of all other
bit positions unchanged.  So if say \textbf{src} comes into this routine
with bits 1, 5 and 7 set, then \textbf{dst} should leave with bits 1,
13 and 15 set.
\index{bitmap\_bitremap (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.bitmap_bitremap}\pysiglinewithargsret{int \bfcode{bitmap\_bitremap}}{int\emph{ oldbit}, const unsigned long *\emph{ old}, const unsigned long *\emph{ new}, int\emph{ bits}}{}
Apply map defined by a pair of bitmaps to a single bit

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{int oldbit}}] \leavevmode
bit position to be mapped

\item[{\code{const unsigned long * old}}] \leavevmode
defines domain of map

\item[{\code{const unsigned long * new}}] \leavevmode
defines range of map

\item[{\code{int bits}}] \leavevmode
number of bits in each of these bitmaps

\end{description}

\textbf{Description}

Let \textbf{old} and \textbf{new} define a mapping of bit positions, such that
whatever position is held by the n-th set bit in \textbf{old} is mapped
to the n-th set bit in \textbf{new}.  In the more general case, allowing
for the possibility that the weight `w' of \textbf{new} is less than the
weight of \textbf{old}, map the position of the n-th set bit in \textbf{old} to
the position of the m-th set bit in \textbf{new}, where m == n \% w.

The positions of unset bits in \textbf{old} are mapped to themselves
(the identify map).

Apply the above specified mapping to bit position \textbf{oldbit}, returning
the new bit position.

For example, lets say that \textbf{old} has bits 4 through 7 set, and
\textbf{new} has bits 12 through 15 set.  This defines the mapping of bit
position 4 to 12, 5 to 13, 6 to 14 and 7 to 15, and of all other
bit positions unchanged.  So if say \textbf{oldbit} is 5, then this routine
returns 13.
\index{bitmap\_onto (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.bitmap_onto}\pysiglinewithargsret{void \bfcode{bitmap\_onto}}{unsigned long *\emph{ dst}, const unsigned long *\emph{ orig}, const unsigned long *\emph{ relmap}, unsigned int\emph{ bits}}{}
translate one bitmap relative to another

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned long * dst}}] \leavevmode
resulting translated bitmap

\item[{\code{const unsigned long * orig}}] \leavevmode
original untranslated bitmap

\item[{\code{const unsigned long * relmap}}] \leavevmode
bitmap relative to which translated

\item[{\code{unsigned int bits}}] \leavevmode
number of bits in each of these bitmaps

\end{description}

\textbf{Description}

Set the n-th bit of \textbf{dst} iff there exists some m such that the
n-th bit of \textbf{relmap} is set, the m-th bit of \textbf{orig} is set, and
the n-th bit of \textbf{relmap} is also the m-th \_set\_ bit of \textbf{relmap}.
(If you understood the previous sentence the first time your
read it, you're overqualified for your current job.)

In other words, \textbf{orig} is mapped onto (surjectively) \textbf{dst},
using the map \{ \textless{}n, m\textgreater{} \textbar{} the n-th bit of \textbf{relmap} is the
m-th set bit of \textbf{relmap} \}.

Any set bits in \textbf{orig} above bit number W, where W is the
weight of (number of set bits in) \textbf{relmap} are mapped nowhere.
In particular, if for all bits m set in \textbf{orig}, m \textgreater{}= W, then
\textbf{dst} will end up empty.  In situations where the possibility
of such an empty result is not desired, one way to avoid it is
to use the {\hyperref[core\string-api/kernel\string-api:c.bitmap_fold]{\emph{\code{bitmap\_fold()}}}} operator, below, to first fold the
\textbf{orig} bitmap over itself so that all its set bits x are in the
range 0 \textless{}= x \textless{} W.  The {\hyperref[core\string-api/kernel\string-api:c.bitmap_fold]{\emph{\code{bitmap\_fold()}}}} operator does this by
setting the bit (m \% W) in \textbf{dst}, for each bit (m) set in \textbf{orig}.
\begin{description}
\item[{Example {[}1{]} for {\hyperref[core\string-api/kernel\string-api:c.bitmap_onto]{\emph{\code{bitmap\_onto()}}}}:}] \leavevmode
Let's say \textbf{relmap} has bits 30-39 set, and \textbf{orig} has bits
1, 3, 5, 7, 9 and 11 set.  Then on return from this routine,
\textbf{dst} will have bits 31, 33, 35, 37 and 39 set.

When bit 0 is set in \textbf{orig}, it means turn on the bit in
\textbf{dst} corresponding to whatever is the first bit (if any)
that is turned on in \textbf{relmap}.  Since bit 0 was off in the
above example, we leave off that bit (bit 30) in \textbf{dst}.

When bit 1 is set in \textbf{orig} (as in the above example), it
means turn on the bit in \textbf{dst} corresponding to whatever
is the second bit that is turned on in \textbf{relmap}.  The second
bit in \textbf{relmap} that was turned on in the above example was
bit 31, so we turned on bit 31 in \textbf{dst}.

Similarly, we turned on bits 33, 35, 37 and 39 in \textbf{dst},
because they were the 4th, 6th, 8th and 10th set bits
set in \textbf{relmap}, and the 4th, 6th, 8th and 10th bits of
\textbf{orig} (i.e. bits 3, 5, 7 and 9) were also set.

When bit 11 is set in \textbf{orig}, it means turn on the bit in
\textbf{dst} corresponding to whatever is the twelfth bit that is
turned on in \textbf{relmap}.  In the above example, there were
only ten bits turned on in \textbf{relmap} (30..39), so that bit
11 was set in \textbf{orig} had no affect on \textbf{dst}.

\item[{Example {[}2{]} for {\hyperref[core\string-api/kernel\string-api:c.bitmap_fold]{\emph{\code{bitmap\_fold()}}}} + {\hyperref[core\string-api/kernel\string-api:c.bitmap_onto]{\emph{\code{bitmap\_onto()}}}}:}] \leavevmode
Let's say \textbf{relmap} has these ten bits set:

\begin{Verbatim}[commandchars=\\\{\}]
40 41 42 43 45 48 53 61 74 95
\end{Verbatim}

(for the curious, that's 40 plus the first ten terms of the
Fibonacci sequence.)

Further lets say we use the following code, invoking
{\hyperref[core\string-api/kernel\string-api:c.bitmap_fold]{\emph{\code{bitmap\_fold()}}}} then bitmap\_onto, as suggested above to
avoid the possibility of an empty \textbf{dst} result:

\begin{Verbatim}[commandchars=\\\{\}]
unsigned long *tmp;     // a temporary bitmap\PYGZsq{}s bits

bitmap\PYGZus{}fold(tmp, orig, bitmap\PYGZus{}weight(relmap, bits), bits);
bitmap\PYGZus{}onto(dst, tmp, relmap, bits);
\end{Verbatim}

Then this table shows what various values of \textbf{dst} would be, for
various \textbf{orig}`s.  I list the zero-based positions of each set bit.
The tmp column shows the intermediate result, as computed by
using {\hyperref[core\string-api/kernel\string-api:c.bitmap_fold]{\emph{\code{bitmap\_fold()}}}} to fold the \textbf{orig} bitmap modulo ten
(the weight of \textbf{relmap}):
\begin{quote}

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline

\textbf{orig}
 & 
tmp
 & 
\textbf{dst}
\\
\hline
0
 & 
0
 & 
40
\\
\hline
1
 & 
1
 & 
41
\\
\hline
9
 & 
9
 & 
95
\\
\hline
10
 & 
0
 & 
40 \protect\footnotemark[1]
\\
\hline
1 3 5 7
 & 
1 3 5 7
 & 
41 43 48 61
\\
\hline
0 1 2 3 4
 & 
0 1 2 3 4
 & 
40 41 42 43 45
\\
\hline
0 9 18 27
 & 
0 9 8 7
 & 
40 61 74 95
\\
\hline
0 10 20 30
 & 
0
 & 
40
\\
\hline
0 11 22 33
 & 
0 1 2 3
 & 
40 41 42 43
\\
\hline
0 12 24 36
 & 
0 2 4 6
 & 
40 42 45 53
\\
\hline
78 102 211
 & 
1 2 8
 & 
41 42 74 \protect\footnotemark[1]
\\
\hline\end{tabulary}

\footnotetext[1]{
For these marked lines, if we hadn't first done {\hyperref[core\string-api/kernel\string-api:c.bitmap_fold]{\emph{\code{bitmap\_fold()}}}}
into tmp, then the \textbf{dst} result would have been empty.
}\end{quote}

\end{description}

If either of \textbf{orig} or \textbf{relmap} is empty (no set bits), then \textbf{dst}
will be returned empty.

If (as explained above) the only set bits in \textbf{orig} are in positions
m where m \textgreater{}= W, (where W is the weight of \textbf{relmap}) then \textbf{dst} will
once again be returned empty.

All bits in \textbf{dst} not set by the above rule are cleared.
\index{bitmap\_fold (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.bitmap_fold}\pysiglinewithargsret{void \bfcode{bitmap\_fold}}{unsigned long *\emph{ dst}, const unsigned long *\emph{ orig}, unsigned int\emph{ sz}, unsigned int\emph{ nbits}}{}
fold larger bitmap into smaller, modulo specified size

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned long * dst}}] \leavevmode
resulting smaller bitmap

\item[{\code{const unsigned long * orig}}] \leavevmode
original larger bitmap

\item[{\code{unsigned int sz}}] \leavevmode
specified size

\item[{\code{unsigned int nbits}}] \leavevmode
number of bits in each of these bitmaps

\end{description}

\textbf{Description}

For each bit oldbit in \textbf{orig}, set bit oldbit mod \textbf{sz} in \textbf{dst}.
Clear all other bits in \textbf{dst}.  See further the comment and
Example {[}2{]} for {\hyperref[core\string-api/kernel\string-api:c.bitmap_onto]{\emph{\code{bitmap\_onto()}}}} for why and how to use this.
\index{bitmap\_find\_free\_region (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.bitmap_find_free_region}\pysiglinewithargsret{int \bfcode{bitmap\_find\_free\_region}}{unsigned long *\emph{ bitmap}, unsigned int\emph{ bits}, int\emph{ order}}{}
find a contiguous aligned mem region

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned long * bitmap}}] \leavevmode
array of unsigned longs corresponding to the bitmap

\item[{\code{unsigned int bits}}] \leavevmode
number of bits in the bitmap

\item[{\code{int order}}] \leavevmode
region size (log base 2 of number of bits) to find

\end{description}

\textbf{Description}

Find a region of free (zero) bits in a \textbf{bitmap} of \textbf{bits} bits and
allocate them (set them to one).  Only consider regions of length
a power (\textbf{order}) of two, aligned to that power of two, which
makes the search algorithm much faster.

Return the bit offset in bitmap of the allocated region,
or -errno on failure.
\index{bitmap\_release\_region (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.bitmap_release_region}\pysiglinewithargsret{void \bfcode{bitmap\_release\_region}}{unsigned long *\emph{ bitmap}, unsigned int\emph{ pos}, int\emph{ order}}{}
release allocated bitmap region

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned long * bitmap}}] \leavevmode
array of unsigned longs corresponding to the bitmap

\item[{\code{unsigned int pos}}] \leavevmode
beginning of bit region to release

\item[{\code{int order}}] \leavevmode
region size (log base 2 of number of bits) to release

\end{description}

\textbf{Description}

This is the complement to \code{\_\_bitmap\_find\_free\_region()} and releases
the found region (by clearing it in the bitmap).

No return value.
\index{bitmap\_allocate\_region (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.bitmap_allocate_region}\pysiglinewithargsret{int \bfcode{bitmap\_allocate\_region}}{unsigned long *\emph{ bitmap}, unsigned int\emph{ pos}, int\emph{ order}}{}
allocate bitmap region

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned long * bitmap}}] \leavevmode
array of unsigned longs corresponding to the bitmap

\item[{\code{unsigned int pos}}] \leavevmode
beginning of bit region to allocate

\item[{\code{int order}}] \leavevmode
region size (log base 2 of number of bits) to allocate

\end{description}

\textbf{Description}

Allocate (set bits in) a specified region of a bitmap.

Return 0 on success, or \code{-EBUSY} if specified region wasn't
free (not all bits were zero).
\index{bitmap\_copy\_le (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.bitmap_copy_le}\pysiglinewithargsret{void \bfcode{bitmap\_copy\_le}}{unsigned long *\emph{ dst}, const unsigned long *\emph{ src}, unsigned int\emph{ nbits}}{}
copy a bitmap, putting the bits into little-endian order.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned long * dst}}] \leavevmode
destination buffer

\item[{\code{const unsigned long * src}}] \leavevmode
bitmap to copy

\item[{\code{unsigned int nbits}}] \leavevmode
number of bits in the bitmap

\end{description}

\textbf{Description}

Require nbits \% BITS\_PER\_LONG == 0.
\index{bitmap\_from\_arr32 (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.bitmap_from_arr32}\pysiglinewithargsret{void \bfcode{bitmap\_from\_arr32}}{unsigned long *\emph{ bitmap}, const u32 *\emph{ buf}, unsigned int\emph{ nbits}}{}
copy the contents of u32 array of bits to bitmap

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned long * bitmap}}] \leavevmode
array of unsigned longs, the destination bitmap

\item[{\code{const u32 * buf}}] \leavevmode
array of u32 (in host byte order), the source bitmap

\item[{\code{unsigned int nbits}}] \leavevmode
number of bits in \textbf{bitmap}

\end{description}
\index{bitmap\_to\_arr32 (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.bitmap_to_arr32}\pysiglinewithargsret{void \bfcode{bitmap\_to\_arr32}}{u32 *\emph{ buf}, const unsigned long *\emph{ bitmap}, unsigned int\emph{ nbits}}{}
copy the contents of bitmap to a u32 array of bits

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{u32 * buf}}] \leavevmode
array of u32 (in host byte order), the dest bitmap

\item[{\code{const unsigned long * bitmap}}] \leavevmode
array of unsigned longs, the source bitmap

\item[{\code{unsigned int nbits}}] \leavevmode
number of bits in \textbf{bitmap}

\end{description}
\index{\_\_bitmap\_parselist (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__bitmap_parselist}\pysiglinewithargsret{int \bfcode{\_\_bitmap\_parselist}}{const char *\emph{ buf}, unsigned int\emph{ buflen}, int\emph{ is\_user}, unsigned long *\emph{ maskp}, int\emph{ nmaskbits}}{}
convert list format ASCII string to bitmap

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * buf}}] \leavevmode
read nul-terminated user string from this buffer

\item[{\code{unsigned int buflen}}] \leavevmode
buffer size in bytes.  If string is smaller than this
then it must be terminated with a 0.

\item[{\code{int is\_user}}] \leavevmode
location of buffer, 0 indicates kernel space

\item[{\code{unsigned long * maskp}}] \leavevmode
write resulting mask here

\item[{\code{int nmaskbits}}] \leavevmode
number of bits in mask to be written

\end{description}

\textbf{Description}

Input format is a comma-separated list of decimal numbers and
ranges.  Consecutively set bits are shown as two hyphen-separated
decimal numbers, the smallest and largest bit numbers set in
the range.
Optionally each range can be postfixed to denote that only parts of it
should be set. The range will divided to groups of specific size.
From each group will be used only defined amount of bits.
Syntax: range:used\_size/group\_size

\textbf{Example}

0-1023:2/256 ==\textgreater{} 0,1,256,257,512,513,768,769

\textbf{Return}

0 on success, -errno on invalid input strings. Error values:
\begin{itemize}
\item {} 
\code{-EINVAL}: second number in range smaller than first

\item {} 
\code{-EINVAL}: invalid character in string

\item {} 
\code{-ERANGE}: bit number specified too large for mask

\end{itemize}
\index{bitmap\_pos\_to\_ord (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.bitmap_pos_to_ord}\pysiglinewithargsret{int \bfcode{bitmap\_pos\_to\_ord}}{const unsigned long *\emph{ buf}, unsigned int\emph{ pos}, unsigned int\emph{ nbits}}{}
find ordinal of set bit at given position in bitmap

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const unsigned long * buf}}] \leavevmode
pointer to a bitmap

\item[{\code{unsigned int pos}}] \leavevmode
a bit position in \textbf{buf} (0 \textless{}= \textbf{pos} \textless{} \textbf{nbits})

\item[{\code{unsigned int nbits}}] \leavevmode
number of valid bit positions in \textbf{buf}

\end{description}

\textbf{Description}

Map the bit at position \textbf{pos} in \textbf{buf} (of length \textbf{nbits}) to the
ordinal of which set bit it is.  If it is not set or if \textbf{pos}
is not a valid bit position, map to -1.

If for example, just bits 4 through 7 are set in \textbf{buf}, then \textbf{pos}
values 4 through 7 will get mapped to 0 through 3, respectively,
and other \textbf{pos} values will get mapped to -1.  When \textbf{pos} value 7
gets mapped to (returns) \textbf{ord} value 3 in this example, that means
that bit 7 is the 3rd (starting with 0th) set bit in \textbf{buf}.

The bit positions 0 through \textbf{bits} are valid positions in \textbf{buf}.
\index{bitmap\_ord\_to\_pos (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.bitmap_ord_to_pos}\pysiglinewithargsret{unsigned int \bfcode{bitmap\_ord\_to\_pos}}{const unsigned long *\emph{ buf}, unsigned int\emph{ ord}, unsigned int\emph{ nbits}}{}
find position of n-th set bit in bitmap

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const unsigned long * buf}}] \leavevmode
pointer to bitmap

\item[{\code{unsigned int ord}}] \leavevmode
ordinal bit position (n-th set bit, n \textgreater{}= 0)

\item[{\code{unsigned int nbits}}] \leavevmode
number of valid bit positions in \textbf{buf}

\end{description}

\textbf{Description}

Map the ordinal offset of bit \textbf{ord} in \textbf{buf} to its position in \textbf{buf}.
Value of \textbf{ord} should be in range 0 \textless{}= \textbf{ord} \textless{} weight(buf). If \textbf{ord}
\textgreater{}= weight(buf), returns \textbf{nbits}.

If for example, just bits 4 through 7 are set in \textbf{buf}, then \textbf{ord}
values 0 through 3 will get mapped to 4 through 7, respectively,
and all other \textbf{ord} values returns \textbf{nbits}.  When \textbf{ord} value 3
gets mapped to (returns) \textbf{pos} value 7 in this example, that means
that the 3rd set bit (starting with 0th) is at position 7 in \textbf{buf}.

The bit positions 0 through \textbf{nbits}-1 are valid positions in \textbf{buf}.
\index{bitmap\_find\_next\_zero\_area (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.bitmap_find_next_zero_area}\pysiglinewithargsret{unsigned long \bfcode{bitmap\_find\_next\_zero\_area}}{unsigned long *\emph{ map}, unsigned long\emph{ size}, unsigned long\emph{ start}, unsigned int\emph{ nr}, unsigned long\emph{ align\_mask}}{}
find a contiguous aligned zero area

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned long * map}}] \leavevmode
The address to base the search on

\item[{\code{unsigned long size}}] \leavevmode
The bitmap size in bits

\item[{\code{unsigned long start}}] \leavevmode
The bitnumber to start searching at

\item[{\code{unsigned int nr}}] \leavevmode
The number of zeroed bits we're looking for

\item[{\code{unsigned long align\_mask}}] \leavevmode
Alignment mask for zero area

\end{description}

\textbf{Description}

The \textbf{align\_mask} should be one less than a power of 2; the effect is that
the bit offset of all zero areas this function finds is multiples of that
power of 2. A \textbf{align\_mask} of 0 means no alignment is required.
\index{BITMAP\_FROM\_U64 (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.BITMAP_FROM_U64}\pysiglinewithargsret{\bfcode{BITMAP\_FROM\_U64}}{\emph{n}}{}
Represent u64 value in the format suitable for bitmap.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{n}}] \leavevmode
u64 value

\end{description}

\textbf{Description}

Linux bitmaps are internally arrays of unsigned longs, i.e. 32-bit
integers in 32-bit environment, and 64-bit integers in 64-bit one.

There are four combinations of endianness and length of the word in linux
ABIs: LE64, BE64, LE32 and BE32.

On 64-bit kernels 64-bit LE and BE numbers are naturally ordered in
bitmaps and therefore don't require any special handling.

On 32-bit kernels 32-bit LE ABI orders lo word of 64-bit number in memory
prior to hi, and 32-bit BE orders hi word prior to lo. The bitmap on the
other hand is represented as an array of 32-bit words and the position of
bit N may therefore be calculated as: word \#(N/32) and bit \#(N{}`{}`32{}`{}`) in that
word.  For example, bit \#42 is located at 10th position of 2nd word.
It matches 32-bit LE ABI, and we can simply let the compiler store 64-bit
values in memory as it usually does. But for BE we need to swap hi and lo
words manually.

With all that, the macro {\hyperref[core\string-api/kernel\string-api:c.BITMAP_FROM_U64]{\emph{\code{BITMAP\_FROM\_U64()}}}} does explicit reordering of hi and
lo parts of u64.  For LE32 it does nothing, and for BE environment it swaps
hi and lo words, as is expected by bitmap.
\index{bitmap\_from\_u64 (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.bitmap_from_u64}\pysiglinewithargsret{void \bfcode{bitmap\_from\_u64}}{unsigned long *\emph{ dst}, u64\emph{ mask}}{}
Check and swap words within u64.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned long * dst}}] \leavevmode
destination bitmap

\item[{\code{u64 mask}}] \leavevmode
source bitmap

\end{description}

\textbf{Description}

In 32-bit Big Endian kernel, when using \code{(u32 *)(:c:type:{}`val{}`){[}*{]}}
to read u64 mask, we will get the wrong word.
That is \code{(u32 *)(:c:type:{}`val{}`){[}0{]}} gets the upper 32 bits,
but we expect the lower 32-bits of u64.


\subsubsection{Command-line Parsing}
\label{core-api/kernel-api:command-line-parsing}\index{get\_option (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.get_option}\pysiglinewithargsret{int \bfcode{get\_option}}{char **\emph{ str}, int *\emph{ pint}}{}
Parse integer from an option string

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{char ** str}}] \leavevmode
option string

\item[{\code{int * pint}}] \leavevmode
(output) integer value parsed from \textbf{str}

\end{description}

\textbf{Description}
\begin{quote}

Read an int from an option string; if available accept a subsequent
comma as well.

Return values:
0 - no int in string
1 - int found, no subsequent comma
2 - int found including a subsequent comma
3 - hyphen found to denote a range
\end{quote}
\index{get\_options (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.get_options}\pysiglinewithargsret{char * \bfcode{get\_options}}{const char *\emph{ str}, int\emph{ nints}, int *\emph{ ints}}{}
Parse a string into a list of integers

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * str}}] \leavevmode
String to be parsed

\item[{\code{int nints}}] \leavevmode
size of integer array

\item[{\code{int * ints}}] \leavevmode
integer array

\end{description}

\textbf{Description}
\begin{quote}

This function parses a string containing a comma-separated
list of integers, a hyphen-separated range of \_positive\_ integers,
or a combination of both.  The parse halts when the array is
full, or when no more numbers can be retrieved from the
string.

Return value is the character in the string which caused
the parse to end (typically a null terminator, if \textbf{str} is
completely parseable).
\end{quote}
\index{memparse (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.memparse}\pysiglinewithargsret{unsigned long long \bfcode{memparse}}{const char *\emph{ ptr}, char **\emph{ retptr}}{}
parse a string with mem suffixes into a number

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * ptr}}] \leavevmode
Where parse begins

\item[{\code{char ** retptr}}] \leavevmode
(output) Optional pointer to next char after parse completes

\end{description}

\textbf{Description}
\begin{quote}

Parses a string into a number.  The number stored at \textbf{ptr} is
potentially suffixed with K, M, G, T, P, E.
\end{quote}


\subsubsection{CRC Functions}
\label{core-api/kernel-api:crc-functions}\index{crc4 (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.crc4}\pysiglinewithargsret{uint8\_t \bfcode{crc4}}{uint8\_t\emph{ c}, uint64\_t\emph{ x}, int\emph{ bits}}{}
calculate the 4-bit crc of a value.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{uint8\_t c}}] \leavevmode
starting crc4

\item[{\code{uint64\_t x}}] \leavevmode
value to checksum

\item[{\code{int bits}}] \leavevmode
number of bits in \textbf{x} to checksum

\end{description}

\textbf{Description}

Returns the crc4 value of \textbf{x}, using polynomial 0b10111.

The \textbf{x} value is treated as left-aligned, and bits above \textbf{bits} are ignored
in the crc calculations.
\index{crc7\_be (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.crc7_be}\pysiglinewithargsret{u8 \bfcode{crc7\_be}}{u8\emph{ crc}, const u8 *\emph{ buffer}, size\_t\emph{ len}}{}
update the CRC7 for the data buffer

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{u8 crc}}] \leavevmode
previous CRC7 value

\item[{\code{const u8 * buffer}}] \leavevmode
data pointer

\item[{\code{size\_t len}}] \leavevmode
number of bytes in the buffer

\end{description}

\textbf{Context}

any

\textbf{Description}

Returns the updated CRC7 value.
The CRC7 is left-aligned in the byte (the lsbit is always 0), as that
makes the computation easier, and all callers want it in that form.
\index{crc8\_populate\_msb (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.crc8_populate_msb}\pysiglinewithargsret{void \bfcode{crc8\_populate\_msb}}{u8\emph{ table}, u8\emph{ polynomial}}{}
fill crc table for given polynomial in reverse bit order.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{u8 table}}] \leavevmode
table to be filled.

\item[{\code{u8 polynomial}}] \leavevmode
polynomial for which table is to be filled.

\end{description}
\index{crc8\_populate\_lsb (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.crc8_populate_lsb}\pysiglinewithargsret{void \bfcode{crc8\_populate\_lsb}}{u8\emph{ table}, u8\emph{ polynomial}}{}
fill crc table for given polynomial in regular bit order.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{u8 table}}] \leavevmode
table to be filled.

\item[{\code{u8 polynomial}}] \leavevmode
polynomial for which table is to be filled.

\end{description}
\index{crc8 (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.crc8}\pysiglinewithargsret{u8 \bfcode{crc8}}{const u8\emph{ table}, u8 *\emph{ pdata}, size\_t\emph{ nbytes}, u8\emph{ crc}}{}
calculate a crc8 over the given input data.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const u8 table}}] \leavevmode
crc table used for calculation.

\item[{\code{u8 * pdata}}] \leavevmode
pointer to data buffer.

\item[{\code{size\_t nbytes}}] \leavevmode
number of bytes in data buffer.

\item[{\code{u8 crc}}] \leavevmode
previous returned crc8 value.

\end{description}
\index{crc16 (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.crc16}\pysiglinewithargsret{u16 \bfcode{crc16}}{u16\emph{ crc}, u8 const *\emph{ buffer}, size\_t\emph{ len}}{}
compute the CRC-16 for the data buffer

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{u16 crc}}] \leavevmode
previous CRC value

\item[{\code{u8 const * buffer}}] \leavevmode
data pointer

\item[{\code{size\_t len}}] \leavevmode
number of bytes in the buffer

\end{description}

\textbf{Description}

Returns the updated CRC value.
\index{crc32\_le\_generic (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.crc32_le_generic}\pysiglinewithargsret{u32 \_\_pure \bfcode{crc32\_le\_generic}}{u32\emph{ crc}, unsigned char const *\emph{ p}, size\_t\emph{ len}, const u32 ( *\emph{ tab}, u32\emph{ polynomial}}{}
Calculate bitwise little-endian Ethernet AUTODIN II CRC32/CRC32C

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{u32 crc}}] \leavevmode
seed value for computation.  \textasciitilde{}0 for Ethernet, sometimes 0 for other
uses, or the previous crc32/crc32c value if computing incrementally.

\item[{\code{unsigned char const * p}}] \leavevmode
pointer to buffer over which CRC32/CRC32C is run

\item[{\code{size\_t len}}] \leavevmode
length of buffer \textbf{p}

\item[{\code{const u32 ( * tab}}] \leavevmode
little-endian Ethernet table

\item[{\code{u32 polynomial}}] \leavevmode
CRC32/CRC32c LE polynomial

\end{description}
\index{crc32\_generic\_shift (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.crc32_generic_shift}\pysiglinewithargsret{u32 \_\_attribute\_const\_\_ \bfcode{crc32\_generic\_shift}}{u32\emph{ crc}, size\_t\emph{ len}, u32\emph{ polynomial}}{}
Append \textbf{len} 0 bytes to crc, in logarithmic time

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{u32 crc}}] \leavevmode
The original little-endian CRC (i.e. lsbit is x\textasciicircum{}31 coefficient)

\item[{\code{size\_t len}}] \leavevmode
The number of bytes. \textbf{crc} is multiplied by x\textasciicircum{}(8***len**)

\item[{\code{u32 polynomial}}] \leavevmode
The modulus used to reduce the result to 32 bits.

\end{description}

\textbf{Description}

It's possible to parallelize CRC computations by computing a CRC
over separate ranges of a buffer, then summing them.
This shifts the given CRC by 8*len bits (i.e. produces the same effect
as appending len bytes of zero to the data), in time proportional
to log(len).
\index{crc32\_be\_generic (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.crc32_be_generic}\pysiglinewithargsret{u32 \_\_pure \bfcode{crc32\_be\_generic}}{u32\emph{ crc}, unsigned char const *\emph{ p}, size\_t\emph{ len}, const u32 ( *\emph{ tab}, u32\emph{ polynomial}}{}
Calculate bitwise big-endian Ethernet AUTODIN II CRC32

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{u32 crc}}] \leavevmode
seed value for computation.  \textasciitilde{}0 for Ethernet, sometimes 0 for
other uses, or the previous crc32 value if computing incrementally.

\item[{\code{unsigned char const * p}}] \leavevmode
pointer to buffer over which CRC32 is run

\item[{\code{size\_t len}}] \leavevmode
length of buffer \textbf{p}

\item[{\code{const u32 ( * tab}}] \leavevmode
big-endian Ethernet table

\item[{\code{u32 polynomial}}] \leavevmode
CRC32 BE polynomial

\end{description}
\index{crc\_ccitt (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.crc_ccitt}\pysiglinewithargsret{u16 \bfcode{crc\_ccitt}}{u16\emph{ crc}, u8 const *\emph{ buffer}, size\_t\emph{ len}}{}
recompute the CRC (CRC-CCITT variant) for the data buffer

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{u16 crc}}] \leavevmode
previous CRC value

\item[{\code{u8 const * buffer}}] \leavevmode
data pointer

\item[{\code{size\_t len}}] \leavevmode
number of bytes in the buffer

\end{description}
\index{crc\_ccitt\_false (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.crc_ccitt_false}\pysiglinewithargsret{u16 \bfcode{crc\_ccitt\_false}}{u16\emph{ crc}, u8 const *\emph{ buffer}, size\_t\emph{ len}}{}
recompute the CRC (CRC-CCITT-FALSE variant) for the data buffer

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{u16 crc}}] \leavevmode
previous CRC value

\item[{\code{u8 const * buffer}}] \leavevmode
data pointer

\item[{\code{size\_t len}}] \leavevmode
number of bytes in the buffer

\end{description}
\index{crc\_itu\_t (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.crc_itu_t}\pysiglinewithargsret{u16 \bfcode{crc\_itu\_t}}{u16\emph{ crc}, const u8 *\emph{ buffer}, size\_t\emph{ len}}{}
Compute the CRC-ITU-T for the data buffer

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{u16 crc}}] \leavevmode
previous CRC value

\item[{\code{const u8 * buffer}}] \leavevmode
data pointer

\item[{\code{size\_t len}}] \leavevmode
number of bytes in the buffer

\end{description}

\textbf{Description}

Returns the updated CRC value


\subsection{Math Functions in Linux}
\label{core-api/kernel-api:math-functions-in-linux}

\subsubsection{Base 2 log and power Functions}
\label{core-api/kernel-api:base-2-log-and-power-functions}\index{is\_power\_of\_2 (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.is_power_of_2}\pysiglinewithargsret{bool \bfcode{is\_power\_of\_2}}{unsigned long\emph{ n}}{}
check if a value is a power of two

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned long n}}] \leavevmode
the value to check

\end{description}

\textbf{Description}

Determine whether some value is a power of two, where zero is
\emph{not} considered a power of two.

\textbf{Return}

true if \textbf{n} is a power of 2, otherwise false.
\index{\_\_roundup\_pow\_of\_two (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__roundup_pow_of_two}\pysiglinewithargsret{unsigned long \bfcode{\_\_roundup\_pow\_of\_two}}{unsigned long\emph{ n}}{}
round up to nearest power of two

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned long n}}] \leavevmode
value to round up

\end{description}
\index{\_\_rounddown\_pow\_of\_two (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__rounddown_pow_of_two}\pysiglinewithargsret{unsigned long \bfcode{\_\_rounddown\_pow\_of\_two}}{unsigned long\emph{ n}}{}
round down to nearest power of two

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned long n}}] \leavevmode
value to round down

\end{description}
\index{ilog2 (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.ilog2}\pysiglinewithargsret{\bfcode{ilog2}}{\emph{n}}{}
log base 2 of 32-bit or a 64-bit unsigned value

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{n}}] \leavevmode
parameter

\end{description}

\textbf{Description}

constant-capable log of base 2 calculation
- this can be used to initialise global variables from constant data, hence
the massive ternary operator construction

selects the appropriately-sized optimised version depending on sizeof(n)
\index{roundup\_pow\_of\_two (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.roundup_pow_of_two}\pysiglinewithargsret{\bfcode{roundup\_pow\_of\_two}}{\emph{n}}{}
round the given value up to nearest power of two

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{n}}] \leavevmode
parameter

\end{description}

\textbf{Description}

round the given value up to the nearest power of two
- the result is undefined when n == 0
- this can be used to initialise global variables from constant data
\index{rounddown\_pow\_of\_two (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rounddown_pow_of_two}\pysiglinewithargsret{\bfcode{rounddown\_pow\_of\_two}}{\emph{n}}{}
round the given value down to nearest power of two

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{n}}] \leavevmode
parameter

\end{description}

\textbf{Description}

round the given value down to the nearest power of two
- the result is undefined when n == 0
- this can be used to initialise global variables from constant data
\index{order\_base\_2 (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.order_base_2}\pysiglinewithargsret{\bfcode{order\_base\_2}}{\emph{n}}{}
calculate the (rounded up) base 2 order of the argument

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{n}}] \leavevmode
parameter

\end{description}

\textbf{Description}
\begin{description}
\item[{The first few values calculated by this routine:}] \leavevmode
ob2(0) = 0
ob2(1) = 0
ob2(2) = 1
ob2(3) = 2
ob2(4) = 2
ob2(5) = 3
... and so on.

\end{description}


\subsubsection{Division Functions}
\label{core-api/kernel-api:division-functions}\index{do\_div (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.do_div}\pysiglinewithargsret{\bfcode{do\_div}}{\emph{n}, \emph{base}}{}
returns 2 values: calculate remainder and update new dividend

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{n}}] \leavevmode
pointer to uint64\_t dividend (will be updated)

\item[{\code{base}}] \leavevmode
uint32\_t divisor

\end{description}

\textbf{Description}

Summary:
\code{uint32\_t remainder = *n \% base;}
\code{*n = *n / base;}

\textbf{Return}

(uint32\_t)remainder

\textbf{NOTE}

macro parameter \textbf{n} is evaluated multiple times,
beware of side effects!
\index{div\_u64\_rem (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.div_u64_rem}\pysiglinewithargsret{u64 \bfcode{div\_u64\_rem}}{u64\emph{ dividend}, u32\emph{ divisor}, u32 *\emph{ remainder}}{}
unsigned 64bit divide with 32bit divisor with remainder

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{u64 dividend}}] \leavevmode
unsigned 64bit dividend

\item[{\code{u32 divisor}}] \leavevmode
unsigned 32bit divisor

\item[{\code{u32 * remainder}}] \leavevmode
pointer to unsigned 32bit remainder

\end{description}

\textbf{Return}

sets \code{*remainder}, then returns dividend / divisor

This is commonly provided by 32bit archs to provide an optimized 64bit
divide.
\index{div\_s64\_rem (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.div_s64_rem}\pysiglinewithargsret{s64 \bfcode{div\_s64\_rem}}{s64\emph{ dividend}, s32\emph{ divisor}, s32 *\emph{ remainder}}{}
signed 64bit divide with 32bit divisor with remainder

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{s64 dividend}}] \leavevmode
signed 64bit dividend

\item[{\code{s32 divisor}}] \leavevmode
signed 32bit divisor

\item[{\code{s32 * remainder}}] \leavevmode
pointer to signed 32bit remainder

\end{description}

\textbf{Return}

sets \code{*remainder}, then returns dividend / divisor
\index{div64\_u64\_rem (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.div64_u64_rem}\pysiglinewithargsret{u64 \bfcode{div64\_u64\_rem}}{u64\emph{ dividend}, u64\emph{ divisor}, u64 *\emph{ remainder}}{}
unsigned 64bit divide with 64bit divisor and remainder

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{u64 dividend}}] \leavevmode
unsigned 64bit dividend

\item[{\code{u64 divisor}}] \leavevmode
unsigned 64bit divisor

\item[{\code{u64 * remainder}}] \leavevmode
pointer to unsigned 64bit remainder

\end{description}

\textbf{Return}

sets \code{*remainder}, then returns dividend / divisor
\index{div64\_u64 (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.div64_u64}\pysiglinewithargsret{u64 \bfcode{div64\_u64}}{u64\emph{ dividend}, u64\emph{ divisor}}{}
unsigned 64bit divide with 64bit divisor

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{u64 dividend}}] \leavevmode
unsigned 64bit dividend

\item[{\code{u64 divisor}}] \leavevmode
unsigned 64bit divisor

\end{description}

\textbf{Return}

dividend / divisor
\index{div64\_s64 (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.div64_s64}\pysiglinewithargsret{s64 \bfcode{div64\_s64}}{s64\emph{ dividend}, s64\emph{ divisor}}{}
signed 64bit divide with 64bit divisor

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{s64 dividend}}] \leavevmode
signed 64bit dividend

\item[{\code{s64 divisor}}] \leavevmode
signed 64bit divisor

\end{description}

\textbf{Return}

dividend / divisor
\index{div\_u64 (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.div_u64}\pysiglinewithargsret{u64 \bfcode{div\_u64}}{u64\emph{ dividend}, u32\emph{ divisor}}{}
unsigned 64bit divide with 32bit divisor

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{u64 dividend}}] \leavevmode
unsigned 64bit dividend

\item[{\code{u32 divisor}}] \leavevmode
unsigned 32bit divisor

\end{description}

\textbf{Description}

This is the most common 64bit divide and should be used if possible,
as many 32bit archs can optimize this variant better than a full 64bit
divide.
\index{div\_s64 (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.div_s64}\pysiglinewithargsret{s64 \bfcode{div\_s64}}{s64\emph{ dividend}, s32\emph{ divisor}}{}
signed 64bit divide with 32bit divisor

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{s64 dividend}}] \leavevmode
signed 64bit dividend

\item[{\code{s32 divisor}}] \leavevmode
signed 32bit divisor

\end{description}
\index{div\_s64\_rem (C function)}

\begin{fulllineitems}
\pysiglinewithargsret{s64 \bfcode{div\_s64\_rem}}{s64\emph{ dividend}, s32\emph{ divisor}, s32 *\emph{ remainder}}{}
signed 64bit divide with 64bit divisor and remainder

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{s64 dividend}}] \leavevmode
64bit dividend

\item[{\code{s32 divisor}}] \leavevmode
64bit divisor

\item[{\code{s32 * remainder}}] \leavevmode
64bit remainder

\end{description}
\index{div64\_u64\_rem (C function)}

\begin{fulllineitems}
\pysiglinewithargsret{u64 \bfcode{div64\_u64\_rem}}{u64\emph{ dividend}, u64\emph{ divisor}, u64 *\emph{ remainder}}{}
unsigned 64bit divide with 64bit divisor and remainder

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{u64 dividend}}] \leavevmode
64bit dividend

\item[{\code{u64 divisor}}] \leavevmode
64bit divisor

\item[{\code{u64 * remainder}}] \leavevmode
64bit remainder

\end{description}

\textbf{Description}

This implementation is a comparable to algorithm used by div64\_u64.
But this operation, which includes math for calculating the remainder,
is kept distinct to avoid slowing down the div64\_u64 operation on 32bit
systems.
\index{div64\_u64 (C function)}

\begin{fulllineitems}
\pysiglinewithargsret{u64 \bfcode{div64\_u64}}{u64\emph{ dividend}, u64\emph{ divisor}}{}
unsigned 64bit divide with 64bit divisor

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{u64 dividend}}] \leavevmode
64bit dividend

\item[{\code{u64 divisor}}] \leavevmode
64bit divisor

\end{description}

\textbf{Description}

This implementation is a modified version of the algorithm proposed
by the book `Hacker's Delight'.  The original source and full proof
can be found here and is available for use without restriction.

`\href{http://www.hackersdelight.org/hdcodetxt/divDouble.c.txt}{http://www.hackersdelight.org/hdcodetxt/divDouble.c.txt}`
\index{div64\_s64 (C function)}

\begin{fulllineitems}
\pysiglinewithargsret{s64 \bfcode{div64\_s64}}{s64\emph{ dividend}, s64\emph{ divisor}}{}
signed 64bit divide with 64bit divisor

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{s64 dividend}}] \leavevmode
64bit dividend

\item[{\code{s64 divisor}}] \leavevmode
64bit divisor

\end{description}
\index{gcd (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.gcd}\pysiglinewithargsret{unsigned long \bfcode{gcd}}{unsigned long\emph{ a}, unsigned long\emph{ b}}{}
calculate and return the greatest common divisor of 2 unsigned longs

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned long a}}] \leavevmode
first value

\item[{\code{unsigned long b}}] \leavevmode
second value

\end{description}


\subsubsection{Sorting}
\label{core-api/kernel-api:sorting}\index{sort (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.sort}\pysiglinewithargsret{void \bfcode{sort}}{void *\emph{ base}, size\_t\emph{ num}, size\_t\emph{ size}, int (*cmp\_func) (const void\emph{ *}, const void\emph{ *}, void (*swap\_func) (void\emph{ *}, void\emph{ *}, int\emph{ size}}{}
sort an array of elements

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void * base}}] \leavevmode
pointer to data to sort

\item[{\code{size\_t num}}] \leavevmode
number of elements

\item[{\code{size\_t size}}] \leavevmode
size of each element

\item[{\code{int (*)(const void *, const void *) cmp\_func}}] \leavevmode
pointer to comparison function

\item[{\code{void (*)(void *, void *, int size) swap\_func}}] \leavevmode
pointer to swap function or NULL

\end{description}

\textbf{Description}

This function does a heapsort on the given array. You may provide a
swap\_func function optimized to your element type.

Sorting time is O(n log n) both on average and worst-case. While
qsort is about 20\% faster on average, it suffers from exploitable
O(n*n) worst-case behavior and extra memory requirements that make
it less suitable for kernel use.
\index{list\_sort (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_sort}\pysiglinewithargsret{void \bfcode{list\_sort}}{void *\emph{ priv}, struct list\_head *\emph{ head}, int (*cmp) (void\emph{ *priv}, struct list\_head\emph{ *a}, struct list\_head\emph{ *b}}{}
sort a list

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void * priv}}] \leavevmode
private data, opaque to {\hyperref[core\string-api/kernel\string-api:c.list_sort]{\emph{\code{list\_sort()}}}}, passed to \textbf{cmp}

\item[{\code{struct list\_head * head}}] \leavevmode
the list to sort

\item[{\code{int (*)(void *priv, struct list\_head *a, struct list\_head *b) cmp}}] \leavevmode
the elements comparison function

\end{description}

\textbf{Description}

This function implements ``merge sort'', which has O(nlog(n))
complexity.

The comparison function \textbf{cmp} must return a negative value if \textbf{a}
should sort before \textbf{b}, and a positive value if \textbf{a} should sort after
\textbf{b}. If \textbf{a} and \textbf{b} are equivalent, and their original relative
ordering is to be preserved, \textbf{cmp} must return 0.


\subsubsection{UUID/GUID}
\label{core-api/kernel-api:uuid-guid}\index{generate\_random\_uuid (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.generate_random_uuid}\pysiglinewithargsret{void \bfcode{generate\_random\_uuid}}{unsigned char\emph{ uuid}}{}
generate a random UUID

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned char uuid}}] \leavevmode
where to put the generated UUID

\end{description}

\textbf{Description}

Random UUID interface

Used to create a Boot ID or a filesystem UUID/GUID, but can be
useful for other kernel drivers.
\index{uuid\_is\_valid (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.uuid_is_valid}\pysiglinewithargsret{bool \bfcode{uuid\_is\_valid}}{const char *\emph{ uuid}}{}
checks if a UUID string is valid

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * uuid}}] \leavevmode
UUID string to check

\end{description}

\textbf{Description}
\begin{description}
\item[{It checks if the UUID string is following the format:}] \leavevmode
xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx

\end{description}

where x is a hex digit.

\textbf{Return}

true if input is valid UUID string.


\subsection{Memory Management in Linux}
\label{core-api/kernel-api:memory-management-in-linux}

\subsubsection{The Slab Cache}
\label{core-api/kernel-api:the-slab-cache}\index{kmalloc (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kmalloc}\pysiglinewithargsret{void * \bfcode{kmalloc}}{size\_t\emph{ size}, gfp\_t\emph{ flags}}{}
allocate memory

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{size\_t size}}] \leavevmode
how many bytes of memory are required.

\item[{\code{gfp\_t flags}}] \leavevmode
the type of memory to allocate.

\end{description}

\textbf{Description}

kmalloc is the normal method of allocating memory
for objects smaller than page size in the kernel.

The \textbf{flags} argument may be one of:

\code{GFP\_USER} - Allocate memory on behalf of user.  May sleep.

\code{GFP\_KERNEL} - Allocate normal kernel ram.  May sleep.
\begin{description}
\item[{\code{GFP\_ATOMIC} - Allocation will not sleep.  May use emergency pools.}] \leavevmode
For example, use this inside interrupt handlers.

\end{description}

\code{GFP\_HIGHUSER} - Allocate pages from high memory.

\code{GFP\_NOIO} - Do not do any I/O at all while trying to get memory.

\code{GFP\_NOFS} - Do not make any fs calls while trying to get memory.

\code{GFP\_NOWAIT} - Allocation will not sleep.

\code{\_\_GFP\_THISNODE} - Allocate node-local memory only.
\begin{description}
\item[{\code{GFP\_DMA} - Allocation suitable for DMA.}] \leavevmode
Should only be used for {\hyperref[core\string-api/kernel\string-api:c.kmalloc]{\emph{\code{kmalloc()}}}} caches. Otherwise, use a
slab created with SLAB\_DMA.

\end{description}

Also it is possible to set different flags by OR'ing
in one or more of the following additional \textbf{flags}:

\code{\_\_GFP\_HIGH} - This allocation has high priority and may use emergency pools.
\begin{description}
\item[{\code{\_\_GFP\_NOFAIL} - Indicate that this allocation is in no way allowed to fail}] \leavevmode
(think twice before using).

\item[{\code{\_\_GFP\_NORETRY} - If memory is not immediately available,}] \leavevmode
then give up at once.

\end{description}

\code{\_\_GFP\_NOWARN} - If allocation fails, don't issue any warnings.
\begin{description}
\item[{\code{\_\_GFP\_RETRY\_MAYFAIL} - Try really hard to succeed the allocation but fail}] \leavevmode
eventually.

\end{description}

There are other flags available as well, but these are not intended
for general use, and so are not documented here. For a full list of
potential flags, always refer to linux/gfp.h.
\index{kmalloc\_array (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kmalloc_array}\pysiglinewithargsret{void * \bfcode{kmalloc\_array}}{size\_t\emph{ n}, size\_t\emph{ size}, gfp\_t\emph{ flags}}{}
allocate memory for an array.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{size\_t n}}] \leavevmode
number of elements.

\item[{\code{size\_t size}}] \leavevmode
element size.

\item[{\code{gfp\_t flags}}] \leavevmode
the type of memory to allocate (see kmalloc).

\end{description}
\index{kcalloc (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kcalloc}\pysiglinewithargsret{void * \bfcode{kcalloc}}{size\_t\emph{ n}, size\_t\emph{ size}, gfp\_t\emph{ flags}}{}
allocate memory for an array. The memory is set to zero.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{size\_t n}}] \leavevmode
number of elements.

\item[{\code{size\_t size}}] \leavevmode
element size.

\item[{\code{gfp\_t flags}}] \leavevmode
the type of memory to allocate (see kmalloc).

\end{description}
\index{kzalloc (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kzalloc}\pysiglinewithargsret{void * \bfcode{kzalloc}}{size\_t\emph{ size}, gfp\_t\emph{ flags}}{}
allocate memory. The memory is set to zero.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{size\_t size}}] \leavevmode
how many bytes of memory are required.

\item[{\code{gfp\_t flags}}] \leavevmode
the type of memory to allocate (see kmalloc).

\end{description}
\index{kzalloc\_node (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kzalloc_node}\pysiglinewithargsret{void * \bfcode{kzalloc\_node}}{size\_t\emph{ size}, gfp\_t\emph{ flags}, int\emph{ node}}{}
allocate zeroed memory from a particular memory node.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{size\_t size}}] \leavevmode
how many bytes of memory are required.

\item[{\code{gfp\_t flags}}] \leavevmode
the type of memory to allocate (see kmalloc).

\item[{\code{int node}}] \leavevmode
memory node from which to allocate

\end{description}
\index{kmem\_cache\_alloc (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kmem_cache_alloc}\pysiglinewithargsret{void * \bfcode{kmem\_cache\_alloc}}{struct kmem\_cache *\emph{ cachep}, gfp\_t\emph{ flags}}{}
Allocate an object

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct kmem\_cache * cachep}}] \leavevmode
The cache to allocate from.

\item[{\code{gfp\_t flags}}] \leavevmode
See {\hyperref[core\string-api/kernel\string-api:c.kmalloc]{\emph{\code{kmalloc()}}}}.

\end{description}

\textbf{Description}

Allocate an object from this cache.  The flags are only relevant
if the cache has no available objects.
\index{kmem\_cache\_alloc\_node (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kmem_cache_alloc_node}\pysiglinewithargsret{void * \bfcode{kmem\_cache\_alloc\_node}}{struct kmem\_cache *\emph{ cachep}, gfp\_t\emph{ flags}, int\emph{ nodeid}}{}
Allocate an object on the specified node

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct kmem\_cache * cachep}}] \leavevmode
The cache to allocate from.

\item[{\code{gfp\_t flags}}] \leavevmode
See {\hyperref[core\string-api/kernel\string-api:c.kmalloc]{\emph{\code{kmalloc()}}}}.

\item[{\code{int nodeid}}] \leavevmode
node number of the target node.

\end{description}

\textbf{Description}

Identical to kmem\_cache\_alloc but it will allocate memory on the given
node, which can improve the performance for cpu bound structures.

Fallback to other node is possible if \_\_GFP\_THISNODE is not set.
\index{kmem\_cache\_free (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kmem_cache_free}\pysiglinewithargsret{void \bfcode{kmem\_cache\_free}}{struct kmem\_cache *\emph{ cachep}, void *\emph{ objp}}{}
Deallocate an object

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct kmem\_cache * cachep}}] \leavevmode
The cache the allocation was from.

\item[{\code{void * objp}}] \leavevmode
The previously allocated object.

\end{description}

\textbf{Description}

Free an object which was previously allocated from this
cache.
\index{kfree (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kfree}\pysiglinewithargsret{void \bfcode{kfree}}{const void *\emph{ objp}}{}
free previously allocated memory

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const void * objp}}] \leavevmode
pointer returned by kmalloc.

\end{description}

\textbf{Description}

If \textbf{objp} is NULL, no operation is performed.

Don't free memory not originally allocated by {\hyperref[core\string-api/kernel\string-api:c.kmalloc]{\emph{\code{kmalloc()}}}}
or you will run into trouble.
\index{ksize (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.ksize}\pysiglinewithargsret{size\_t \bfcode{ksize}}{const void *\emph{ objp}}{}
get the actual amount of memory allocated for a given object

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const void * objp}}] \leavevmode
Pointer to the object

\end{description}

\textbf{Description}

kmalloc may internally round up allocations and return more memory
than requested. {\hyperref[core\string-api/kernel\string-api:c.ksize]{\emph{\code{ksize()}}}} can be used to determine the actual amount of
memory allocated. The caller may use this additional memory, even though
a smaller amount of memory was initially specified with the kmalloc call.
The caller must guarantee that objp points to a valid object previously
allocated with either {\hyperref[core\string-api/kernel\string-api:c.kmalloc]{\emph{\code{kmalloc()}}}} or {\hyperref[core\string-api/kernel\string-api:c.kmem_cache_alloc]{\emph{\code{kmem\_cache\_alloc()}}}}. The object
must not be freed during the duration of the call.
\index{kfree\_const (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kfree_const}\pysiglinewithargsret{void \bfcode{kfree\_const}}{const void *\emph{ x}}{}
conditionally free memory

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const void * x}}] \leavevmode
pointer to the memory

\end{description}

\textbf{Description}

Function calls kfree only if \textbf{x} is not in .rodata section.
\index{kstrdup (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kstrdup}\pysiglinewithargsret{char * \bfcode{kstrdup}}{const char *\emph{ s}, gfp\_t\emph{ gfp}}{}
allocate space for and copy an existing string

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * s}}] \leavevmode
the string to duplicate

\item[{\code{gfp\_t gfp}}] \leavevmode
the GFP mask used in the {\hyperref[core\string-api/kernel\string-api:c.kmalloc]{\emph{\code{kmalloc()}}}} call when allocating memory

\end{description}
\index{kstrdup\_const (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kstrdup_const}\pysiglinewithargsret{const char * \bfcode{kstrdup\_const}}{const char *\emph{ s}, gfp\_t\emph{ gfp}}{}
conditionally duplicate an existing const string

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * s}}] \leavevmode
the string to duplicate

\item[{\code{gfp\_t gfp}}] \leavevmode
the GFP mask used in the {\hyperref[core\string-api/kernel\string-api:c.kmalloc]{\emph{\code{kmalloc()}}}} call when allocating memory

\end{description}

\textbf{Description}

Function returns source string if it is in .rodata section otherwise it
fallbacks to kstrdup.
Strings allocated by kstrdup\_const should be freed by kfree\_const.
\index{kstrndup (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kstrndup}\pysiglinewithargsret{char * \bfcode{kstrndup}}{const char *\emph{ s}, size\_t\emph{ max}, gfp\_t\emph{ gfp}}{}
allocate space for and copy an existing string

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * s}}] \leavevmode
the string to duplicate

\item[{\code{size\_t max}}] \leavevmode
read at most \textbf{max} chars from \textbf{s}

\item[{\code{gfp\_t gfp}}] \leavevmode
the GFP mask used in the {\hyperref[core\string-api/kernel\string-api:c.kmalloc]{\emph{\code{kmalloc()}}}} call when allocating memory

\end{description}

\textbf{Note}

Use {\hyperref[core\string-api/kernel\string-api:c.kmemdup_nul]{\emph{\code{kmemdup\_nul()}}}} instead if the size is known exactly.
\index{kmemdup (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kmemdup}\pysiglinewithargsret{void * \bfcode{kmemdup}}{const void *\emph{ src}, size\_t\emph{ len}, gfp\_t\emph{ gfp}}{}
duplicate region of memory

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const void * src}}] \leavevmode
memory region to duplicate

\item[{\code{size\_t len}}] \leavevmode
memory region length

\item[{\code{gfp\_t gfp}}] \leavevmode
GFP mask to use

\end{description}
\index{kmemdup\_nul (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kmemdup_nul}\pysiglinewithargsret{char * \bfcode{kmemdup\_nul}}{const char *\emph{ s}, size\_t\emph{ len}, gfp\_t\emph{ gfp}}{}
Create a NUL-terminated string from unterminated data

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * s}}] \leavevmode
The data to stringify

\item[{\code{size\_t len}}] \leavevmode
The size of the data

\item[{\code{gfp\_t gfp}}] \leavevmode
the GFP mask used in the {\hyperref[core\string-api/kernel\string-api:c.kmalloc]{\emph{\code{kmalloc()}}}} call when allocating memory

\end{description}
\index{memdup\_user (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.memdup_user}\pysiglinewithargsret{void * \bfcode{memdup\_user}}{const void \_\_user *\emph{ src}, size\_t\emph{ len}}{}
duplicate memory region from user space

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const void \_\_user * src}}] \leavevmode
source address in user space

\item[{\code{size\_t len}}] \leavevmode
number of bytes to copy

\end{description}

\textbf{Description}

Returns an \code{ERR\_PTR()} on failure.  Result is physically
contiguous, to be freed by {\hyperref[core\string-api/kernel\string-api:c.kfree]{\emph{\code{kfree()}}}}.
\index{vmemdup\_user (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.vmemdup_user}\pysiglinewithargsret{void * \bfcode{vmemdup\_user}}{const void \_\_user *\emph{ src}, size\_t\emph{ len}}{}
duplicate memory region from user space

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const void \_\_user * src}}] \leavevmode
source address in user space

\item[{\code{size\_t len}}] \leavevmode
number of bytes to copy

\end{description}

\textbf{Description}

Returns an \code{ERR\_PTR()} on failure.  Result may be not
physically contiguous.  Use \code{kvfree()} to free.
\index{memdup\_user\_nul (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.memdup_user_nul}\pysiglinewithargsret{void * \bfcode{memdup\_user\_nul}}{const void \_\_user *\emph{ src}, size\_t\emph{ len}}{}
duplicate memory region from user space and NUL-terminate

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const void \_\_user * src}}] \leavevmode
source address in user space

\item[{\code{size\_t len}}] \leavevmode
number of bytes to copy

\end{description}

\textbf{Description}

Returns an \code{ERR\_PTR()} on failure.
\index{get\_user\_pages\_fast (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.get_user_pages_fast}\pysiglinewithargsret{int \bfcode{get\_user\_pages\_fast}}{unsigned long\emph{ start}, int\emph{ nr\_pages}, int\emph{ write}, struct page **\emph{ pages}}{}
pin user pages in memory

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned long start}}] \leavevmode
starting user address

\item[{\code{int nr\_pages}}] \leavevmode
number of pages from start to pin

\item[{\code{int write}}] \leavevmode
whether pages will be written to

\item[{\code{struct page ** pages}}] \leavevmode
array that receives pointers to the pages pinned.
Should be at least nr\_pages long.

\end{description}

\textbf{Description}

Returns number of pages pinned. This may be fewer than the number
requested. If nr\_pages is 0 or negative, returns 0. If no pages
were pinned, returns -errno.

get\_user\_pages\_fast provides equivalent functionality to get\_user\_pages,
operating on current and current-\textgreater{}mm, with force=0 and vma=NULL. However
unlike get\_user\_pages, it must be called without mmap\_sem held.

get\_user\_pages\_fast may take mmap\_sem and page table locks, so no
assumptions can be made about lack of locking. get\_user\_pages\_fast is to be
implemented in a way that is advantageous (vs \code{get\_user\_pages()}) when the
user memory area is already faulted in and present in ptes. However if the
pages have to be faulted in, it may turn out to be slightly slower so
callers need to carefully consider what to use. On many architectures,
get\_user\_pages\_fast simply falls back to get\_user\_pages.
\index{kvmalloc\_node (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kvmalloc_node}\pysiglinewithargsret{void * \bfcode{kvmalloc\_node}}{size\_t\emph{ size}, gfp\_t\emph{ flags}, int\emph{ node}}{}
attempt to allocate physically contiguous memory, but upon failure, fall back to non-contiguous (vmalloc) allocation.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{size\_t size}}] \leavevmode
size of the request.

\item[{\code{gfp\_t flags}}] \leavevmode
gfp mask for the allocation - must be compatible (superset) with GFP\_KERNEL.

\item[{\code{int node}}] \leavevmode
numa node to allocate from

\end{description}

\textbf{Description}

Uses kmalloc to get the memory but if the allocation fails then falls back
to the vmalloc allocator. Use kvfree for freeing the memory.

Reclaim modifiers - \_\_GFP\_NORETRY and \_\_GFP\_NOFAIL are not supported.
\_\_GFP\_RETRY\_MAYFAIL is supported, and it should be used only if kmalloc is
preferable to the vmalloc fallback, due to visible performance drawbacks.

Any use of gfp flags outside of GFP\_KERNEL should be consulted with mm people.


\subsubsection{User Space Memory Access}
\label{core-api/kernel-api:user-space-memory-access}\index{access\_ok (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.access_ok}\pysiglinewithargsret{\bfcode{access\_ok}}{\emph{type}, \emph{addr}, \emph{size}}{}
Checks if a user space pointer is valid

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{type}}] \leavevmode
Type of access: \code{VERIFY\_READ} or \code{VERIFY\_WRITE}.  Note that
\code{VERIFY\_WRITE} is a superset of \code{VERIFY\_READ} - if it is safe
to write to a block, it is always safe to read from it.

\item[{\code{addr}}] \leavevmode
User space pointer to start of block to check

\item[{\code{size}}] \leavevmode
Size of block to check

\end{description}

\textbf{Context}

User context only. This function may sleep if pagefaults are
enabled.

\textbf{Description}

Checks if a pointer to a block of memory in user space is valid.

Returns true (nonzero) if the memory block may be valid, false (zero)
if it is definitely invalid.

Note that, depending on architecture, this function probably just
checks that the pointer is in the user space range - after calling
this function, memory access functions may still return -EFAULT.
\index{get\_user (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.get_user}\pysiglinewithargsret{\bfcode{get\_user}}{\emph{x}, \emph{ptr}}{}
Get a simple variable from user space.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{x}}] \leavevmode
Variable to store result.

\item[{\code{ptr}}] \leavevmode
Source address, in user space.

\end{description}

\textbf{Context}

User context only. This function may sleep if pagefaults are
enabled.

\textbf{Description}

This macro copies a single simple variable from user space to kernel
space.  It supports simple types like char and int, but not larger
data types like structures or arrays.

\textbf{ptr} must have pointer-to-simple-variable type, and the result of
dereferencing \textbf{ptr} must be assignable to \textbf{x} without a cast.

Returns zero on success, or -EFAULT on error.
On error, the variable \textbf{x} is set to zero.
\index{put\_user (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.put_user}\pysiglinewithargsret{\bfcode{put\_user}}{\emph{x}, \emph{ptr}}{}
Write a simple value into user space.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{x}}] \leavevmode
Value to copy to user space.

\item[{\code{ptr}}] \leavevmode
Destination address, in user space.

\end{description}

\textbf{Context}

User context only. This function may sleep if pagefaults are
enabled.

\textbf{Description}

This macro copies a single simple value from kernel space to user
space.  It supports simple types like char and int, but not larger
data types like structures or arrays.

\textbf{ptr} must have pointer-to-simple-variable type, and \textbf{x} must be assignable
to the result of dereferencing \textbf{ptr}.

Returns zero on success, or -EFAULT on error.
\index{\_\_get\_user (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__get_user}\pysiglinewithargsret{\bfcode{\_\_get\_user}}{\emph{x}, \emph{ptr}}{}
Get a simple variable from user space, with less checking.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{x}}] \leavevmode
Variable to store result.

\item[{\code{ptr}}] \leavevmode
Source address, in user space.

\end{description}

\textbf{Context}

User context only. This function may sleep if pagefaults are
enabled.

\textbf{Description}

This macro copies a single simple variable from user space to kernel
space.  It supports simple types like char and int, but not larger
data types like structures or arrays.

\textbf{ptr} must have pointer-to-simple-variable type, and the result of
dereferencing \textbf{ptr} must be assignable to \textbf{x} without a cast.

Caller must check the pointer with {\hyperref[core\string-api/kernel\string-api:c.access_ok]{\emph{\code{access\_ok()}}}} before calling this
function.

Returns zero on success, or -EFAULT on error.
On error, the variable \textbf{x} is set to zero.
\index{\_\_put\_user (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__put_user}\pysiglinewithargsret{\bfcode{\_\_put\_user}}{\emph{x}, \emph{ptr}}{}
Write a simple value into user space, with less checking.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{x}}] \leavevmode
Value to copy to user space.

\item[{\code{ptr}}] \leavevmode
Destination address, in user space.

\end{description}

\textbf{Context}

User context only. This function may sleep if pagefaults are
enabled.

\textbf{Description}

This macro copies a single simple value from kernel space to user
space.  It supports simple types like char and int, but not larger
data types like structures or arrays.

\textbf{ptr} must have pointer-to-simple-variable type, and \textbf{x} must be assignable
to the result of dereferencing \textbf{ptr}.

Caller must check the pointer with {\hyperref[core\string-api/kernel\string-api:c.access_ok]{\emph{\code{access\_ok()}}}} before calling this
function.

Returns zero on success, or -EFAULT on error.
\index{clear\_user (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.clear_user}\pysiglinewithargsret{unsigned long \bfcode{clear\_user}}{void \_\_user *\emph{ to}, unsigned long\emph{ n}}{}
Zero a block of memory in user space.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void \_\_user * to}}] \leavevmode
Destination address, in user space.

\item[{\code{unsigned long n}}] \leavevmode
Number of bytes to zero.

\end{description}

\textbf{Description}

Zero a block of memory in user space.

Returns number of bytes that could not be cleared.
On success, this will be zero.
\index{\_\_clear\_user (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__clear_user}\pysiglinewithargsret{unsigned long \bfcode{\_\_clear\_user}}{void \_\_user *\emph{ to}, unsigned long\emph{ n}}{}
Zero a block of memory in user space, with less checking.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void \_\_user * to}}] \leavevmode
Destination address, in user space.

\item[{\code{unsigned long n}}] \leavevmode
Number of bytes to zero.

\end{description}

\textbf{Description}

Zero a block of memory in user space.  Caller must check
the specified block with {\hyperref[core\string-api/kernel\string-api:c.access_ok]{\emph{\code{access\_ok()}}}} before calling this function.

Returns number of bytes that could not be cleared.
On success, this will be zero.


\subsubsection{More Memory Management Functions}
\label{core-api/kernel-api:more-memory-management-functions}\index{read\_cache\_pages (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.read_cache_pages}\pysiglinewithargsret{int \bfcode{read\_cache\_pages}}{struct address\_space *\emph{ mapping}, struct list\_head *\emph{ pages}, int (*filler) (void\emph{ *}, struct page\emph{ *}, void *\emph{ data}}{}
populate an address space with some pages \& start reads against them

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct address\_space * mapping}}] \leavevmode
the address\_space

\item[{\code{struct list\_head * pages}}] \leavevmode
The address of a list\_head which contains the target pages.  These
pages have their -\textgreater{}index populated and are otherwise uninitialised.

\item[{\code{int (*)(void *, struct page *) filler}}] \leavevmode
callback routine for filling a single page.

\item[{\code{void * data}}] \leavevmode
private data for the callback routine.

\end{description}

\textbf{Description}

Hides the details of the LRU cache etc from the filesystems.
\index{page\_cache\_sync\_readahead (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.page_cache_sync_readahead}\pysiglinewithargsret{void \bfcode{page\_cache\_sync\_readahead}}{struct address\_space *\emph{ mapping}, struct file\_ra\_state *\emph{ ra}, struct file *\emph{ filp}, pgoff\_t\emph{ offset}, unsigned long\emph{ req\_size}}{}
generic file readahead

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct address\_space * mapping}}] \leavevmode
address\_space which holds the pagecache and I/O vectors

\item[{\code{struct file\_ra\_state * ra}}] \leavevmode
file\_ra\_state which holds the readahead state

\item[{\code{struct file * filp}}] \leavevmode
passed on to -\textgreater{}:c:func:\emph{readpage()} and -\textgreater{}:c:func:\emph{readpages()}

\item[{\code{pgoff\_t offset}}] \leavevmode
start offset into \textbf{mapping}, in pagecache page-sized units

\item[{\code{unsigned long req\_size}}] \leavevmode
hint: total size of the read which the caller is performing in
pagecache pages

\end{description}

\textbf{Description}

{\hyperref[core\string-api/kernel\string-api:c.page_cache_sync_readahead]{\emph{\code{page\_cache\_sync\_readahead()}}}} should be called when a cache miss happened:
it will submit the read.  The readahead logic may decide to piggyback more
pages onto the read request if access patterns suggest it will improve
performance.
\index{page\_cache\_async\_readahead (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.page_cache_async_readahead}\pysiglinewithargsret{void \bfcode{page\_cache\_async\_readahead}}{struct address\_space *\emph{ mapping}, struct file\_ra\_state *\emph{ ra}, struct file *\emph{ filp}, struct page *\emph{ page}, pgoff\_t\emph{ offset}, unsigned long\emph{ req\_size}}{}
file readahead for marked pages

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct address\_space * mapping}}] \leavevmode
address\_space which holds the pagecache and I/O vectors

\item[{\code{struct file\_ra\_state * ra}}] \leavevmode
file\_ra\_state which holds the readahead state

\item[{\code{struct file * filp}}] \leavevmode
passed on to -\textgreater{}:c:func:\emph{readpage()} and -\textgreater{}:c:func:\emph{readpages()}

\item[{\code{struct page * page}}] \leavevmode
the page at \textbf{offset} which has the PG\_readahead flag set

\item[{\code{pgoff\_t offset}}] \leavevmode
start offset into \textbf{mapping}, in pagecache page-sized units

\item[{\code{unsigned long req\_size}}] \leavevmode
hint: total size of the read which the caller is performing in
pagecache pages

\end{description}

\textbf{Description}

{\hyperref[core\string-api/kernel\string-api:c.page_cache_async_readahead]{\emph{\code{page\_cache\_async\_readahead()}}}} should be called when a page is used which
has the PG\_readahead flag; this is a marker to suggest that the application
has used up enough of the readahead window that we should start pulling in
more pages.
\index{delete\_from\_page\_cache (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.delete_from_page_cache}\pysiglinewithargsret{void \bfcode{delete\_from\_page\_cache}}{struct page *\emph{ page}}{}
delete page from page cache

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct page * page}}] \leavevmode
the page which the kernel is trying to remove from page cache

\end{description}

\textbf{Description}

This must be called only on pages that have been verified to be in the page
cache and locked.  It will never put the page into the free list, the caller
has a reference on the page.
\index{filemap\_flush (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.filemap_flush}\pysiglinewithargsret{int \bfcode{filemap\_flush}}{struct address\_space *\emph{ mapping}}{}
mostly a non-blocking flush

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct address\_space * mapping}}] \leavevmode
target address\_space

\end{description}

\textbf{Description}

This is a mostly non-blocking flush.  Not suitable for data-integrity
purposes - I/O may not be started against all dirty pages.
\index{filemap\_range\_has\_page (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.filemap_range_has_page}\pysiglinewithargsret{bool \bfcode{filemap\_range\_has\_page}}{struct address\_space *\emph{ mapping}, loff\_t\emph{ start\_byte}, loff\_t\emph{ end\_byte}}{}
check if a page exists in range.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct address\_space * mapping}}] \leavevmode
address space within which to check

\item[{\code{loff\_t start\_byte}}] \leavevmode
offset in bytes where the range starts

\item[{\code{loff\_t end\_byte}}] \leavevmode
offset in bytes where the range ends (inclusive)

\end{description}

\textbf{Description}

Find at least one page in the range supplied, usually used to check if
direct writing in this range will trigger a writeback.
\index{filemap\_fdatawait\_range (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.filemap_fdatawait_range}\pysiglinewithargsret{int \bfcode{filemap\_fdatawait\_range}}{struct address\_space *\emph{ mapping}, loff\_t\emph{ start\_byte}, loff\_t\emph{ end\_byte}}{}
wait for writeback to complete

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct address\_space * mapping}}] \leavevmode
address space structure to wait for

\item[{\code{loff\_t start\_byte}}] \leavevmode
offset in bytes where the range starts

\item[{\code{loff\_t end\_byte}}] \leavevmode
offset in bytes where the range ends (inclusive)

\end{description}

\textbf{Description}

Walk the list of under-writeback pages of the given address space
in the given range and wait for all of them.  Check error status of
the address space and return it.

Since the error status of the address space is cleared by this function,
callers are responsible for checking the return value and handling and/or
reporting the error.
\index{file\_fdatawait\_range (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.file_fdatawait_range}\pysiglinewithargsret{int \bfcode{file\_fdatawait\_range}}{struct file *\emph{ file}, loff\_t\emph{ start\_byte}, loff\_t\emph{ end\_byte}}{}
wait for writeback to complete

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct file * file}}] \leavevmode
file pointing to address space structure to wait for

\item[{\code{loff\_t start\_byte}}] \leavevmode
offset in bytes where the range starts

\item[{\code{loff\_t end\_byte}}] \leavevmode
offset in bytes where the range ends (inclusive)

\end{description}

\textbf{Description}

Walk the list of under-writeback pages of the address space that file
refers to, in the given range and wait for all of them.  Check error
status of the address space vs. the file-\textgreater{}f\_wb\_err cursor and return it.

Since the error status of the file is advanced by this function,
callers are responsible for checking the return value and handling and/or
reporting the error.
\index{filemap\_fdatawait\_keep\_errors (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.filemap_fdatawait_keep_errors}\pysiglinewithargsret{int \bfcode{filemap\_fdatawait\_keep\_errors}}{struct address\_space *\emph{ mapping}}{}
wait for writeback without clearing errors

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct address\_space * mapping}}] \leavevmode
address space structure to wait for

\end{description}

\textbf{Description}

Walk the list of under-writeback pages of the given address space
and wait for all of them.  Unlike \code{filemap\_fdatawait()}, this function
does not clear error status of the address space.

Use this function if callers don't handle errors themselves.  Expected
call sites are system-wide / filesystem-wide data flushers: e.g. sync(2),
fsfreeze(8)
\index{filemap\_write\_and\_wait\_range (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.filemap_write_and_wait_range}\pysiglinewithargsret{int \bfcode{filemap\_write\_and\_wait\_range}}{struct address\_space *\emph{ mapping}, loff\_t\emph{ lstart}, loff\_t\emph{ lend}}{}
write out \& wait on a file range

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct address\_space * mapping}}] \leavevmode
the address\_space for the pages

\item[{\code{loff\_t lstart}}] \leavevmode
offset in bytes where the range starts

\item[{\code{loff\_t lend}}] \leavevmode
offset in bytes where the range ends (inclusive)

\end{description}

\textbf{Description}

Write out and wait upon file offsets lstart-\textgreater{}lend, inclusive.

Note that \textbf{lend} is inclusive (describes the last byte to be written) so
that this function can be used to write to the very end-of-file (end = -1).
\index{file\_check\_and\_advance\_wb\_err (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.file_check_and_advance_wb_err}\pysiglinewithargsret{int \bfcode{file\_check\_and\_advance\_wb\_err}}{struct file *\emph{ file}}{}
report wb error (if any) that was previously and advance wb\_err to current one

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct file * file}}] \leavevmode
struct file on which the error is being reported

\end{description}

\textbf{Description}

When userland calls fsync (or something like nfsd does the equivalent), we
want to report any writeback errors that occurred since the last fsync (or
since the file was opened if there haven't been any).

Grab the wb\_err from the mapping. If it matches what we have in the file,
then just quickly return 0. The file is all caught up.

If it doesn't match, then take the mapping value, set the ``seen'' flag in
it and try to swap it into place. If it works, or another task beat us
to it with the new value, then update the f\_wb\_err and return the error
portion. The error at this point must be reported via proper channels
(a'la fsync, or NFS COMMIT operation, etc.).

While we handle mapping-\textgreater{}wb\_err with atomic operations, the f\_wb\_err
value is protected by the f\_lock since we must ensure that it reflects
the latest value swapped in for this file descriptor.
\index{file\_write\_and\_wait\_range (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.file_write_and_wait_range}\pysiglinewithargsret{int \bfcode{file\_write\_and\_wait\_range}}{struct file *\emph{ file}, loff\_t\emph{ lstart}, loff\_t\emph{ lend}}{}
write out \& wait on a file range

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct file * file}}] \leavevmode
file pointing to address\_space with pages

\item[{\code{loff\_t lstart}}] \leavevmode
offset in bytes where the range starts

\item[{\code{loff\_t lend}}] \leavevmode
offset in bytes where the range ends (inclusive)

\end{description}

\textbf{Description}

Write out and wait upon file offsets lstart-\textgreater{}lend, inclusive.

Note that \textbf{lend} is inclusive (describes the last byte to be written) so
that this function can be used to write to the very end-of-file (end = -1).

After writing out and waiting on the data, we check and advance the
f\_wb\_err cursor to the latest value, and return any errors detected there.
\index{replace\_page\_cache\_page (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.replace_page_cache_page}\pysiglinewithargsret{int \bfcode{replace\_page\_cache\_page}}{struct page *\emph{ old}, struct page *\emph{ new}, gfp\_t\emph{ gfp\_mask}}{}
replace a pagecache page with a new one

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct page * old}}] \leavevmode
page to be replaced

\item[{\code{struct page * new}}] \leavevmode
page to replace with

\item[{\code{gfp\_t gfp\_mask}}] \leavevmode
allocation mode

\end{description}

\textbf{Description}

This function replaces a page in the pagecache with a new one.  On
success it acquires the pagecache reference for the new page and
drops it for the old page.  Both the old and new pages must be
locked.  This function does not add the new page to the LRU, the
caller must do that.

The remove + add is atomic.  The only way this function can fail is
memory allocation failure.
\index{add\_to\_page\_cache\_locked (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.add_to_page_cache_locked}\pysiglinewithargsret{int \bfcode{add\_to\_page\_cache\_locked}}{struct page *\emph{ page}, struct address\_space *\emph{ mapping}, pgoff\_t\emph{ offset}, gfp\_t\emph{ gfp\_mask}}{}
add a locked page to the pagecache

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct page * page}}] \leavevmode
page to add

\item[{\code{struct address\_space * mapping}}] \leavevmode
the page's address\_space

\item[{\code{pgoff\_t offset}}] \leavevmode
page index

\item[{\code{gfp\_t gfp\_mask}}] \leavevmode
page allocation mode

\end{description}

\textbf{Description}

This function is used to add a page to the pagecache. It must be locked.
This function does not add the page to the LRU.  The caller must do that.
\index{add\_page\_wait\_queue (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.add_page_wait_queue}\pysiglinewithargsret{void \bfcode{add\_page\_wait\_queue}}{struct page *\emph{ page}, wait\_queue\_entry\_t *\emph{ waiter}}{}
Add an arbitrary waiter to a page's wait queue

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct page * page}}] \leavevmode
Page defining the wait queue of interest

\item[{\code{wait\_queue\_entry\_t * waiter}}] \leavevmode
Waiter to add to the queue

\end{description}

\textbf{Description}

Add an arbitrary \textbf{waiter} to the wait queue for the nominated \textbf{page}.
\index{unlock\_page (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.unlock_page}\pysiglinewithargsret{void \bfcode{unlock\_page}}{struct page *\emph{ page}}{}
unlock a locked page

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct page * page}}] \leavevmode
the page

\end{description}

\textbf{Description}

Unlocks the page and wakes up sleepers in \code{\_\_\_wait\_on\_page\_locked()}.
Also wakes sleepers in \code{wait\_on\_page\_writeback()} because the wakeup
mechanism between PageLocked pages and PageWriteback pages is shared.
But that's OK - sleepers in \code{wait\_on\_page\_writeback()} just go back to sleep.

Note that this depends on PG\_waiters being the sign bit in the byte
that contains PG\_locked - thus the \code{BUILD\_BUG\_ON()}. That allows us to
clear the PG\_locked bit and test PG\_waiters at the same time fairly
portably (architectures that do LL/SC can test any bit, while x86 can
test the sign bit).
\index{end\_page\_writeback (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.end_page_writeback}\pysiglinewithargsret{void \bfcode{end\_page\_writeback}}{struct page *\emph{ page}}{}
end writeback against a page

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct page * page}}] \leavevmode
the page

\end{description}
\index{\_\_lock\_page (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__lock_page}\pysiglinewithargsret{void \bfcode{\_\_lock\_page}}{struct page *\emph{ \_\_page}}{}
get a lock on the page, assuming we need to sleep to get it

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct page * \_\_page}}] \leavevmode
the page to lock

\end{description}
\index{page\_cache\_next\_hole (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.page_cache_next_hole}\pysiglinewithargsret{pgoff\_t \bfcode{page\_cache\_next\_hole}}{struct address\_space *\emph{ mapping}, pgoff\_t\emph{ index}, unsigned long\emph{ max\_scan}}{}
find the next hole (not-present entry)

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct address\_space * mapping}}] \leavevmode
mapping

\item[{\code{pgoff\_t index}}] \leavevmode
index

\item[{\code{unsigned long max\_scan}}] \leavevmode
maximum range to search

\end{description}

\textbf{Description}

Search the set {[}index, min(index+max\_scan-1, MAX\_INDEX){]} for the
lowest indexed hole.

\textbf{Return}

the index of the hole if found, otherwise returns an index
outside of the set specified (in which case `return - index \textgreater{}=
max\_scan' will be true). In rare cases of index wrap-around, 0 will
be returned.

page\_cache\_next\_hole may be called under rcu\_read\_lock. However,
like radix\_tree\_gang\_lookup, this will not atomically search a
snapshot of the tree at a single point in time. For example, if a
hole is created at index 5, then subsequently a hole is created at
index 10, page\_cache\_next\_hole covering both indexes may return 10
if called under rcu\_read\_lock.
\index{page\_cache\_prev\_hole (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.page_cache_prev_hole}\pysiglinewithargsret{pgoff\_t \bfcode{page\_cache\_prev\_hole}}{struct address\_space *\emph{ mapping}, pgoff\_t\emph{ index}, unsigned long\emph{ max\_scan}}{}
find the prev hole (not-present entry)

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct address\_space * mapping}}] \leavevmode
mapping

\item[{\code{pgoff\_t index}}] \leavevmode
index

\item[{\code{unsigned long max\_scan}}] \leavevmode
maximum range to search

\end{description}

\textbf{Description}

Search backwards in the range {[}max(index-max\_scan+1, 0), index{]} for
the first hole.

\textbf{Return}

the index of the hole if found, otherwise returns an index
outside of the set specified (in which case `index - return \textgreater{}=
max\_scan' will be true). In rare cases of wrap-around, ULONG\_MAX
will be returned.

page\_cache\_prev\_hole may be called under rcu\_read\_lock. However,
like radix\_tree\_gang\_lookup, this will not atomically search a
snapshot of the tree at a single point in time. For example, if a
hole is created at index 10, then subsequently a hole is created at
index 5, page\_cache\_prev\_hole covering both indexes may return 5 if
called under rcu\_read\_lock.
\index{find\_get\_entry (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.find_get_entry}\pysiglinewithargsret{struct page * \bfcode{find\_get\_entry}}{struct address\_space *\emph{ mapping}, pgoff\_t\emph{ offset}}{}
find and get a page cache entry

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct address\_space * mapping}}] \leavevmode
the address\_space to search

\item[{\code{pgoff\_t offset}}] \leavevmode
the page cache index

\end{description}

\textbf{Description}

Looks up the page cache slot at \textbf{mapping} \& \textbf{offset}.  If there is a
page cache page, it is returned with an increased refcount.

If the slot holds a shadow entry of a previously evicted page, or a
swap entry from shmem/tmpfs, it is returned.

Otherwise, \code{NULL} is returned.
\index{find\_lock\_entry (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.find_lock_entry}\pysiglinewithargsret{struct page * \bfcode{find\_lock\_entry}}{struct address\_space *\emph{ mapping}, pgoff\_t\emph{ offset}}{}
locate, pin and lock a page cache entry

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct address\_space * mapping}}] \leavevmode
the address\_space to search

\item[{\code{pgoff\_t offset}}] \leavevmode
the page cache index

\end{description}

\textbf{Description}

Looks up the page cache slot at \textbf{mapping} \& \textbf{offset}.  If there is a
page cache page, it is returned locked and with an increased
refcount.

If the slot holds a shadow entry of a previously evicted page, or a
swap entry from shmem/tmpfs, it is returned.

Otherwise, \code{NULL} is returned.

{\hyperref[core\string-api/kernel\string-api:c.find_lock_entry]{\emph{\code{find\_lock\_entry()}}}} may sleep.
\index{pagecache\_get\_page (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.pagecache_get_page}\pysiglinewithargsret{struct page * \bfcode{pagecache\_get\_page}}{struct address\_space *\emph{ mapping}, pgoff\_t\emph{ offset}, int\emph{ fgp\_flags}, gfp\_t\emph{ gfp\_mask}}{}
find and get a page reference

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct address\_space * mapping}}] \leavevmode
the address\_space to search

\item[{\code{pgoff\_t offset}}] \leavevmode
the page index

\item[{\code{int fgp\_flags}}] \leavevmode
PCG flags

\item[{\code{gfp\_t gfp\_mask}}] \leavevmode
gfp mask to use for the page cache data page allocation

\end{description}

\textbf{Description}

Looks up the page cache slot at \textbf{mapping} \& \textbf{offset}.

PCG flags modify how the page is returned.

\textbf{fgp\_flags} can be:
\begin{itemize}
\item {} 
FGP\_ACCESSED: the page will be marked accessed

\item {} 
FGP\_LOCK: Page is return locked

\item {} 
FGP\_CREAT: If page is not present then a new page is allocated using
\textbf{gfp\_mask} and added to the page cache and the VM's LRU
list. The page is returned locked and with an increased
refcount. Otherwise, NULL is returned.

\end{itemize}

If FGP\_LOCK or FGP\_CREAT are specified then the function may sleep even
if the GFP flags specified for FGP\_CREAT are atomic.

If there is a page cache page, it is returned with an increased refcount.
\index{find\_get\_pages\_contig (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.find_get_pages_contig}\pysiglinewithargsret{unsigned \bfcode{find\_get\_pages\_contig}}{struct address\_space *\emph{ mapping}, pgoff\_t\emph{ index}, unsigned int\emph{ nr\_pages}, struct page **\emph{ pages}}{}
gang contiguous pagecache lookup

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct address\_space * mapping}}] \leavevmode
The address\_space to search

\item[{\code{pgoff\_t index}}] \leavevmode
The starting page index

\item[{\code{unsigned int nr\_pages}}] \leavevmode
The maximum number of pages

\item[{\code{struct page ** pages}}] \leavevmode
Where the resulting pages are placed

\end{description}

\textbf{Description}

{\hyperref[core\string-api/kernel\string-api:c.find_get_pages_contig]{\emph{\code{find\_get\_pages\_contig()}}}} works exactly like \code{find\_get\_pages()}, except
that the returned number of pages are guaranteed to be contiguous.

{\hyperref[core\string-api/kernel\string-api:c.find_get_pages_contig]{\emph{\code{find\_get\_pages\_contig()}}}} returns the number of pages which were found.
\index{find\_get\_pages\_range\_tag (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.find_get_pages_range_tag}\pysiglinewithargsret{unsigned \bfcode{find\_get\_pages\_range\_tag}}{struct address\_space *\emph{ mapping}, pgoff\_t *\emph{ index}, pgoff\_t\emph{ end}, int\emph{ tag}, unsigned int\emph{ nr\_pages}, struct page **\emph{ pages}}{}
find and return pages in given range matching \textbf{tag}

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct address\_space * mapping}}] \leavevmode
the address\_space to search

\item[{\code{pgoff\_t * index}}] \leavevmode
the starting page index

\item[{\code{pgoff\_t end}}] \leavevmode
The final page index (inclusive)

\item[{\code{int tag}}] \leavevmode
the tag index

\item[{\code{unsigned int nr\_pages}}] \leavevmode
the maximum number of pages

\item[{\code{struct page ** pages}}] \leavevmode
where the resulting pages are placed

\end{description}

\textbf{Description}

Like find\_get\_pages, except we only return pages which are tagged with
\textbf{tag}.   We update \textbf{index} to index the next page for the traversal.
\index{find\_get\_entries\_tag (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.find_get_entries_tag}\pysiglinewithargsret{unsigned \bfcode{find\_get\_entries\_tag}}{struct address\_space *\emph{ mapping}, pgoff\_t\emph{ start}, int\emph{ tag}, unsigned int\emph{ nr\_entries}, struct page **\emph{ entries}, pgoff\_t *\emph{ indices}}{}
find and return entries that match \textbf{tag}

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct address\_space * mapping}}] \leavevmode
the address\_space to search

\item[{\code{pgoff\_t start}}] \leavevmode
the starting page cache index

\item[{\code{int tag}}] \leavevmode
the tag index

\item[{\code{unsigned int nr\_entries}}] \leavevmode
the maximum number of entries

\item[{\code{struct page ** entries}}] \leavevmode
where the resulting entries are placed

\item[{\code{pgoff\_t * indices}}] \leavevmode
the cache indices corresponding to the entries in \textbf{entries}

\end{description}

\textbf{Description}

Like find\_get\_entries, except we only return entries which are tagged with
\textbf{tag}.
\index{generic\_file\_read\_iter (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.generic_file_read_iter}\pysiglinewithargsret{ssize\_t \bfcode{generic\_file\_read\_iter}}{struct kiocb *\emph{ iocb}, struct iov\_iter *\emph{ iter}}{}
generic filesystem read routine

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct kiocb * iocb}}] \leavevmode
kernel I/O control block

\item[{\code{struct iov\_iter * iter}}] \leavevmode
destination for the data read

\end{description}

\textbf{Description}

This is the ``\code{read\_iter()}'' routine for all filesystems
that can use the page cache directly.
\index{filemap\_fault (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.filemap_fault}\pysiglinewithargsret{int \bfcode{filemap\_fault}}{struct vm\_fault *\emph{ vmf}}{}
read in file data for page fault handling

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct vm\_fault * vmf}}] \leavevmode
struct vm\_fault containing details of the fault

\end{description}

\textbf{Description}

{\hyperref[core\string-api/kernel\string-api:c.filemap_fault]{\emph{\code{filemap\_fault()}}}} is invoked via the vma operations vector for a
mapped memory region to read in file data during a page fault.

The goto's are kind of ugly, but this streamlines the normal case of having
it in the page cache, and handles the special cases reasonably without
having a lot of duplicated code.

vma-\textgreater{}vm\_mm-\textgreater{}mmap\_sem must be held on entry.

If our return value has VM\_FAULT\_RETRY set, it's because
\code{lock\_page\_or\_retry()} returned 0.
The mmap\_sem has usually been released in this case.
See \code{\_\_lock\_page\_or\_retry()} for the exception.

If our return value does not have VM\_FAULT\_RETRY set, the mmap\_sem
has not been released.

We never return with VM\_FAULT\_RETRY and a bit from VM\_FAULT\_ERROR set.
\index{read\_cache\_page (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.read_cache_page}\pysiglinewithargsret{struct page * \bfcode{read\_cache\_page}}{struct address\_space *\emph{ mapping}, pgoff\_t\emph{ index}, int (*filler) (void\emph{ *}, struct page\emph{ *}, void *\emph{ data}}{}
read into page cache, fill it if needed

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct address\_space * mapping}}] \leavevmode
the page's address\_space

\item[{\code{pgoff\_t index}}] \leavevmode
the page index

\item[{\code{int (*)(void *, struct page *) filler}}] \leavevmode
function to perform the read

\item[{\code{void * data}}] \leavevmode
first arg to filler(data, page) function, often left as NULL

\end{description}

\textbf{Description}

Read into the page cache. If a page already exists, and \code{PageUptodate()} is
not set, try to fill the page and wait for it to become unlocked.

If the page does not get brought uptodate, return -EIO.
\index{read\_cache\_page\_gfp (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.read_cache_page_gfp}\pysiglinewithargsret{struct page * \bfcode{read\_cache\_page\_gfp}}{struct address\_space *\emph{ mapping}, pgoff\_t\emph{ index}, gfp\_t\emph{ gfp}}{}
read into page cache, using specified page allocation flags.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct address\_space * mapping}}] \leavevmode
the page's address\_space

\item[{\code{pgoff\_t index}}] \leavevmode
the page index

\item[{\code{gfp\_t gfp}}] \leavevmode
the page allocator flags to use if allocating

\end{description}

\textbf{Description}

This is the same as ``read\_mapping\_page(mapping, index, NULL)'', but with
any new page allocations done using the specified allocation flags.

If the page does not get brought uptodate, return -EIO.
\index{\_\_generic\_file\_write\_iter (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__generic_file_write_iter}\pysiglinewithargsret{ssize\_t \bfcode{\_\_generic\_file\_write\_iter}}{struct kiocb *\emph{ iocb}, struct iov\_iter *\emph{ from}}{}
write data to a file

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct kiocb * iocb}}] \leavevmode
IO state structure (file, offset, etc.)

\item[{\code{struct iov\_iter * from}}] \leavevmode
iov\_iter with data to write

\end{description}

\textbf{Description}

This function does all the work needed for actually writing data to a
file. It does all basic checks, removes SUID from the file, updates
modification times and calls proper subroutines depending on whether we
do direct IO or a standard buffered write.

It expects i\_mutex to be grabbed unless we work on a block device or similar
object which does not need locking at all.

This function does \emph{not} take care of syncing data in case of O\_SYNC write.
A caller has to handle it. This is mainly due to the fact that we want to
avoid syncing under i\_mutex.
\index{generic\_file\_write\_iter (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.generic_file_write_iter}\pysiglinewithargsret{ssize\_t \bfcode{generic\_file\_write\_iter}}{struct kiocb *\emph{ iocb}, struct iov\_iter *\emph{ from}}{}
write data to a file

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct kiocb * iocb}}] \leavevmode
IO state structure

\item[{\code{struct iov\_iter * from}}] \leavevmode
iov\_iter with data to write

\end{description}

\textbf{Description}

This is a wrapper around {\hyperref[core\string-api/kernel\string-api:c.__generic_file_write_iter]{\emph{\code{\_\_generic\_file\_write\_iter()}}}} to be used by most
filesystems. It takes care of syncing the file in case of O\_SYNC file
and acquires i\_mutex as needed.
\index{try\_to\_release\_page (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.try_to_release_page}\pysiglinewithargsret{int \bfcode{try\_to\_release\_page}}{struct page *\emph{ page}, gfp\_t\emph{ gfp\_mask}}{}
release old fs-specific metadata on a page

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct page * page}}] \leavevmode
the page which the kernel is trying to free

\item[{\code{gfp\_t gfp\_mask}}] \leavevmode
memory allocation flags (and I/O mode)

\end{description}

\textbf{Description}

The address\_space is to try to release any data against the page
(presumably at page-\textgreater{}private).  If the release was successful, return `1'.
Otherwise return zero.

This may also be called if PG\_fscache is set on a page, indicating that the
page is known to the local caching routines.

The \textbf{gfp\_mask} argument specifies whether I/O may be performed to release
this page (\_\_GFP\_IO), and whether the call may block (\_\_GFP\_RECLAIM \& \_\_GFP\_FS).
\index{zap\_vma\_ptes (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.zap_vma_ptes}\pysiglinewithargsret{int \bfcode{zap\_vma\_ptes}}{struct vm\_area\_struct *\emph{ vma}, unsigned long\emph{ address}, unsigned long\emph{ size}}{}
remove ptes mapping the vma

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct vm\_area\_struct * vma}}] \leavevmode
vm\_area\_struct holding ptes to be zapped

\item[{\code{unsigned long address}}] \leavevmode
starting address of pages to zap

\item[{\code{unsigned long size}}] \leavevmode
number of bytes to zap

\end{description}

\textbf{Description}

This function only unmaps ptes assigned to VM\_PFNMAP vmas.

The entire address range must be fully contained within the vma.

Returns 0 if successful.
\index{vm\_insert\_page (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.vm_insert_page}\pysiglinewithargsret{int \bfcode{vm\_insert\_page}}{struct vm\_area\_struct *\emph{ vma}, unsigned long\emph{ addr}, struct page *\emph{ page}}{}
insert single page into user vma

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct vm\_area\_struct * vma}}] \leavevmode
user vma to map to

\item[{\code{unsigned long addr}}] \leavevmode
target user address of this page

\item[{\code{struct page * page}}] \leavevmode
source kernel page

\end{description}

\textbf{Description}

This allows drivers to insert individual pages they've allocated
into a user vma.

The page has to be a nice clean \_individual\_ kernel allocation.
If you allocate a compound page, you need to have marked it as
such (\_\_GFP\_COMP), or manually just split the page up yourself
(see \code{split\_page()}).

NOTE! Traditionally this was done with ``{\hyperref[core\string-api/kernel\string-api:c.remap_pfn_range]{\emph{\code{remap\_pfn\_range()}}}}'' which
took an arbitrary page protection parameter. This doesn't allow
that. Your vma protection will have to be set up correctly, which
means that if you want a shared writable mapping, you'd better
ask for a shared writable mapping!

The page does not need to be reserved.

Usually this function is called from f\_op-\textgreater{}:c:func:\emph{mmap()} handler
under mm-\textgreater{}mmap\_sem write-lock, so it can change vma-\textgreater{}vm\_flags.
Caller must set VM\_MIXEDMAP on vma if it wants to call this
function from other places, for example from page-fault handler.
\index{vm\_insert\_pfn (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.vm_insert_pfn}\pysiglinewithargsret{int \bfcode{vm\_insert\_pfn}}{struct vm\_area\_struct *\emph{ vma}, unsigned long\emph{ addr}, unsigned long\emph{ pfn}}{}
insert single pfn into user vma

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct vm\_area\_struct * vma}}] \leavevmode
user vma to map to

\item[{\code{unsigned long addr}}] \leavevmode
target user address of this page

\item[{\code{unsigned long pfn}}] \leavevmode
source kernel pfn

\end{description}

\textbf{Description}

Similar to vm\_insert\_page, this allows drivers to insert individual pages
they've allocated into a user vma. Same comments apply.

This function should only be called from a vm\_ops-\textgreater{}fault handler, and
in that case the handler should return NULL.

vma cannot be a COW mapping.

As this is called only for pages that do not currently exist, we
do not need to flush old virtual caches or the TLB.
\index{vm\_insert\_pfn\_prot (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.vm_insert_pfn_prot}\pysiglinewithargsret{int \bfcode{vm\_insert\_pfn\_prot}}{struct vm\_area\_struct *\emph{ vma}, unsigned long\emph{ addr}, unsigned long\emph{ pfn}, pgprot\_t\emph{ pgprot}}{}
insert single pfn into user vma with specified pgprot

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct vm\_area\_struct * vma}}] \leavevmode
user vma to map to

\item[{\code{unsigned long addr}}] \leavevmode
target user address of this page

\item[{\code{unsigned long pfn}}] \leavevmode
source kernel pfn

\item[{\code{pgprot\_t pgprot}}] \leavevmode
pgprot flags for the inserted page

\end{description}

\textbf{Description}

This is exactly like vm\_insert\_pfn, except that it allows drivers to
to override pgprot on a per-page basis.

This only makes sense for IO mappings, and it makes no sense for
cow mappings.  In general, using multiple vmas is preferable;
vm\_insert\_pfn\_prot should only be used if using multiple VMAs is
impractical.
\index{remap\_pfn\_range (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.remap_pfn_range}\pysiglinewithargsret{int \bfcode{remap\_pfn\_range}}{struct vm\_area\_struct *\emph{ vma}, unsigned long\emph{ addr}, unsigned long\emph{ pfn}, unsigned long\emph{ size}, pgprot\_t\emph{ prot}}{}
remap kernel memory to userspace

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct vm\_area\_struct * vma}}] \leavevmode
user vma to map to

\item[{\code{unsigned long addr}}] \leavevmode
target user address to start at

\item[{\code{unsigned long pfn}}] \leavevmode
physical address of kernel memory

\item[{\code{unsigned long size}}] \leavevmode
size of map area

\item[{\code{pgprot\_t prot}}] \leavevmode
page protection flags for this mapping

\end{description}

\textbf{Note}

this is only safe if the mm semaphore is held when called.
\index{vm\_iomap\_memory (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.vm_iomap_memory}\pysiglinewithargsret{int \bfcode{vm\_iomap\_memory}}{struct vm\_area\_struct *\emph{ vma}, phys\_addr\_t\emph{ start}, unsigned long\emph{ len}}{}
remap memory to userspace

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct vm\_area\_struct * vma}}] \leavevmode
user vma to map to

\item[{\code{phys\_addr\_t start}}] \leavevmode
start of area

\item[{\code{unsigned long len}}] \leavevmode
size of area

\end{description}

\textbf{Description}

This is a simplified \code{io\_remap\_pfn\_range()} for common driver use. The
driver just needs to give us the physical memory range to be mapped,
we'll figure out the rest from the vma information.

NOTE! Some drivers might want to tweak vma-\textgreater{}vm\_page\_prot first to get
whatever write-combining details or similar.
\index{unmap\_mapping\_range (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.unmap_mapping_range}\pysiglinewithargsret{void \bfcode{unmap\_mapping\_range}}{struct address\_space *\emph{ mapping}, loff\_t const\emph{ holebegin}, loff\_t const\emph{ holelen}, int\emph{ even\_cows}}{}
unmap the portion of all mmaps in the specified address\_space corresponding to the specified byte range in the underlying file.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct address\_space * mapping}}] \leavevmode
the address space containing mmaps to be unmapped.

\item[{\code{loff\_t const holebegin}}] \leavevmode
byte in first page to unmap, relative to the start of
the underlying file.  This will be rounded down to a PAGE\_SIZE
boundary.  Note that this is different from {\hyperref[core\string-api/kernel\string-api:c.truncate_pagecache]{\emph{\code{truncate\_pagecache()}}}}, which
must keep the partial page.  In contrast, we must get rid of
partial pages.

\item[{\code{loff\_t const holelen}}] \leavevmode
size of prospective hole in bytes.  This will be rounded
up to a PAGE\_SIZE boundary.  A holelen of zero truncates to the
end of the file.

\item[{\code{int even\_cows}}] \leavevmode
1 when truncating a file, unmap even private COWed pages;
but 0 when invalidating pagecache, don't throw away private data.

\end{description}
\index{follow\_pfn (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.follow_pfn}\pysiglinewithargsret{int \bfcode{follow\_pfn}}{struct vm\_area\_struct *\emph{ vma}, unsigned long\emph{ address}, unsigned long *\emph{ pfn}}{}
look up PFN at a user virtual address

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct vm\_area\_struct * vma}}] \leavevmode
memory mapping

\item[{\code{unsigned long address}}] \leavevmode
user virtual address

\item[{\code{unsigned long * pfn}}] \leavevmode
location to store found PFN

\end{description}

\textbf{Description}

Only IO mappings and raw PFN mappings are allowed.

Returns zero and the pfn at \textbf{pfn} on success, -ve otherwise.
\index{vm\_unmap\_aliases (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.vm_unmap_aliases}\pysiglinewithargsret{void \bfcode{vm\_unmap\_aliases}}{void}{}
unmap outstanding lazy aliases in the vmap layer

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

The vmap/vmalloc layer lazily flushes kernel virtual mappings primarily
to amortize TLB flushing overheads. What this means is that any page you
have now, may, in a former life, have been mapped into kernel virtual
address by the vmap layer and so there might be some CPUs with TLB entries
still referencing that page (additional to the regular 1:1 kernel mapping).

vm\_unmap\_aliases flushes all such lazy mappings. After it returns, we can
be sure that none of the pages we have control over will have any aliases
from the vmap layer.
\index{vm\_unmap\_ram (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.vm_unmap_ram}\pysiglinewithargsret{void \bfcode{vm\_unmap\_ram}}{const void *\emph{ mem}, unsigned int\emph{ count}}{}
unmap linear kernel address space set up by vm\_map\_ram

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const void * mem}}] \leavevmode
the pointer returned by vm\_map\_ram

\item[{\code{unsigned int count}}] \leavevmode
the count passed to that vm\_map\_ram call (cannot unmap partial)

\end{description}
\index{vm\_map\_ram (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.vm_map_ram}\pysiglinewithargsret{void * \bfcode{vm\_map\_ram}}{struct page **\emph{ pages}, unsigned int\emph{ count}, int\emph{ node}, pgprot\_t\emph{ prot}}{}
map pages linearly into kernel virtual address (vmalloc space)

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct page ** pages}}] \leavevmode
an array of pointers to the pages to be mapped

\item[{\code{unsigned int count}}] \leavevmode
number of pages

\item[{\code{int node}}] \leavevmode
prefer to allocate data structures on this node

\item[{\code{pgprot\_t prot}}] \leavevmode
memory protection to use. PAGE\_KERNEL for regular RAM

\end{description}

\textbf{Description}

If you use this function for less than VMAP\_MAX\_ALLOC pages, it could be
faster than vmap so it's good.  But if you mix long-life and short-life
objects with {\hyperref[core\string-api/kernel\string-api:c.vm_map_ram]{\emph{\code{vm\_map\_ram()}}}}, it could consume lots of address space through
fragmentation (especially on a 32bit machine).  You could see failures in
the end.  Please use this function for short-lived objects.

\textbf{Return}

a pointer to the address that has been mapped, or \code{NULL} on failure
\index{unmap\_kernel\_range\_noflush (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.unmap_kernel_range_noflush}\pysiglinewithargsret{void \bfcode{unmap\_kernel\_range\_noflush}}{unsigned long\emph{ addr}, unsigned long\emph{ size}}{}
unmap kernel VM area

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned long addr}}] \leavevmode
start of the VM area to unmap

\item[{\code{unsigned long size}}] \leavevmode
size of the VM area to unmap

\end{description}

\textbf{Description}

Unmap PFN\_UP(\textbf{size}) pages at \textbf{addr}.  The VM area \textbf{addr} and \textbf{size}
specify should have been allocated using \code{get\_vm\_area()} and its
friends.

\textbf{NOTE}

This function does NOT do any cache flushing.  The caller is
responsible for calling \code{flush\_cache\_vunmap()} on to-be-mapped areas
before calling this function and \code{flush\_tlb\_kernel\_range()} after.
\index{unmap\_kernel\_range (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.unmap_kernel_range}\pysiglinewithargsret{void \bfcode{unmap\_kernel\_range}}{unsigned long\emph{ addr}, unsigned long\emph{ size}}{}
unmap kernel VM area and flush cache and TLB

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned long addr}}] \leavevmode
start of the VM area to unmap

\item[{\code{unsigned long size}}] \leavevmode
size of the VM area to unmap

\end{description}

\textbf{Description}

Similar to {\hyperref[core\string-api/kernel\string-api:c.unmap_kernel_range_noflush]{\emph{\code{unmap\_kernel\_range\_noflush()}}}} but flushes vcache before
the unmapping and tlb after.
\index{vfree (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.vfree}\pysiglinewithargsret{void \bfcode{vfree}}{const void *\emph{ addr}}{}
release memory allocated by {\hyperref[core\string-api/kernel\string-api:c.vmalloc]{\emph{\code{vmalloc()}}}}

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const void * addr}}] \leavevmode
memory base address

\end{description}

\textbf{Description}
\begin{quote}

Free the virtually continuous memory area starting at \textbf{addr}, as
obtained from {\hyperref[core\string-api/kernel\string-api:c.vmalloc]{\emph{\code{vmalloc()}}}}, {\hyperref[core\string-api/kernel\string-api:c.vmalloc_32]{\emph{\code{vmalloc\_32()}}}} or \code{\_\_vmalloc()}. If \textbf{addr} is
NULL, no operation is performed.

Must not be called in NMI context (strictly speaking, only if we don't
have CONFIG\_ARCH\_HAVE\_NMI\_SAFE\_CMPXCHG, but making the calling
conventions for {\hyperref[core\string-api/kernel\string-api:c.vfree]{\emph{\code{vfree()}}}} arch-depenedent would be a really bad idea)
\end{quote}

\textbf{NOTE}

assumes that the object at \textbf{addr} has a size \textgreater{}= sizeof(llist\_node)
\index{vunmap (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.vunmap}\pysiglinewithargsret{void \bfcode{vunmap}}{const void *\emph{ addr}}{}
release virtual mapping obtained by {\hyperref[core\string-api/kernel\string-api:c.vmap]{\emph{\code{vmap()}}}}

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const void * addr}}] \leavevmode
memory base address

\end{description}

\textbf{Description}
\begin{quote}

Free the virtually contiguous memory area starting at \textbf{addr},
which was created from the page array passed to {\hyperref[core\string-api/kernel\string-api:c.vmap]{\emph{\code{vmap()}}}}.

Must not be called in interrupt context.
\end{quote}
\index{vmap (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.vmap}\pysiglinewithargsret{void * \bfcode{vmap}}{struct page **\emph{ pages}, unsigned int\emph{ count}, unsigned long\emph{ flags}, pgprot\_t\emph{ prot}}{}
map an array of pages into virtually contiguous space

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct page ** pages}}] \leavevmode
array of page pointers

\item[{\code{unsigned int count}}] \leavevmode
number of pages to map

\item[{\code{unsigned long flags}}] \leavevmode
vm\_area-\textgreater{}flags

\item[{\code{pgprot\_t prot}}] \leavevmode
page protection for the mapping

\end{description}

\textbf{Description}
\begin{quote}

Maps \textbf{count} pages from \textbf{pages} into contiguous kernel virtual
space.
\end{quote}
\index{vmalloc (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.vmalloc}\pysiglinewithargsret{void * \bfcode{vmalloc}}{unsigned long\emph{ size}}{}
allocate virtually contiguous memory

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned long size}}] \leavevmode
allocation size
Allocate enough pages to cover \textbf{size} from the page level
allocator and map them into contiguous kernel virtual space.

\end{description}

\textbf{Description}
\begin{quote}

For tight control over page level allocator and protection flags
use \code{\_\_vmalloc()} instead.
\end{quote}
\index{vzalloc (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.vzalloc}\pysiglinewithargsret{void * \bfcode{vzalloc}}{unsigned long\emph{ size}}{}
allocate virtually contiguous memory with zero fill

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned long size}}] \leavevmode
allocation size
Allocate enough pages to cover \textbf{size} from the page level
allocator and map them into contiguous kernel virtual space.
The memory allocated is set to zero.

\end{description}

\textbf{Description}
\begin{quote}

For tight control over page level allocator and protection flags
use \code{\_\_vmalloc()} instead.
\end{quote}
\index{vmalloc\_user (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.vmalloc_user}\pysiglinewithargsret{void * \bfcode{vmalloc\_user}}{unsigned long\emph{ size}}{}
allocate zeroed virtually contiguous memory for userspace

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned long size}}] \leavevmode
allocation size

\end{description}

\textbf{Description}

The resulting memory area is zeroed so it can be mapped to userspace
without leaking data.
\index{vmalloc\_node (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.vmalloc_node}\pysiglinewithargsret{void * \bfcode{vmalloc\_node}}{unsigned long\emph{ size}, int\emph{ node}}{}
allocate memory on a specific node

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned long size}}] \leavevmode
allocation size

\item[{\code{int node}}] \leavevmode
numa node

\end{description}

\textbf{Description}
\begin{quote}

Allocate enough pages to cover \textbf{size} from the page level
allocator and map them into contiguous kernel virtual space.

For tight control over page level allocator and protection flags
use \code{\_\_vmalloc()} instead.
\end{quote}
\index{vzalloc\_node (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.vzalloc_node}\pysiglinewithargsret{void * \bfcode{vzalloc\_node}}{unsigned long\emph{ size}, int\emph{ node}}{}
allocate memory on a specific node with zero fill

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned long size}}] \leavevmode
allocation size

\item[{\code{int node}}] \leavevmode
numa node

\end{description}

\textbf{Description}

Allocate enough pages to cover \textbf{size} from the page level
allocator and map them into contiguous kernel virtual space.
The memory allocated is set to zero.

For tight control over page level allocator and protection flags
use \code{\_\_vmalloc\_node()} instead.
\index{vmalloc\_32 (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.vmalloc_32}\pysiglinewithargsret{void * \bfcode{vmalloc\_32}}{unsigned long\emph{ size}}{}
allocate virtually contiguous memory (32bit addressable)

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned long size}}] \leavevmode
allocation size

\end{description}

\textbf{Description}
\begin{quote}

Allocate enough 32bit PA addressable pages to cover \textbf{size} from the
page level allocator and map them into contiguous kernel virtual space.
\end{quote}
\index{vmalloc\_32\_user (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.vmalloc_32_user}\pysiglinewithargsret{void * \bfcode{vmalloc\_32\_user}}{unsigned long\emph{ size}}{}
allocate zeroed virtually contiguous 32bit memory

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned long size}}] \leavevmode
allocation size

\end{description}

\textbf{Description}

The resulting memory area is 32bit addressable and zeroed so it can be
mapped to userspace without leaking data.
\index{remap\_vmalloc\_range\_partial (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.remap_vmalloc_range_partial}\pysiglinewithargsret{int \bfcode{remap\_vmalloc\_range\_partial}}{struct vm\_area\_struct *\emph{ vma}, unsigned long\emph{ uaddr}, void *\emph{ kaddr}, unsigned long\emph{ size}}{}
map vmalloc pages to userspace

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct vm\_area\_struct * vma}}] \leavevmode
vma to cover

\item[{\code{unsigned long uaddr}}] \leavevmode
target user address to start at

\item[{\code{void * kaddr}}] \leavevmode
virtual address of vmalloc kernel memory

\item[{\code{unsigned long size}}] \leavevmode
size of map area

\end{description}

\textbf{Return}

0 for success, -Exxx on failure
\begin{quote}

This function checks that \textbf{kaddr} is a valid vmalloc'ed area,
and that it is big enough to cover the range starting at
\textbf{uaddr} in \textbf{vma}. Will return failure if that criteria isn't
met.

Similar to {\hyperref[core\string-api/kernel\string-api:c.remap_pfn_range]{\emph{\code{remap\_pfn\_range()}}}} (see mm/memory.c)
\end{quote}
\index{remap\_vmalloc\_range (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.remap_vmalloc_range}\pysiglinewithargsret{int \bfcode{remap\_vmalloc\_range}}{struct vm\_area\_struct *\emph{ vma}, void *\emph{ addr}, unsigned long\emph{ pgoff}}{}
map vmalloc pages to userspace

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct vm\_area\_struct * vma}}] \leavevmode
vma to cover (map full range of vma)

\item[{\code{void * addr}}] \leavevmode
vmalloc memory

\item[{\code{unsigned long pgoff}}] \leavevmode
number of pages into addr before first page to map

\end{description}

\textbf{Return}

0 for success, -Exxx on failure
\begin{quote}

This function checks that addr is a valid vmalloc'ed area, and
that it is big enough to cover the vma. Will return failure if
that criteria isn't met.

Similar to {\hyperref[core\string-api/kernel\string-api:c.remap_pfn_range]{\emph{\code{remap\_pfn\_range()}}}} (see mm/memory.c)
\end{quote}
\index{alloc\_vm\_area (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.alloc_vm_area}\pysiglinewithargsret{struct vm\_struct * \bfcode{alloc\_vm\_area}}{size\_t\emph{ size}, pte\_t **\emph{ ptes}}{}
allocate a range of kernel address space

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{size\_t size}}] \leavevmode
size of the area

\item[{\code{pte\_t ** ptes}}] \leavevmode
returns the PTEs for the address space

\end{description}

\textbf{Return}

NULL on failure, vm\_struct on success
\begin{quote}

This function reserves a range of kernel address space, and
allocates pagetables to map that range.  No actual mappings
are created.

If \textbf{ptes} is non-NULL, pointers to the PTEs (in init\_mm)
allocated for the VM area are returned.
\end{quote}
\index{\_\_get\_pfnblock\_flags\_mask (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__get_pfnblock_flags_mask}\pysiglinewithargsret{unsigned long \bfcode{\_\_get\_pfnblock\_flags\_mask}}{struct page *\emph{ page}, unsigned long\emph{ pfn}, unsigned long\emph{ end\_bitidx}, unsigned long\emph{ mask}}{}
Return the requested group of flags for the pageblock\_nr\_pages block of pages

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct page * page}}] \leavevmode
The page within the block of interest

\item[{\code{unsigned long pfn}}] \leavevmode
The target page frame number

\item[{\code{unsigned long end\_bitidx}}] \leavevmode
The last bit of interest to retrieve

\item[{\code{unsigned long mask}}] \leavevmode
mask of bits that the caller is interested in

\end{description}

\textbf{Return}

pageblock\_bits flags
\index{set\_pfnblock\_flags\_mask (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.set_pfnblock_flags_mask}\pysiglinewithargsret{void \bfcode{set\_pfnblock\_flags\_mask}}{struct page *\emph{ page}, unsigned long\emph{ flags}, unsigned long\emph{ pfn}, unsigned long\emph{ end\_bitidx}, unsigned long\emph{ mask}}{}
Set the requested group of flags for a pageblock\_nr\_pages block of pages

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct page * page}}] \leavevmode
The page within the block of interest

\item[{\code{unsigned long flags}}] \leavevmode
The flags to set

\item[{\code{unsigned long pfn}}] \leavevmode
The target page frame number

\item[{\code{unsigned long end\_bitidx}}] \leavevmode
The last bit of interest

\item[{\code{unsigned long mask}}] \leavevmode
mask of bits that the caller is interested in

\end{description}
\index{alloc\_pages\_exact\_nid (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.alloc_pages_exact_nid}\pysiglinewithargsret{void * \bfcode{alloc\_pages\_exact\_nid}}{int\emph{ nid}, size\_t\emph{ size}, gfp\_t\emph{ gfp\_mask}}{}
allocate an exact number of physically-contiguous pages on a node.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{int nid}}] \leavevmode
the preferred node ID where memory should be allocated

\item[{\code{size\_t size}}] \leavevmode
the number of bytes to allocate

\item[{\code{gfp\_t gfp\_mask}}] \leavevmode
GFP flags for the allocation

\end{description}

\textbf{Description}

Like \code{alloc\_pages\_exact()}, but try to allocate on node nid first before falling
back.
\index{nr\_free\_zone\_pages (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.nr_free_zone_pages}\pysiglinewithargsret{unsigned long \bfcode{nr\_free\_zone\_pages}}{int\emph{ offset}}{}
count number of pages beyond high watermark

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{int offset}}] \leavevmode
The zone index of the highest zone

\end{description}

\textbf{Description}

{\hyperref[core\string-api/kernel\string-api:c.nr_free_zone_pages]{\emph{\code{nr\_free\_zone\_pages()}}}} counts the number of counts pages which are beyond the
high watermark within all zones at or below a given zone index.  For each
zone, the number of pages is calculated as:
\begin{quote}

nr\_free\_zone\_pages = managed\_pages - high\_pages
\end{quote}
\index{nr\_free\_pagecache\_pages (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.nr_free_pagecache_pages}\pysiglinewithargsret{unsigned long \bfcode{nr\_free\_pagecache\_pages}}{void}{}
count number of pages beyond high watermark

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

{\hyperref[core\string-api/kernel\string-api:c.nr_free_pagecache_pages]{\emph{\code{nr\_free\_pagecache\_pages()}}}} counts the number of pages which are beyond the
high watermark within all zones.
\index{find\_next\_best\_node (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.find_next_best_node}\pysiglinewithargsret{int \bfcode{find\_next\_best\_node}}{int\emph{ node}, nodemask\_t *\emph{ used\_node\_mask}}{}
find the next node that should appear in a given node's fallback list

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{int node}}] \leavevmode
node whose fallback list we're appending

\item[{\code{nodemask\_t * used\_node\_mask}}] \leavevmode
nodemask\_t of already used nodes

\end{description}

\textbf{Description}

We use a number of factors to determine which is the next node that should
appear on a given node's fallback list.  The node should not have appeared
already in \textbf{node}`s fallback list, and it should be the next closest node
according to the distance array (which contains arbitrary distance values
from each node to each node in the system), and should also prefer nodes
with no CPUs, since presumably they'll have very little allocation pressure
on them otherwise.
It returns -1 if no node is found.
\index{free\_bootmem\_with\_active\_regions (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.free_bootmem_with_active_regions}\pysiglinewithargsret{void \bfcode{free\_bootmem\_with\_active\_regions}}{int\emph{ nid}, unsigned long\emph{ max\_low\_pfn}}{}
Call memblock\_free\_early\_nid for each active range

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{int nid}}] \leavevmode
The node to free memory on. If MAX\_NUMNODES, all nodes are freed.

\item[{\code{unsigned long max\_low\_pfn}}] \leavevmode
The highest PFN that will be passed to memblock\_free\_early\_nid

\end{description}

\textbf{Description}

If an architecture guarantees that all ranges registered contain no holes
and may be freed, this this function may be used instead of calling
\code{memblock\_free\_early\_nid()} manually.
\index{sparse\_memory\_present\_with\_active\_regions (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.sparse_memory_present_with_active_regions}\pysiglinewithargsret{void \bfcode{sparse\_memory\_present\_with\_active\_regions}}{int\emph{ nid}}{}
Call memory\_present for each active range

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{int nid}}] \leavevmode
The node to call memory\_present for. If MAX\_NUMNODES, all nodes will be used.

\end{description}

\textbf{Description}

If an architecture guarantees that all ranges registered contain no holes and may
be freed, this function may be used instead of calling \code{memory\_present()} manually.
\index{get\_pfn\_range\_for\_nid (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.get_pfn_range_for_nid}\pysiglinewithargsret{void \bfcode{get\_pfn\_range\_for\_nid}}{unsigned int\emph{ nid}, unsigned long *\emph{ start\_pfn}, unsigned long *\emph{ end\_pfn}}{}
Return the start and end page frames for a node

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int nid}}] \leavevmode
The nid to return the range for. If MAX\_NUMNODES, the min and max PFN are returned.

\item[{\code{unsigned long * start\_pfn}}] \leavevmode
Passed by reference. On return, it will have the node start\_pfn.

\item[{\code{unsigned long * end\_pfn}}] \leavevmode
Passed by reference. On return, it will have the node end\_pfn.

\end{description}

\textbf{Description}

It returns the start and end page frame of a node based on information
provided by \code{memblock\_set\_node()}. If called for a node
with no available memory, a warning is printed and the start and end
PFNs will be 0.
\index{absent\_pages\_in\_range (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.absent_pages_in_range}\pysiglinewithargsret{unsigned long \bfcode{absent\_pages\_in\_range}}{unsigned long\emph{ start\_pfn}, unsigned long\emph{ end\_pfn}}{}
Return number of page frames in holes within a range

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned long start\_pfn}}] \leavevmode
The start PFN to start searching for holes

\item[{\code{unsigned long end\_pfn}}] \leavevmode
The end PFN to stop searching for holes

\end{description}

\textbf{Description}

It returns the number of pages frames in memory holes within a range.
\index{node\_map\_pfn\_alignment (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.node_map_pfn_alignment}\pysiglinewithargsret{unsigned long \bfcode{node\_map\_pfn\_alignment}}{void}{}
determine the maximum internode alignment

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

This function should be called after node map is populated and sorted.
It calculates the maximum power of two alignment which can distinguish
all the nodes.

For example, if all nodes are 1GiB and aligned to 1GiB, the return value
would indicate 1GiB alignment with (1 \textless{}\textless{} (30 - PAGE\_SHIFT)).  If the
nodes are shifted by 256MiB, 256MiB.  Note that if only the last node is
shifted, 1GiB is enough and this function will indicate so.

This is used to test whether pfn -\textgreater{} nid mapping of the chosen memory
model has fine enough granularity to avoid incorrect mapping for the
populated node map.

Returns the determined alignment in pfn's.  0 if there is no alignment
requirement (single node).
\index{find\_min\_pfn\_with\_active\_regions (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.find_min_pfn_with_active_regions}\pysiglinewithargsret{unsigned long \bfcode{find\_min\_pfn\_with\_active\_regions}}{void}{}
Find the minimum PFN registered

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

It returns the minimum PFN based on information provided via
\code{memblock\_set\_node()}.
\index{free\_area\_init\_nodes (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.free_area_init_nodes}\pysiglinewithargsret{void \bfcode{free\_area\_init\_nodes}}{unsigned long *\emph{ max\_zone\_pfn}}{}
Initialise all pg\_data\_t and zone data

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned long * max\_zone\_pfn}}] \leavevmode
an array of max PFNs for each zone

\end{description}

\textbf{Description}

This will call \code{free\_area\_init\_node()} for each active node in the system.
Using the page ranges provided by \code{memblock\_set\_node()}, the size of each
zone in each node and their holes is calculated. If the maximum PFN
between two adjacent zones match, it is assumed that the zone is empty.
For example, if arch\_max\_dma\_pfn == arch\_max\_dma32\_pfn, it is assumed
that arch\_max\_dma32\_pfn has no pages. It is also assumed that a zone
starts where the previous one ended. For example, ZONE\_DMA32 starts
at arch\_max\_dma\_pfn.
\index{set\_dma\_reserve (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.set_dma_reserve}\pysiglinewithargsret{void \bfcode{set\_dma\_reserve}}{unsigned long\emph{ new\_dma\_reserve}}{}
set the specified number of pages reserved in the first zone

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned long new\_dma\_reserve}}] \leavevmode
The number of pages to mark reserved

\end{description}

\textbf{Description}

The per-cpu batchsize and zone watermarks are determined by managed\_pages.
In the DMA zone, a significant percentage may be consumed by kernel image
and other unfreeable allocations which can skew the watermarks badly. This
function may optionally be used to account for unfreeable pages in the
first zone (e.g., ZONE\_DMA). The effect will be lower watermarks and
smaller per-cpu batchsize.
\index{setup\_per\_zone\_wmarks (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.setup_per_zone_wmarks}\pysiglinewithargsret{void \bfcode{setup\_per\_zone\_wmarks}}{void}{}
called when min\_free\_kbytes changes or when memory is hot-\{added\textbar{}removed\}

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

Ensures that the watermark{[}min,low,high{]} values for each zone are set
correctly with respect to min\_free\_kbytes.
\index{alloc\_contig\_range (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.alloc_contig_range}\pysiglinewithargsret{int \bfcode{alloc\_contig\_range}}{unsigned long\emph{ start}, unsigned long\emph{ end}, unsigned\emph{ migratetype}, gfp\_t\emph{ gfp\_mask}}{}~\begin{itemize}
\item {} 
tries to allocate given range of pages

\end{itemize}

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned long start}}] \leavevmode
start PFN to allocate

\item[{\code{unsigned long end}}] \leavevmode
one-past-the-last PFN to allocate

\item[{\code{unsigned migratetype}}] \leavevmode
migratetype of the underlaying pageblocks (either
\#MIGRATE\_MOVABLE or \#MIGRATE\_CMA).  All pageblocks
in range must have the same migratetype and it must
be either of the two.

\item[{\code{gfp\_t gfp\_mask}}] \leavevmode
GFP mask to use during compaction

\end{description}

\textbf{Description}

The PFN range does not have to be pageblock or MAX\_ORDER\_NR\_PAGES
aligned, however it's the caller's responsibility to guarantee that
we are the only thread that changes migrate type of pageblocks the
pages fall in.

The PFN range must belong to a single zone.

Returns zero on success or negative error code.  On success all
pages which PFN is in {[}start, end) are allocated for the caller and
need to be freed with \code{free\_contig\_range()}.
\index{mempool\_destroy (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.mempool_destroy}\pysiglinewithargsret{void \bfcode{mempool\_destroy}}{mempool\_t *\emph{ pool}}{}
deallocate a memory pool

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{mempool\_t * pool}}] \leavevmode
pointer to the memory pool which was allocated via
{\hyperref[core\string-api/kernel\string-api:c.mempool_create]{\emph{\code{mempool\_create()}}}}.

\end{description}

\textbf{Description}

Free all reserved elements in \textbf{pool} and \textbf{pool} itself.  This function
only sleeps if the \code{free\_fn()} function sleeps.
\index{mempool\_create (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.mempool_create}\pysiglinewithargsret{mempool\_t * \bfcode{mempool\_create}}{int\emph{ min\_nr}, mempool\_alloc\_t *\emph{ alloc\_fn}, mempool\_free\_t *\emph{ free\_fn}, void *\emph{ pool\_data}}{}
create a memory pool

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{int min\_nr}}] \leavevmode
the minimum number of elements guaranteed to be
allocated for this pool.

\item[{\code{mempool\_alloc\_t * alloc\_fn}}] \leavevmode
user-defined element-allocation function.

\item[{\code{mempool\_free\_t * free\_fn}}] \leavevmode
user-defined element-freeing function.

\item[{\code{void * pool\_data}}] \leavevmode
optional private data available to the user-defined functions.

\end{description}

\textbf{Description}

this function creates and allocates a guaranteed size, preallocated
memory pool. The pool can be used from the {\hyperref[core\string-api/kernel\string-api:c.mempool_alloc]{\emph{\code{mempool\_alloc()}}}} and {\hyperref[core\string-api/kernel\string-api:c.mempool_free]{\emph{\code{mempool\_free()}}}}
functions. This function might sleep. Both the \code{alloc\_fn()} and the \code{free\_fn()}
functions might sleep - as long as the {\hyperref[core\string-api/kernel\string-api:c.mempool_alloc]{\emph{\code{mempool\_alloc()}}}} function is not called
from IRQ contexts.
\index{mempool\_resize (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.mempool_resize}\pysiglinewithargsret{int \bfcode{mempool\_resize}}{mempool\_t *\emph{ pool}, int\emph{ new\_min\_nr}}{}
resize an existing memory pool

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{mempool\_t * pool}}] \leavevmode
pointer to the memory pool which was allocated via
{\hyperref[core\string-api/kernel\string-api:c.mempool_create]{\emph{\code{mempool\_create()}}}}.

\item[{\code{int new\_min\_nr}}] \leavevmode
the new minimum number of elements guaranteed to be
allocated for this pool.

\end{description}

\textbf{Description}

This function shrinks/grows the pool. In the case of growing,
it cannot be guaranteed that the pool will be grown to the new
size immediately, but new {\hyperref[core\string-api/kernel\string-api:c.mempool_free]{\emph{\code{mempool\_free()}}}} calls will refill it.
This function may sleep.

Note, the caller must guarantee that no mempool\_destroy is called
while this function is running. {\hyperref[core\string-api/kernel\string-api:c.mempool_alloc]{\emph{\code{mempool\_alloc()}}}} \& {\hyperref[core\string-api/kernel\string-api:c.mempool_free]{\emph{\code{mempool\_free()}}}}
might be called (eg. from IRQ contexts) while this function executes.
\index{mempool\_alloc (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.mempool_alloc}\pysiglinewithargsret{void * \bfcode{mempool\_alloc}}{mempool\_t *\emph{ pool}, gfp\_t\emph{ gfp\_mask}}{}
allocate an element from a specific memory pool

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{mempool\_t * pool}}] \leavevmode
pointer to the memory pool which was allocated via
{\hyperref[core\string-api/kernel\string-api:c.mempool_create]{\emph{\code{mempool\_create()}}}}.

\item[{\code{gfp\_t gfp\_mask}}] \leavevmode
the usual allocation bitmask.

\end{description}

\textbf{Description}

this function only sleeps if the \code{alloc\_fn()} function sleeps or
returns NULL. Note that due to preallocation, this function
\emph{never} fails when called from process contexts. (it might
fail if called from an IRQ context.)

\textbf{Note}

using \_\_GFP\_ZERO is not supported.
\index{mempool\_free (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.mempool_free}\pysiglinewithargsret{void \bfcode{mempool\_free}}{void *\emph{ element}, mempool\_t *\emph{ pool}}{}
return an element to the pool.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void * element}}] \leavevmode
pool element pointer.

\item[{\code{mempool\_t * pool}}] \leavevmode
pointer to the memory pool which was allocated via
{\hyperref[core\string-api/kernel\string-api:c.mempool_create]{\emph{\code{mempool\_create()}}}}.

\end{description}

\textbf{Description}

this function only sleeps if the \code{free\_fn()} function sleeps.
\index{dma\_pool\_create (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.dma_pool_create}\pysiglinewithargsret{struct dma\_pool * \bfcode{dma\_pool\_create}}{const char *\emph{ name}, struct device *\emph{ dev}, size\_t\emph{ size}, size\_t\emph{ align}, size\_t\emph{ boundary}}{}
Creates a pool of consistent memory blocks, for dma.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * name}}] \leavevmode
name of pool, for diagnostics

\item[{\code{struct device * dev}}] \leavevmode
device that will be doing the DMA

\item[{\code{size\_t size}}] \leavevmode
size of the blocks in this pool.

\item[{\code{size\_t align}}] \leavevmode
alignment requirement for blocks; must be a power of two

\item[{\code{size\_t boundary}}] \leavevmode
returned blocks won't cross this power of two boundary

\end{description}

\textbf{Context}

!:c:func:\emph{in\_interrupt()}

\textbf{Description}

Returns a dma allocation pool with the requested characteristics, or
null if one can't be created.  Given one of these pools, {\hyperref[core\string-api/kernel\string-api:c.dma_pool_alloc]{\emph{\code{dma\_pool\_alloc()}}}}
may be used to allocate memory.  Such memory will all have ``consistent''
DMA mappings, accessible by the device and its driver without using
cache flushing primitives.  The actual size of blocks allocated may be
larger than requested because of alignment.

If \textbf{boundary} is nonzero, objects returned from {\hyperref[core\string-api/kernel\string-api:c.dma_pool_alloc]{\emph{\code{dma\_pool\_alloc()}}}} won't
cross that size boundary.  This is useful for devices which have
addressing restrictions on individual DMA transfers, such as not crossing
boundaries of 4KBytes.
\index{dma\_pool\_destroy (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.dma_pool_destroy}\pysiglinewithargsret{void \bfcode{dma\_pool\_destroy}}{struct dma\_pool *\emph{ pool}}{}
destroys a pool of dma memory blocks.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct dma\_pool * pool}}] \leavevmode
dma pool that will be destroyed

\end{description}

\textbf{Context}

!:c:func:\emph{in\_interrupt()}

\textbf{Description}

Caller guarantees that no more memory from the pool is in use,
and that nothing will try to use the pool after this call.
\index{dma\_pool\_alloc (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.dma_pool_alloc}\pysiglinewithargsret{void * \bfcode{dma\_pool\_alloc}}{struct dma\_pool *\emph{ pool}, gfp\_t\emph{ mem\_flags}, dma\_addr\_t *\emph{ handle}}{}
get a block of consistent memory

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct dma\_pool * pool}}] \leavevmode
dma pool that will produce the block

\item[{\code{gfp\_t mem\_flags}}] \leavevmode
GFP\_* bitmask

\item[{\code{dma\_addr\_t * handle}}] \leavevmode
pointer to dma address of block

\end{description}

\textbf{Description}

This returns the kernel virtual address of a currently unused block,
and reports its dma address through the handle.
If such a memory block can't be allocated, \code{NULL} is returned.
\index{dma\_pool\_free (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.dma_pool_free}\pysiglinewithargsret{void \bfcode{dma\_pool\_free}}{struct dma\_pool *\emph{ pool}, void *\emph{ vaddr}, dma\_addr\_t\emph{ dma}}{}
put block back into dma pool

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct dma\_pool * pool}}] \leavevmode
the dma pool holding the block

\item[{\code{void * vaddr}}] \leavevmode
virtual address of block

\item[{\code{dma\_addr\_t dma}}] \leavevmode
dma address of block

\end{description}

\textbf{Description}

Caller promises neither device nor driver will again touch this block
unless it is first re-allocated.
\index{dmam\_pool\_create (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.dmam_pool_create}\pysiglinewithargsret{struct dma\_pool * \bfcode{dmam\_pool\_create}}{const char *\emph{ name}, struct device *\emph{ dev}, size\_t\emph{ size}, size\_t\emph{ align}, size\_t\emph{ allocation}}{}
Managed {\hyperref[core\string-api/kernel\string-api:c.dma_pool_create]{\emph{\code{dma\_pool\_create()}}}}

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * name}}] \leavevmode
name of pool, for diagnostics

\item[{\code{struct device * dev}}] \leavevmode
device that will be doing the DMA

\item[{\code{size\_t size}}] \leavevmode
size of the blocks in this pool.

\item[{\code{size\_t align}}] \leavevmode
alignment requirement for blocks; must be a power of two

\item[{\code{size\_t allocation}}] \leavevmode
returned blocks won't cross this boundary (or zero)

\end{description}

\textbf{Description}

Managed {\hyperref[core\string-api/kernel\string-api:c.dma_pool_create]{\emph{\code{dma\_pool\_create()}}}}.  DMA pool created with this function is
automatically destroyed on driver detach.
\index{dmam\_pool\_destroy (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.dmam_pool_destroy}\pysiglinewithargsret{void \bfcode{dmam\_pool\_destroy}}{struct dma\_pool *\emph{ pool}}{}
Managed {\hyperref[core\string-api/kernel\string-api:c.dma_pool_destroy]{\emph{\code{dma\_pool\_destroy()}}}}

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct dma\_pool * pool}}] \leavevmode
dma pool that will be destroyed

\end{description}

\textbf{Description}

Managed {\hyperref[core\string-api/kernel\string-api:c.dma_pool_destroy]{\emph{\code{dma\_pool\_destroy()}}}}.
\index{balance\_dirty\_pages\_ratelimited (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.balance_dirty_pages_ratelimited}\pysiglinewithargsret{void \bfcode{balance\_dirty\_pages\_ratelimited}}{struct address\_space *\emph{ mapping}}{}
balance dirty memory state

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct address\_space * mapping}}] \leavevmode
address\_space which was dirtied

\end{description}

\textbf{Description}

Processes which are dirtying memory should call in here once for each page
which was newly dirtied.  The function will periodically check the system's
dirty state and will initiate writeback if needed.

On really big machines, get\_writeback\_state is expensive, so try to avoid
calling it too often (ratelimiting).  But once we're over the dirty memory
limit we decrease the ratelimiting by a lot, to prevent individual processes
from overshooting the limit by (ratelimit\_pages) each.
\index{tag\_pages\_for\_writeback (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.tag_pages_for_writeback}\pysiglinewithargsret{void \bfcode{tag\_pages\_for\_writeback}}{struct address\_space *\emph{ mapping}, pgoff\_t\emph{ start}, pgoff\_t\emph{ end}}{}
tag pages to be written by write\_cache\_pages

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct address\_space * mapping}}] \leavevmode
address space structure to write

\item[{\code{pgoff\_t start}}] \leavevmode
starting page index

\item[{\code{pgoff\_t end}}] \leavevmode
ending page index (inclusive)

\end{description}

\textbf{Description}

This function scans the page range from \textbf{start} to \textbf{end} (inclusive) and tags
all pages that have DIRTY tag set with a special TOWRITE tag. The idea is
that write\_cache\_pages (or whoever calls this function) will then use
TOWRITE tag to identify pages eligible for writeback.  This mechanism is
used to avoid livelocking of writeback by a process steadily creating new
dirty pages in the file (thus it is important for this function to be quick
so that it can tag pages faster than a dirtying process can create them).
\index{write\_cache\_pages (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.write_cache_pages}\pysiglinewithargsret{int \bfcode{write\_cache\_pages}}{struct address\_space *\emph{ mapping}, struct writeback\_control *\emph{ wbc}, writepage\_t\emph{ writepage}, void *\emph{ data}}{}
walk the list of dirty pages of the given address space and write all of them.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct address\_space * mapping}}] \leavevmode
address space structure to write

\item[{\code{struct writeback\_control * wbc}}] \leavevmode
subtract the number of written pages from \textbf{*wbc}-\textgreater{}nr\_to\_write

\item[{\code{writepage\_t writepage}}] \leavevmode
function called for each page

\item[{\code{void * data}}] \leavevmode
data passed to writepage function

\end{description}

\textbf{Description}

If a page is already under I/O, {\hyperref[core\string-api/kernel\string-api:c.write_cache_pages]{\emph{\code{write\_cache\_pages()}}}} skips it, even
if it's dirty.  This is desirable behaviour for memory-cleaning writeback,
but it is INCORRECT for data-integrity system calls such as \code{fsync()}.  \code{fsync()}
and \code{msync()} need to guarantee that all the data which was dirty at the time
the call was made get new I/O started against them.  If wbc-\textgreater{}sync\_mode is
WB\_SYNC\_ALL then we were called for data integrity and we must wait for
existing IO to complete.

To avoid livelocks (when other process dirties new pages), we first tag
pages which should be written back with TOWRITE tag and only then start
writing them. For data-integrity sync we have to be careful so that we do
not miss some pages (e.g., because some other process has cleared TOWRITE
tag we set). The rule we follow is that TOWRITE tag can be cleared only
by the process clearing the DIRTY tag (and submitting the page for IO).
\index{generic\_writepages (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.generic_writepages}\pysiglinewithargsret{int \bfcode{generic\_writepages}}{struct address\_space *\emph{ mapping}, struct writeback\_control *\emph{ wbc}}{}
walk the list of dirty pages of the given address space and \code{writepage()} all of them.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct address\_space * mapping}}] \leavevmode
address space structure to write

\item[{\code{struct writeback\_control * wbc}}] \leavevmode
subtract the number of written pages from \textbf{*wbc}-\textgreater{}nr\_to\_write

\end{description}

\textbf{Description}

This is a library function, which implements the \code{writepages()}
address\_space\_operation.
\index{write\_one\_page (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.write_one_page}\pysiglinewithargsret{int \bfcode{write\_one\_page}}{struct page *\emph{ page}}{}
write out a single page and wait on I/O

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct page * page}}] \leavevmode
the page to write

\end{description}

\textbf{Description}

The page must be locked by the caller and will be unlocked upon return.

Note that the mapping's AS\_EIO/AS\_ENOSPC flags will be cleared when this
function returns.
\index{wait\_for\_stable\_page (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.wait_for_stable_page}\pysiglinewithargsret{void \bfcode{wait\_for\_stable\_page}}{struct page *\emph{ page}}{}
wait for writeback to finish, if necessary.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct page * page}}] \leavevmode
The page to wait on.

\end{description}

\textbf{Description}

This function determines if the given page is related to a backing device
that requires page contents to be held stable during writeback.  If so, then
it will wait for any pending writeback to complete.
\index{truncate\_inode\_pages\_range (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.truncate_inode_pages_range}\pysiglinewithargsret{void \bfcode{truncate\_inode\_pages\_range}}{struct address\_space *\emph{ mapping}, loff\_t\emph{ lstart}, loff\_t\emph{ lend}}{}
truncate range of pages specified by start \& end byte offsets

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct address\_space * mapping}}] \leavevmode
mapping to truncate

\item[{\code{loff\_t lstart}}] \leavevmode
offset from which to truncate

\item[{\code{loff\_t lend}}] \leavevmode
offset to which to truncate (inclusive)

\end{description}

\textbf{Description}

Truncate the page cache, removing the pages that are between
specified offsets (and zeroing out partial pages
if lstart or lend + 1 is not page aligned).

Truncate takes two passes - the first pass is nonblocking.  It will not
block on page locks and it will not block on writeback.  The second pass
will wait.  This is to prevent as much IO as possible in the affected region.
The first pass will remove most pages, so the search cost of the second pass
is low.

We pass down the cache-hot hint to the page freeing code.  Even if the
mapping is large, it is probably the case that the final pages are the most
recently touched, and freeing happens in ascending file offset order.

Note that since -\textgreater{}:c:func:\emph{invalidatepage()} accepts range to invalidate
truncate\_inode\_pages\_range is able to handle cases where lend + 1 is not
page aligned properly.
\index{truncate\_inode\_pages (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.truncate_inode_pages}\pysiglinewithargsret{void \bfcode{truncate\_inode\_pages}}{struct address\_space *\emph{ mapping}, loff\_t\emph{ lstart}}{}
truncate \emph{all} the pages from an offset

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct address\_space * mapping}}] \leavevmode
mapping to truncate

\item[{\code{loff\_t lstart}}] \leavevmode
offset from which to truncate

\end{description}

\textbf{Description}

Called under (and serialised by) inode-\textgreater{}i\_mutex.

\textbf{Note}

When this function returns, there can be a page in the process of
deletion (inside \code{\_\_delete\_from\_page\_cache()}) in the specified range.  Thus
mapping-\textgreater{}nrpages can be non-zero when this function returns even after
truncation of the whole mapping.
\index{truncate\_inode\_pages\_final (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.truncate_inode_pages_final}\pysiglinewithargsret{void \bfcode{truncate\_inode\_pages\_final}}{struct address\_space *\emph{ mapping}}{}
truncate \emph{all} pages before inode dies

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct address\_space * mapping}}] \leavevmode
mapping to truncate

\end{description}

\textbf{Description}

Called under (and serialized by) inode-\textgreater{}i\_mutex.

Filesystems have to use this in the .evict\_inode path to inform the
VM that this is the final truncate and the inode is going away.
\index{invalidate\_mapping\_pages (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.invalidate_mapping_pages}\pysiglinewithargsret{unsigned long \bfcode{invalidate\_mapping\_pages}}{struct address\_space *\emph{ mapping}, pgoff\_t\emph{ start}, pgoff\_t\emph{ end}}{}
Invalidate all the unlocked pages of one inode

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct address\_space * mapping}}] \leavevmode
the address\_space which holds the pages to invalidate

\item[{\code{pgoff\_t start}}] \leavevmode
the offset `from' which to invalidate

\item[{\code{pgoff\_t end}}] \leavevmode
the offset `to' which to invalidate (inclusive)

\end{description}

\textbf{Description}

This function only removes the unlocked pages, if you want to
remove all the pages of one inode, you must call truncate\_inode\_pages.

{\hyperref[core\string-api/kernel\string-api:c.invalidate_mapping_pages]{\emph{\code{invalidate\_mapping\_pages()}}}} will not block on IO activity. It will not
invalidate pages which are dirty, locked, under writeback or mapped into
pagetables.
\index{invalidate\_inode\_pages2\_range (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.invalidate_inode_pages2_range}\pysiglinewithargsret{int \bfcode{invalidate\_inode\_pages2\_range}}{struct address\_space *\emph{ mapping}, pgoff\_t\emph{ start}, pgoff\_t\emph{ end}}{}
remove range of pages from an address\_space

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct address\_space * mapping}}] \leavevmode
the address\_space

\item[{\code{pgoff\_t start}}] \leavevmode
the page offset `from' which to invalidate

\item[{\code{pgoff\_t end}}] \leavevmode
the page offset `to' which to invalidate (inclusive)

\end{description}

\textbf{Description}

Any pages which are found to be mapped into pagetables are unmapped prior to
invalidation.

Returns -EBUSY if any pages could not be invalidated.
\index{invalidate\_inode\_pages2 (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.invalidate_inode_pages2}\pysiglinewithargsret{int \bfcode{invalidate\_inode\_pages2}}{struct address\_space *\emph{ mapping}}{}
remove all pages from an address\_space

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct address\_space * mapping}}] \leavevmode
the address\_space

\end{description}

\textbf{Description}

Any pages which are found to be mapped into pagetables are unmapped prior to
invalidation.

Returns -EBUSY if any pages could not be invalidated.
\index{truncate\_pagecache (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.truncate_pagecache}\pysiglinewithargsret{void \bfcode{truncate\_pagecache}}{struct inode *\emph{ inode}, loff\_t\emph{ newsize}}{}
unmap and remove pagecache that has been truncated

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct inode * inode}}] \leavevmode
inode

\item[{\code{loff\_t newsize}}] \leavevmode
new file size

\end{description}

\textbf{Description}

inode's new i\_size must already be written before truncate\_pagecache
is called.

This function should typically be called before the filesystem
releases resources associated with the freed range (eg. deallocates
blocks). This way, pagecache will always stay logically coherent
with on-disk format, and the filesystem would not have to deal with
situations such as writepage being called for a page that has already
had its underlying blocks deallocated.
\index{truncate\_setsize (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.truncate_setsize}\pysiglinewithargsret{void \bfcode{truncate\_setsize}}{struct inode *\emph{ inode}, loff\_t\emph{ newsize}}{}
update inode and pagecache for a new file size

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct inode * inode}}] \leavevmode
inode

\item[{\code{loff\_t newsize}}] \leavevmode
new file size

\end{description}

\textbf{Description}

truncate\_setsize updates i\_size and performs pagecache truncation (if
necessary) to \textbf{newsize}. It will be typically be called from the filesystem's
setattr function when ATTR\_SIZE is passed in.

Must be called with a lock serializing truncates and writes (generally
i\_mutex but e.g. xfs uses a different lock) and before all filesystem
specific block truncation has been performed.
\index{pagecache\_isize\_extended (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.pagecache_isize_extended}\pysiglinewithargsret{void \bfcode{pagecache\_isize\_extended}}{struct inode *\emph{ inode}, loff\_t\emph{ from}, loff\_t\emph{ to}}{}
update pagecache after extension of i\_size

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct inode * inode}}] \leavevmode
inode for which i\_size was extended

\item[{\code{loff\_t from}}] \leavevmode
original inode size

\item[{\code{loff\_t to}}] \leavevmode
new inode size

\end{description}

\textbf{Description}

Handle extension of inode size either caused by extending truncate or by
write starting after current i\_size. We mark the page straddling current
i\_size RO so that \code{page\_mkwrite()} is called on the nearest write access to
the page.  This way filesystem can be sure that \code{page\_mkwrite()} is called on
the page before user writes to the page via mmap after the i\_size has been
changed.

The function must be called after i\_size is updated so that page fault
coming after we unlock the page will already see the new i\_size.
The function must be called while we still hold i\_mutex - this not only
makes sure i\_size is stable but also that userspace cannot observe new
i\_size value before we are prepared to store mmap writes at new inode size.
\index{truncate\_pagecache\_range (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.truncate_pagecache_range}\pysiglinewithargsret{void \bfcode{truncate\_pagecache\_range}}{struct inode *\emph{ inode}, loff\_t\emph{ lstart}, loff\_t\emph{ lend}}{}
unmap and remove pagecache that is hole-punched

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct inode * inode}}] \leavevmode
inode

\item[{\code{loff\_t lstart}}] \leavevmode
offset of beginning of hole

\item[{\code{loff\_t lend}}] \leavevmode
offset of last byte of hole

\end{description}

\textbf{Description}

This function should typically be called before the filesystem
releases resources associated with the freed range (eg. deallocates
blocks). This way, pagecache will always stay logically coherent
with on-disk format, and the filesystem would not have to deal with
situations such as writepage being called for a page that has already
had its underlying blocks deallocated.


\subsection{Kernel IPC facilities}
\label{core-api/kernel-api:kernel-ipc-facilities}

\subsubsection{IPC utilities}
\label{core-api/kernel-api:ipc-utilities}\index{ipc\_init (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.ipc_init}\pysiglinewithargsret{int \bfcode{ipc\_init}}{void}{}
initialise ipc subsystem

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

The various sysv ipc resources (semaphores, messages and shared
memory) are initialised.

A callback routine is registered into the memory hotplug notifier
chain: since msgmni scales to lowmem this callback routine will be
called upon successful memory add / remove to recompute msmgni.
\index{ipc\_init\_ids (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.ipc_init_ids}\pysiglinewithargsret{int \bfcode{ipc\_init\_ids}}{struct ipc\_ids *\emph{ ids}}{}
initialise ipc identifiers

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct ipc\_ids * ids}}] \leavevmode
ipc identifier set

\end{description}

\textbf{Description}

Set up the sequence range to use for the ipc identifier range (limited
below IPCMNI) then initialise the keys hashtable and ids idr.
\index{ipc\_init\_proc\_interface (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.ipc_init_proc_interface}\pysiglinewithargsret{void \bfcode{ipc\_init\_proc\_interface}}{const char *\emph{ path}, const char *\emph{ header}, int\emph{ ids}, int (*show) (struct seq\_file\emph{ *}, void\emph{ *}}{}
create a proc interface for sysipc types using a seq\_file interface.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * path}}] \leavevmode
Path in procfs

\item[{\code{const char * header}}] \leavevmode
Banner to be printed at the beginning of the file.

\item[{\code{int ids}}] \leavevmode
ipc id table to iterate.

\item[{\code{int (*)(struct seq\_file *, void *) show}}] \leavevmode
show routine.

\end{description}
\index{ipc\_findkey (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.ipc_findkey}\pysiglinewithargsret{struct kern\_ipc\_perm * \bfcode{ipc\_findkey}}{struct ipc\_ids *\emph{ ids}, key\_t\emph{ key}}{}
find a key in an ipc identifier set

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct ipc\_ids * ids}}] \leavevmode
ipc identifier set

\item[{\code{key\_t key}}] \leavevmode
key to find

\end{description}

\textbf{Description}

Returns the locked pointer to the ipc structure if found or NULL
otherwise. If key is found ipc points to the owning ipc structure

Called with writer ipc\_ids.rwsem held.
\index{ipc\_addid (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.ipc_addid}\pysiglinewithargsret{int \bfcode{ipc\_addid}}{struct ipc\_ids *\emph{ ids}, struct kern\_ipc\_perm *\emph{ new}, int\emph{ limit}}{}
add an ipc identifier

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct ipc\_ids * ids}}] \leavevmode
ipc identifier set

\item[{\code{struct kern\_ipc\_perm * new}}] \leavevmode
new ipc permission set

\item[{\code{int limit}}] \leavevmode
limit for the number of used ids

\end{description}

\textbf{Description}

Add an entry `new' to the ipc ids idr. The permissions object is
initialised and the first free entry is set up and the id assigned
is returned. The `new' entry is returned in a locked state on success.
On failure the entry is not locked and a negative err-code is returned.

Called with writer ipc\_ids.rwsem held.
\index{ipcget\_new (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.ipcget_new}\pysiglinewithargsret{int \bfcode{ipcget\_new}}{struct ipc\_namespace *\emph{ ns}, struct ipc\_ids *\emph{ ids}, const struct ipc\_ops *\emph{ ops}, struct ipc\_params *\emph{ params}}{}
create a new ipc object

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct ipc\_namespace * ns}}] \leavevmode
ipc namespace

\item[{\code{struct ipc\_ids * ids}}] \leavevmode
ipc identifier set

\item[{\code{const struct ipc\_ops * ops}}] \leavevmode
the actual creation routine to call

\item[{\code{struct ipc\_params * params}}] \leavevmode
its parameters

\end{description}

\textbf{Description}

This routine is called by sys\_msgget, \code{sys\_semget()} and \code{sys\_shmget()}
when the key is IPC\_PRIVATE.
\index{ipc\_check\_perms (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.ipc_check_perms}\pysiglinewithargsret{int \bfcode{ipc\_check\_perms}}{struct ipc\_namespace *\emph{ ns}, struct kern\_ipc\_perm *\emph{ ipcp}, const struct ipc\_ops *\emph{ ops}, struct ipc\_params *\emph{ params}}{}
check security and permissions for an ipc object

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct ipc\_namespace * ns}}] \leavevmode
ipc namespace

\item[{\code{struct kern\_ipc\_perm * ipcp}}] \leavevmode
ipc permission set

\item[{\code{const struct ipc\_ops * ops}}] \leavevmode
the actual security routine to call

\item[{\code{struct ipc\_params * params}}] \leavevmode
its parameters

\end{description}

\textbf{Description}

This routine is called by \code{sys\_msgget()}, \code{sys\_semget()} and \code{sys\_shmget()}
when the key is not IPC\_PRIVATE and that key already exists in the
ds IDR.

On success, the ipc id is returned.

It is called with ipc\_ids.rwsem and ipcp-\textgreater{}lock held.
\index{ipcget\_public (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.ipcget_public}\pysiglinewithargsret{int \bfcode{ipcget\_public}}{struct ipc\_namespace *\emph{ ns}, struct ipc\_ids *\emph{ ids}, const struct ipc\_ops *\emph{ ops}, struct ipc\_params *\emph{ params}}{}
get an ipc object or create a new one

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct ipc\_namespace * ns}}] \leavevmode
ipc namespace

\item[{\code{struct ipc\_ids * ids}}] \leavevmode
ipc identifier set

\item[{\code{const struct ipc\_ops * ops}}] \leavevmode
the actual creation routine to call

\item[{\code{struct ipc\_params * params}}] \leavevmode
its parameters

\end{description}

\textbf{Description}

This routine is called by sys\_msgget, \code{sys\_semget()} and \code{sys\_shmget()}
when the key is not IPC\_PRIVATE.
It adds a new entry if the key is not found and does some permission
/ security checkings if the key is found.

On success, the ipc id is returned.
\index{ipc\_kht\_remove (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.ipc_kht_remove}\pysiglinewithargsret{void \bfcode{ipc\_kht\_remove}}{struct ipc\_ids *\emph{ ids}, struct kern\_ipc\_perm *\emph{ ipcp}}{}
remove an ipc from the key hashtable

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct ipc\_ids * ids}}] \leavevmode
ipc identifier set

\item[{\code{struct kern\_ipc\_perm * ipcp}}] \leavevmode
ipc perm structure containing the key to remove

\end{description}

\textbf{Description}

ipc\_ids.rwsem (as a writer) and the spinlock for this ID are held
before this function is called, and remain locked on the exit.
\index{ipc\_rmid (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.ipc_rmid}\pysiglinewithargsret{void \bfcode{ipc\_rmid}}{struct ipc\_ids *\emph{ ids}, struct kern\_ipc\_perm *\emph{ ipcp}}{}
remove an ipc identifier

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct ipc\_ids * ids}}] \leavevmode
ipc identifier set

\item[{\code{struct kern\_ipc\_perm * ipcp}}] \leavevmode
ipc perm structure containing the identifier to remove

\end{description}

\textbf{Description}

ipc\_ids.rwsem (as a writer) and the spinlock for this ID are held
before this function is called, and remain locked on the exit.
\index{ipc\_set\_key\_private (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.ipc_set_key_private}\pysiglinewithargsret{void \bfcode{ipc\_set\_key\_private}}{struct ipc\_ids *\emph{ ids}, struct kern\_ipc\_perm *\emph{ ipcp}}{}
switch the key of an existing ipc to IPC\_PRIVATE

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct ipc\_ids * ids}}] \leavevmode
ipc identifier set

\item[{\code{struct kern\_ipc\_perm * ipcp}}] \leavevmode
ipc perm structure containing the key to modify

\end{description}

\textbf{Description}

ipc\_ids.rwsem (as a writer) and the spinlock for this ID are held
before this function is called, and remain locked on the exit.
\index{ipcperms (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.ipcperms}\pysiglinewithargsret{int \bfcode{ipcperms}}{struct ipc\_namespace *\emph{ ns}, struct kern\_ipc\_perm *\emph{ ipcp}, short\emph{ flag}}{}
check ipc permissions

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct ipc\_namespace * ns}}] \leavevmode
ipc namespace

\item[{\code{struct kern\_ipc\_perm * ipcp}}] \leavevmode
ipc permission set

\item[{\code{short flag}}] \leavevmode
desired permission set

\end{description}

\textbf{Description}

Check user, group, other permissions for access
to ipc resources. return 0 if allowed

\textbf{flag} will most probably be 0 or \code{S\_...UGO} from \textless{}linux/stat.h\textgreater{}
\index{kernel\_to\_ipc64\_perm (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kernel_to_ipc64_perm}\pysiglinewithargsret{void \bfcode{kernel\_to\_ipc64\_perm}}{struct kern\_ipc\_perm *\emph{ in}, struct ipc64\_perm *\emph{ out}}{}
convert kernel ipc permissions to user

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct kern\_ipc\_perm * in}}] \leavevmode
kernel permissions

\item[{\code{struct ipc64\_perm * out}}] \leavevmode
new style ipc permissions

\end{description}

\textbf{Description}

Turn the kernel object \textbf{in} into a set of permissions descriptions
for returning to userspace (\textbf{out}).
\index{ipc64\_perm\_to\_ipc\_perm (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.ipc64_perm_to_ipc_perm}\pysiglinewithargsret{void \bfcode{ipc64\_perm\_to\_ipc\_perm}}{struct ipc64\_perm *\emph{ in}, struct ipc\_perm *\emph{ out}}{}
convert new ipc permissions to old

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct ipc64\_perm * in}}] \leavevmode
new style ipc permissions

\item[{\code{struct ipc\_perm * out}}] \leavevmode
old style ipc permissions

\end{description}

\textbf{Description}

Turn the new style permissions object \textbf{in} into a compatibility
object and store it into the \textbf{out} pointer.
\index{ipc\_obtain\_object\_idr (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.ipc_obtain_object_idr}\pysiglinewithargsret{struct kern\_ipc\_perm * \bfcode{ipc\_obtain\_object\_idr}}{struct ipc\_ids *\emph{ ids}, int\emph{ id}}{}
\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct ipc\_ids * ids}}] \leavevmode
ipc identifier set

\item[{\code{int id}}] \leavevmode
ipc id to look for

\end{description}

\textbf{Description}

Look for an id in the ipc ids idr and return associated ipc object.

Call inside the RCU critical section.
The ipc object is \emph{not} locked on exit.
\index{ipc\_lock (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.ipc_lock}\pysiglinewithargsret{struct kern\_ipc\_perm * \bfcode{ipc\_lock}}{struct ipc\_ids *\emph{ ids}, int\emph{ id}}{}
lock an ipc structure without rwsem held

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct ipc\_ids * ids}}] \leavevmode
ipc identifier set

\item[{\code{int id}}] \leavevmode
ipc id to look for

\end{description}

\textbf{Description}

Look for an id in the ipc ids idr and lock the associated ipc object.

The ipc object is locked on successful exit.
\index{ipc\_obtain\_object\_check (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.ipc_obtain_object_check}\pysiglinewithargsret{struct kern\_ipc\_perm * \bfcode{ipc\_obtain\_object\_check}}{struct ipc\_ids *\emph{ ids}, int\emph{ id}}{}
\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct ipc\_ids * ids}}] \leavevmode
ipc identifier set

\item[{\code{int id}}] \leavevmode
ipc id to look for

\end{description}

\textbf{Description}

Similar to {\hyperref[core\string-api/kernel\string-api:c.ipc_obtain_object_idr]{\emph{\code{ipc\_obtain\_object\_idr()}}}} but also checks
the ipc object reference counter.

Call inside the RCU critical section.
The ipc object is \emph{not} locked on exit.
\index{ipcget (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.ipcget}\pysiglinewithargsret{int \bfcode{ipcget}}{struct ipc\_namespace *\emph{ ns}, struct ipc\_ids *\emph{ ids}, const struct ipc\_ops *\emph{ ops}, struct ipc\_params *\emph{ params}}{}
Common sys\_*:c:func:\emph{get()} code

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct ipc\_namespace * ns}}] \leavevmode
namespace

\item[{\code{struct ipc\_ids * ids}}] \leavevmode
ipc identifier set

\item[{\code{const struct ipc\_ops * ops}}] \leavevmode
operations to be called on ipc object creation, permission checks
and further checks

\item[{\code{struct ipc\_params * params}}] \leavevmode
the parameters needed by the previous operations.

\end{description}

\textbf{Description}

Common routine called by \code{sys\_msgget()}, \code{sys\_semget()} and \code{sys\_shmget()}.
\index{ipc\_update\_perm (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.ipc_update_perm}\pysiglinewithargsret{int \bfcode{ipc\_update\_perm}}{struct ipc64\_perm *\emph{ in}, struct kern\_ipc\_perm *\emph{ out}}{}
update the permissions of an ipc object

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct ipc64\_perm * in}}] \leavevmode
the permission given as input.

\item[{\code{struct kern\_ipc\_perm * out}}] \leavevmode
the permission of the ipc to set.

\end{description}
\index{ipcctl\_pre\_down\_nolock (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.ipcctl_pre_down_nolock}\pysiglinewithargsret{struct kern\_ipc\_perm * \bfcode{ipcctl\_pre\_down\_nolock}}{struct ipc\_namespace *\emph{ ns}, struct ipc\_ids *\emph{ ids}, int\emph{ id}, int\emph{ cmd}, struct ipc64\_perm *\emph{ perm}, int\emph{ extra\_perm}}{}
retrieve an ipc and check permissions for some IPC\_XXX cmd

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct ipc\_namespace * ns}}] \leavevmode
ipc namespace

\item[{\code{struct ipc\_ids * ids}}] \leavevmode
the table of ids where to look for the ipc

\item[{\code{int id}}] \leavevmode
the id of the ipc to retrieve

\item[{\code{int cmd}}] \leavevmode
the cmd to check

\item[{\code{struct ipc64\_perm * perm}}] \leavevmode
the permission to set

\item[{\code{int extra\_perm}}] \leavevmode
one extra permission parameter used by msq

\end{description}

\textbf{Description}

This function does some common audit and permissions check for some IPC\_XXX
cmd and is called from semctl\_down, shmctl\_down and msgctl\_down.
It must be called without any lock held and:
\begin{itemize}
\item {} 
retrieves the ipc with the given id in the given table.

\item {} 
performs some audit and permission check, depending on the given cmd

\item {} 
returns a pointer to the ipc object or otherwise, the corresponding
error.

\end{itemize}

Call holding the both the rwsem and the rcu read lock.
\index{ipc\_parse\_version (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.ipc_parse_version}\pysiglinewithargsret{int \bfcode{ipc\_parse\_version}}{int *\emph{ cmd}}{}
ipc call version

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{int * cmd}}] \leavevmode
pointer to command

\end{description}

\textbf{Description}

Return IPC\_64 for new style IPC and IPC\_OLD for old style IPC.
The \textbf{cmd} value is turned from an encoding command and version into
just the command code.


\subsection{FIFO Buffer}
\label{core-api/kernel-api:fifo-buffer}

\subsubsection{kfifo interface}
\label{core-api/kernel-api:kfifo-interface}\index{DECLARE\_KFIFO\_PTR (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.DECLARE_KFIFO_PTR}\pysiglinewithargsret{\bfcode{DECLARE\_KFIFO\_PTR}}{\emph{fifo}, \emph{type}}{}
macro to declare a fifo pointer object

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{fifo}}] \leavevmode
name of the declared fifo

\item[{\code{type}}] \leavevmode
type of the fifo elements

\end{description}
\index{DECLARE\_KFIFO (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.DECLARE_KFIFO}\pysiglinewithargsret{\bfcode{DECLARE\_KFIFO}}{\emph{fifo}, \emph{type}, \emph{size}}{}
macro to declare a fifo object

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{fifo}}] \leavevmode
name of the declared fifo

\item[{\code{type}}] \leavevmode
type of the fifo elements

\item[{\code{size}}] \leavevmode
the number of elements in the fifo, this must be a power of 2

\end{description}
\index{INIT\_KFIFO (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.INIT_KFIFO}\pysiglinewithargsret{\bfcode{INIT\_KFIFO}}{\emph{fifo}}{}
Initialize a fifo declared by DECLARE\_KFIFO

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{fifo}}] \leavevmode
name of the declared fifo datatype

\end{description}
\index{DEFINE\_KFIFO (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.DEFINE_KFIFO}\pysiglinewithargsret{\bfcode{DEFINE\_KFIFO}}{\emph{fifo}, \emph{type}, \emph{size}}{}
macro to define and initialize a fifo

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{fifo}}] \leavevmode
name of the declared fifo datatype

\item[{\code{type}}] \leavevmode
type of the fifo elements

\item[{\code{size}}] \leavevmode
the number of elements in the fifo, this must be a power of 2

\end{description}

\textbf{Note}

the macro can be used for global and local fifo data type variables.
\index{kfifo\_initialized (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kfifo_initialized}\pysiglinewithargsret{\bfcode{kfifo\_initialized}}{\emph{fifo}}{}
Check if the fifo is initialized

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{fifo}}] \leavevmode
address of the fifo to check

\end{description}

\textbf{Description}

Return \code{true} if fifo is initialized, otherwise \code{false}.
Assumes the fifo was 0 before.
\index{kfifo\_esize (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kfifo_esize}\pysiglinewithargsret{\bfcode{kfifo\_esize}}{\emph{fifo}}{}
returns the size of the element managed by the fifo

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{fifo}}] \leavevmode
address of the fifo to be used

\end{description}
\index{kfifo\_recsize (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kfifo_recsize}\pysiglinewithargsret{\bfcode{kfifo\_recsize}}{\emph{fifo}}{}
returns the size of the record length field

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{fifo}}] \leavevmode
address of the fifo to be used

\end{description}
\index{kfifo\_size (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kfifo_size}\pysiglinewithargsret{\bfcode{kfifo\_size}}{\emph{fifo}}{}
returns the size of the fifo in elements

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{fifo}}] \leavevmode
address of the fifo to be used

\end{description}
\index{kfifo\_reset (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kfifo_reset}\pysiglinewithargsret{\bfcode{kfifo\_reset}}{\emph{fifo}}{}
removes the entire fifo content

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{fifo}}] \leavevmode
address of the fifo to be used

\end{description}

\textbf{Note}

usage of {\hyperref[core\string-api/kernel\string-api:c.kfifo_reset]{\emph{\code{kfifo\_reset()}}}} is dangerous. It should be only called when the
fifo is exclusived locked or when it is secured that no other thread is
accessing the fifo.
\index{kfifo\_reset\_out (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kfifo_reset_out}\pysiglinewithargsret{\bfcode{kfifo\_reset\_out}}{\emph{fifo}}{}
skip fifo content

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{fifo}}] \leavevmode
address of the fifo to be used

\end{description}

\textbf{Note}

The usage of {\hyperref[core\string-api/kernel\string-api:c.kfifo_reset_out]{\emph{\code{kfifo\_reset\_out()}}}} is safe until it will be only called
from the reader thread and there is only one concurrent reader. Otherwise
it is dangerous and must be handled in the same way as {\hyperref[core\string-api/kernel\string-api:c.kfifo_reset]{\emph{\code{kfifo\_reset()}}}}.
\index{kfifo\_len (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kfifo_len}\pysiglinewithargsret{\bfcode{kfifo\_len}}{\emph{fifo}}{}
returns the number of used elements in the fifo

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{fifo}}] \leavevmode
address of the fifo to be used

\end{description}
\index{kfifo\_is\_empty (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kfifo_is_empty}\pysiglinewithargsret{\bfcode{kfifo\_is\_empty}}{\emph{fifo}}{}
returns true if the fifo is empty

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{fifo}}] \leavevmode
address of the fifo to be used

\end{description}
\index{kfifo\_is\_full (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kfifo_is_full}\pysiglinewithargsret{\bfcode{kfifo\_is\_full}}{\emph{fifo}}{}
returns true if the fifo is full

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{fifo}}] \leavevmode
address of the fifo to be used

\end{description}
\index{kfifo\_avail (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kfifo_avail}\pysiglinewithargsret{\bfcode{kfifo\_avail}}{\emph{fifo}}{}
returns the number of unused elements in the fifo

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{fifo}}] \leavevmode
address of the fifo to be used

\end{description}
\index{kfifo\_skip (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kfifo_skip}\pysiglinewithargsret{\bfcode{kfifo\_skip}}{\emph{fifo}}{}
skip output data

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{fifo}}] \leavevmode
address of the fifo to be used

\end{description}
\index{kfifo\_peek\_len (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kfifo_peek_len}\pysiglinewithargsret{\bfcode{kfifo\_peek\_len}}{\emph{fifo}}{}
gets the size of the next fifo record

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{fifo}}] \leavevmode
address of the fifo to be used

\end{description}

\textbf{Description}

This function returns the size of the next fifo record in number of bytes.
\index{kfifo\_alloc (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kfifo_alloc}\pysiglinewithargsret{\bfcode{kfifo\_alloc}}{\emph{fifo}, \emph{size}, \emph{gfp\_mask}}{}
dynamically allocates a new fifo buffer

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{fifo}}] \leavevmode
pointer to the fifo

\item[{\code{size}}] \leavevmode
the number of elements in the fifo, this must be a power of 2

\item[{\code{gfp\_mask}}] \leavevmode
get\_free\_pages mask, passed to {\hyperref[core\string-api/kernel\string-api:c.kmalloc]{\emph{\code{kmalloc()}}}}

\end{description}

\textbf{Description}

This macro dynamically allocates a new fifo buffer.

The number of elements will be rounded-up to a power of 2.
The fifo will be release with {\hyperref[core\string-api/kernel\string-api:c.kfifo_free]{\emph{\code{kfifo\_free()}}}}.
Return 0 if no error, otherwise an error code.
\index{kfifo\_free (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kfifo_free}\pysiglinewithargsret{\bfcode{kfifo\_free}}{\emph{fifo}}{}
frees the fifo

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{fifo}}] \leavevmode
the fifo to be freed

\end{description}
\index{kfifo\_init (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kfifo_init}\pysiglinewithargsret{\bfcode{kfifo\_init}}{\emph{fifo}, \emph{buffer}, \emph{size}}{}
initialize a fifo using a preallocated buffer

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{fifo}}] \leavevmode
the fifo to assign the buffer

\item[{\code{buffer}}] \leavevmode
the preallocated buffer to be used

\item[{\code{size}}] \leavevmode
the size of the internal buffer, this have to be a power of 2

\end{description}

\textbf{Description}

This macro initializes a fifo using a preallocated buffer.

The number of elements will be rounded-up to a power of 2.
Return 0 if no error, otherwise an error code.
\index{kfifo\_put (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kfifo_put}\pysiglinewithargsret{\bfcode{kfifo\_put}}{\emph{fifo}, \emph{val}}{}
put data into the fifo

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{fifo}}] \leavevmode
address of the fifo to be used

\item[{\code{val}}] \leavevmode
the data to be added

\end{description}

\textbf{Description}

This macro copies the given value into the fifo.
It returns 0 if the fifo was full. Otherwise it returns the number
processed elements.

Note that with only one concurrent reader and one concurrent
writer, you don't need extra locking to use these macro.
\index{kfifo\_get (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kfifo_get}\pysiglinewithargsret{\bfcode{kfifo\_get}}{\emph{fifo}, \emph{val}}{}
get data from the fifo

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{fifo}}] \leavevmode
address of the fifo to be used

\item[{\code{val}}] \leavevmode
address where to store the data

\end{description}

\textbf{Description}

This macro reads the data from the fifo.
It returns 0 if the fifo was empty. Otherwise it returns the number
processed elements.

Note that with only one concurrent reader and one concurrent
writer, you don't need extra locking to use these macro.
\index{kfifo\_peek (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kfifo_peek}\pysiglinewithargsret{\bfcode{kfifo\_peek}}{\emph{fifo}, \emph{val}}{}
get data from the fifo without removing

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{fifo}}] \leavevmode
address of the fifo to be used

\item[{\code{val}}] \leavevmode
address where to store the data

\end{description}

\textbf{Description}

This reads the data from the fifo without removing it from the fifo.
It returns 0 if the fifo was empty. Otherwise it returns the number
processed elements.

Note that with only one concurrent reader and one concurrent
writer, you don't need extra locking to use these macro.
\index{kfifo\_in (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kfifo_in}\pysiglinewithargsret{\bfcode{kfifo\_in}}{\emph{fifo}, \emph{buf}, \emph{n}}{}
put data into the fifo

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{fifo}}] \leavevmode
address of the fifo to be used

\item[{\code{buf}}] \leavevmode
the data to be added

\item[{\code{n}}] \leavevmode
number of elements to be added

\end{description}

\textbf{Description}

This macro copies the given buffer into the fifo and returns the
number of copied elements.

Note that with only one concurrent reader and one concurrent
writer, you don't need extra locking to use these macro.
\index{kfifo\_in\_spinlocked (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kfifo_in_spinlocked}\pysiglinewithargsret{\bfcode{kfifo\_in\_spinlocked}}{\emph{fifo}, \emph{buf}, \emph{n}, \emph{lock}}{}
put data into the fifo using a spinlock for locking

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{fifo}}] \leavevmode
address of the fifo to be used

\item[{\code{buf}}] \leavevmode
the data to be added

\item[{\code{n}}] \leavevmode
number of elements to be added

\item[{\code{lock}}] \leavevmode
pointer to the spinlock to use for locking

\end{description}

\textbf{Description}

This macro copies the given values buffer into the fifo and returns the
number of copied elements.
\index{kfifo\_out (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kfifo_out}\pysiglinewithargsret{\bfcode{kfifo\_out}}{\emph{fifo}, \emph{buf}, \emph{n}}{}
get data from the fifo

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{fifo}}] \leavevmode
address of the fifo to be used

\item[{\code{buf}}] \leavevmode
pointer to the storage buffer

\item[{\code{n}}] \leavevmode
max. number of elements to get

\end{description}

\textbf{Description}

This macro get some data from the fifo and return the numbers of elements
copied.

Note that with only one concurrent reader and one concurrent
writer, you don't need extra locking to use these macro.
\index{kfifo\_out\_spinlocked (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kfifo_out_spinlocked}\pysiglinewithargsret{\bfcode{kfifo\_out\_spinlocked}}{\emph{fifo}, \emph{buf}, \emph{n}, \emph{lock}}{}
get data from the fifo using a spinlock for locking

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{fifo}}] \leavevmode
address of the fifo to be used

\item[{\code{buf}}] \leavevmode
pointer to the storage buffer

\item[{\code{n}}] \leavevmode
max. number of elements to get

\item[{\code{lock}}] \leavevmode
pointer to the spinlock to use for locking

\end{description}

\textbf{Description}

This macro get the data from the fifo and return the numbers of elements
copied.
\index{kfifo\_from\_user (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kfifo_from_user}\pysiglinewithargsret{\bfcode{kfifo\_from\_user}}{\emph{fifo}, \emph{from}, \emph{len}, \emph{copied}}{}
puts some data from user space into the fifo

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{fifo}}] \leavevmode
address of the fifo to be used

\item[{\code{from}}] \leavevmode
pointer to the data to be added

\item[{\code{len}}] \leavevmode
the length of the data to be added

\item[{\code{copied}}] \leavevmode
pointer to output variable to store the number of copied bytes

\end{description}

\textbf{Description}

This macro copies at most \textbf{len} bytes from the \textbf{from} into the
fifo, depending of the available space and returns -EFAULT/0.

Note that with only one concurrent reader and one concurrent
writer, you don't need extra locking to use these macro.
\index{kfifo\_to\_user (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kfifo_to_user}\pysiglinewithargsret{\bfcode{kfifo\_to\_user}}{\emph{fifo}, \emph{to}, \emph{len}, \emph{copied}}{}
copies data from the fifo into user space

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{fifo}}] \leavevmode
address of the fifo to be used

\item[{\code{to}}] \leavevmode
where the data must be copied

\item[{\code{len}}] \leavevmode
the size of the destination buffer

\item[{\code{copied}}] \leavevmode
pointer to output variable to store the number of copied bytes

\end{description}

\textbf{Description}

This macro copies at most \textbf{len} bytes from the fifo into the
\textbf{to} buffer and returns -EFAULT/0.

Note that with only one concurrent reader and one concurrent
writer, you don't need extra locking to use these macro.
\index{kfifo\_dma\_in\_prepare (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kfifo_dma_in_prepare}\pysiglinewithargsret{\bfcode{kfifo\_dma\_in\_prepare}}{\emph{fifo}, \emph{sgl}, \emph{nents}, \emph{len}}{}
setup a scatterlist for DMA input

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{fifo}}] \leavevmode
address of the fifo to be used

\item[{\code{sgl}}] \leavevmode
pointer to the scatterlist array

\item[{\code{nents}}] \leavevmode
number of entries in the scatterlist array

\item[{\code{len}}] \leavevmode
number of elements to transfer

\end{description}

\textbf{Description}

This macro fills a scatterlist for DMA input.
It returns the number entries in the scatterlist array.

Note that with only one concurrent reader and one concurrent
writer, you don't need extra locking to use these macros.
\index{kfifo\_dma\_in\_finish (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kfifo_dma_in_finish}\pysiglinewithargsret{\bfcode{kfifo\_dma\_in\_finish}}{\emph{fifo}, \emph{len}}{}
finish a DMA IN operation

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{fifo}}] \leavevmode
address of the fifo to be used

\item[{\code{len}}] \leavevmode
number of bytes to received

\end{description}

\textbf{Description}

This macro finish a DMA IN operation. The in counter will be updated by
the len parameter. No error checking will be done.

Note that with only one concurrent reader and one concurrent
writer, you don't need extra locking to use these macros.
\index{kfifo\_dma\_out\_prepare (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kfifo_dma_out_prepare}\pysiglinewithargsret{\bfcode{kfifo\_dma\_out\_prepare}}{\emph{fifo}, \emph{sgl}, \emph{nents}, \emph{len}}{}
setup a scatterlist for DMA output

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{fifo}}] \leavevmode
address of the fifo to be used

\item[{\code{sgl}}] \leavevmode
pointer to the scatterlist array

\item[{\code{nents}}] \leavevmode
number of entries in the scatterlist array

\item[{\code{len}}] \leavevmode
number of elements to transfer

\end{description}

\textbf{Description}

This macro fills a scatterlist for DMA output which at most \textbf{len} bytes
to transfer.
It returns the number entries in the scatterlist array.
A zero means there is no space available and the scatterlist is not filled.

Note that with only one concurrent reader and one concurrent
writer, you don't need extra locking to use these macros.
\index{kfifo\_dma\_out\_finish (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kfifo_dma_out_finish}\pysiglinewithargsret{\bfcode{kfifo\_dma\_out\_finish}}{\emph{fifo}, \emph{len}}{}
finish a DMA OUT operation

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{fifo}}] \leavevmode
address of the fifo to be used

\item[{\code{len}}] \leavevmode
number of bytes transferred

\end{description}

\textbf{Description}

This macro finish a DMA OUT operation. The out counter will be updated by
the len parameter. No error checking will be done.

Note that with only one concurrent reader and one concurrent
writer, you don't need extra locking to use these macros.
\index{kfifo\_out\_peek (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kfifo_out_peek}\pysiglinewithargsret{\bfcode{kfifo\_out\_peek}}{\emph{fifo}, \emph{buf}, \emph{n}}{}
gets some data from the fifo

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{fifo}}] \leavevmode
address of the fifo to be used

\item[{\code{buf}}] \leavevmode
pointer to the storage buffer

\item[{\code{n}}] \leavevmode
max. number of elements to get

\end{description}

\textbf{Description}

This macro get the data from the fifo and return the numbers of elements
copied. The data is not removed from the fifo.

Note that with only one concurrent reader and one concurrent
writer, you don't need extra locking to use these macro.


\subsection{relay interface support}
\label{core-api/kernel-api:relay-interface-support}
Relay interface support is designed to provide an efficient mechanism
for tools and facilities to relay large amounts of data from kernel
space to user space.


\subsubsection{relay interface}
\label{core-api/kernel-api:relay-interface}\index{relay\_buf\_full (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.relay_buf_full}\pysiglinewithargsret{int \bfcode{relay\_buf\_full}}{struct rchan\_buf *\emph{ buf}}{}
boolean, is the channel buffer full?

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct rchan\_buf * buf}}] \leavevmode
channel buffer

\end{description}

\textbf{Description}
\begin{quote}

Returns 1 if the buffer is full, 0 otherwise.
\end{quote}
\index{relay\_reset (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.relay_reset}\pysiglinewithargsret{void \bfcode{relay\_reset}}{struct rchan *\emph{ chan}}{}
reset the channel

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct rchan * chan}}] \leavevmode
the channel

\end{description}

\textbf{Description}
\begin{quote}

This has the effect of erasing all data from all channel buffers
and restarting the channel in its initial state.  The buffers
are not freed, so any mappings are still in effect.

NOTE. Care should be taken that the channel isn't actually
being used by anything when this call is made.
\end{quote}
\index{relay\_open (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.relay_open}\pysiglinewithargsret{struct rchan * \bfcode{relay\_open}}{const char *\emph{ base\_filename}, struct dentry *\emph{ parent}, size\_t\emph{ subbuf\_size}, size\_t\emph{ n\_subbufs}, struct rchan\_callbacks *\emph{ cb}, void *\emph{ private\_data}}{}
create a new relay channel

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * base\_filename}}] \leavevmode
base name of files to create, \code{NULL} for buffering only

\item[{\code{struct dentry * parent}}] \leavevmode
dentry of parent directory, \code{NULL} for root directory or buffer

\item[{\code{size\_t subbuf\_size}}] \leavevmode
size of sub-buffers

\item[{\code{size\_t n\_subbufs}}] \leavevmode
number of sub-buffers

\item[{\code{struct rchan\_callbacks * cb}}] \leavevmode
client callback functions

\item[{\code{void * private\_data}}] \leavevmode
user-defined data

\end{description}

\textbf{Description}
\begin{quote}

Returns channel pointer if successful, \code{NULL} otherwise.

Creates a channel buffer for each cpu using the sizes and
attributes specified.  The created channel buffer files
will be named base\_filename0...base\_filenameN-1.  File
permissions will be \code{S\_IRUSR}.

If opening a buffer (\textbf{parent} = NULL) that you later wish to register
in a filesystem, call {\hyperref[core\string-api/kernel\string-api:c.relay_late_setup_files]{\emph{\code{relay\_late\_setup\_files()}}}} once the \textbf{parent} dentry
is available.
\end{quote}
\index{relay\_late\_setup\_files (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.relay_late_setup_files}\pysiglinewithargsret{int \bfcode{relay\_late\_setup\_files}}{struct rchan *\emph{ chan}, const char *\emph{ base\_filename}, struct dentry *\emph{ parent}}{}
triggers file creation

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct rchan * chan}}] \leavevmode
channel to operate on

\item[{\code{const char * base\_filename}}] \leavevmode
base name of files to create

\item[{\code{struct dentry * parent}}] \leavevmode
dentry of parent directory, \code{NULL} for root directory

\end{description}

\textbf{Description}
\begin{quote}

Returns 0 if successful, non-zero otherwise.

Use to setup files for a previously buffer-only channel created
by {\hyperref[core\string-api/kernel\string-api:c.relay_open]{\emph{\code{relay\_open()}}}} with a NULL parent dentry.

For example, this is useful for perfomring early tracing in kernel,
before VFS is up and then exposing the early results once the dentry
is available.
\end{quote}
\index{relay\_switch\_subbuf (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.relay_switch_subbuf}\pysiglinewithargsret{size\_t \bfcode{relay\_switch\_subbuf}}{struct rchan\_buf *\emph{ buf}, size\_t\emph{ length}}{}
switch to a new sub-buffer

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct rchan\_buf * buf}}] \leavevmode
channel buffer

\item[{\code{size\_t length}}] \leavevmode
size of current event

\end{description}

\textbf{Description}
\begin{quote}

Returns either the length passed in or 0 if full.

Performs sub-buffer-switch tasks such as invoking callbacks,
updating padding counts, waking up readers, etc.
\end{quote}
\index{relay\_subbufs\_consumed (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.relay_subbufs_consumed}\pysiglinewithargsret{void \bfcode{relay\_subbufs\_consumed}}{struct rchan *\emph{ chan}, unsigned int\emph{ cpu}, size\_t\emph{ subbufs\_consumed}}{}
update the buffer's sub-buffers-consumed count

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct rchan * chan}}] \leavevmode
the channel

\item[{\code{unsigned int cpu}}] \leavevmode
the cpu associated with the channel buffer to update

\item[{\code{size\_t subbufs\_consumed}}] \leavevmode
number of sub-buffers to add to current buf's count

\end{description}

\textbf{Description}
\begin{quote}

Adds to the channel buffer's consumed sub-buffer count.
subbufs\_consumed should be the number of sub-buffers newly consumed,
not the total consumed.

NOTE. Kernel clients don't need to call this function if the channel
mode is `overwrite'.
\end{quote}
\index{relay\_close (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.relay_close}\pysiglinewithargsret{void \bfcode{relay\_close}}{struct rchan *\emph{ chan}}{}
close the channel

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct rchan * chan}}] \leavevmode
the channel

\end{description}

\textbf{Description}
\begin{quote}

Closes all channel buffers and frees the channel.
\end{quote}
\index{relay\_flush (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.relay_flush}\pysiglinewithargsret{void \bfcode{relay\_flush}}{struct rchan *\emph{ chan}}{}
close the channel

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct rchan * chan}}] \leavevmode
the channel

\end{description}

\textbf{Description}
\begin{quote}

Flushes all channel buffers, i.e. forces buffer switch.
\end{quote}
\index{relay\_mmap\_buf (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.relay_mmap_buf}\pysiglinewithargsret{int \bfcode{relay\_mmap\_buf}}{struct rchan\_buf *\emph{ buf}, struct vm\_area\_struct *\emph{ vma}}{}
mmap channel buffer to process address space

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct rchan\_buf * buf}}] \leavevmode
relay channel buffer

\item[{\code{struct vm\_area\_struct * vma}}] \leavevmode
vm\_area\_struct describing memory to be mapped

\end{description}

\textbf{Description}
\begin{quote}

Returns 0 if ok, negative on error

Caller should already have grabbed mmap\_sem.
\end{quote}
\index{relay\_alloc\_buf (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.relay_alloc_buf}\pysiglinewithargsret{void * \bfcode{relay\_alloc\_buf}}{struct rchan\_buf *\emph{ buf}, size\_t *\emph{ size}}{}
allocate a channel buffer

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct rchan\_buf * buf}}] \leavevmode
the buffer struct

\item[{\code{size\_t * size}}] \leavevmode
total size of the buffer

\end{description}

\textbf{Description}
\begin{quote}

Returns a pointer to the resulting buffer, \code{NULL} if unsuccessful. The
passed in size will get page aligned, if it isn't already.
\end{quote}
\index{relay\_create\_buf (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.relay_create_buf}\pysiglinewithargsret{struct rchan\_buf * \bfcode{relay\_create\_buf}}{struct rchan *\emph{ chan}}{}
allocate and initialize a channel buffer

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct rchan * chan}}] \leavevmode
the relay channel

\end{description}

\textbf{Description}
\begin{quote}

Returns channel buffer if successful, \code{NULL} otherwise.
\end{quote}
\index{relay\_destroy\_channel (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.relay_destroy_channel}\pysiglinewithargsret{void \bfcode{relay\_destroy\_channel}}{struct kref *\emph{ kref}}{}
free the channel struct

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct kref * kref}}] \leavevmode
target kernel reference that contains the relay channel

\end{description}

\textbf{Description}
\begin{quote}

Should only be called from \code{kref\_put()}.
\end{quote}
\index{relay\_destroy\_buf (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.relay_destroy_buf}\pysiglinewithargsret{void \bfcode{relay\_destroy\_buf}}{struct rchan\_buf *\emph{ buf}}{}
destroy an rchan\_buf struct and associated buffer

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct rchan\_buf * buf}}] \leavevmode
the buffer struct

\end{description}
\index{relay\_remove\_buf (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.relay_remove_buf}\pysiglinewithargsret{void \bfcode{relay\_remove\_buf}}{struct kref *\emph{ kref}}{}
remove a channel buffer

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct kref * kref}}] \leavevmode
target kernel reference that contains the relay buffer

\end{description}

\textbf{Description}
\begin{quote}

Removes the file from the filesystem, which also frees the
rchan\_buf\_struct and the channel buffer.  Should only be called from
\code{kref\_put()}.
\end{quote}
\index{relay\_buf\_empty (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.relay_buf_empty}\pysiglinewithargsret{int \bfcode{relay\_buf\_empty}}{struct rchan\_buf *\emph{ buf}}{}
boolean, is the channel buffer empty?

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct rchan\_buf * buf}}] \leavevmode
channel buffer

\end{description}

\textbf{Description}
\begin{quote}

Returns 1 if the buffer is empty, 0 otherwise.
\end{quote}
\index{wakeup\_readers (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.wakeup_readers}\pysiglinewithargsret{void \bfcode{wakeup\_readers}}{struct irq\_work *\emph{ work}}{}
wake up readers waiting on a channel

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_work * work}}] \leavevmode
contains the channel buffer

\end{description}

\textbf{Description}
\begin{quote}

This is the function used to defer reader waking
\end{quote}
\index{\_\_relay\_reset (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__relay_reset}\pysiglinewithargsret{void \bfcode{\_\_relay\_reset}}{struct rchan\_buf *\emph{ buf}, unsigned int\emph{ init}}{}
reset a channel buffer

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct rchan\_buf * buf}}] \leavevmode
the channel buffer

\item[{\code{unsigned int init}}] \leavevmode
1 if this is a first-time initialization

\end{description}

\textbf{Description}
\begin{quote}

See {\hyperref[core\string-api/kernel\string-api:c.relay_reset]{\emph{\code{relay\_reset()}}}} for description of effect.
\end{quote}
\index{relay\_close\_buf (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.relay_close_buf}\pysiglinewithargsret{void \bfcode{relay\_close\_buf}}{struct rchan\_buf *\emph{ buf}}{}
close a channel buffer

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct rchan\_buf * buf}}] \leavevmode
channel buffer

\end{description}

\textbf{Description}
\begin{quote}

Marks the buffer finalized and restores the default callbacks.
The channel buffer and channel buffer data structure are then freed
automatically when the last reference is given up.
\end{quote}
\index{relay\_file\_open (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.relay_file_open}\pysiglinewithargsret{int \bfcode{relay\_file\_open}}{struct inode *\emph{ inode}, struct file *\emph{ filp}}{}
open file op for relay files

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct inode * inode}}] \leavevmode
the inode

\item[{\code{struct file * filp}}] \leavevmode
the file

\end{description}

\textbf{Description}
\begin{quote}

Increments the channel buffer refcount.
\end{quote}
\index{relay\_file\_mmap (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.relay_file_mmap}\pysiglinewithargsret{int \bfcode{relay\_file\_mmap}}{struct file *\emph{ filp}, struct vm\_area\_struct *\emph{ vma}}{}
mmap file op for relay files

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct file * filp}}] \leavevmode
the file

\item[{\code{struct vm\_area\_struct * vma}}] \leavevmode
the vma describing what to map

\end{description}

\textbf{Description}
\begin{quote}

Calls upon {\hyperref[core\string-api/kernel\string-api:c.relay_mmap_buf]{\emph{\code{relay\_mmap\_buf()}}}} to map the file into user space.
\end{quote}
\index{relay\_file\_poll (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.relay_file_poll}\pysiglinewithargsret{\_\_poll\_t \bfcode{relay\_file\_poll}}{struct file *\emph{ filp}, poll\_table *\emph{ wait}}{}
poll file op for relay files

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct file * filp}}] \leavevmode
the file

\item[{\code{poll\_table * wait}}] \leavevmode
poll table

\end{description}

\textbf{Description}
\begin{quote}

Poll implemention.
\end{quote}
\index{relay\_file\_release (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.relay_file_release}\pysiglinewithargsret{int \bfcode{relay\_file\_release}}{struct inode *\emph{ inode}, struct file *\emph{ filp}}{}
release file op for relay files

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct inode * inode}}] \leavevmode
the inode

\item[{\code{struct file * filp}}] \leavevmode
the file

\end{description}

\textbf{Description}
\begin{quote}

Decrements the channel refcount, as the filesystem is
no longer using it.
\end{quote}
\index{relay\_file\_read\_subbuf\_avail (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.relay_file_read_subbuf_avail}\pysiglinewithargsret{size\_t \bfcode{relay\_file\_read\_subbuf\_avail}}{size\_t\emph{ read\_pos}, struct rchan\_buf *\emph{ buf}}{}
return bytes available in sub-buffer

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{size\_t read\_pos}}] \leavevmode
file read position

\item[{\code{struct rchan\_buf * buf}}] \leavevmode
relay channel buffer

\end{description}
\index{relay\_file\_read\_start\_pos (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.relay_file_read_start_pos}\pysiglinewithargsret{size\_t \bfcode{relay\_file\_read\_start\_pos}}{size\_t\emph{ read\_pos}, struct rchan\_buf *\emph{ buf}}{}
find the first available byte to read

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{size\_t read\_pos}}] \leavevmode
file read position

\item[{\code{struct rchan\_buf * buf}}] \leavevmode
relay channel buffer

\end{description}

\textbf{Description}
\begin{quote}

If the \textbf{read\_pos} is in the middle of padding, return the
position of the first actually available byte, otherwise
return the original value.
\end{quote}
\index{relay\_file\_read\_end\_pos (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.relay_file_read_end_pos}\pysiglinewithargsret{size\_t \bfcode{relay\_file\_read\_end\_pos}}{struct rchan\_buf *\emph{ buf}, size\_t\emph{ read\_pos}, size\_t\emph{ count}}{}
return the new read position

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct rchan\_buf * buf}}] \leavevmode
relay channel buffer

\item[{\code{size\_t read\_pos}}] \leavevmode
file read position

\item[{\code{size\_t count}}] \leavevmode
number of bytes to be read

\end{description}


\subsection{Module Support}
\label{core-api/kernel-api:module-support}

\subsubsection{Module Loading}
\label{core-api/kernel-api:module-loading}\index{\_\_request\_module (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__request_module}\pysiglinewithargsret{int \bfcode{\_\_request\_module}}{bool\emph{ wait}, const char *\emph{ fmt}, ...}{}
try to load a kernel module

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{bool wait}}] \leavevmode
wait (or not) for the operation to complete

\item[{\code{const char * fmt}}] \leavevmode
printf style format string for the name of the module

\item[{\code{...}}] \leavevmode
arguments as specified in the format string

\end{description}

\textbf{Description}

Load a module using the user mode module loader. The function returns
zero on success or a negative errno code or positive exit code from
``modprobe'' on failure. Note that a successful module load does not mean
the module did not then unload and exit on an error of its own. Callers
must check that the service they requested is now available not blindly
invoke it.

If module auto-loading support is disabled then this function
becomes a no-operation.


\subsubsection{Inter Module support}
\label{core-api/kernel-api:inter-module-support}
Refer to the file kernel/module.c for more information.


\subsection{Hardware Interfaces}
\label{core-api/kernel-api:hardware-interfaces}

\subsubsection{Interrupt Handling}
\label{core-api/kernel-api:interrupt-handling}\index{synchronize\_hardirq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.synchronize_hardirq}\pysiglinewithargsret{bool \bfcode{synchronize\_hardirq}}{unsigned int\emph{ irq}}{}
wait for pending hard IRQ handlers (on other CPUs)

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
interrupt number to wait for

\end{description}

\textbf{Description}
\begin{quote}

This function waits for any pending hard IRQ handlers for this
interrupt to complete before returning. If you use this
function while holding a resource the IRQ handler may need you
will deadlock. It does not take associated threaded handlers
into account.

Do not use this for shutdown scenarios where you must be sure
that all parts (hardirq and threaded handler) have completed.
\end{quote}

\textbf{Return}

false if a threaded handler is active.
\begin{quote}

This function may be called - with care - from IRQ context.
\end{quote}
\index{synchronize\_irq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.synchronize_irq}\pysiglinewithargsret{void \bfcode{synchronize\_irq}}{unsigned int\emph{ irq}}{}
wait for pending IRQ handlers (on other CPUs)

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
interrupt number to wait for

\end{description}

\textbf{Description}
\begin{quote}

This function waits for any pending IRQ handlers for this interrupt
to complete before returning. If you use this function while
holding a resource the IRQ handler may need you will deadlock.

This function may be called - with care - from IRQ context.
\end{quote}
\index{irq\_set\_affinity\_notifier (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.irq_set_affinity_notifier}\pysiglinewithargsret{int \bfcode{irq\_set\_affinity\_notifier}}{unsigned int\emph{ irq}, struct {\hyperref[core\string-api/genericirq:c.irq_affinity_notify]{\emph{irq\_affinity\_notify}}} *\emph{ notify}}{}
control notification of IRQ affinity changes

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt for which to enable/disable notification

\item[{\code{struct irq\_affinity\_notify * notify}}] \leavevmode
Context for notification, or \code{NULL} to disable
notification.  Function pointers must be initialised;
the other fields will be initialised by this function.

\end{description}

\textbf{Description}
\begin{quote}

Must be called in process context.  Notification may only be enabled
after the IRQ is allocated and must be disabled before the IRQ is
freed using {\hyperref[core\string-api/kernel\string-api:c.free_irq]{\emph{\code{free\_irq()}}}}.
\end{quote}
\index{irq\_set\_vcpu\_affinity (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.irq_set_vcpu_affinity}\pysiglinewithargsret{int \bfcode{irq\_set\_vcpu\_affinity}}{unsigned int\emph{ irq}, void *\emph{ vcpu\_info}}{}
Set vcpu affinity for the interrupt

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
interrupt number to set affinity

\item[{\code{void * vcpu\_info}}] \leavevmode
vCPU specific data or pointer to a percpu array of vCPU
specific data for percpu\_devid interrupts

\end{description}

\textbf{Description}
\begin{quote}

This function uses the vCPU specific data to set the vCPU
affinity for an irq. The vCPU specific data is passed from
outside, such as KVM. One example code path is as below:
KVM -\textgreater{} IOMMU -\textgreater{} {\hyperref[core\string-api/kernel\string-api:c.irq_set_vcpu_affinity]{\emph{\code{irq\_set\_vcpu\_affinity()}}}}.
\end{quote}
\index{disable\_irq\_nosync (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.disable_irq_nosync}\pysiglinewithargsret{void \bfcode{disable\_irq\_nosync}}{unsigned int\emph{ irq}}{}
disable an irq without waiting

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt to disable

\end{description}

\textbf{Description}
\begin{quote}

Disable the selected interrupt line.  Disables and Enables are
nested.
Unlike {\hyperref[core\string-api/kernel\string-api:c.disable_irq]{\emph{\code{disable\_irq()}}}}, this function does not ensure existing
instances of the IRQ handler have completed before returning.

This function may be called from IRQ context.
\end{quote}
\index{disable\_irq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.disable_irq}\pysiglinewithargsret{void \bfcode{disable\_irq}}{unsigned int\emph{ irq}}{}
disable an irq and wait for completion

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt to disable

\end{description}

\textbf{Description}
\begin{quote}

Disable the selected interrupt line.  Enables and Disables are
nested.
This function waits for any pending IRQ handlers for this interrupt
to complete before returning. If you use this function while
holding a resource the IRQ handler may need you will deadlock.

This function may be called - with care - from IRQ context.
\end{quote}
\index{disable\_hardirq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.disable_hardirq}\pysiglinewithargsret{bool \bfcode{disable\_hardirq}}{unsigned int\emph{ irq}}{}
disables an irq and waits for hardirq completion

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt to disable

\end{description}

\textbf{Description}
\begin{quote}

Disable the selected interrupt line.  Enables and Disables are
nested.
This function waits for any pending hard IRQ handlers for this
interrupt to complete before returning. If you use this function while
holding a resource the hard IRQ handler may need you will deadlock.

When used to optimistically disable an interrupt from atomic context
the return value must be checked.
\end{quote}

\textbf{Return}

false if a threaded handler is active.
\begin{quote}

This function may be called - with care - from IRQ context.
\end{quote}
\index{enable\_irq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.enable_irq}\pysiglinewithargsret{void \bfcode{enable\_irq}}{unsigned int\emph{ irq}}{}
enable handling of an irq

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt to enable

\end{description}

\textbf{Description}
\begin{quote}

Undoes the effect of one call to {\hyperref[core\string-api/kernel\string-api:c.disable_irq]{\emph{\code{disable\_irq()}}}}.  If this
matches the last disable, processing of interrupts on this
IRQ line is re-enabled.

This function may be called from IRQ context only when
desc-\textgreater{}irq\_data.chip-\textgreater{}bus\_lock and desc-\textgreater{}chip-\textgreater{}bus\_sync\_unlock are NULL !
\end{quote}
\index{irq\_set\_irq\_wake (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.irq_set_irq_wake}\pysiglinewithargsret{int \bfcode{irq\_set\_irq\_wake}}{unsigned int\emph{ irq}, unsigned int\emph{ on}}{}
control irq power management wakeup

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
interrupt to control

\item[{\code{unsigned int on}}] \leavevmode
enable/disable power management wakeup

\end{description}

\textbf{Description}
\begin{quote}

Enable/disable power management wakeup mode, which is
disabled by default.  Enables and disables must match,
just as they match for non-wakeup mode support.

Wakeup mode lets this IRQ wake the system from sleep
states like ``suspend to RAM''.
\end{quote}
\index{irq\_wake\_thread (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.irq_wake_thread}\pysiglinewithargsret{void \bfcode{irq\_wake\_thread}}{unsigned int\emph{ irq}, void *\emph{ dev\_id}}{}
wake the irq thread for the action identified by dev\_id

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt line

\item[{\code{void * dev\_id}}] \leavevmode
Device identity for which the thread should be woken

\end{description}
\index{setup\_irq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.setup_irq}\pysiglinewithargsret{int \bfcode{setup\_irq}}{unsigned int\emph{ irq}, struct {\hyperref[core\string-api/genericirq:c.irqaction]{\emph{irqaction}}} *\emph{ act}}{}
setup an interrupt

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt line to setup

\item[{\code{struct irqaction * act}}] \leavevmode
irqaction for the interrupt

\end{description}

\textbf{Description}

Used to statically setup interrupts in the early boot process.
\index{remove\_irq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.remove_irq}\pysiglinewithargsret{void \bfcode{remove\_irq}}{unsigned int\emph{ irq}, struct {\hyperref[core\string-api/genericirq:c.irqaction]{\emph{irqaction}}} *\emph{ act}}{}
free an interrupt

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt line to free

\item[{\code{struct irqaction * act}}] \leavevmode
irqaction for the interrupt

\end{description}

\textbf{Description}

Used to remove interrupts statically setup by the early boot process.
\index{free\_irq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.free_irq}\pysiglinewithargsret{const void * \bfcode{free\_irq}}{unsigned int\emph{ irq}, void *\emph{ dev\_id}}{}
free an interrupt allocated with request\_irq

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt line to free

\item[{\code{void * dev\_id}}] \leavevmode
Device identity to free

\end{description}

\textbf{Description}
\begin{quote}

Remove an interrupt handler. The handler is removed and if the
interrupt line is no longer in use by any driver it is disabled.
On a shared IRQ the caller must ensure the interrupt is disabled
on the card it drives before calling this function. The function
does not return until any executing interrupts for this IRQ
have completed.

This function must not be called from interrupt context.

Returns the devname argument passed to request\_irq.
\end{quote}
\index{request\_threaded\_irq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.request_threaded_irq}\pysiglinewithargsret{int \bfcode{request\_threaded\_irq}}{unsigned int\emph{ irq}, irq\_handler\_t\emph{ handler}, irq\_handler\_t\emph{ thread\_fn}, unsigned long\emph{ irqflags}, const char *\emph{ devname}, void *\emph{ dev\_id}}{}
allocate an interrupt line

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt line to allocate

\item[{\code{irq\_handler\_t handler}}] \leavevmode
Function to be called when the IRQ occurs.
Primary handler for threaded interrupts
If NULL and thread\_fn != NULL the default
primary handler is installed

\item[{\code{irq\_handler\_t thread\_fn}}] \leavevmode
Function called from the irq handler thread
If NULL, no irq thread is created

\item[{\code{unsigned long irqflags}}] \leavevmode
Interrupt type flags

\item[{\code{const char * devname}}] \leavevmode
An ascii name for the claiming device

\item[{\code{void * dev\_id}}] \leavevmode
A cookie passed back to the handler function

\end{description}

\textbf{Description}
\begin{quote}

This call allocates interrupt resources and enables the
interrupt line and IRQ handling. From the point this
call is made your handler function may be invoked. Since
your handler function must clear any interrupt the board
raises, you must take care both to initialise your hardware
and to set up the interrupt handler in the right order.

If you want to set up a threaded irq handler for your device
then you need to supply \textbf{handler} and \textbf{thread\_fn}. \textbf{handler} is
still called in hard interrupt context and has to check
whether the interrupt originates from the device. If yes it
needs to disable the interrupt on the device and return
IRQ\_WAKE\_THREAD which will wake up the handler thread and run
\textbf{thread\_fn}. This split handler design is necessary to support
shared interrupts.

Dev\_id must be globally unique. Normally the address of the
device data structure is used as the cookie. Since the handler
receives this value it makes sense to use it.

If your interrupt is shared you must pass a non NULL dev\_id
as this is required when freeing the interrupt.

Flags:

IRQF\_SHARED             Interrupt is shared
IRQF\_TRIGGER\_*          Specify active edge(s) or level
\end{quote}
\index{request\_any\_context\_irq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.request_any_context_irq}\pysiglinewithargsret{int \bfcode{request\_any\_context\_irq}}{unsigned int\emph{ irq}, irq\_handler\_t\emph{ handler}, unsigned long\emph{ flags}, const char *\emph{ name}, void *\emph{ dev\_id}}{}
allocate an interrupt line

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt line to allocate

\item[{\code{irq\_handler\_t handler}}] \leavevmode
Function to be called when the IRQ occurs.
Threaded handler for threaded interrupts.

\item[{\code{unsigned long flags}}] \leavevmode
Interrupt type flags

\item[{\code{const char * name}}] \leavevmode
An ascii name for the claiming device

\item[{\code{void * dev\_id}}] \leavevmode
A cookie passed back to the handler function

\end{description}

\textbf{Description}
\begin{quote}

This call allocates interrupt resources and enables the
interrupt line and IRQ handling. It selects either a
hardirq or threaded handling method depending on the
context.

On failure, it returns a negative value. On success,
it returns either IRQC\_IS\_HARDIRQ or IRQC\_IS\_NESTED.
\end{quote}
\index{irq\_percpu\_is\_enabled (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.irq_percpu_is_enabled}\pysiglinewithargsret{bool \bfcode{irq\_percpu\_is\_enabled}}{unsigned int\emph{ irq}}{}
Check whether the per cpu irq is enabled

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Linux irq number to check for

\end{description}

\textbf{Description}

Must be called from a non migratable context. Returns the enable
state of a per cpu interrupt on the current cpu.
\index{free\_percpu\_irq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.free_percpu_irq}\pysiglinewithargsret{void \bfcode{free\_percpu\_irq}}{unsigned int\emph{ irq}, void \_\_percpu *\emph{ dev\_id}}{}
free an interrupt allocated with request\_percpu\_irq

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt line to free

\item[{\code{void \_\_percpu * dev\_id}}] \leavevmode
Device identity to free

\end{description}

\textbf{Description}
\begin{quote}

Remove a percpu interrupt handler. The handler is removed, but
the interrupt line is not disabled. This must be done on each
CPU before calling this function. The function does not return
until any executing interrupts for this IRQ have completed.

This function must not be called from interrupt context.
\end{quote}
\index{\_\_request\_percpu\_irq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__request_percpu_irq}\pysiglinewithargsret{int \bfcode{\_\_request\_percpu\_irq}}{unsigned int\emph{ irq}, irq\_handler\_t\emph{ handler}, unsigned long\emph{ flags}, const char *\emph{ devname}, void \_\_percpu *\emph{ dev\_id}}{}
allocate a percpu interrupt line

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt line to allocate

\item[{\code{irq\_handler\_t handler}}] \leavevmode
Function to be called when the IRQ occurs.

\item[{\code{unsigned long flags}}] \leavevmode
Interrupt type flags (IRQF\_TIMER only)

\item[{\code{const char * devname}}] \leavevmode
An ascii name for the claiming device

\item[{\code{void \_\_percpu * dev\_id}}] \leavevmode
A percpu cookie passed back to the handler function

\end{description}

\textbf{Description}
\begin{quote}

This call allocates interrupt resources and enables the
interrupt on the local CPU. If the interrupt is supposed to be
enabled on other CPUs, it has to be done on each CPU using
\code{enable\_percpu\_irq()}.

Dev\_id must be globally unique. It is a per-cpu variable, and
the handler gets called with the interrupted CPU's instance of
that variable.
\end{quote}
\index{irq\_get\_irqchip\_state (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.irq_get_irqchip_state}\pysiglinewithargsret{int \bfcode{irq\_get\_irqchip\_state}}{unsigned int\emph{ irq}, enum irqchip\_irq\_state\emph{ which}, bool *\emph{ state}}{}
returns the irqchip state of a interrupt.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt line that is forwarded to a VM

\item[{\code{enum irqchip\_irq\_state which}}] \leavevmode
One of IRQCHIP\_STATE\_* the caller wants to know about

\item[{\code{bool * state}}] \leavevmode
a pointer to a boolean where the state is to be storeed

\end{description}

\textbf{Description}
\begin{quote}

This call snapshots the internal irqchip state of an
interrupt, returning into \textbf{state} the bit corresponding to
stage \textbf{which}

This function should be called with preemption disabled if the
interrupt controller has per-cpu registers.
\end{quote}
\index{irq\_set\_irqchip\_state (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.irq_set_irqchip_state}\pysiglinewithargsret{int \bfcode{irq\_set\_irqchip\_state}}{unsigned int\emph{ irq}, enum irqchip\_irq\_state\emph{ which}, bool\emph{ val}}{}
set the state of a forwarded interrupt.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt line that is forwarded to a VM

\item[{\code{enum irqchip\_irq\_state which}}] \leavevmode
State to be restored (one of IRQCHIP\_STATE\_*)

\item[{\code{bool val}}] \leavevmode
Value corresponding to \textbf{which}

\end{description}

\textbf{Description}
\begin{quote}

This call sets the internal irqchip state of an interrupt,
depending on the value of \textbf{which}.

This function should be called with preemption disabled if the
interrupt controller has per-cpu registers.
\end{quote}


\subsubsection{DMA Channels}
\label{core-api/kernel-api:dma-channels}\index{request\_dma (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.request_dma}\pysiglinewithargsret{int \bfcode{request\_dma}}{unsigned int\emph{ dmanr}, const char *\emph{ device\_id}}{}
request and reserve a system DMA channel

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int dmanr}}] \leavevmode
DMA channel number

\item[{\code{const char * device\_id}}] \leavevmode
reserving device ID string, used in /proc/dma

\end{description}
\index{free\_dma (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.free_dma}\pysiglinewithargsret{void \bfcode{free\_dma}}{unsigned int\emph{ dmanr}}{}
free a reserved system DMA channel

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int dmanr}}] \leavevmode
DMA channel number

\end{description}


\subsubsection{Resources Management}
\label{core-api/kernel-api:resources-management}\index{request\_resource\_conflict (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.request_resource_conflict}\pysiglinewithargsret{struct resource * \bfcode{request\_resource\_conflict}}{struct resource *\emph{ root}, struct resource *\emph{ new}}{}
request and reserve an I/O or memory resource

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct resource * root}}] \leavevmode
root resource descriptor

\item[{\code{struct resource * new}}] \leavevmode
resource descriptor desired by caller

\end{description}

\textbf{Description}

Returns 0 for success, conflict resource on error.
\index{reallocate\_resource (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.reallocate_resource}\pysiglinewithargsret{int \bfcode{reallocate\_resource}}{struct resource *\emph{ root}, struct resource *\emph{ old}, resource\_size\_t\emph{ newsize}, struct resource\_constraint *\emph{ constraint}}{}
allocate a slot in the resource tree given range \& alignment. The resource will be relocated if the new size cannot be reallocated in the current location.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct resource * root}}] \leavevmode
root resource descriptor

\item[{\code{struct resource * old}}] \leavevmode
resource descriptor desired by caller

\item[{\code{resource\_size\_t newsize}}] \leavevmode
new size of the resource descriptor

\item[{\code{struct resource\_constraint * constraint}}] \leavevmode
the size and alignment constraints to be met.

\end{description}
\index{lookup\_resource (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.lookup_resource}\pysiglinewithargsret{struct resource * \bfcode{lookup\_resource}}{struct resource *\emph{ root}, resource\_size\_t\emph{ start}}{}
find an existing resource by a resource start address

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct resource * root}}] \leavevmode
root resource descriptor

\item[{\code{resource\_size\_t start}}] \leavevmode
resource start address

\end{description}

\textbf{Description}

Returns a pointer to the resource if found, NULL otherwise
\index{insert\_resource\_conflict (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.insert_resource_conflict}\pysiglinewithargsret{struct resource * \bfcode{insert\_resource\_conflict}}{struct resource *\emph{ parent}, struct resource *\emph{ new}}{}
Inserts resource in the resource tree

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct resource * parent}}] \leavevmode
parent of the new resource

\item[{\code{struct resource * new}}] \leavevmode
new resource to insert

\end{description}

\textbf{Description}

Returns 0 on success, conflict resource if the resource can't be inserted.

This function is equivalent to request\_resource\_conflict when no conflict
happens. If a conflict happens, and the conflicting resources
entirely fit within the range of the new resource, then the new
resource is inserted and the conflicting resources become children of
the new resource.

This function is intended for producers of resources, such as FW modules
and bus drivers.
\index{insert\_resource\_expand\_to\_fit (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.insert_resource_expand_to_fit}\pysiglinewithargsret{void \bfcode{insert\_resource\_expand\_to\_fit}}{struct resource *\emph{ root}, struct resource *\emph{ new}}{}
Insert a resource into the resource tree

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct resource * root}}] \leavevmode
root resource descriptor

\item[{\code{struct resource * new}}] \leavevmode
new resource to insert

\end{description}

\textbf{Description}

Insert a resource into the resource tree, possibly expanding it in order
to make it encompass any conflicting resources.
\index{resource\_alignment (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.resource_alignment}\pysiglinewithargsret{resource\_size\_t \bfcode{resource\_alignment}}{struct resource *\emph{ res}}{}
calculate resource's alignment

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct resource * res}}] \leavevmode
resource pointer

\end{description}

\textbf{Description}

Returns alignment on success, 0 (invalid alignment) on failure.
\index{release\_mem\_region\_adjustable (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.release_mem_region_adjustable}\pysiglinewithargsret{int \bfcode{release\_mem\_region\_adjustable}}{struct resource *\emph{ parent}, resource\_size\_t\emph{ start}, resource\_size\_t\emph{ size}}{}
release a previously reserved memory region

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct resource * parent}}] \leavevmode
parent resource descriptor

\item[{\code{resource\_size\_t start}}] \leavevmode
resource start address

\item[{\code{resource\_size\_t size}}] \leavevmode
resource region size

\end{description}

\textbf{Description}

This interface is intended for memory hot-delete.  The requested region
is released from a currently busy memory resource.  The requested region
must either match exactly or fit into a single busy resource entry.  In
the latter case, the remaining resource is adjusted accordingly.
Existing children of the busy memory resource must be immutable in the
request.

\textbf{Note}
\begin{itemize}
\item {} 
Additional release conditions, such as overlapping region, can be
supported after they are confirmed as valid cases.

\item {} 
When a busy memory resource gets split into two entries, the code
assumes that all children remain in the lower address entry for
simplicity.  Enhance this logic when necessary.

\end{itemize}
\index{request\_resource (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.request_resource}\pysiglinewithargsret{int \bfcode{request\_resource}}{struct resource *\emph{ root}, struct resource *\emph{ new}}{}
request and reserve an I/O or memory resource

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct resource * root}}] \leavevmode
root resource descriptor

\item[{\code{struct resource * new}}] \leavevmode
resource descriptor desired by caller

\end{description}

\textbf{Description}

Returns 0 for success, negative error code on error.
\index{release\_resource (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.release_resource}\pysiglinewithargsret{int \bfcode{release\_resource}}{struct resource *\emph{ old}}{}
release a previously reserved resource

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct resource * old}}] \leavevmode
resource pointer

\end{description}
\index{region\_intersects (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.region_intersects}\pysiglinewithargsret{int \bfcode{region\_intersects}}{resource\_size\_t\emph{ start}, size\_t\emph{ size}, unsigned long\emph{ flags}, unsigned long\emph{ desc}}{}
determine intersection of region with known resources

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{resource\_size\_t start}}] \leavevmode
region start address

\item[{\code{size\_t size}}] \leavevmode
size of region

\item[{\code{unsigned long flags}}] \leavevmode
flags of resource (in iomem\_resource)

\item[{\code{unsigned long desc}}] \leavevmode
descriptor of resource (in iomem\_resource) or IORES\_DESC\_NONE

\end{description}

\textbf{Description}

Check if the specified region partially overlaps or fully eclipses a
resource identified by \textbf{flags} and \textbf{desc} (optional with IORES\_DESC\_NONE).
Return REGION\_DISJOINT if the region does not overlap \textbf{flags}/\textbf{desc},
return REGION\_MIXED if the region overlaps \textbf{flags}/\textbf{desc} and another
resource, and return REGION\_INTERSECTS if the region overlaps \textbf{flags}/\textbf{desc}
and no other defined resource. Note that REGION\_INTERSECTS is also
returned in the case when the specified region overlaps RAM and undefined
memory holes.

\code{region\_intersect()} is used by memory remapping functions to ensure
the user is not remapping RAM and is a vast speed up over walking
through the resource table page by page.
\index{allocate\_resource (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.allocate_resource}\pysiglinewithargsret{int \bfcode{allocate\_resource}}{struct resource *\emph{ root}, struct resource *\emph{ new}, resource\_size\_t\emph{ size}, resource\_size\_t\emph{ min}, resource\_size\_t\emph{ max}, resource\_size\_t\emph{ align}, resource\_size\_t (*alignf) (void\emph{ *}, const struct resource\emph{ *}, resource\_size\_t, resource\_size\_t, void *\emph{ alignf\_data}}{}
allocate empty slot in the resource tree given range \& alignment. The resource will be reallocated with a new size if it was already allocated

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct resource * root}}] \leavevmode
root resource descriptor

\item[{\code{struct resource * new}}] \leavevmode
resource descriptor desired by caller

\item[{\code{resource\_size\_t size}}] \leavevmode
requested resource region size

\item[{\code{resource\_size\_t min}}] \leavevmode
minimum boundary to allocate

\item[{\code{resource\_size\_t max}}] \leavevmode
maximum boundary to allocate

\item[{\code{resource\_size\_t align}}] \leavevmode
alignment requested, in bytes

\item[{\code{resource\_size\_t (*)(void *, const struct resource *, resource\_size\_t, resource\_size\_t) alignf}}] \leavevmode
alignment function, optional, called if not NULL

\item[{\code{void * alignf\_data}}] \leavevmode
arbitrary data to pass to the \textbf{alignf} function

\end{description}
\index{insert\_resource (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.insert_resource}\pysiglinewithargsret{int \bfcode{insert\_resource}}{struct resource *\emph{ parent}, struct resource *\emph{ new}}{}
Inserts a resource in the resource tree

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct resource * parent}}] \leavevmode
parent of the new resource

\item[{\code{struct resource * new}}] \leavevmode
new resource to insert

\end{description}

\textbf{Description}

Returns 0 on success, -EBUSY if the resource can't be inserted.

This function is intended for producers of resources, such as FW modules
and bus drivers.
\index{remove\_resource (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.remove_resource}\pysiglinewithargsret{int \bfcode{remove\_resource}}{struct resource *\emph{ old}}{}
Remove a resource in the resource tree

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct resource * old}}] \leavevmode
resource to remove

\end{description}

\textbf{Description}

Returns 0 on success, -EINVAL if the resource is not valid.

This function removes a resource previously inserted by {\hyperref[core\string-api/kernel\string-api:c.insert_resource]{\emph{\code{insert\_resource()}}}}
or {\hyperref[core\string-api/kernel\string-api:c.insert_resource_conflict]{\emph{\code{insert\_resource\_conflict()}}}}, and moves the children (if any) up to
where they were before.  {\hyperref[core\string-api/kernel\string-api:c.insert_resource]{\emph{\code{insert\_resource()}}}} and {\hyperref[core\string-api/kernel\string-api:c.insert_resource_conflict]{\emph{\code{insert\_resource\_conflict()}}}}
insert a new resource, and move any conflicting resources down to the
children of the new resource.

{\hyperref[core\string-api/kernel\string-api:c.insert_resource]{\emph{\code{insert\_resource()}}}}, {\hyperref[core\string-api/kernel\string-api:c.insert_resource_conflict]{\emph{\code{insert\_resource\_conflict()}}}} and {\hyperref[core\string-api/kernel\string-api:c.remove_resource]{\emph{\code{remove\_resource()}}}} are
intended for producers of resources, such as FW modules and bus drivers.
\index{adjust\_resource (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.adjust_resource}\pysiglinewithargsret{int \bfcode{adjust\_resource}}{struct resource *\emph{ res}, resource\_size\_t\emph{ start}, resource\_size\_t\emph{ size}}{}
modify a resource's start and size

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct resource * res}}] \leavevmode
resource to modify

\item[{\code{resource\_size\_t start}}] \leavevmode
new start value

\item[{\code{resource\_size\_t size}}] \leavevmode
new size

\end{description}

\textbf{Description}

Given an existing resource, change its start and size to match the
arguments.  Returns 0 on success, -EBUSY if it can't fit.
Existing children of the resource are assumed to be immutable.
\index{\_\_request\_region (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__request_region}\pysiglinewithargsret{struct resource * \bfcode{\_\_request\_region}}{struct resource *\emph{ parent}, resource\_size\_t\emph{ start}, resource\_size\_t\emph{ n}, const char *\emph{ name}, int\emph{ flags}}{}
create a new busy resource region

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct resource * parent}}] \leavevmode
parent resource descriptor

\item[{\code{resource\_size\_t start}}] \leavevmode
resource start address

\item[{\code{resource\_size\_t n}}] \leavevmode
resource region size

\item[{\code{const char * name}}] \leavevmode
reserving caller's ID string

\item[{\code{int flags}}] \leavevmode
IO resource flags

\end{description}
\index{\_\_release\_region (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__release_region}\pysiglinewithargsret{void \bfcode{\_\_release\_region}}{struct resource *\emph{ parent}, resource\_size\_t\emph{ start}, resource\_size\_t\emph{ n}}{}
release a previously reserved resource region

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct resource * parent}}] \leavevmode
parent resource descriptor

\item[{\code{resource\_size\_t start}}] \leavevmode
resource start address

\item[{\code{resource\_size\_t n}}] \leavevmode
resource region size

\end{description}

\textbf{Description}

The described resource region must match a currently busy region.
\index{devm\_request\_resource (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.devm_request_resource}\pysiglinewithargsret{int \bfcode{devm\_request\_resource}}{struct device *\emph{ dev}, struct resource *\emph{ root}, struct resource *\emph{ new}}{}
request and reserve an I/O or memory resource

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct device * dev}}] \leavevmode
device for which to request the resource

\item[{\code{struct resource * root}}] \leavevmode
root of the resource tree from which to request the resource

\item[{\code{struct resource * new}}] \leavevmode
descriptor of the resource to request

\end{description}

\textbf{Description}

This is a device-managed version of {\hyperref[core\string-api/kernel\string-api:c.request_resource]{\emph{\code{request\_resource()}}}}. There is usually
no need to release resources requested by this function explicitly since
that will be taken care of when the device is unbound from its driver.
If for some reason the resource needs to be released explicitly, because
of ordering issues for example, drivers must call {\hyperref[core\string-api/kernel\string-api:c.devm_release_resource]{\emph{\code{devm\_release\_resource()}}}}
rather than the regular {\hyperref[core\string-api/kernel\string-api:c.release_resource]{\emph{\code{release\_resource()}}}}.

When a conflict is detected between any existing resources and the newly
requested resource, an error message will be printed.

Returns 0 on success or a negative error code on failure.
\index{devm\_release\_resource (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.devm_release_resource}\pysiglinewithargsret{void \bfcode{devm\_release\_resource}}{struct device *\emph{ dev}, struct resource *\emph{ new}}{}
release a previously requested resource

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct device * dev}}] \leavevmode
device for which to release the resource

\item[{\code{struct resource * new}}] \leavevmode
descriptor of the resource to release

\end{description}

\textbf{Description}

Releases a resource previously requested using {\hyperref[core\string-api/kernel\string-api:c.devm_request_resource]{\emph{\code{devm\_request\_resource()}}}}.


\subsubsection{MTRR Handling}
\label{core-api/kernel-api:mtrr-handling}\index{arch\_phys\_wc\_add (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.arch_phys_wc_add}\pysiglinewithargsret{int \bfcode{arch\_phys\_wc\_add}}{unsigned long\emph{ base}, unsigned long\emph{ size}}{}
add a WC MTRR and handle errors if PAT is unavailable

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned long base}}] \leavevmode
Physical base address

\item[{\code{unsigned long size}}] \leavevmode
Size of region

\end{description}

\textbf{Description}

If PAT is available, this does nothing.  If PAT is unavailable, it
attempts to add a WC MTRR covering size bytes starting at base and
logs an error if this fails.

The called should provide a power of two size on an equivalent
power of two boundary.

Drivers must store the return value to pass to mtrr\_del\_wc\_if\_needed,
but drivers should not try to interpret that return value.


\subsection{Security Framework}
\label{core-api/kernel-api:security-framework}\index{security\_init (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.security_init}\pysiglinewithargsret{int \bfcode{security\_init}}{void}{}
initializes the security framework

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

This should be called early in the kernel initialization sequence.
\index{security\_module\_enable (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.security_module_enable}\pysiglinewithargsret{int \bfcode{security\_module\_enable}}{const char *\emph{ module}}{}
Load given security module on boot ?

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * module}}] \leavevmode
the name of the module

\end{description}

\textbf{Description}

Each LSM must pass this method before registering its own operations
to avoid security registration races. This method may also be used
to check if your LSM is currently loaded during kernel initialization.

\textbf{Return}

true if:
\begin{itemize}
\item {} 
The passed LSM is the one chosen by user at boot time,

\item {} 
or the passed LSM is configured as the default and the user did not
choose an alternate LSM at boot time.

\end{itemize}

Otherwise, return false.
\index{security\_add\_hooks (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.security_add_hooks}\pysiglinewithargsret{void \bfcode{security\_add\_hooks}}{struct security\_hook\_list *\emph{ hooks}, int\emph{ count}, char *\emph{ lsm}}{}
Add a modules hooks to the hook lists.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct security\_hook\_list * hooks}}] \leavevmode
the hooks to add

\item[{\code{int count}}] \leavevmode
the number of hooks to add

\item[{\code{char * lsm}}] \leavevmode
the name of the security module

\end{description}

\textbf{Description}

Each LSM has to register its hooks with the infrastructure.
\index{securityfs\_create\_file (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.securityfs_create_file}\pysiglinewithargsret{struct dentry * \bfcode{securityfs\_create\_file}}{const char *\emph{ name}, umode\_t\emph{ mode}, struct dentry *\emph{ parent}, void *\emph{ data}, const struct file\_operations *\emph{ fops}}{}
create a file in the securityfs filesystem

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * name}}] \leavevmode
a pointer to a string containing the name of the file to create.

\item[{\code{umode\_t mode}}] \leavevmode
the permission that the file should have

\item[{\code{struct dentry * parent}}] \leavevmode
a pointer to the parent dentry for this file.  This should be a
directory dentry if set.  If this parameter is \code{NULL}, then the
file will be created in the root of the securityfs filesystem.

\item[{\code{void * data}}] \leavevmode
a pointer to something that the caller will want to get to later
on.  The inode.i\_private pointer will point to this value on
the \code{open()} call.

\item[{\code{const struct file\_operations * fops}}] \leavevmode
a pointer to a struct file\_operations that should be used for
this file.

\end{description}

\textbf{Description}

This function creates a file in securityfs with the given \textbf{name}.

This function returns a pointer to a dentry if it succeeds.  This
pointer must be passed to the {\hyperref[core\string-api/kernel\string-api:c.securityfs_remove]{\emph{\code{securityfs\_remove()}}}} function when the file is
to be removed (no automatic cleanup happens if your module is unloaded,
you are responsible here).  If an error occurs, the function will return
the error value (via ERR\_PTR).

If securityfs is not enabled in the kernel, the value \code{-ENODEV} is
returned.
\index{securityfs\_create\_dir (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.securityfs_create_dir}\pysiglinewithargsret{struct dentry * \bfcode{securityfs\_create\_dir}}{const char *\emph{ name}, struct dentry *\emph{ parent}}{}
create a directory in the securityfs filesystem

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * name}}] \leavevmode
a pointer to a string containing the name of the directory to
create.

\item[{\code{struct dentry * parent}}] \leavevmode
a pointer to the parent dentry for this file.  This should be a
directory dentry if set.  If this parameter is \code{NULL}, then the
directory will be created in the root of the securityfs filesystem.

\end{description}

\textbf{Description}

This function creates a directory in securityfs with the given \textbf{name}.

This function returns a pointer to a dentry if it succeeds.  This
pointer must be passed to the {\hyperref[core\string-api/kernel\string-api:c.securityfs_remove]{\emph{\code{securityfs\_remove()}}}} function when the file is
to be removed (no automatic cleanup happens if your module is unloaded,
you are responsible here).  If an error occurs, the function will return
the error value (via ERR\_PTR).

If securityfs is not enabled in the kernel, the value \code{-ENODEV} is
returned.
\index{securityfs\_create\_symlink (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.securityfs_create_symlink}\pysiglinewithargsret{struct dentry * \bfcode{securityfs\_create\_symlink}}{const char *\emph{ name}, struct dentry *\emph{ parent}, const char *\emph{ target}, const struct inode\_operations *\emph{ iops}}{}
create a symlink in the securityfs filesystem

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * name}}] \leavevmode
a pointer to a string containing the name of the symlink to
create.

\item[{\code{struct dentry * parent}}] \leavevmode
a pointer to the parent dentry for the symlink.  This should be a
directory dentry if set.  If this parameter is \code{NULL}, then the
directory will be created in the root of the securityfs filesystem.

\item[{\code{const char * target}}] \leavevmode
a pointer to a string containing the name of the symlink's target.
If this parameter is \code{NULL}, then the \textbf{iops} parameter needs to be
setup to handle .readlink and .get\_link inode\_operations.

\item[{\code{const struct inode\_operations * iops}}] \leavevmode
a pointer to the struct inode\_operations to use for the symlink. If
this parameter is \code{NULL}, then the default simple\_symlink\_inode
operations will be used.

\end{description}

\textbf{Description}

This function creates a symlink in securityfs with the given \textbf{name}.

This function returns a pointer to a dentry if it succeeds.  This
pointer must be passed to the {\hyperref[core\string-api/kernel\string-api:c.securityfs_remove]{\emph{\code{securityfs\_remove()}}}} function when the file is
to be removed (no automatic cleanup happens if your module is unloaded,
you are responsible here).  If an error occurs, the function will return
the error value (via ERR\_PTR).

If securityfs is not enabled in the kernel, the value \code{-ENODEV} is
returned.
\index{securityfs\_remove (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.securityfs_remove}\pysiglinewithargsret{void \bfcode{securityfs\_remove}}{struct dentry *\emph{ dentry}}{}
removes a file or directory from the securityfs filesystem

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct dentry * dentry}}] \leavevmode
a pointer to a the dentry of the file or directory to be removed.

\end{description}

\textbf{Description}

This function removes a file or directory in securityfs that was previously
created with a call to another securityfs function (like
{\hyperref[core\string-api/kernel\string-api:c.securityfs_create_file]{\emph{\code{securityfs\_create\_file()}}}} or variants thereof.)

This function is required to be called in order for the file to be
removed. No automatic cleanup of files will happen when a module is
removed; you are responsible here.


\subsection{Audit Interfaces}
\label{core-api/kernel-api:audit-interfaces}\index{audit\_log\_start (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.audit_log_start}\pysiglinewithargsret{struct audit\_buffer * \bfcode{audit\_log\_start}}{struct audit\_context *\emph{ ctx}, gfp\_t\emph{ gfp\_mask}, int\emph{ type}}{}
obtain an audit buffer

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct audit\_context * ctx}}] \leavevmode
audit\_context (may be NULL)

\item[{\code{gfp\_t gfp\_mask}}] \leavevmode
type of allocation

\item[{\code{int type}}] \leavevmode
audit message type

\end{description}

\textbf{Description}

Returns audit\_buffer pointer on success or NULL on error.

Obtain an audit buffer.  This routine does locking to obtain the
audit buffer, but then no locking is required for calls to
audit\_log\_*format.  If the task (ctx) is a task that is currently in a
syscall, then the syscall is marked as auditable and an audit record
will be written at syscall exit.  If there is no associated task, then
task context (ctx) should be NULL.
\index{audit\_log\_format (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.audit_log_format}\pysiglinewithargsret{void \bfcode{audit\_log\_format}}{struct audit\_buffer *\emph{ ab}, const char *\emph{ fmt}, ...}{}
format a message into the audit buffer.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct audit\_buffer * ab}}] \leavevmode
audit\_buffer

\item[{\code{const char * fmt}}] \leavevmode
format string

\item[{\code{...}}] \leavevmode
optional parameters matching \textbf{fmt} string

\end{description}

\textbf{Description}

All the work is done in audit\_log\_vformat.
\index{audit\_log\_end (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.audit_log_end}\pysiglinewithargsret{void \bfcode{audit\_log\_end}}{struct audit\_buffer *\emph{ ab}}{}
end one audit record

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct audit\_buffer * ab}}] \leavevmode
the audit\_buffer

\end{description}

\textbf{Description}

We can not do a netlink send inside an irq context because it blocks (last
arg, flags, is not set to MSG\_DONTWAIT), so the audit buffer is placed on a
queue and a tasklet is scheduled to remove them from the queue outside the
irq context.  May be called in any context.
\index{audit\_log (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.audit_log}\pysiglinewithargsret{void \bfcode{audit\_log}}{struct audit\_context *\emph{ ctx}, gfp\_t\emph{ gfp\_mask}, int\emph{ type}, const char *\emph{ fmt}, ...}{}
Log an audit record

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct audit\_context * ctx}}] \leavevmode
audit context

\item[{\code{gfp\_t gfp\_mask}}] \leavevmode
type of allocation

\item[{\code{int type}}] \leavevmode
audit message type

\item[{\code{const char * fmt}}] \leavevmode
format string to use

\item[{\code{...}}] \leavevmode
variable parameters matching the format string

\end{description}

\textbf{Description}

This is a convenience function that calls audit\_log\_start,
audit\_log\_vformat, and audit\_log\_end.  It may be called
in any context.
\index{audit\_alloc (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.audit_alloc}\pysiglinewithargsret{int \bfcode{audit\_alloc}}{struct task\_struct *\emph{ tsk}}{}
allocate an audit context block for a task

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct task\_struct * tsk}}] \leavevmode
task

\end{description}

\textbf{Description}

Filter on the task information and allocate a per-task audit context
if necessary.  Doing so turns on system call auditing for the
specified task.  This is called from copy\_process, so no lock is
needed.
\index{\_\_audit\_free (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__audit_free}\pysiglinewithargsret{void \bfcode{\_\_audit\_free}}{struct task\_struct *\emph{ tsk}}{}
free a per-task audit context

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct task\_struct * tsk}}] \leavevmode
task whose audit context block to free

\end{description}

\textbf{Description}

Called from copy\_process and do\_exit
\index{\_\_audit\_syscall\_entry (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__audit_syscall_entry}\pysiglinewithargsret{void \bfcode{\_\_audit\_syscall\_entry}}{int\emph{ major}, unsigned long\emph{ a1}, unsigned long\emph{ a2}, unsigned long\emph{ a3}, unsigned long\emph{ a4}}{}
fill in an audit record at syscall entry

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{int major}}] \leavevmode
major syscall type (function)

\item[{\code{unsigned long a1}}] \leavevmode
additional syscall register 1

\item[{\code{unsigned long a2}}] \leavevmode
additional syscall register 2

\item[{\code{unsigned long a3}}] \leavevmode
additional syscall register 3

\item[{\code{unsigned long a4}}] \leavevmode
additional syscall register 4

\end{description}

\textbf{Description}

Fill in audit context at syscall entry.  This only happens if the
audit context was created when the task was created and the state or
filters demand the audit context be built.  If the state from the
per-task filter or from the per-syscall filter is AUDIT\_RECORD\_CONTEXT,
then the record will be written at syscall exit time (otherwise, it
will only be written if another part of the kernel requests that it
be written).
\index{\_\_audit\_syscall\_exit (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__audit_syscall_exit}\pysiglinewithargsret{void \bfcode{\_\_audit\_syscall\_exit}}{int\emph{ success}, long\emph{ return\_code}}{}
deallocate audit context after a system call

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{int success}}] \leavevmode
success value of the syscall

\item[{\code{long return\_code}}] \leavevmode
return value of the syscall

\end{description}

\textbf{Description}

Tear down after system call.  If the audit context has been marked as
auditable (either because of the AUDIT\_RECORD\_CONTEXT state from
filtering, or because some other part of the kernel wrote an audit
message), then write out the syscall information.  In call cases,
free the names stored from \code{getname()}.
\index{\_\_audit\_reusename (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__audit_reusename}\pysiglinewithargsret{struct filename * \bfcode{\_\_audit\_reusename}}{const \_\_user char *\emph{ uptr}}{}
fill out filename with info from existing entry

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const \_\_user char * uptr}}] \leavevmode
userland ptr to pathname

\end{description}

\textbf{Description}

Search the audit\_names list for the current audit context. If there is an
existing entry with a matching ``uptr'' then return the filename
associated with that audit\_name. If not, return NULL.
\index{\_\_audit\_getname (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__audit_getname}\pysiglinewithargsret{void \bfcode{\_\_audit\_getname}}{struct filename *\emph{ name}}{}
add a name to the list

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct filename * name}}] \leavevmode
name to add

\end{description}

\textbf{Description}

Add a name to the list of audit names for this context.
Called from fs/namei.c:\code{getname()}.
\index{\_\_audit\_inode (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__audit_inode}\pysiglinewithargsret{void \bfcode{\_\_audit\_inode}}{struct filename *\emph{ name}, const struct dentry *\emph{ dentry}, unsigned int\emph{ flags}}{}
store the inode and device from a lookup

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct filename * name}}] \leavevmode
name being audited

\item[{\code{const struct dentry * dentry}}] \leavevmode
dentry being audited

\item[{\code{unsigned int flags}}] \leavevmode
attributes for this particular entry

\end{description}
\index{auditsc\_get\_stamp (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.auditsc_get_stamp}\pysiglinewithargsret{int \bfcode{auditsc\_get\_stamp}}{struct audit\_context *\emph{ ctx}, struct timespec64 *\emph{ t}, unsigned int *\emph{ serial}}{}
get local copies of audit\_context values

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct audit\_context * ctx}}] \leavevmode
audit\_context for the task

\item[{\code{struct timespec64 * t}}] \leavevmode
timespec64 to store time recorded in the audit\_context

\item[{\code{unsigned int * serial}}] \leavevmode
serial value that is recorded in the audit\_context

\end{description}

\textbf{Description}

Also sets the context as auditable.
\index{audit\_set\_loginuid (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.audit_set_loginuid}\pysiglinewithargsret{int \bfcode{audit\_set\_loginuid}}{kuid\_t\emph{ loginuid}}{}
set current task's audit\_context loginuid

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{kuid\_t loginuid}}] \leavevmode
loginuid value

\end{description}

\textbf{Description}

Returns 0.

Called (set) from fs/proc/base.c::\code{proc\_loginuid\_write()}.
\index{\_\_audit\_mq\_open (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__audit_mq_open}\pysiglinewithargsret{void \bfcode{\_\_audit\_mq\_open}}{int\emph{ oflag}, umode\_t\emph{ mode}, struct mq\_attr *\emph{ attr}}{}
record audit data for a POSIX MQ open

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{int oflag}}] \leavevmode
open flag

\item[{\code{umode\_t mode}}] \leavevmode
mode bits

\item[{\code{struct mq\_attr * attr}}] \leavevmode
queue attributes

\end{description}
\index{\_\_audit\_mq\_sendrecv (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__audit_mq_sendrecv}\pysiglinewithargsret{void \bfcode{\_\_audit\_mq\_sendrecv}}{mqd\_t\emph{ mqdes}, size\_t\emph{ msg\_len}, unsigned int\emph{ msg\_prio}, const struct timespec64 *\emph{ abs\_timeout}}{}
record audit data for a POSIX MQ timed send/receive

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{mqd\_t mqdes}}] \leavevmode
MQ descriptor

\item[{\code{size\_t msg\_len}}] \leavevmode
Message length

\item[{\code{unsigned int msg\_prio}}] \leavevmode
Message priority

\item[{\code{const struct timespec64 * abs\_timeout}}] \leavevmode
Message timeout in absolute time

\end{description}
\index{\_\_audit\_mq\_notify (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__audit_mq_notify}\pysiglinewithargsret{void \bfcode{\_\_audit\_mq\_notify}}{mqd\_t\emph{ mqdes}, const struct sigevent *\emph{ notification}}{}
record audit data for a POSIX MQ notify

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{mqd\_t mqdes}}] \leavevmode
MQ descriptor

\item[{\code{const struct sigevent * notification}}] \leavevmode
Notification event

\end{description}
\index{\_\_audit\_mq\_getsetattr (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__audit_mq_getsetattr}\pysiglinewithargsret{void \bfcode{\_\_audit\_mq\_getsetattr}}{mqd\_t\emph{ mqdes}, struct mq\_attr *\emph{ mqstat}}{}
record audit data for a POSIX MQ get/set attribute

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{mqd\_t mqdes}}] \leavevmode
MQ descriptor

\item[{\code{struct mq\_attr * mqstat}}] \leavevmode
MQ flags

\end{description}
\index{\_\_audit\_ipc\_obj (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__audit_ipc_obj}\pysiglinewithargsret{void \bfcode{\_\_audit\_ipc\_obj}}{struct kern\_ipc\_perm *\emph{ ipcp}}{}
record audit data for ipc object

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct kern\_ipc\_perm * ipcp}}] \leavevmode
ipc permissions

\end{description}
\index{\_\_audit\_ipc\_set\_perm (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__audit_ipc_set_perm}\pysiglinewithargsret{void \bfcode{\_\_audit\_ipc\_set\_perm}}{unsigned long\emph{ qbytes}, uid\_t\emph{ uid}, gid\_t\emph{ gid}, umode\_t\emph{ mode}}{}
record audit data for new ipc permissions

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned long qbytes}}] \leavevmode
msgq bytes

\item[{\code{uid\_t uid}}] \leavevmode
msgq user id

\item[{\code{gid\_t gid}}] \leavevmode
msgq group id

\item[{\code{umode\_t mode}}] \leavevmode
msgq mode (permissions)

\end{description}

\textbf{Description}

Called only after \code{audit\_ipc\_obj()}.
\index{\_\_audit\_socketcall (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__audit_socketcall}\pysiglinewithargsret{int \bfcode{\_\_audit\_socketcall}}{int\emph{ nargs}, unsigned long *\emph{ args}}{}
record audit data for sys\_socketcall

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{int nargs}}] \leavevmode
number of args, which should not be more than AUDITSC\_ARGS.

\item[{\code{unsigned long * args}}] \leavevmode
args array

\end{description}
\index{\_\_audit\_fd\_pair (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__audit_fd_pair}\pysiglinewithargsret{void \bfcode{\_\_audit\_fd\_pair}}{int\emph{ fd1}, int\emph{ fd2}}{}
record audit data for pipe and socketpair

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{int fd1}}] \leavevmode
the first file descriptor

\item[{\code{int fd2}}] \leavevmode
the second file descriptor

\end{description}
\index{\_\_audit\_sockaddr (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__audit_sockaddr}\pysiglinewithargsret{int \bfcode{\_\_audit\_sockaddr}}{int\emph{ len}, void *\emph{ a}}{}
record audit data for sys\_bind, sys\_connect, sys\_sendto

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{int len}}] \leavevmode
data length in user space

\item[{\code{void * a}}] \leavevmode
data address in kernel space

\end{description}

\textbf{Description}

Returns 0 for success or NULL context or \textless{} 0 on error.
\index{audit\_signal\_info (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.audit_signal_info}\pysiglinewithargsret{int \bfcode{audit\_signal\_info}}{int\emph{ sig}, struct task\_struct *\emph{ t}}{}
record signal info for shutting down audit subsystem

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{int sig}}] \leavevmode
signal value

\item[{\code{struct task\_struct * t}}] \leavevmode
task being signaled

\end{description}

\textbf{Description}

If the audit subsystem is being terminated, record the task (pid)
and uid that is doing that.
\index{\_\_audit\_log\_bprm\_fcaps (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__audit_log_bprm_fcaps}\pysiglinewithargsret{int \bfcode{\_\_audit\_log\_bprm\_fcaps}}{struct linux\_binprm *\emph{ bprm}, const struct cred *\emph{ new}, const struct cred *\emph{ old}}{}
store information about a loading bprm and relevant fcaps

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct linux\_binprm * bprm}}] \leavevmode
pointer to the bprm being processed

\item[{\code{const struct cred * new}}] \leavevmode
the proposed new credentials

\item[{\code{const struct cred * old}}] \leavevmode
the old credentials

\end{description}

\textbf{Description}

Simply check if the proc already has the caps given by the file and if not
store the priv escalation info for later auditing at the end of the syscall

-Eric
\index{\_\_audit\_log\_capset (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__audit_log_capset}\pysiglinewithargsret{void \bfcode{\_\_audit\_log\_capset}}{const struct cred *\emph{ new}, const struct cred *\emph{ old}}{}
store information about the arguments to the capset syscall

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const struct cred * new}}] \leavevmode
the new credentials

\item[{\code{const struct cred * old}}] \leavevmode
the old (current) credentials

\end{description}

\textbf{Description}

Record the arguments userspace sent to sys\_capset for later printing by the
audit system if applicable
\index{audit\_core\_dumps (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.audit_core_dumps}\pysiglinewithargsret{void \bfcode{audit\_core\_dumps}}{long\emph{ signr}}{}
record information about processes that end abnormally

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{long signr}}] \leavevmode
signal value

\end{description}

\textbf{Description}

If a process ends with a core dump, something fishy is going on and we
should record the event for investigation.
\index{audit\_rule\_change (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.audit_rule_change}\pysiglinewithargsret{int \bfcode{audit\_rule\_change}}{int\emph{ type}, int\emph{ seq}, void *\emph{ data}, size\_t\emph{ datasz}}{}
apply all rules to the specified message type

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{int type}}] \leavevmode
audit message type

\item[{\code{int seq}}] \leavevmode
netlink audit message sequence (serial) number

\item[{\code{void * data}}] \leavevmode
payload data

\item[{\code{size\_t datasz}}] \leavevmode
size of payload data

\end{description}
\index{audit\_list\_rules\_send (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.audit_list_rules_send}\pysiglinewithargsret{int \bfcode{audit\_list\_rules\_send}}{struct sk\_buff *\emph{ request\_skb}, int\emph{ seq}}{}
list the audit rules

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct sk\_buff * request\_skb}}] \leavevmode
skb of request we are replying to (used to target the reply)

\item[{\code{int seq}}] \leavevmode
netlink audit message sequence (serial) number

\end{description}
\index{parent\_len (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.parent_len}\pysiglinewithargsret{int \bfcode{parent\_len}}{const char *\emph{ path}}{}
find the length of the parent portion of a pathname

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * path}}] \leavevmode
pathname of which to determine length

\end{description}
\index{audit\_compare\_dname\_path (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.audit_compare_dname_path}\pysiglinewithargsret{int \bfcode{audit\_compare\_dname\_path}}{const char *\emph{ dname}, const char *\emph{ path}, int\emph{ parentlen}}{}
compare given dentry name with last component in given path. Return of 0 indicates a match.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * dname}}] \leavevmode
dentry name that we're comparing

\item[{\code{const char * path}}] \leavevmode
full pathname that we're comparing

\item[{\code{int parentlen}}] \leavevmode
length of the parent if known. Passing in AUDIT\_NAME\_FULL
here indicates that we must compute this value.

\end{description}


\subsection{Accounting Framework}
\label{core-api/kernel-api:accounting-framework}\index{sys\_acct (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.sys_acct}\pysiglinewithargsret{long \bfcode{sys\_acct}}{const char \_\_user *\emph{ name}}{}
enable/disable process accounting

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char \_\_user * name}}] \leavevmode
file name for accounting records or NULL to shutdown accounting

\end{description}

\textbf{Description}

Returns 0 for success or negative errno values for failure.

{\hyperref[core\string-api/kernel\string-api:c.sys_acct]{\emph{\code{sys\_acct()}}}} is the only system call needed to implement process
accounting. It takes the name of the file where accounting records
should be written. If the filename is NULL, accounting will be
shutdown.
\index{acct\_collect (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.acct_collect}\pysiglinewithargsret{void \bfcode{acct\_collect}}{long\emph{ exitcode}, int\emph{ group\_dead}}{}
collect accounting information into pacct\_struct

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{long exitcode}}] \leavevmode
task exit code

\item[{\code{int group\_dead}}] \leavevmode
not 0, if this thread is the last one in the process.

\end{description}
\index{acct\_process (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.acct_process}\pysiglinewithargsret{void \bfcode{acct\_process}}{void}{}
\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

handles process accounting for an exiting task


\subsection{Block Devices}
\label{core-api/kernel-api:block-devices}\index{blk\_delay\_queue (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_delay_queue}\pysiglinewithargsret{void \bfcode{blk\_delay\_queue}}{struct request\_queue *\emph{ q}, unsigned long\emph{ msecs}}{}
restart queueing after defined interval

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
The \code{struct request\_queue} in question

\item[{\code{unsigned long msecs}}] \leavevmode
Delay in msecs

\end{description}

\textbf{Description}
\begin{quote}

Sometimes queueing needs to be postponed for a little while, to allow
resources to come back. This function will make sure that queueing is
restarted around the specified time.
\end{quote}
\index{blk\_start\_queue\_async (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_start_queue_async}\pysiglinewithargsret{void \bfcode{blk\_start\_queue\_async}}{struct request\_queue *\emph{ q}}{}
asynchronously restart a previously stopped queue

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
The \code{struct request\_queue} in question

\end{description}

\textbf{Description}
\begin{quote}

{\hyperref[core\string-api/kernel\string-api:c.blk_start_queue_async]{\emph{\code{blk\_start\_queue\_async()}}}} will clear the stop flag on the queue, and
ensure that the request\_fn for the queue is run from an async
context.
\end{quote}
\index{blk\_start\_queue (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_start_queue}\pysiglinewithargsret{void \bfcode{blk\_start\_queue}}{struct request\_queue *\emph{ q}}{}
restart a previously stopped queue

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
The \code{struct request\_queue} in question

\end{description}

\textbf{Description}
\begin{quote}

{\hyperref[core\string-api/kernel\string-api:c.blk_start_queue]{\emph{\code{blk\_start\_queue()}}}} will clear the stop flag on the queue, and call
the request\_fn for the queue if it was in a stopped state when
entered. Also see {\hyperref[core\string-api/kernel\string-api:c.blk_stop_queue]{\emph{\code{blk\_stop\_queue()}}}}.
\end{quote}
\index{blk\_stop\_queue (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_stop_queue}\pysiglinewithargsret{void \bfcode{blk\_stop\_queue}}{struct request\_queue *\emph{ q}}{}
stop a queue

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
The \code{struct request\_queue} in question

\end{description}

\textbf{Description}
\begin{quote}

The Linux block layer assumes that a block driver will consume all
entries on the request queue when the request\_fn strategy is called.
Often this will not happen, because of hardware limitations (queue
depth settings). If a device driver gets a `queue full' response,
or if it simply chooses not to queue more I/O at one point, it can
call this function to prevent the request\_fn from being called until
the driver has signalled it's ready to go again. This happens by calling
{\hyperref[core\string-api/kernel\string-api:c.blk_start_queue]{\emph{\code{blk\_start\_queue()}}}} to restart queue operations.
\end{quote}
\index{blk\_sync\_queue (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_sync_queue}\pysiglinewithargsret{void \bfcode{blk\_sync\_queue}}{struct request\_queue *\emph{ q}}{}
cancel any pending callbacks on a queue

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the queue

\end{description}

\textbf{Description}
\begin{quote}

The block layer may perform asynchronous callback activity
on a queue, such as calling the unplug function after a timeout.
A block device may call blk\_sync\_queue to ensure that any
such activity is cancelled, thus allowing it to release resources
that the callbacks might use. The caller must already have made sure
that its -\textgreater{}make\_request\_fn will not re-add plugging prior to calling
this function.

This function does not cancel any asynchronous activity arising
out of elevator or throttling code. That would require \code{elevator\_exit()}
and \code{blkcg\_exit\_queue()} to be called with queue lock initialized.
\end{quote}
\index{blk\_set\_preempt\_only (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_set_preempt_only}\pysiglinewithargsret{int \bfcode{blk\_set\_preempt\_only}}{struct request\_queue *\emph{ q}}{}
set QUEUE\_FLAG\_PREEMPT\_ONLY

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
request queue pointer

\end{description}

\textbf{Description}

Returns the previous value of the PREEMPT\_ONLY flag - 0 if the flag was not
set and 1 if the flag was already set.
\index{\_\_blk\_run\_queue\_uncond (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__blk_run_queue_uncond}\pysiglinewithargsret{void \bfcode{\_\_blk\_run\_queue\_uncond}}{struct request\_queue *\emph{ q}}{}
run a queue whether or not it has been stopped

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
The queue to run

\end{description}

\textbf{Description}
\begin{quote}

Invoke request handling on a queue if there are any pending requests.
May be used to restart request handling after a request has completed.
This variant runs the queue whether or not the queue has been
stopped. Must be called with the queue lock held and interrupts
disabled. See also \textbf{blk\_run\_queue}.
\end{quote}
\index{\_\_blk\_run\_queue (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__blk_run_queue}\pysiglinewithargsret{void \bfcode{\_\_blk\_run\_queue}}{struct request\_queue *\emph{ q}}{}
run a single device queue

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
The queue to run

\end{description}

\textbf{Description}
\begin{quote}

See \textbf{blk\_run\_queue}.
\end{quote}
\index{blk\_run\_queue\_async (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_run_queue_async}\pysiglinewithargsret{void \bfcode{blk\_run\_queue\_async}}{struct request\_queue *\emph{ q}}{}
run a single device queue in workqueue context

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
The queue to run

\end{description}

\textbf{Description}
\begin{quote}

Tells kblockd to perform the equivalent of \textbf{blk\_run\_queue} on behalf
of us.
\end{quote}

\textbf{Note}
\begin{quote}

Since it is not allowed to run q-\textgreater{}delay\_work after {\hyperref[core\string-api/kernel\string-api:c.blk_cleanup_queue]{\emph{\code{blk\_cleanup\_queue()}}}}
has canceled q-\textgreater{}delay\_work, callers must hold the queue lock to avoid
race conditions between {\hyperref[core\string-api/kernel\string-api:c.blk_cleanup_queue]{\emph{\code{blk\_cleanup\_queue()}}}} and {\hyperref[core\string-api/kernel\string-api:c.blk_run_queue_async]{\emph{\code{blk\_run\_queue\_async()}}}}.
\end{quote}
\index{blk\_run\_queue (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_run_queue}\pysiglinewithargsret{void \bfcode{blk\_run\_queue}}{struct request\_queue *\emph{ q}}{}
run a single device queue

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
The queue to run

\end{description}

\textbf{Description}
\begin{quote}

Invoke request handling on this queue, if it has pending work to do.
May be used to restart queueing when a request has completed.
\end{quote}
\index{blk\_queue\_bypass\_start (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_queue_bypass_start}\pysiglinewithargsret{void \bfcode{blk\_queue\_bypass\_start}}{struct request\_queue *\emph{ q}}{}
enter queue bypass mode

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
queue of interest

\end{description}

\textbf{Description}

In bypass mode, only the dispatch FIFO queue of \textbf{q} is used.  This
function makes \textbf{q} enter bypass mode and drains all requests which were
throttled or issued before.  On return, it's guaranteed that no request
is being throttled or has ELVPRIV set and \code{blk\_queue\_bypass()} \code{true}
inside queue or RCU read lock.
\index{blk\_queue\_bypass\_end (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_queue_bypass_end}\pysiglinewithargsret{void \bfcode{blk\_queue\_bypass\_end}}{struct request\_queue *\emph{ q}}{}
leave queue bypass mode

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
queue of interest

\end{description}

\textbf{Description}

Leave bypass mode and restore the normal queueing behavior.

\textbf{Note}

although {\hyperref[core\string-api/kernel\string-api:c.blk_queue_bypass_start]{\emph{\code{blk\_queue\_bypass\_start()}}}} is only called for blk-sq queues,
this function is called for both blk-sq and blk-mq queues.
\index{blk\_cleanup\_queue (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_cleanup_queue}\pysiglinewithargsret{void \bfcode{blk\_cleanup\_queue}}{struct request\_queue *\emph{ q}}{}
shutdown a request queue

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
request queue to shutdown

\end{description}

\textbf{Description}

Mark \textbf{q} DYING, drain all pending requests, mark \textbf{q} DEAD, destroy and
put it.  All future requests will be failed immediately with -ENODEV.
\index{blk\_init\_queue (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_init_queue}\pysiglinewithargsret{struct request\_queue * \bfcode{blk\_init\_queue}}{request\_fn\_proc *\emph{ rfn}, spinlock\_t *\emph{ lock}}{}
prepare a request queue for use with a block device

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{request\_fn\_proc * rfn}}] \leavevmode
The function to be called to process requests that have been
placed on the queue.

\item[{\code{spinlock\_t * lock}}] \leavevmode
Request queue spin lock

\end{description}

\textbf{Description}
\begin{quote}

If a block device wishes to use the standard request handling procedures,
which sorts requests and coalesces adjacent requests, then it must
call {\hyperref[core\string-api/kernel\string-api:c.blk_init_queue]{\emph{\code{blk\_init\_queue()}}}}.  The function \textbf{rfn} will be called when there
are requests on the queue that need to be processed.  If the device
supports plugging, then \textbf{rfn} may not be called immediately when requests
are available on the queue, but may be called at some time later instead.
Plugged queues are generally unplugged when a buffer belonging to one
of the requests on the queue is needed, or due to memory pressure.

\textbf{rfn} is not required, or even expected, to remove all requests off the
queue, but only as many as it can handle at a time.  If it does leave
requests on the queue, it is responsible for arranging that the requests
get dealt with eventually.

The queue spin lock must be held while manipulating the requests on the
request queue; this lock will be taken also from interrupt context, so irq
disabling is needed for it.

Function returns a pointer to the initialized request queue, or \code{NULL} if
it didn't succeed.
\end{quote}

\textbf{Note}
\begin{quote}

{\hyperref[core\string-api/kernel\string-api:c.blk_init_queue]{\emph{\code{blk\_init\_queue()}}}} must be paired with a {\hyperref[core\string-api/kernel\string-api:c.blk_cleanup_queue]{\emph{\code{blk\_cleanup\_queue()}}}} call
when the block device is deactivated (such as at module unload).
\end{quote}
\index{blk\_get\_request\_flags (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_get_request_flags}\pysiglinewithargsret{struct request * \bfcode{blk\_get\_request\_flags}}{struct request\_queue *\emph{ q}, unsigned int\emph{ op}, blk\_mq\_req\_flags\_t\emph{ flags}}{}
allocate a request

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
request queue to allocate a request for

\item[{\code{unsigned int op}}] \leavevmode
operation (REQ\_OP\_*) and REQ\_* flags, e.g. REQ\_SYNC.

\item[{\code{blk\_mq\_req\_flags\_t flags}}] \leavevmode
BLK\_MQ\_REQ\_* flags, e.g. BLK\_MQ\_REQ\_NOWAIT.

\end{description}
\index{blk\_requeue\_request (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_requeue_request}\pysiglinewithargsret{void \bfcode{blk\_requeue\_request}}{struct request\_queue *\emph{ q}, struct request *\emph{ rq}}{}
put a request back on queue

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
request queue where request should be inserted

\item[{\code{struct request * rq}}] \leavevmode
request to be inserted

\end{description}

\textbf{Description}
\begin{quote}

Drivers often keep queueing requests until the hardware cannot accept
more, when that condition happens we need to put the request back
on the queue. Must be called with queue lock held.
\end{quote}
\index{part\_round\_stats (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.part_round_stats}\pysiglinewithargsret{void \bfcode{part\_round\_stats}}{struct request\_queue *\emph{ q}, int\emph{ cpu}, struct hd\_struct *\emph{ part}}{}
Round off the performance stats on a struct disk\_stats.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
target block queue

\item[{\code{int cpu}}] \leavevmode
cpu number for stats access

\item[{\code{struct hd\_struct * part}}] \leavevmode
target partition

\end{description}

\textbf{Description}

The average IO queue length and utilisation statistics are maintained
by observing the current state of the queue length and the amount of
time it has been in this state for.

Normally, that accounting is done on IO completion, but that can result
in more than a second's worth of IO being accounted for within any one
second, leading to \textgreater{}100\% utilisation.  To deal with that, we call this
function to do a round-off before returning the results when reading
/proc/diskstats.  This accounts immediately for all queue usage up to
the current jiffies and restarts the counters again.
\index{generic\_make\_request (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.generic_make_request}\pysiglinewithargsret{blk\_qc\_t \bfcode{generic\_make\_request}}{struct bio *\emph{ bio}}{}
hand a buffer to its device driver for I/O

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct bio * bio}}] \leavevmode
The bio describing the location in memory and on the device.

\end{description}

\textbf{Description}

{\hyperref[core\string-api/kernel\string-api:c.generic_make_request]{\emph{\code{generic\_make\_request()}}}} is used to make I/O requests of block
devices. It is passed a \code{struct bio}, which describes the I/O that needs
to be done.

{\hyperref[core\string-api/kernel\string-api:c.generic_make_request]{\emph{\code{generic\_make\_request()}}}} does not return any status.  The
success/failure status of the request, along with notification of
completion, is delivered asynchronously through the bio-\textgreater{}bi\_end\_io
function described (one day) else where.

The caller of generic\_make\_request must make sure that bi\_io\_vec
are set to describe the memory buffer, and that bi\_dev and bi\_sector are
set to describe the device address, and the
bi\_end\_io and optionally bi\_private are set to describe how
completion notification should be signaled.

generic\_make\_request and the drivers it calls may use bi\_next if this
bio happens to be merged with someone else, and may resubmit the bio to
a lower device by calling into generic\_make\_request recursively, which
means the bio should NOT be touched after the call to -\textgreater{}make\_request\_fn.
\index{direct\_make\_request (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.direct_make_request}\pysiglinewithargsret{blk\_qc\_t \bfcode{direct\_make\_request}}{struct bio *\emph{ bio}}{}
hand a buffer directly to its device driver for I/O

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct bio * bio}}] \leavevmode
The bio describing the location in memory and on the device.

\end{description}

\textbf{Description}

This function behaves like {\hyperref[core\string-api/kernel\string-api:c.generic_make_request]{\emph{\code{generic\_make\_request()}}}}, but does not protect
against recursion.  Must only be used if the called driver is known
to not call generic\_make\_request (or direct\_make\_request) again from
its make\_request function.  (Calling direct\_make\_request again from
a workqueue is perfectly fine as that doesn't recurse).
\index{submit\_bio (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.submit_bio}\pysiglinewithargsret{blk\_qc\_t \bfcode{submit\_bio}}{struct bio *\emph{ bio}}{}
submit a bio to the block device layer for I/O

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct bio * bio}}] \leavevmode
The \code{struct bio} which describes the I/O

\end{description}

\textbf{Description}

{\hyperref[core\string-api/kernel\string-api:c.submit_bio]{\emph{\code{submit\_bio()}}}} is very similar in purpose to {\hyperref[core\string-api/kernel\string-api:c.generic_make_request]{\emph{\code{generic\_make\_request()}}}}, and
uses that function to do most of the work. Both are fairly rough
interfaces; \textbf{bio} must be presetup and ready for I/O.
\index{blk\_insert\_cloned\_request (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_insert_cloned_request}\pysiglinewithargsret{blk\_status\_t \bfcode{blk\_insert\_cloned\_request}}{struct request\_queue *\emph{ q}, struct request *\emph{ rq}}{}
Helper for stacking drivers to submit a request

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the queue to submit the request

\item[{\code{struct request * rq}}] \leavevmode
the request being queued

\end{description}
\index{blk\_rq\_err\_bytes (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_rq_err_bytes}\pysiglinewithargsret{unsigned int \bfcode{blk\_rq\_err\_bytes}}{const struct request *\emph{ rq}}{}
determine number of bytes till the next failure boundary

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const struct request * rq}}] \leavevmode
request to examine

\end{description}

\textbf{Description}
\begin{quote}

A request could be merge of IOs which require different failure
handling.  This function determines the number of bytes which
can be failed from the beginning of the request without
crossing into area which need to be retried further.
\end{quote}

\textbf{Return}
\begin{quote}

The number of bytes to fail.
\end{quote}
\index{blk\_peek\_request (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_peek_request}\pysiglinewithargsret{struct request * \bfcode{blk\_peek\_request}}{struct request\_queue *\emph{ q}}{}
peek at the top of a request queue

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
request queue to peek at

\end{description}

\textbf{Description}
\begin{quote}

Return the request at the top of \textbf{q}.  The returned request
should be started using {\hyperref[core\string-api/kernel\string-api:c.blk_start_request]{\emph{\code{blk\_start\_request()}}}} before LLD starts
processing it.
\end{quote}

\textbf{Return}
\begin{quote}

Pointer to the request at the top of \textbf{q} if available.  Null
otherwise.
\end{quote}
\index{blk\_start\_request (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_start_request}\pysiglinewithargsret{void \bfcode{blk\_start\_request}}{struct request *\emph{ req}}{}
start request processing on the driver

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request * req}}] \leavevmode
request to dequeue

\end{description}

\textbf{Description}
\begin{quote}

Dequeue \textbf{req} and start timeout timer on it.  This hands off the
request to the driver.
\end{quote}
\index{blk\_fetch\_request (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_fetch_request}\pysiglinewithargsret{struct request * \bfcode{blk\_fetch\_request}}{struct request\_queue *\emph{ q}}{}
fetch a request from a request queue

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
request queue to fetch a request from

\end{description}

\textbf{Description}
\begin{quote}

Return the request at the top of \textbf{q}.  The request is started on
return and LLD can start processing it immediately.
\end{quote}

\textbf{Return}
\begin{quote}

Pointer to the request at the top of \textbf{q} if available.  Null
otherwise.
\end{quote}
\index{blk\_update\_request (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_update_request}\pysiglinewithargsret{bool \bfcode{blk\_update\_request}}{struct request *\emph{ req}, blk\_status\_t\emph{ error}, unsigned int\emph{ nr\_bytes}}{}
Special helper function for request stacking drivers

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request * req}}] \leavevmode
the request being processed

\item[{\code{blk\_status\_t error}}] \leavevmode
block status code

\item[{\code{unsigned int nr\_bytes}}] \leavevmode
number of bytes to complete \textbf{req}

\end{description}

\textbf{Description}
\begin{quote}

Ends I/O on a number of bytes attached to \textbf{req}, but doesn't complete
the request structure even if \textbf{req} doesn't have leftover.
If \textbf{req} has leftover, sets it up for the next range of segments.

This special helper function is only for request stacking drivers
(e.g. request-based dm) so that they can handle partial completion.
Actual device drivers should use blk\_end\_request instead.

Passing the result of \code{blk\_rq\_bytes()} as \textbf{nr\_bytes} guarantees
\code{false} return from this function.
\end{quote}

\textbf{Return}
\begin{quote}

\code{false} - this request doesn't have any more data
\code{true}  - this request has more data
\end{quote}
\index{blk\_unprep\_request (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_unprep_request}\pysiglinewithargsret{void \bfcode{blk\_unprep\_request}}{struct request *\emph{ req}}{}
unprepare a request

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request * req}}] \leavevmode
the request

\end{description}

\textbf{Description}

This function makes a request ready for complete resubmission (or
completion).  It happens only after all error handling is complete,
so represents the appropriate moment to deallocate any resources
that were allocated to the request in the prep\_rq\_fn.  The queue
lock is held when calling this.
\index{blk\_end\_request (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_end_request}\pysiglinewithargsret{bool \bfcode{blk\_end\_request}}{struct request *\emph{ rq}, blk\_status\_t\emph{ error}, unsigned int\emph{ nr\_bytes}}{}
Helper function for drivers to complete the request.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request * rq}}] \leavevmode
the request being processed

\item[{\code{blk\_status\_t error}}] \leavevmode
block status code

\item[{\code{unsigned int nr\_bytes}}] \leavevmode
number of bytes to complete

\end{description}

\textbf{Description}
\begin{quote}

Ends I/O on a number of bytes attached to \textbf{rq}.
If \textbf{rq} has leftover, sets it up for the next range of segments.
\end{quote}

\textbf{Return}
\begin{quote}

\code{false} - we are done with this request
\code{true}  - still buffers pending for this request
\end{quote}
\index{blk\_end\_request\_all (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_end_request_all}\pysiglinewithargsret{void \bfcode{blk\_end\_request\_all}}{struct request *\emph{ rq}, blk\_status\_t\emph{ error}}{}
Helper function for drives to finish the request.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request * rq}}] \leavevmode
the request to finish

\item[{\code{blk\_status\_t error}}] \leavevmode
block status code

\end{description}

\textbf{Description}
\begin{quote}

Completely finish \textbf{rq}.
\end{quote}
\index{\_\_blk\_end\_request (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__blk_end_request}\pysiglinewithargsret{bool \bfcode{\_\_blk\_end\_request}}{struct request *\emph{ rq}, blk\_status\_t\emph{ error}, unsigned int\emph{ nr\_bytes}}{}
Helper function for drivers to complete the request.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request * rq}}] \leavevmode
the request being processed

\item[{\code{blk\_status\_t error}}] \leavevmode
block status code

\item[{\code{unsigned int nr\_bytes}}] \leavevmode
number of bytes to complete

\end{description}

\textbf{Description}
\begin{quote}

Must be called with queue lock held unlike {\hyperref[core\string-api/kernel\string-api:c.blk_end_request]{\emph{\code{blk\_end\_request()}}}}.
\end{quote}

\textbf{Return}
\begin{quote}

\code{false} - we are done with this request
\code{true}  - still buffers pending for this request
\end{quote}
\index{\_\_blk\_end\_request\_all (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__blk_end_request_all}\pysiglinewithargsret{void \bfcode{\_\_blk\_end\_request\_all}}{struct request *\emph{ rq}, blk\_status\_t\emph{ error}}{}
Helper function for drives to finish the request.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request * rq}}] \leavevmode
the request to finish

\item[{\code{blk\_status\_t error}}] \leavevmode
block status code

\end{description}

\textbf{Description}
\begin{quote}

Completely finish \textbf{rq}.  Must be called with queue lock held.
\end{quote}
\index{\_\_blk\_end\_request\_cur (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__blk_end_request_cur}\pysiglinewithargsret{bool \bfcode{\_\_blk\_end\_request\_cur}}{struct request *\emph{ rq}, blk\_status\_t\emph{ error}}{}
Helper function to finish the current request chunk.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request * rq}}] \leavevmode
the request to finish the current chunk for

\item[{\code{blk\_status\_t error}}] \leavevmode
block status code

\end{description}

\textbf{Description}
\begin{quote}

Complete the current consecutively mapped chunk from \textbf{rq}.  Must
be called with queue lock held.
\end{quote}

\textbf{Return}
\begin{quote}

\code{false} - we are done with this request
\code{true}  - still buffers pending for this request
\end{quote}
\index{rq\_flush\_dcache\_pages (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rq_flush_dcache_pages}\pysiglinewithargsret{void \bfcode{rq\_flush\_dcache\_pages}}{struct request *\emph{ rq}}{}
Helper function to flush all pages in a request

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request * rq}}] \leavevmode
the request to be flushed

\end{description}

\textbf{Description}
\begin{quote}

Flush all pages in \textbf{rq}.
\end{quote}
\index{blk\_lld\_busy (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_lld_busy}\pysiglinewithargsret{int \bfcode{blk\_lld\_busy}}{struct request\_queue *\emph{ q}}{}
Check if underlying low-level drivers of a device are busy

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the queue of the device being checked

\end{description}

\textbf{Description}
\begin{quote}

Check if underlying low-level drivers of a device are busy.
If the drivers want to export their busy state, they must set own
exporting function using \code{blk\_queue\_lld\_busy()} first.

Basically, this function is used only by request stacking drivers
to stop dispatching requests to underlying devices when underlying
devices are busy.  This behavior helps more I/O merging on the queue
of the request stacking driver and prevents I/O throughput regression
on burst I/O load.
\end{quote}

\textbf{Return}
\begin{quote}

0 - Not busy (The request stacking driver should dispatch request)
1 - Busy (The request stacking driver should stop dispatching request)
\end{quote}
\index{blk\_rq\_unprep\_clone (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_rq_unprep_clone}\pysiglinewithargsret{void \bfcode{blk\_rq\_unprep\_clone}}{struct request *\emph{ rq}}{}
Helper function to free all bios in a cloned request

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request * rq}}] \leavevmode
the clone request to be cleaned up

\end{description}

\textbf{Description}
\begin{quote}

Free all bios in \textbf{rq} for a cloned request.
\end{quote}
\index{blk\_rq\_prep\_clone (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_rq_prep_clone}\pysiglinewithargsret{int \bfcode{blk\_rq\_prep\_clone}}{struct request *\emph{ rq}, struct request *\emph{ rq\_src}, struct bio\_set *\emph{ bs}, gfp\_t\emph{ gfp\_mask}, int (*bio\_ctr) (struct bio\emph{ *}, struct bio\emph{ *}, void\emph{ *}, void *\emph{ data}}{}
Helper function to setup clone request

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request * rq}}] \leavevmode
the request to be setup

\item[{\code{struct request * rq\_src}}] \leavevmode
original request to be cloned

\item[{\code{struct bio\_set * bs}}] \leavevmode
bio\_set that bios for clone are allocated from

\item[{\code{gfp\_t gfp\_mask}}] \leavevmode
memory allocation mask for bio

\item[{\code{int (*)(struct bio *, struct bio *, void *) bio\_ctr}}] \leavevmode
setup function to be called for each clone bio.
Returns \code{0} for success, non \code{0} for failure.

\item[{\code{void * data}}] \leavevmode
private data to be passed to \textbf{bio\_ctr}

\end{description}

\textbf{Description}
\begin{quote}

Clones bios in \textbf{rq\_src} to \textbf{rq}, and copies attributes of \textbf{rq\_src} to \textbf{rq}.
The actual data parts of \textbf{rq\_src} (e.g. -\textgreater{}cmd, -\textgreater{}sense)
are not copied, and copying such parts is the caller's responsibility.
Also, pages which the original bios are pointing to are not copied
and the cloned bios just point same pages.
So cloned bios must be completed before original bios, which means
the caller must complete \textbf{rq} before \textbf{rq\_src}.
\end{quote}
\index{blk\_start\_plug (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_start_plug}\pysiglinewithargsret{void \bfcode{blk\_start\_plug}}{struct blk\_plug *\emph{ plug}}{}
initialize blk\_plug and track it inside the task\_struct

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct blk\_plug * plug}}] \leavevmode
The \code{struct blk\_plug} that needs to be initialized

\end{description}

\textbf{Description}
\begin{quote}

Tracking blk\_plug inside the task\_struct will help with auto-flushing the
pending I/O should the task end up blocking between {\hyperref[core\string-api/kernel\string-api:c.blk_start_plug]{\emph{\code{blk\_start\_plug()}}}} and
\code{blk\_finish\_plug()}. This is important from a performance perspective, but
also ensures that we don't deadlock. For instance, if the task is blocking
for a memory allocation, memory reclaim could end up wanting to free a
page belonging to that request that is currently residing in our private
plug. By flushing the pending I/O when the process goes to sleep, we avoid
this kind of deadlock.
\end{quote}
\index{blk\_pm\_runtime\_init (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_pm_runtime_init}\pysiglinewithargsret{void \bfcode{blk\_pm\_runtime\_init}}{struct request\_queue *\emph{ q}, struct device *\emph{ dev}}{}
Block layer runtime PM initialization routine

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the queue of the device

\item[{\code{struct device * dev}}] \leavevmode
the device the queue belongs to

\end{description}

\textbf{Description}
\begin{quote}

Initialize runtime-PM-related fields for \textbf{q} and start auto suspend for
\textbf{dev}. Drivers that want to take advantage of request-based runtime PM
should call this function after \textbf{dev} has been initialized, and its
request queue \textbf{q} has been allocated, and runtime PM for it can not happen
yet(either due to disabled/forbidden or its usage\_count \textgreater{} 0). In most
cases, driver should call this function before any I/O has taken place.

This function takes care of setting up using auto suspend for the device,
the autosuspend delay is set to -1 to make runtime suspend impossible
until an updated value is either set by user or by driver. Drivers do
not need to touch other autosuspend settings.

The block layer runtime PM is request based, so only works for drivers
that use request as their IO unit instead of those directly use bio's.
\end{quote}
\index{blk\_pre\_runtime\_suspend (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_pre_runtime_suspend}\pysiglinewithargsret{int \bfcode{blk\_pre\_runtime\_suspend}}{struct request\_queue *\emph{ q}}{}
Pre runtime suspend check

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the queue of the device

\end{description}

\textbf{Description}
\begin{quote}

This function will check if runtime suspend is allowed for the device
by examining if there are any requests pending in the queue. If there
are requests pending, the device can not be runtime suspended; otherwise,
the queue's status will be updated to SUSPENDING and the driver can
proceed to suspend the device.

For the not allowed case, we mark last busy for the device so that
runtime PM core will try to autosuspend it some time later.

This function should be called near the start of the device's
runtime\_suspend callback.
\end{quote}

\textbf{Return}
\begin{quote}

0         - OK to runtime suspend the device
-EBUSY    - Device should not be runtime suspended
\end{quote}
\index{blk\_post\_runtime\_suspend (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_post_runtime_suspend}\pysiglinewithargsret{void \bfcode{blk\_post\_runtime\_suspend}}{struct request\_queue *\emph{ q}, int\emph{ err}}{}
Post runtime suspend processing

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the queue of the device

\item[{\code{int err}}] \leavevmode
return value of the device's runtime\_suspend function

\end{description}

\textbf{Description}
\begin{quote}

Update the queue's runtime status according to the return value of the
device's runtime suspend function and mark last busy for the device so
that PM core will try to auto suspend the device at a later time.

This function should be called near the end of the device's
runtime\_suspend callback.
\end{quote}
\index{blk\_pre\_runtime\_resume (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_pre_runtime_resume}\pysiglinewithargsret{void \bfcode{blk\_pre\_runtime\_resume}}{struct request\_queue *\emph{ q}}{}
Pre runtime resume processing

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the queue of the device

\end{description}

\textbf{Description}
\begin{quote}

Update the queue's runtime status to RESUMING in preparation for the
runtime resume of the device.

This function should be called near the start of the device's
runtime\_resume callback.
\end{quote}
\index{blk\_post\_runtime\_resume (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_post_runtime_resume}\pysiglinewithargsret{void \bfcode{blk\_post\_runtime\_resume}}{struct request\_queue *\emph{ q}, int\emph{ err}}{}
Post runtime resume processing

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the queue of the device

\item[{\code{int err}}] \leavevmode
return value of the device's runtime\_resume function

\end{description}

\textbf{Description}
\begin{quote}

Update the queue's runtime status according to the return value of the
device's runtime\_resume function. If it is successfully resumed, process
the requests that are queued into the device's queue when it is resuming
and then mark last busy and initiate autosuspend for it.

This function should be called near the end of the device's
runtime\_resume callback.
\end{quote}
\index{blk\_set\_runtime\_active (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_set_runtime_active}\pysiglinewithargsret{void \bfcode{blk\_set\_runtime\_active}}{struct request\_queue *\emph{ q}}{}
Force runtime status of the queue to be active

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the queue of the device

\end{description}

\textbf{Description}

If the device is left runtime suspended during system suspend the resume
hook typically resumes the device and corrects runtime status
accordingly. However, that does not affect the queue runtime PM status
which is still ``suspended''. This prevents processing requests from the
queue.

This function can be used in driver's resume hook to correct queue
runtime PM status and re-enable peeking requests from the queue. It
should be called before first request is added to the queue.
\index{\_\_blk\_drain\_queue (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__blk_drain_queue}\pysiglinewithargsret{void \bfcode{\_\_blk\_drain\_queue}}{struct request\_queue *\emph{ q}, bool\emph{ drain\_all}}{}
drain requests from request\_queue

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
queue to drain

\item[{\code{bool drain\_all}}] \leavevmode
whether to drain all requests or only the ones w/ ELVPRIV

\end{description}

\textbf{Description}

Drain requests from \textbf{q}.  If \textbf{drain\_all} is set, all requests are drained.
If not, only ELVPRIV requests are drained.  The caller is responsible
for ensuring that no new requests which need to be drained are queued.
\index{blk\_queue\_enter (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_queue_enter}\pysiglinewithargsret{int \bfcode{blk\_queue\_enter}}{struct request\_queue *\emph{ q}, blk\_mq\_req\_flags\_t\emph{ flags}}{}
try to increase q-\textgreater{}q\_usage\_counter

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
request queue pointer

\item[{\code{blk\_mq\_req\_flags\_t flags}}] \leavevmode
BLK\_MQ\_REQ\_NOWAIT and/or BLK\_MQ\_REQ\_PREEMPT

\end{description}
\index{\_\_get\_request (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__get_request}\pysiglinewithargsret{struct request * \bfcode{\_\_get\_request}}{struct request\_list *\emph{ rl}, unsigned int\emph{ op}, struct bio *\emph{ bio}, blk\_mq\_req\_flags\_t\emph{ flags}}{}
get a free request

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_list * rl}}] \leavevmode
request list to allocate from

\item[{\code{unsigned int op}}] \leavevmode
operation and flags

\item[{\code{struct bio * bio}}] \leavevmode
bio to allocate request for (can be \code{NULL})

\item[{\code{blk\_mq\_req\_flags\_t flags}}] \leavevmode
BLQ\_MQ\_REQ\_* flags

\end{description}

\textbf{Description}

Get a free request from \textbf{q}.  This function may fail under memory
pressure or if \textbf{q} is dead.

Must be called with \textbf{q}-\textgreater{}queue\_lock held and,
Returns ERR\_PTR on failure, with \textbf{q}-\textgreater{}queue\_lock held.
Returns request pointer on success, with \textbf{q}-\textgreater{}queue\_lock \emph{not held}.
\index{get\_request (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.get_request}\pysiglinewithargsret{struct request * \bfcode{get\_request}}{struct request\_queue *\emph{ q}, unsigned int\emph{ op}, struct bio *\emph{ bio}, blk\_mq\_req\_flags\_t\emph{ flags}}{}
get a free request

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
request\_queue to allocate request from

\item[{\code{unsigned int op}}] \leavevmode
operation and flags

\item[{\code{struct bio * bio}}] \leavevmode
bio to allocate request for (can be \code{NULL})

\item[{\code{blk\_mq\_req\_flags\_t flags}}] \leavevmode
BLK\_MQ\_REQ\_* flags.

\end{description}

\textbf{Description}

Get a free request from \textbf{q}.  If \code{\_\_GFP\_DIRECT\_RECLAIM} is set in \textbf{gfp\_mask},
this function keeps retrying under memory pressure and fails iff \textbf{q} is dead.

Must be called with \textbf{q}-\textgreater{}queue\_lock held and,
Returns ERR\_PTR on failure, with \textbf{q}-\textgreater{}queue\_lock held.
Returns request pointer on success, with \textbf{q}-\textgreater{}queue\_lock \emph{not held}.
\index{blk\_attempt\_plug\_merge (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_attempt_plug_merge}\pysiglinewithargsret{bool \bfcode{blk\_attempt\_plug\_merge}}{struct request\_queue *\emph{ q}, struct bio *\emph{ bio}, unsigned int *\emph{ request\_count}, struct request **\emph{ same\_queue\_rq}}{}
try to merge with \code{current}`s plugged list

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
request\_queue new bio is being queued at

\item[{\code{struct bio * bio}}] \leavevmode
new bio being queued

\item[{\code{unsigned int * request\_count}}] \leavevmode
out parameter for number of traversed plugged requests

\item[{\code{struct request ** same\_queue\_rq}}] \leavevmode
pointer to \code{struct request} that gets filled in when
another request associated with \textbf{q} is found on the plug list
(optional, may be \code{NULL})

\end{description}

\textbf{Description}

Determine whether \textbf{bio} being queued on \textbf{q} can be merged with a request
on \code{current}`s plugged list.  Returns \code{true} if merge was successful,
otherwise \code{false}.

Plugging coalesces IOs from the same issuer for the same purpose without
going through \textbf{q}-\textgreater{}queue\_lock.  As such it's more of an issuing mechanism
than scheduling, and the request, while may have elvpriv data, is not
added on the elevator at this point.  In addition, we don't have
reliable access to the elevator outside queue lock.  Only check basic
merging parameters without querying the elevator.

Caller must ensure !blk\_queue\_nomerges(q) beforehand.
\index{blk\_cloned\_rq\_check\_limits (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_cloned_rq_check_limits}\pysiglinewithargsret{int \bfcode{blk\_cloned\_rq\_check\_limits}}{struct request\_queue *\emph{ q}, struct request *\emph{ rq}}{}
Helper function to check a cloned request for new the queue limits

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the queue

\item[{\code{struct request * rq}}] \leavevmode
the request being checked

\end{description}

\textbf{Description}
\begin{quote}

\textbf{rq} may have been made based on weaker limitations of upper-level queues
in request stacking drivers, and it may violate the limitation of \textbf{q}.
Since the block layer and the underlying device driver trust \textbf{rq}
after it is inserted to \textbf{q}, it should be checked against \textbf{q} before
the insertion using this generic function.

Request stacking drivers like request-based dm may change the queue
limits when retrying requests on other queues. Those requests need
to be checked against the new queue limits again during dispatch.
\end{quote}
\index{blk\_end\_bidi\_request (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_end_bidi_request}\pysiglinewithargsret{bool \bfcode{blk\_end\_bidi\_request}}{struct request *\emph{ rq}, blk\_status\_t\emph{ error}, unsigned int\emph{ nr\_bytes}, unsigned int\emph{ bidi\_bytes}}{}
Complete a bidi request

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request * rq}}] \leavevmode
the request to complete

\item[{\code{blk\_status\_t error}}] \leavevmode
block status code

\item[{\code{unsigned int nr\_bytes}}] \leavevmode
number of bytes to complete \textbf{rq}

\item[{\code{unsigned int bidi\_bytes}}] \leavevmode
number of bytes to complete \textbf{rq}-\textgreater{}next\_rq

\end{description}

\textbf{Description}
\begin{quote}

Ends I/O on a number of bytes attached to \textbf{rq} and \textbf{rq}-\textgreater{}next\_rq.
Drivers that supports bidi can safely call this member for any
type of request, bidi or uni.  In the later case \textbf{bidi\_bytes} is
just ignored.
\end{quote}

\textbf{Return}
\begin{quote}

\code{false} - we are done with this request
\code{true}  - still buffers pending for this request
\end{quote}
\index{\_\_blk\_end\_bidi\_request (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__blk_end_bidi_request}\pysiglinewithargsret{bool \bfcode{\_\_blk\_end\_bidi\_request}}{struct request *\emph{ rq}, blk\_status\_t\emph{ error}, unsigned int\emph{ nr\_bytes}, unsigned int\emph{ bidi\_bytes}}{}
Complete a bidi request with queue lock held

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request * rq}}] \leavevmode
the request to complete

\item[{\code{blk\_status\_t error}}] \leavevmode
block status code

\item[{\code{unsigned int nr\_bytes}}] \leavevmode
number of bytes to complete \textbf{rq}

\item[{\code{unsigned int bidi\_bytes}}] \leavevmode
number of bytes to complete \textbf{rq}-\textgreater{}next\_rq

\end{description}

\textbf{Description}
\begin{quote}

Identical to {\hyperref[core\string-api/kernel\string-api:c.blk_end_bidi_request]{\emph{\code{blk\_end\_bidi\_request()}}}} except that queue lock is
assumed to be locked on entry and remains so on return.
\end{quote}

\textbf{Return}
\begin{quote}

\code{false} - we are done with this request
\code{true}  - still buffers pending for this request
\end{quote}
\index{blk\_rq\_map\_user\_iov (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_rq_map_user_iov}\pysiglinewithargsret{int \bfcode{blk\_rq\_map\_user\_iov}}{struct request\_queue *\emph{ q}, struct request *\emph{ rq}, struct rq\_map\_data *\emph{ map\_data}, const struct iov\_iter *\emph{ iter}, gfp\_t\emph{ gfp\_mask}}{}
map user data to a request, for passthrough requests

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
request queue where request should be inserted

\item[{\code{struct request * rq}}] \leavevmode
request to map data to

\item[{\code{struct rq\_map\_data * map\_data}}] \leavevmode
pointer to the rq\_map\_data holding pages (if necessary)

\item[{\code{const struct iov\_iter * iter}}] \leavevmode
iovec iterator

\item[{\code{gfp\_t gfp\_mask}}] \leavevmode
memory allocation flags

\end{description}

\textbf{Description}
\begin{quote}

Data will be mapped directly for zero copy I/O, if possible. Otherwise
a kernel bounce buffer is used.

A matching {\hyperref[core\string-api/kernel\string-api:c.blk_rq_unmap_user]{\emph{\code{blk\_rq\_unmap\_user()}}}} must be issued at the end of I/O, while
still in process context.
\end{quote}

\textbf{Note}
\begin{description}
\item[{The mapped bio may need to be bounced through \code{blk\_queue\_bounce()}}] \leavevmode
before being submitted to the device, as pages mapped may be out of
reach. It's the callers responsibility to make sure this happens. The
original bio must be passed back in to {\hyperref[core\string-api/kernel\string-api:c.blk_rq_unmap_user]{\emph{\code{blk\_rq\_unmap\_user()}}}} for proper
unmapping.

\end{description}
\index{blk\_rq\_unmap\_user (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_rq_unmap_user}\pysiglinewithargsret{int \bfcode{blk\_rq\_unmap\_user}}{struct bio *\emph{ bio}}{}
unmap a request with user data

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct bio * bio}}] \leavevmode
start of bio list

\end{description}

\textbf{Description}
\begin{quote}

Unmap a rq previously mapped by \code{blk\_rq\_map\_user()}. The caller must
supply the original rq-\textgreater{}bio from the \code{blk\_rq\_map\_user()} return, since
the I/O completion may have changed rq-\textgreater{}bio.
\end{quote}
\index{blk\_rq\_map\_kern (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_rq_map_kern}\pysiglinewithargsret{int \bfcode{blk\_rq\_map\_kern}}{struct request\_queue *\emph{ q}, struct request *\emph{ rq}, void *\emph{ kbuf}, unsigned int\emph{ len}, gfp\_t\emph{ gfp\_mask}}{}
map kernel data to a request, for passthrough requests

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
request queue where request should be inserted

\item[{\code{struct request * rq}}] \leavevmode
request to fill

\item[{\code{void * kbuf}}] \leavevmode
the kernel buffer

\item[{\code{unsigned int len}}] \leavevmode
length of user data

\item[{\code{gfp\_t gfp\_mask}}] \leavevmode
memory allocation flags

\end{description}

\textbf{Description}
\begin{quote}

Data will be mapped directly if possible. Otherwise a bounce
buffer is used. Can be called multiple times to append multiple
buffers.
\end{quote}
\index{\_\_blk\_release\_queue (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__blk_release_queue}\pysiglinewithargsret{void \bfcode{\_\_blk\_release\_queue}}{struct work\_struct *\emph{ work}}{}
release a request queue when it is no longer needed

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct work\_struct * work}}] \leavevmode
pointer to the release\_work member of the request queue to be released

\end{description}

\textbf{Description}
\begin{quote}

blk\_release\_queue is the counterpart of {\hyperref[core\string-api/kernel\string-api:c.blk_init_queue]{\emph{\code{blk\_init\_queue()}}}}. It should be
called when a request queue is being released; typically when a block
device is being de-registered. Its primary task it to free the queue
itself.
\end{quote}

\textbf{Notes}
\begin{quote}

The low level driver must have finished any outstanding requests first
via {\hyperref[core\string-api/kernel\string-api:c.blk_cleanup_queue]{\emph{\code{blk\_cleanup\_queue()}}}}.

Although \code{blk\_release\_queue()} may be called with preemption disabled,
{\hyperref[core\string-api/kernel\string-api:c.__blk_release_queue]{\emph{\code{\_\_blk\_release\_queue()}}}} may sleep.
\end{quote}
\index{blk\_unregister\_queue (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_unregister_queue}\pysiglinewithargsret{void \bfcode{blk\_unregister\_queue}}{struct gendisk *\emph{ disk}}{}
counterpart of \code{blk\_register\_queue()}

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct gendisk * disk}}] \leavevmode
Disk of which the request queue should be unregistered from sysfs.

\end{description}

\textbf{Note}

the caller is responsible for guaranteeing that this function is called
after \code{blk\_register\_queue()} has finished.
\index{blk\_queue\_prep\_rq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_queue_prep_rq}\pysiglinewithargsret{void \bfcode{blk\_queue\_prep\_rq}}{struct request\_queue *\emph{ q}, prep\_rq\_fn *\emph{ pfn}}{}
set a prepare\_request function for queue

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
queue

\item[{\code{prep\_rq\_fn * pfn}}] \leavevmode
prepare\_request function

\end{description}

\textbf{Description}

It's possible for a queue to register a prepare\_request callback which
is invoked before the request is handed to the request\_fn. The goal of
the function is to prepare a request for I/O, it can be used to build a
cdb from the request data for instance.
\index{blk\_queue\_unprep\_rq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_queue_unprep_rq}\pysiglinewithargsret{void \bfcode{blk\_queue\_unprep\_rq}}{struct request\_queue *\emph{ q}, unprep\_rq\_fn *\emph{ ufn}}{}
set an unprepare\_request function for queue

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
queue

\item[{\code{unprep\_rq\_fn * ufn}}] \leavevmode
unprepare\_request function

\end{description}

\textbf{Description}

It's possible for a queue to register an unprepare\_request callback
which is invoked before the request is finally completed. The goal
of the function is to deallocate any data that was allocated in the
prepare\_request callback.
\index{blk\_set\_default\_limits (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_set_default_limits}\pysiglinewithargsret{void \bfcode{blk\_set\_default\_limits}}{struct queue\_limits *\emph{ lim}}{}
reset limits to default values

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct queue\_limits * lim}}] \leavevmode
the queue\_limits structure to reset

\end{description}

\textbf{Description}
\begin{quote}

Returns a queue\_limit struct to its default state.
\end{quote}
\index{blk\_set\_stacking\_limits (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_set_stacking_limits}\pysiglinewithargsret{void \bfcode{blk\_set\_stacking\_limits}}{struct queue\_limits *\emph{ lim}}{}
set default limits for stacking devices

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct queue\_limits * lim}}] \leavevmode
the queue\_limits structure to reset

\end{description}

\textbf{Description}
\begin{quote}

Returns a queue\_limit struct to its default state. Should be used
by stacking drivers like DM that have no internal limits.
\end{quote}
\index{blk\_queue\_make\_request (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_queue_make_request}\pysiglinewithargsret{void \bfcode{blk\_queue\_make\_request}}{struct request\_queue *\emph{ q}, make\_request\_fn *\emph{ mfn}}{}
define an alternate make\_request function for a device

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the request queue for the device to be affected

\item[{\code{make\_request\_fn * mfn}}] \leavevmode
the alternate make\_request function

\end{description}

\textbf{Description}
\begin{quote}

The normal way for \code{struct bios} to be passed to a device
driver is for them to be collected into requests on a request
queue, and then to allow the device driver to select requests
off that queue when it is ready.  This works well for many block
devices. However some block devices (typically virtual devices
such as md or lvm) do not benefit from the processing on the
request queue, and are served best by having the requests passed
directly to them.  This can be achieved by providing a function
to {\hyperref[core\string-api/kernel\string-api:c.blk_queue_make_request]{\emph{\code{blk\_queue\_make\_request()}}}}.
\end{quote}
\begin{description}
\item[{Caveat:}] \leavevmode
The driver that does this \emph{must} be able to deal appropriately
with buffers in ``highmemory''. This can be accomplished by either calling
\code{kmap\_atomic()} to get a temporary kernel mapping, or by calling
\code{blk\_queue\_bounce()} to create a buffer in normal memory.

\end{description}
\index{blk\_queue\_bounce\_limit (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_queue_bounce_limit}\pysiglinewithargsret{void \bfcode{blk\_queue\_bounce\_limit}}{struct request\_queue *\emph{ q}, u64\emph{ max\_addr}}{}
set bounce buffer limit for queue

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the request queue for the device

\item[{\code{u64 max\_addr}}] \leavevmode
the maximum address the device can handle

\end{description}

\textbf{Description}
\begin{quote}

Different hardware can have different requirements as to what pages
it can do I/O directly to. A low level driver can call
blk\_queue\_bounce\_limit to have lower memory pages allocated as bounce
buffers for doing I/O to pages residing above \textbf{max\_addr}.
\end{quote}
\index{blk\_queue\_max\_hw\_sectors (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_queue_max_hw_sectors}\pysiglinewithargsret{void \bfcode{blk\_queue\_max\_hw\_sectors}}{struct request\_queue *\emph{ q}, unsigned int\emph{ max\_hw\_sectors}}{}
set max sectors for a request for this queue

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the request queue for the device

\item[{\code{unsigned int max\_hw\_sectors}}] \leavevmode
max hardware sectors in the usual 512b unit

\end{description}

\textbf{Description}
\begin{quote}

Enables a low level driver to set a hard upper limit,
max\_hw\_sectors, on the size of requests.  max\_hw\_sectors is set by
the device driver based upon the capabilities of the I/O
controller.

max\_dev\_sectors is a hard limit imposed by the storage device for
READ/WRITE requests. It is set by the disk driver.

max\_sectors is a soft limit imposed by the block layer for
filesystem type requests.  This value can be overridden on a
per-device basis in /sys/block/\textless{}device\textgreater{}/queue/max\_sectors\_kb.
The soft limit can not exceed max\_hw\_sectors.
\end{quote}
\index{blk\_queue\_chunk\_sectors (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_queue_chunk_sectors}\pysiglinewithargsret{void \bfcode{blk\_queue\_chunk\_sectors}}{struct request\_queue *\emph{ q}, unsigned int\emph{ chunk\_sectors}}{}
set size of the chunk for this queue

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the request queue for the device

\item[{\code{unsigned int chunk\_sectors}}] \leavevmode
chunk sectors in the usual 512b unit

\end{description}

\textbf{Description}
\begin{quote}

If a driver doesn't want IOs to cross a given chunk size, it can set
this limit and prevent merging across chunks. Note that the chunk size
must currently be a power-of-2 in sectors. Also note that the block
layer must accept a page worth of data at any offset. So if the
crossing of chunks is a hard limitation in the driver, it must still be
prepared to split single page bios.
\end{quote}
\index{blk\_queue\_max\_discard\_sectors (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_queue_max_discard_sectors}\pysiglinewithargsret{void \bfcode{blk\_queue\_max\_discard\_sectors}}{struct request\_queue *\emph{ q}, unsigned int\emph{ max\_discard\_sectors}}{}
set max sectors for a single discard

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the request queue for the device

\item[{\code{unsigned int max\_discard\_sectors}}] \leavevmode
maximum number of sectors to discard

\end{description}
\index{blk\_queue\_max\_write\_same\_sectors (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_queue_max_write_same_sectors}\pysiglinewithargsret{void \bfcode{blk\_queue\_max\_write\_same\_sectors}}{struct request\_queue *\emph{ q}, unsigned int\emph{ max\_write\_same\_sectors}}{}
set max sectors for a single write same

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the request queue for the device

\item[{\code{unsigned int max\_write\_same\_sectors}}] \leavevmode
maximum number of sectors to write per command

\end{description}
\index{blk\_queue\_max\_write\_zeroes\_sectors (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_queue_max_write_zeroes_sectors}\pysiglinewithargsret{void \bfcode{blk\_queue\_max\_write\_zeroes\_sectors}}{struct request\_queue *\emph{ q}, unsigned int\emph{ max\_write\_zeroes\_sectors}}{}
set max sectors for a single write zeroes

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the request queue for the device

\item[{\code{unsigned int max\_write\_zeroes\_sectors}}] \leavevmode
maximum number of sectors to write per command

\end{description}
\index{blk\_queue\_max\_segments (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_queue_max_segments}\pysiglinewithargsret{void \bfcode{blk\_queue\_max\_segments}}{struct request\_queue *\emph{ q}, unsigned short\emph{ max\_segments}}{}
set max hw segments for a request for this queue

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the request queue for the device

\item[{\code{unsigned short max\_segments}}] \leavevmode
max number of segments

\end{description}

\textbf{Description}
\begin{quote}

Enables a low level driver to set an upper limit on the number of
hw data segments in a request.
\end{quote}
\index{blk\_queue\_max\_discard\_segments (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_queue_max_discard_segments}\pysiglinewithargsret{void \bfcode{blk\_queue\_max\_discard\_segments}}{struct request\_queue *\emph{ q}, unsigned short\emph{ max\_segments}}{}
set max segments for discard requests

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the request queue for the device

\item[{\code{unsigned short max\_segments}}] \leavevmode
max number of segments

\end{description}

\textbf{Description}
\begin{quote}

Enables a low level driver to set an upper limit on the number of
segments in a discard request.
\end{quote}
\index{blk\_queue\_max\_segment\_size (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_queue_max_segment_size}\pysiglinewithargsret{void \bfcode{blk\_queue\_max\_segment\_size}}{struct request\_queue *\emph{ q}, unsigned int\emph{ max\_size}}{}
set max segment size for blk\_rq\_map\_sg

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the request queue for the device

\item[{\code{unsigned int max\_size}}] \leavevmode
max size of segment in bytes

\end{description}

\textbf{Description}
\begin{quote}

Enables a low level driver to set an upper limit on the size of a
coalesced segment
\end{quote}
\index{blk\_queue\_logical\_block\_size (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_queue_logical_block_size}\pysiglinewithargsret{void \bfcode{blk\_queue\_logical\_block\_size}}{struct request\_queue *\emph{ q}, unsigned short\emph{ size}}{}
set logical block size for the queue

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the request queue for the device

\item[{\code{unsigned short size}}] \leavevmode
the logical block size, in bytes

\end{description}

\textbf{Description}
\begin{quote}

This should be set to the lowest possible block size that the
storage device can address.  The default of 512 covers most
hardware.
\end{quote}
\index{blk\_queue\_physical\_block\_size (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_queue_physical_block_size}\pysiglinewithargsret{void \bfcode{blk\_queue\_physical\_block\_size}}{struct request\_queue *\emph{ q}, unsigned int\emph{ size}}{}
set physical block size for the queue

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the request queue for the device

\item[{\code{unsigned int size}}] \leavevmode
the physical block size, in bytes

\end{description}

\textbf{Description}
\begin{quote}

This should be set to the lowest possible sector size that the
hardware can operate on without reverting to read-modify-write
operations.
\end{quote}
\index{blk\_queue\_alignment\_offset (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_queue_alignment_offset}\pysiglinewithargsret{void \bfcode{blk\_queue\_alignment\_offset}}{struct request\_queue *\emph{ q}, unsigned int\emph{ offset}}{}
set physical block alignment offset

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the request queue for the device

\item[{\code{unsigned int offset}}] \leavevmode
alignment offset in bytes

\end{description}

\textbf{Description}
\begin{quote}

Some devices are naturally misaligned to compensate for things like
the legacy DOS partition table 63-sector offset.  Low-level drivers
should call this function for devices whose first sector is not
naturally aligned.
\end{quote}
\index{blk\_limits\_io\_min (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_limits_io_min}\pysiglinewithargsret{void \bfcode{blk\_limits\_io\_min}}{struct queue\_limits *\emph{ limits}, unsigned int\emph{ min}}{}
set minimum request size for a device

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct queue\_limits * limits}}] \leavevmode
the queue limits

\item[{\code{unsigned int min}}] \leavevmode
smallest I/O size in bytes

\end{description}

\textbf{Description}
\begin{quote}

Some devices have an internal block size bigger than the reported
hardware sector size.  This function can be used to signal the
smallest I/O the device can perform without incurring a performance
penalty.
\end{quote}
\index{blk\_queue\_io\_min (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_queue_io_min}\pysiglinewithargsret{void \bfcode{blk\_queue\_io\_min}}{struct request\_queue *\emph{ q}, unsigned int\emph{ min}}{}
set minimum request size for the queue

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the request queue for the device

\item[{\code{unsigned int min}}] \leavevmode
smallest I/O size in bytes

\end{description}

\textbf{Description}
\begin{quote}

Storage devices may report a granularity or preferred minimum I/O
size which is the smallest request the device can perform without
incurring a performance penalty.  For disk drives this is often the
physical block size.  For RAID arrays it is often the stripe chunk
size.  A properly aligned multiple of minimum\_io\_size is the
preferred request size for workloads where a high number of I/O
operations is desired.
\end{quote}
\index{blk\_limits\_io\_opt (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_limits_io_opt}\pysiglinewithargsret{void \bfcode{blk\_limits\_io\_opt}}{struct queue\_limits *\emph{ limits}, unsigned int\emph{ opt}}{}
set optimal request size for a device

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct queue\_limits * limits}}] \leavevmode
the queue limits

\item[{\code{unsigned int opt}}] \leavevmode
smallest I/O size in bytes

\end{description}

\textbf{Description}
\begin{quote}

Storage devices may report an optimal I/O size, which is the
device's preferred unit for sustained I/O.  This is rarely reported
for disk drives.  For RAID arrays it is usually the stripe width or
the internal track size.  A properly aligned multiple of
optimal\_io\_size is the preferred request size for workloads where
sustained throughput is desired.
\end{quote}
\index{blk\_queue\_io\_opt (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_queue_io_opt}\pysiglinewithargsret{void \bfcode{blk\_queue\_io\_opt}}{struct request\_queue *\emph{ q}, unsigned int\emph{ opt}}{}
set optimal request size for the queue

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the request queue for the device

\item[{\code{unsigned int opt}}] \leavevmode
optimal request size in bytes

\end{description}

\textbf{Description}
\begin{quote}

Storage devices may report an optimal I/O size, which is the
device's preferred unit for sustained I/O.  This is rarely reported
for disk drives.  For RAID arrays it is usually the stripe width or
the internal track size.  A properly aligned multiple of
optimal\_io\_size is the preferred request size for workloads where
sustained throughput is desired.
\end{quote}
\index{blk\_queue\_stack\_limits (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_queue_stack_limits}\pysiglinewithargsret{void \bfcode{blk\_queue\_stack\_limits}}{struct request\_queue *\emph{ t}, struct request\_queue *\emph{ b}}{}
inherit underlying queue limits for stacked drivers

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * t}}] \leavevmode
the stacking driver (top)

\item[{\code{struct request\_queue * b}}] \leavevmode
the underlying device (bottom)

\end{description}
\index{blk\_stack\_limits (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_stack_limits}\pysiglinewithargsret{int \bfcode{blk\_stack\_limits}}{struct queue\_limits *\emph{ t}, struct queue\_limits *\emph{ b}, sector\_t\emph{ start}}{}
adjust queue\_limits for stacked devices

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct queue\_limits * t}}] \leavevmode
the stacking driver limits (top device)

\item[{\code{struct queue\_limits * b}}] \leavevmode
the underlying queue limits (bottom, component device)

\item[{\code{sector\_t start}}] \leavevmode
first data sector within component device

\end{description}

\textbf{Description}
\begin{quote}

This function is used by stacking drivers like MD and DM to ensure
that all component devices have compatible block sizes and
alignments.  The stacking driver must provide a queue\_limits
struct (top) and then iteratively call the stacking function for
all component (bottom) devices.  The stacking function will
attempt to combine the values and ensure proper alignment.

Returns 0 if the top and bottom queue\_limits are compatible.  The
top device's block sizes and alignment offsets may be adjusted to
ensure alignment with the bottom device. If no compatible sizes
and alignments exist, -1 is returned and the resulting top
queue\_limits will have the misaligned flag set to indicate that
the alignment\_offset is undefined.
\end{quote}
\index{bdev\_stack\_limits (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.bdev_stack_limits}\pysiglinewithargsret{int \bfcode{bdev\_stack\_limits}}{struct queue\_limits *\emph{ t}, struct block\_device *\emph{ bdev}, sector\_t\emph{ start}}{}
adjust queue limits for stacked drivers

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct queue\_limits * t}}] \leavevmode
the stacking driver limits (top device)

\item[{\code{struct block\_device * bdev}}] \leavevmode
the component block\_device (bottom)

\item[{\code{sector\_t start}}] \leavevmode
first data sector within component device

\end{description}

\textbf{Description}
\begin{quote}

Merges queue limits for a top device and a block\_device.  Returns
0 if alignment didn't change.  Returns -1 if adding the bottom
device caused misalignment.
\end{quote}
\index{disk\_stack\_limits (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.disk_stack_limits}\pysiglinewithargsret{void \bfcode{disk\_stack\_limits}}{struct gendisk *\emph{ disk}, struct block\_device *\emph{ bdev}, sector\_t\emph{ offset}}{}
adjust queue limits for stacked drivers

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct gendisk * disk}}] \leavevmode
MD/DM gendisk (top)

\item[{\code{struct block\_device * bdev}}] \leavevmode
the underlying block device (bottom)

\item[{\code{sector\_t offset}}] \leavevmode
offset to beginning of data within component device

\end{description}

\textbf{Description}
\begin{quote}

Merges the limits for a top level gendisk and a bottom level
block\_device.
\end{quote}
\index{blk\_queue\_dma\_pad (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_queue_dma_pad}\pysiglinewithargsret{void \bfcode{blk\_queue\_dma\_pad}}{struct request\_queue *\emph{ q}, unsigned int\emph{ mask}}{}
set pad mask

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the request queue for the device

\item[{\code{unsigned int mask}}] \leavevmode
pad mask

\end{description}

\textbf{Description}

Set dma pad mask.

Appending pad buffer to a request modifies the last entry of a
scatter list such that it includes the pad buffer.
\index{blk\_queue\_update\_dma\_pad (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_queue_update_dma_pad}\pysiglinewithargsret{void \bfcode{blk\_queue\_update\_dma\_pad}}{struct request\_queue *\emph{ q}, unsigned int\emph{ mask}}{}
update pad mask

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the request queue for the device

\item[{\code{unsigned int mask}}] \leavevmode
pad mask

\end{description}

\textbf{Description}

Update dma pad mask.

Appending pad buffer to a request modifies the last entry of a
scatter list such that it includes the pad buffer.
\index{blk\_queue\_dma\_drain (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_queue_dma_drain}\pysiglinewithargsret{int \bfcode{blk\_queue\_dma\_drain}}{struct request\_queue *\emph{ q}, dma\_drain\_needed\_fn *\emph{ dma\_drain\_needed}, void *\emph{ buf}, unsigned int\emph{ size}}{}
Set up a drain buffer for excess dma.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the request queue for the device

\item[{\code{dma\_drain\_needed\_fn * dma\_drain\_needed}}] \leavevmode
fn which returns non-zero if drain is necessary

\item[{\code{void * buf}}] \leavevmode
physically contiguous buffer

\item[{\code{unsigned int size}}] \leavevmode
size of the buffer in bytes

\end{description}

\textbf{Description}

Some devices have excess DMA problems and can't simply discard (or
zero fill) the unwanted piece of the transfer.  They have to have a
real area of memory to transfer it into.  The use case for this is
ATAPI devices in DMA mode.  If the packet command causes a transfer
bigger than the transfer size some HBAs will lock up if there
aren't DMA elements to contain the excess transfer.  What this API
does is adjust the queue so that the buf is always appended
silently to the scatterlist.

\textbf{Note}

This routine adjusts max\_hw\_segments to make room for appending
the drain buffer.  If you call {\hyperref[core\string-api/kernel\string-api:c.blk_queue_max_segments]{\emph{\code{blk\_queue\_max\_segments()}}}} after calling
this routine, you must set the limit to one fewer than your device
can support otherwise there won't be room for the drain buffer.
\index{blk\_queue\_segment\_boundary (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_queue_segment_boundary}\pysiglinewithargsret{void \bfcode{blk\_queue\_segment\_boundary}}{struct request\_queue *\emph{ q}, unsigned long\emph{ mask}}{}
set boundary rules for segment merging

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the request queue for the device

\item[{\code{unsigned long mask}}] \leavevmode
the memory boundary mask

\end{description}
\index{blk\_queue\_virt\_boundary (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_queue_virt_boundary}\pysiglinewithargsret{void \bfcode{blk\_queue\_virt\_boundary}}{struct request\_queue *\emph{ q}, unsigned long\emph{ mask}}{}
set boundary rules for bio merging

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the request queue for the device

\item[{\code{unsigned long mask}}] \leavevmode
the memory boundary mask

\end{description}
\index{blk\_queue\_dma\_alignment (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_queue_dma_alignment}\pysiglinewithargsret{void \bfcode{blk\_queue\_dma\_alignment}}{struct request\_queue *\emph{ q}, int\emph{ mask}}{}
set dma length and memory alignment

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the request queue for the device

\item[{\code{int mask}}] \leavevmode
alignment mask

\end{description}

\textbf{Description}
\begin{quote}

set required memory and length alignment for direct dma transactions.
this is used when building direct io requests for the queue.
\end{quote}
\index{blk\_queue\_update\_dma\_alignment (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_queue_update_dma_alignment}\pysiglinewithargsret{void \bfcode{blk\_queue\_update\_dma\_alignment}}{struct request\_queue *\emph{ q}, int\emph{ mask}}{}
update dma length and memory alignment

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the request queue for the device

\item[{\code{int mask}}] \leavevmode
alignment mask

\end{description}

\textbf{Description}
\begin{quote}

update required memory and length alignment for direct dma transactions.
If the requested alignment is larger than the current alignment, then
the current queue alignment is updated to the new value, otherwise it
is left alone.  The design of this is to allow multiple objects
(driver, device, transport etc) to set their respective
alignments without having them interfere.
\end{quote}
\index{blk\_set\_queue\_depth (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_set_queue_depth}\pysiglinewithargsret{void \bfcode{blk\_set\_queue\_depth}}{struct request\_queue *\emph{ q}, unsigned int\emph{ depth}}{}
tell the block layer about the device queue depth

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the request queue for the device

\item[{\code{unsigned int depth}}] \leavevmode
queue depth

\end{description}
\index{blk\_queue\_write\_cache (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_queue_write_cache}\pysiglinewithargsret{void \bfcode{blk\_queue\_write\_cache}}{struct request\_queue *\emph{ q}, bool\emph{ wc}, bool\emph{ fua}}{}
configure queue's write cache

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the request queue for the device

\item[{\code{bool wc}}] \leavevmode
write back cache on or off

\item[{\code{bool fua}}] \leavevmode
device supports FUA writes, if true

\end{description}

\textbf{Description}

Tell the block layer about the write cache of \textbf{q}.
\index{blk\_execute\_rq\_nowait (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_execute_rq_nowait}\pysiglinewithargsret{void \bfcode{blk\_execute\_rq\_nowait}}{struct request\_queue *\emph{ q}, struct gendisk *\emph{ bd\_disk}, struct request *\emph{ rq}, int\emph{ at\_head}, rq\_end\_io\_fn *\emph{ done}}{}
insert a request into queue for execution

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
queue to insert the request in

\item[{\code{struct gendisk * bd\_disk}}] \leavevmode
matching gendisk

\item[{\code{struct request * rq}}] \leavevmode
request to insert

\item[{\code{int at\_head}}] \leavevmode
insert request at head or tail of queue

\item[{\code{rq\_end\_io\_fn * done}}] \leavevmode
I/O completion handler

\end{description}

\textbf{Description}
\begin{quote}

Insert a fully prepared request at the back of the I/O scheduler queue
for execution.  Don't wait for completion.
\end{quote}

\textbf{Note}
\begin{quote}

This function will invoke \textbf{done} directly if the queue is dead.
\end{quote}
\index{blk\_execute\_rq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_execute_rq}\pysiglinewithargsret{void \bfcode{blk\_execute\_rq}}{struct request\_queue *\emph{ q}, struct gendisk *\emph{ bd\_disk}, struct request *\emph{ rq}, int\emph{ at\_head}}{}
insert a request into queue for execution

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
queue to insert the request in

\item[{\code{struct gendisk * bd\_disk}}] \leavevmode
matching gendisk

\item[{\code{struct request * rq}}] \leavevmode
request to insert

\item[{\code{int at\_head}}] \leavevmode
insert request at head or tail of queue

\end{description}

\textbf{Description}
\begin{quote}

Insert a fully prepared request at the back of the I/O scheduler queue
for execution and wait for completion.
\end{quote}
\index{blkdev\_issue\_flush (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blkdev_issue_flush}\pysiglinewithargsret{int \bfcode{blkdev\_issue\_flush}}{struct block\_device *\emph{ bdev}, gfp\_t\emph{ gfp\_mask}, sector\_t *\emph{ error\_sector}}{}
queue a flush

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct block\_device * bdev}}] \leavevmode
blockdev to issue flush for

\item[{\code{gfp\_t gfp\_mask}}] \leavevmode
memory allocation flags (for bio\_alloc)

\item[{\code{sector\_t * error\_sector}}] \leavevmode
error sector

\end{description}

\textbf{Description}
\begin{quote}

Issue a flush for the block device in question. Caller can supply
room for storing the error offset in case of a flush error, if they
wish to.
\end{quote}
\index{blkdev\_issue\_discard (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blkdev_issue_discard}\pysiglinewithargsret{int \bfcode{blkdev\_issue\_discard}}{struct block\_device *\emph{ bdev}, sector\_t\emph{ sector}, sector\_t\emph{ nr\_sects}, gfp\_t\emph{ gfp\_mask}, unsigned long\emph{ flags}}{}
queue a discard

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct block\_device * bdev}}] \leavevmode
blockdev to issue discard for

\item[{\code{sector\_t sector}}] \leavevmode
start sector

\item[{\code{sector\_t nr\_sects}}] \leavevmode
number of sectors to discard

\item[{\code{gfp\_t gfp\_mask}}] \leavevmode
memory allocation flags (for bio\_alloc)

\item[{\code{unsigned long flags}}] \leavevmode
BLKDEV\_DISCARD\_* flags to control behaviour

\end{description}

\textbf{Description}
\begin{quote}

Issue a discard request for the sectors in question.
\end{quote}
\index{blkdev\_issue\_write\_same (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blkdev_issue_write_same}\pysiglinewithargsret{int \bfcode{blkdev\_issue\_write\_same}}{struct block\_device *\emph{ bdev}, sector\_t\emph{ sector}, sector\_t\emph{ nr\_sects}, gfp\_t\emph{ gfp\_mask}, struct page *\emph{ page}}{}
queue a write same operation

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct block\_device * bdev}}] \leavevmode
target blockdev

\item[{\code{sector\_t sector}}] \leavevmode
start sector

\item[{\code{sector\_t nr\_sects}}] \leavevmode
number of sectors to write

\item[{\code{gfp\_t gfp\_mask}}] \leavevmode
memory allocation flags (for bio\_alloc)

\item[{\code{struct page * page}}] \leavevmode
page containing data

\end{description}

\textbf{Description}
\begin{quote}

Issue a write same request for the sectors in question.
\end{quote}
\index{\_\_blkdev\_issue\_zeroout (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__blkdev_issue_zeroout}\pysiglinewithargsret{int \bfcode{\_\_blkdev\_issue\_zeroout}}{struct block\_device *\emph{ bdev}, sector\_t\emph{ sector}, sector\_t\emph{ nr\_sects}, gfp\_t\emph{ gfp\_mask}, struct bio **\emph{ biop}, unsigned\emph{ flags}}{}
generate number of zero filed write bios

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct block\_device * bdev}}] \leavevmode
blockdev to issue

\item[{\code{sector\_t sector}}] \leavevmode
start sector

\item[{\code{sector\_t nr\_sects}}] \leavevmode
number of sectors to write

\item[{\code{gfp\_t gfp\_mask}}] \leavevmode
memory allocation flags (for bio\_alloc)

\item[{\code{struct bio ** biop}}] \leavevmode
pointer to anchor bio

\item[{\code{unsigned flags}}] \leavevmode
controls detailed behavior

\end{description}

\textbf{Description}
\begin{quote}

Zero-fill a block range, either using hardware offload or by explicitly
writing zeroes to the device.

If a device is using logical block provisioning, the underlying space will
not be released if \code{flags} contains BLKDEV\_ZERO\_NOUNMAP.

If \code{flags} contains BLKDEV\_ZERO\_NOFALLBACK, the function will return
-EOPNOTSUPP if no explicit hardware offload for zeroing is provided.
\end{quote}
\index{blkdev\_issue\_zeroout (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blkdev_issue_zeroout}\pysiglinewithargsret{int \bfcode{blkdev\_issue\_zeroout}}{struct block\_device *\emph{ bdev}, sector\_t\emph{ sector}, sector\_t\emph{ nr\_sects}, gfp\_t\emph{ gfp\_mask}, unsigned\emph{ flags}}{}
zero-fill a block range

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct block\_device * bdev}}] \leavevmode
blockdev to write

\item[{\code{sector\_t sector}}] \leavevmode
start sector

\item[{\code{sector\_t nr\_sects}}] \leavevmode
number of sectors to write

\item[{\code{gfp\_t gfp\_mask}}] \leavevmode
memory allocation flags (for bio\_alloc)

\item[{\code{unsigned flags}}] \leavevmode
controls detailed behavior

\end{description}

\textbf{Description}
\begin{quote}

Zero-fill a block range, either using hardware offload or by explicitly
writing zeroes to the device.  See {\hyperref[core\string-api/kernel\string-api:c.__blkdev_issue_zeroout]{\emph{\code{\_\_blkdev\_issue\_zeroout()}}}} for the
valid values for \code{flags}.
\end{quote}
\index{blk\_queue\_find\_tag (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_queue_find_tag}\pysiglinewithargsret{struct request * \bfcode{blk\_queue\_find\_tag}}{struct request\_queue *\emph{ q}, int\emph{ tag}}{}
find a request by its tag and queue

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
The request queue for the device

\item[{\code{int tag}}] \leavevmode
The tag of the request

\end{description}

\textbf{Notes}
\begin{quote}

Should be used when a device returns a tag and you want to match
it with a request.

no locks need be held.
\end{quote}
\index{blk\_free\_tags (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_free_tags}\pysiglinewithargsret{void \bfcode{blk\_free\_tags}}{struct blk\_queue\_tag *\emph{ bqt}}{}
release a given set of tag maintenance info

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct blk\_queue\_tag * bqt}}] \leavevmode
the tag map to free

\end{description}

\textbf{Description}

Drop the reference count on \textbf{bqt} and frees it when the last reference
is dropped.
\index{blk\_queue\_free\_tags (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_queue_free_tags}\pysiglinewithargsret{void \bfcode{blk\_queue\_free\_tags}}{struct request\_queue *\emph{ q}}{}
release tag maintenance info

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the request queue for the device

\end{description}

\textbf{Notes}
\begin{quote}

This is used to disable tagged queuing to a device, yet leave
queue in function.
\end{quote}
\index{blk\_init\_tags (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_init_tags}\pysiglinewithargsret{struct blk\_queue\_tag * \bfcode{blk\_init\_tags}}{int\emph{ depth}, int\emph{ alloc\_policy}}{}
initialize the tag info for an external tag map

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{int depth}}] \leavevmode
the maximum queue depth supported

\item[{\code{int alloc\_policy}}] \leavevmode
tag allocation policy

\end{description}
\index{blk\_queue\_init\_tags (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_queue_init_tags}\pysiglinewithargsret{int \bfcode{blk\_queue\_init\_tags}}{struct request\_queue *\emph{ q}, int\emph{ depth}, struct blk\_queue\_tag *\emph{ tags}, int\emph{ alloc\_policy}}{}
initialize the queue tag info

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the request queue for the device

\item[{\code{int depth}}] \leavevmode
the maximum queue depth supported

\item[{\code{struct blk\_queue\_tag * tags}}] \leavevmode
the tag to use

\item[{\code{int alloc\_policy}}] \leavevmode
tag allocation policy

\end{description}

\textbf{Description}

Queue lock must be held here if the function is called to resize an
existing map.
\index{blk\_queue\_resize\_tags (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_queue_resize_tags}\pysiglinewithargsret{int \bfcode{blk\_queue\_resize\_tags}}{struct request\_queue *\emph{ q}, int\emph{ new\_depth}}{}
change the queueing depth

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the request queue for the device

\item[{\code{int new\_depth}}] \leavevmode
the new max command queueing depth

\end{description}

\textbf{Notes}
\begin{quote}

Must be called with the queue lock held.
\end{quote}
\index{blk\_queue\_start\_tag (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_queue_start_tag}\pysiglinewithargsret{int \bfcode{blk\_queue\_start\_tag}}{struct request\_queue *\emph{ q}, struct request *\emph{ rq}}{}
find a free tag and assign it

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the request queue for the device

\item[{\code{struct request * rq}}] \leavevmode
the block request that needs tagging

\end{description}

\textbf{Description}
\begin{quote}

This can either be used as a stand-alone helper, or possibly be
assigned as the queue \code{prep\_rq\_fn} (in which case \code{struct request}
automagically gets a tag assigned). Note that this function
assumes that any type of request can be queued! if this is not
true for your device, you must check the request type before
calling this function.  The request will also be removed from
the request queue, so it's the drivers responsibility to readd
it if it should need to be restarted for some reason.
\end{quote}
\index{blk\_queue\_invalidate\_tags (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_queue_invalidate_tags}\pysiglinewithargsret{void \bfcode{blk\_queue\_invalidate\_tags}}{struct request\_queue *\emph{ q}}{}
invalidate all pending tags

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the request queue for the device

\end{description}

\textbf{Description}
\begin{quote}

Hardware conditions may dictate a need to stop all pending requests.
In this case, we will safely clear the block side of the tag queue and
readd all requests to the request queue in the right order.
\end{quote}
\index{\_\_blk\_queue\_free\_tags (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__blk_queue_free_tags}\pysiglinewithargsret{void \bfcode{\_\_blk\_queue\_free\_tags}}{struct request\_queue *\emph{ q}}{}
release tag maintenance info

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the request queue for the device

\end{description}

\textbf{Notes}
\begin{quote}

{\hyperref[core\string-api/kernel\string-api:c.blk_cleanup_queue]{\emph{\code{blk\_cleanup\_queue()}}}} will take care of calling this function, if tagging
has been used. So there's no need to call this directly.
\end{quote}
\index{blk\_queue\_end\_tag (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_queue_end_tag}\pysiglinewithargsret{void \bfcode{blk\_queue\_end\_tag}}{struct request\_queue *\emph{ q}, struct request *\emph{ rq}}{}
end tag operations for a request

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the request queue for the device

\item[{\code{struct request * rq}}] \leavevmode
the request that has completed

\end{description}

\textbf{Description}
\begin{quote}

Typically called when \code{end\_that\_request\_first()} returns \code{0}, meaning
all transfers have been done for a request. It's important to call
this function before \code{end\_that\_request\_last()}, as that will put the
request back on the free list thus corrupting the internal tag list.
\end{quote}
\index{blk\_rq\_count\_integrity\_sg (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_rq_count_integrity_sg}\pysiglinewithargsret{int \bfcode{blk\_rq\_count\_integrity\_sg}}{struct request\_queue *\emph{ q}, struct bio *\emph{ bio}}{}
Count number of integrity scatterlist elements

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
request queue

\item[{\code{struct bio * bio}}] \leavevmode
bio with integrity metadata attached

\end{description}

\textbf{Description}

Returns the number of elements required in a
scatterlist corresponding to the integrity metadata in a bio.
\index{blk\_rq\_map\_integrity\_sg (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_rq_map_integrity_sg}\pysiglinewithargsret{int \bfcode{blk\_rq\_map\_integrity\_sg}}{struct request\_queue *\emph{ q}, struct bio *\emph{ bio}, struct scatterlist *\emph{ sglist}}{}
Map integrity metadata into a scatterlist

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
request queue

\item[{\code{struct bio * bio}}] \leavevmode
bio with integrity metadata attached

\item[{\code{struct scatterlist * sglist}}] \leavevmode
target scatterlist

\end{description}

\textbf{Description}

Map the integrity vectors in request into a
scatterlist.  The scatterlist must be big enough to hold all
elements.  I.e. sized using {\hyperref[core\string-api/kernel\string-api:c.blk_rq_count_integrity_sg]{\emph{\code{blk\_rq\_count\_integrity\_sg()}}}}.
\index{blk\_integrity\_compare (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_integrity_compare}\pysiglinewithargsret{int \bfcode{blk\_integrity\_compare}}{struct gendisk *\emph{ gd1}, struct gendisk *\emph{ gd2}}{}
Compare integrity profile of two disks

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct gendisk * gd1}}] \leavevmode
Disk to compare

\item[{\code{struct gendisk * gd2}}] \leavevmode
Disk to compare

\end{description}

\textbf{Description}

Meta-devices like DM and MD need to verify that all
sub-devices use the same integrity format before advertising to
upper layers that they can send/receive integrity metadata.  This
function can be used to check whether two gendisk devices have
compatible integrity formats.
\index{blk\_integrity\_register (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_integrity_register}\pysiglinewithargsret{void \bfcode{blk\_integrity\_register}}{struct gendisk *\emph{ disk}, struct blk\_integrity *\emph{ template}}{}
Register a gendisk as being integrity-capable

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct gendisk * disk}}] \leavevmode
struct gendisk pointer to make integrity-aware

\item[{\code{struct blk\_integrity * template}}] \leavevmode
block integrity profile to register

\end{description}

\textbf{Description}

When a device needs to advertise itself as being able to
send/receive integrity metadata it must use this function to register
the capability with the block layer. The template is a blk\_integrity
struct with values appropriate for the underlying hardware. See
Documentation/block/data-integrity.txt.
\index{blk\_integrity\_unregister (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_integrity_unregister}\pysiglinewithargsret{void \bfcode{blk\_integrity\_unregister}}{struct gendisk *\emph{ disk}}{}
Unregister block integrity profile

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct gendisk * disk}}] \leavevmode
disk whose integrity profile to unregister

\end{description}

\textbf{Description}

This function unregisters the integrity capability from
a block device.
\index{blk\_trace\_ioctl (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_trace_ioctl}\pysiglinewithargsret{int \bfcode{blk\_trace\_ioctl}}{struct block\_device *\emph{ bdev}, unsigned\emph{ cmd}, char \_\_user *\emph{ arg}}{}
handle the ioctls associated with tracing

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct block\_device * bdev}}] \leavevmode
the block device

\item[{\code{unsigned cmd}}] \leavevmode
the ioctl cmd

\item[{\code{char \_\_user * arg}}] \leavevmode
the argument data, if any

\end{description}
\index{blk\_trace\_shutdown (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_trace_shutdown}\pysiglinewithargsret{void \bfcode{blk\_trace\_shutdown}}{struct request\_queue *\emph{ q}}{}
stop and cleanup trace structures

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
the request queue associated with the device

\end{description}
\index{blk\_add\_trace\_rq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_add_trace_rq}\pysiglinewithargsret{void \bfcode{blk\_add\_trace\_rq}}{struct request *\emph{ rq}, int\emph{ error}, unsigned int\emph{ nr\_bytes}, u32\emph{ what}, union kernfs\_node\_id *\emph{ cgid}}{}
Add a trace for a request oriented action

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request * rq}}] \leavevmode
the source request

\item[{\code{int error}}] \leavevmode
return status to log

\item[{\code{unsigned int nr\_bytes}}] \leavevmode
number of completed bytes

\item[{\code{u32 what}}] \leavevmode
the action

\item[{\code{union kernfs\_node\_id * cgid}}] \leavevmode
the cgroup info

\end{description}

\textbf{Description}
\begin{quote}

Records an action against a request. Will log the bio offset + size.
\end{quote}
\index{blk\_add\_trace\_bio (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_add_trace_bio}\pysiglinewithargsret{void \bfcode{blk\_add\_trace\_bio}}{struct request\_queue *\emph{ q}, struct bio *\emph{ bio}, u32\emph{ what}, int\emph{ error}}{}
Add a trace for a bio oriented action

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
queue the io is for

\item[{\code{struct bio * bio}}] \leavevmode
the source bio

\item[{\code{u32 what}}] \leavevmode
the action

\item[{\code{int error}}] \leavevmode
error, if any

\end{description}

\textbf{Description}
\begin{quote}

Records an action against a bio. Will log the bio offset + size.
\end{quote}
\index{blk\_add\_trace\_bio\_remap (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_add_trace_bio_remap}\pysiglinewithargsret{void \bfcode{blk\_add\_trace\_bio\_remap}}{void *\emph{ ignore}, struct request\_queue *\emph{ q}, struct bio *\emph{ bio}, dev\_t\emph{ dev}, sector\_t\emph{ from}}{}
Add a trace for a bio-remap operation

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void * ignore}}] \leavevmode
trace callback data parameter (not used)

\item[{\code{struct request\_queue * q}}] \leavevmode
queue the io is for

\item[{\code{struct bio * bio}}] \leavevmode
the source bio

\item[{\code{dev\_t dev}}] \leavevmode
target device

\item[{\code{sector\_t from}}] \leavevmode
source sector

\end{description}

\textbf{Description}
\begin{quote}

Device mapper or raid target sometimes need to split a bio because
it spans a stripe (or similar). Add a trace for that action.
\end{quote}
\index{blk\_add\_trace\_rq\_remap (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_add_trace_rq_remap}\pysiglinewithargsret{void \bfcode{blk\_add\_trace\_rq\_remap}}{void *\emph{ ignore}, struct request\_queue *\emph{ q}, struct request *\emph{ rq}, dev\_t\emph{ dev}, sector\_t\emph{ from}}{}
Add a trace for a request-remap operation

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void * ignore}}] \leavevmode
trace callback data parameter (not used)

\item[{\code{struct request\_queue * q}}] \leavevmode
queue the io is for

\item[{\code{struct request * rq}}] \leavevmode
the source request

\item[{\code{dev\_t dev}}] \leavevmode
target device

\item[{\code{sector\_t from}}] \leavevmode
source sector

\end{description}

\textbf{Description}
\begin{quote}

Device mapper remaps request to other devices.
Add a trace for that action.
\end{quote}
\index{blk\_mangle\_minor (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_mangle_minor}\pysiglinewithargsret{int \bfcode{blk\_mangle\_minor}}{int\emph{ minor}}{}
scatter minor numbers apart

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{int minor}}] \leavevmode
minor number to mangle

\end{description}

\textbf{Description}

Scatter consecutively allocated \textbf{minor} number apart if MANGLE\_DEVT
is enabled.  Mangling twice gives the original value.

\textbf{Return}

Mangled value.

\textbf{Context}

Don't care.
\index{blk\_alloc\_devt (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_alloc_devt}\pysiglinewithargsret{int \bfcode{blk\_alloc\_devt}}{struct hd\_struct *\emph{ part}, dev\_t *\emph{ devt}}{}
allocate a dev\_t for a partition

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct hd\_struct * part}}] \leavevmode
partition to allocate dev\_t for

\item[{\code{dev\_t * devt}}] \leavevmode
out parameter for resulting dev\_t

\end{description}

\textbf{Description}

Allocate a dev\_t for block device.

\textbf{Return}

0 on success, allocated dev\_t is returned in \textbf{*devt}.  -errno on
failure.

\textbf{Context}

Might sleep.
\index{blk\_free\_devt (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.blk_free_devt}\pysiglinewithargsret{void \bfcode{blk\_free\_devt}}{dev\_t\emph{ devt}}{}
free a dev\_t

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{dev\_t devt}}] \leavevmode
dev\_t to free

\end{description}

\textbf{Description}

Free \textbf{devt} which was allocated using {\hyperref[core\string-api/kernel\string-api:c.blk_alloc_devt]{\emph{\code{blk\_alloc\_devt()}}}}.

\textbf{Context}

Might sleep.
\index{\_\_device\_add\_disk (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__device_add_disk}\pysiglinewithargsret{void \bfcode{\_\_device\_add\_disk}}{struct device *\emph{ parent}, struct gendisk *\emph{ disk}, bool\emph{ register\_queue}}{}
add disk information to kernel list

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct device * parent}}] \leavevmode
parent device for the disk

\item[{\code{struct gendisk * disk}}] \leavevmode
per-device partitioning information

\item[{\code{bool register\_queue}}] \leavevmode
register the queue if set to true

\end{description}

\textbf{Description}

This function registers the partitioning information in \textbf{disk}
with the kernel.

FIXME: error handling
\index{disk\_replace\_part\_tbl (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.disk_replace_part_tbl}\pysiglinewithargsret{void \bfcode{disk\_replace\_part\_tbl}}{struct gendisk *\emph{ disk}, struct disk\_part\_tbl *\emph{ new\_ptbl}}{}
replace disk-\textgreater{}part\_tbl in RCU-safe way

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct gendisk * disk}}] \leavevmode
disk to replace part\_tbl for

\item[{\code{struct disk\_part\_tbl * new\_ptbl}}] \leavevmode
new part\_tbl to install

\end{description}

\textbf{Description}

Replace disk-\textgreater{}part\_tbl with \textbf{new\_ptbl} in RCU-safe way.  The
original ptbl is freed using RCU callback.

LOCKING:
Matching bd\_mutex locked or the caller is the only user of \textbf{disk}.
\index{disk\_expand\_part\_tbl (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.disk_expand_part_tbl}\pysiglinewithargsret{int \bfcode{disk\_expand\_part\_tbl}}{struct gendisk *\emph{ disk}, int\emph{ partno}}{}
expand disk-\textgreater{}part\_tbl

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct gendisk * disk}}] \leavevmode
disk to expand part\_tbl for

\item[{\code{int partno}}] \leavevmode
expand such that this partno can fit in

\end{description}

\textbf{Description}

Expand disk-\textgreater{}part\_tbl such that \textbf{partno} can fit in.  disk-\textgreater{}part\_tbl
uses RCU to allow unlocked dereferencing for stats and other stuff.

LOCKING:
Matching bd\_mutex locked or the caller is the only user of \textbf{disk}.
Might sleep.

\textbf{Return}

0 on success, -errno on failure.
\index{disk\_block\_events (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.disk_block_events}\pysiglinewithargsret{void \bfcode{disk\_block\_events}}{struct gendisk *\emph{ disk}}{}
block and flush disk event checking

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct gendisk * disk}}] \leavevmode
disk to block events for

\end{description}

\textbf{Description}

On return from this function, it is guaranteed that event checking
isn't in progress and won't happen until unblocked by
{\hyperref[core\string-api/kernel\string-api:c.disk_unblock_events]{\emph{\code{disk\_unblock\_events()}}}}.  Events blocking is counted and the actual
unblocking happens after the matching number of unblocks are done.

Note that this intentionally does not block event checking from
{\hyperref[core\string-api/kernel\string-api:c.disk_clear_events]{\emph{\code{disk\_clear\_events()}}}}.

\textbf{Context}

Might sleep.
\index{disk\_unblock\_events (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.disk_unblock_events}\pysiglinewithargsret{void \bfcode{disk\_unblock\_events}}{struct gendisk *\emph{ disk}}{}
unblock disk event checking

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct gendisk * disk}}] \leavevmode
disk to unblock events for

\end{description}

\textbf{Description}

Undo {\hyperref[core\string-api/kernel\string-api:c.disk_block_events]{\emph{\code{disk\_block\_events()}}}}.  When the block count reaches zero, it
starts events polling if configured.

\textbf{Context}

Don't care.  Safe to call from irq context.
\index{disk\_flush\_events (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.disk_flush_events}\pysiglinewithargsret{void \bfcode{disk\_flush\_events}}{struct gendisk *\emph{ disk}, unsigned int\emph{ mask}}{}
schedule immediate event checking and flushing

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct gendisk * disk}}] \leavevmode
disk to check and flush events for

\item[{\code{unsigned int mask}}] \leavevmode
events to flush

\end{description}

\textbf{Description}

Schedule immediate event checking on \textbf{disk} if not blocked.  Events in
\textbf{mask} are scheduled to be cleared from the driver.  Note that this
doesn't clear the events from \textbf{disk}-\textgreater{}ev.

\textbf{Context}

If \textbf{mask} is non-zero must be called with bdev-\textgreater{}bd\_mutex held.
\index{disk\_clear\_events (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.disk_clear_events}\pysiglinewithargsret{unsigned int \bfcode{disk\_clear\_events}}{struct gendisk *\emph{ disk}, unsigned int\emph{ mask}}{}
synchronously check, clear and return pending events

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct gendisk * disk}}] \leavevmode
disk to fetch and clear events from

\item[{\code{unsigned int mask}}] \leavevmode
mask of events to be fetched and cleared

\end{description}

\textbf{Description}

Disk events are synchronously checked and pending events in \textbf{mask}
are cleared and returned.  This ignores the block count.

\textbf{Context}

Might sleep.
\index{disk\_get\_part (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.disk_get_part}\pysiglinewithargsret{struct hd\_struct * \bfcode{disk\_get\_part}}{struct gendisk *\emph{ disk}, int\emph{ partno}}{}
get partition

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct gendisk * disk}}] \leavevmode
disk to look partition from

\item[{\code{int partno}}] \leavevmode
partition number

\end{description}

\textbf{Description}

Look for partition \textbf{partno} from \textbf{disk}.  If found, increment
reference count and return it.

\textbf{Context}

Don't care.

\textbf{Return}

Pointer to the found partition on success, NULL if not found.
\index{disk\_part\_iter\_init (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.disk_part_iter_init}\pysiglinewithargsret{void \bfcode{disk\_part\_iter\_init}}{struct disk\_part\_iter *\emph{ piter}, struct gendisk *\emph{ disk}, unsigned int\emph{ flags}}{}
initialize partition iterator

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct disk\_part\_iter * piter}}] \leavevmode
iterator to initialize

\item[{\code{struct gendisk * disk}}] \leavevmode
disk to iterate over

\item[{\code{unsigned int flags}}] \leavevmode
DISK\_PITER\_* flags

\end{description}

\textbf{Description}

Initialize \textbf{piter} so that it iterates over partitions of \textbf{disk}.

\textbf{Context}

Don't care.
\index{disk\_part\_iter\_next (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.disk_part_iter_next}\pysiglinewithargsret{struct hd\_struct * \bfcode{disk\_part\_iter\_next}}{struct disk\_part\_iter *\emph{ piter}}{}
proceed iterator to the next partition and return it

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct disk\_part\_iter * piter}}] \leavevmode
iterator of interest

\end{description}

\textbf{Description}

Proceed \textbf{piter} to the next partition and return it.

\textbf{Context}

Don't care.
\index{disk\_part\_iter\_exit (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.disk_part_iter_exit}\pysiglinewithargsret{void \bfcode{disk\_part\_iter\_exit}}{struct disk\_part\_iter *\emph{ piter}}{}
finish up partition iteration

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct disk\_part\_iter * piter}}] \leavevmode
iter of interest

\end{description}

\textbf{Description}

Called when iteration is over.  Cleans up \textbf{piter}.

\textbf{Context}

Don't care.
\index{disk\_map\_sector\_rcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.disk_map_sector_rcu}\pysiglinewithargsret{struct hd\_struct * \bfcode{disk\_map\_sector\_rcu}}{struct gendisk *\emph{ disk}, sector\_t\emph{ sector}}{}
map sector to partition

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct gendisk * disk}}] \leavevmode
gendisk of interest

\item[{\code{sector\_t sector}}] \leavevmode
sector to map

\end{description}

\textbf{Description}

Find out which partition \textbf{sector} maps to on \textbf{disk}.  This is
primarily used for stats accounting.

\textbf{Context}

RCU read locked.  The returned partition pointer is valid only
while preemption is disabled.

\textbf{Return}

Found partition on success, part0 is returned if no partition matches
\index{register\_blkdev (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.register_blkdev}\pysiglinewithargsret{int \bfcode{register\_blkdev}}{unsigned int\emph{ major}, const char *\emph{ name}}{}
register a new block device

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int major}}] \leavevmode
the requested major device number {[}1..255{]}. If \textbf{major} = 0, try to
allocate any unused major number.

\item[{\code{const char * name}}] \leavevmode
the name of the new block device as a zero terminated string

\end{description}

\textbf{Description}

The \textbf{name} must be unique within the system.

The return value depends on the \textbf{major} input parameter:
\begin{itemize}
\item {} 
if a major device number was requested in range {[}1..255{]} then the
function returns zero on success, or a negative error code

\item {} 
if any unused major number was requested with \textbf{major} = 0 parameter
then the return value is the allocated major number in range
{[}1..255{]} or a negative error code otherwise

\end{itemize}
\index{get\_gendisk (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.get_gendisk}\pysiglinewithargsret{struct gendisk * \bfcode{get\_gendisk}}{dev\_t\emph{ devt}, int *\emph{ partno}}{}
get partitioning information for a given device

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{dev\_t devt}}] \leavevmode
device to get partitioning information for

\item[{\code{int * partno}}] \leavevmode
returned partition index

\end{description}

\textbf{Description}

This function gets the structure containing partitioning
information for the given device \textbf{devt}.
\index{bdget\_disk (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.bdget_disk}\pysiglinewithargsret{struct block\_device * \bfcode{bdget\_disk}}{struct gendisk *\emph{ disk}, int\emph{ partno}}{}
do \code{bdget()} by gendisk and partition number

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct gendisk * disk}}] \leavevmode
gendisk of interest

\item[{\code{int partno}}] \leavevmode
partition number

\end{description}

\textbf{Description}

Find partition \textbf{partno} from \textbf{disk}, do \code{bdget()} on it.

\textbf{Context}

Don't care.

\textbf{Return}

Resulting block\_device on success, NULL on failure.


\subsection{Char devices}
\label{core-api/kernel-api:char-devices}\index{register\_chrdev\_region (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.register_chrdev_region}\pysiglinewithargsret{int \bfcode{register\_chrdev\_region}}{dev\_t\emph{ from}, unsigned\emph{ count}, const char *\emph{ name}}{}
register a range of device numbers

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{dev\_t from}}] \leavevmode
the first in the desired range of device numbers; must include
the major number.

\item[{\code{unsigned count}}] \leavevmode
the number of consecutive device numbers required

\item[{\code{const char * name}}] \leavevmode
the name of the device or driver.

\end{description}

\textbf{Description}

Return value is zero on success, a negative error code on failure.
\index{alloc\_chrdev\_region (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.alloc_chrdev_region}\pysiglinewithargsret{int \bfcode{alloc\_chrdev\_region}}{dev\_t *\emph{ dev}, unsigned\emph{ baseminor}, unsigned\emph{ count}, const char *\emph{ name}}{}
register a range of char device numbers

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{dev\_t * dev}}] \leavevmode
output parameter for first assigned number

\item[{\code{unsigned baseminor}}] \leavevmode
first of the requested range of minor numbers

\item[{\code{unsigned count}}] \leavevmode
the number of minor numbers required

\item[{\code{const char * name}}] \leavevmode
the name of the associated device or driver

\end{description}

\textbf{Description}

Allocates a range of char device numbers.  The major number will be
chosen dynamically, and returned (along with the first minor number)
in \textbf{dev}.  Returns zero or a negative error code.
\index{\_\_register\_chrdev (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__register_chrdev}\pysiglinewithargsret{int \bfcode{\_\_register\_chrdev}}{unsigned int\emph{ major}, unsigned int\emph{ baseminor}, unsigned int\emph{ count}, const char *\emph{ name}, const struct file\_operations *\emph{ fops}}{}
create and register a cdev occupying a range of minors

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int major}}] \leavevmode
major device number or 0 for dynamic allocation

\item[{\code{unsigned int baseminor}}] \leavevmode
first of the requested range of minor numbers

\item[{\code{unsigned int count}}] \leavevmode
the number of minor numbers required

\item[{\code{const char * name}}] \leavevmode
name of this range of devices

\item[{\code{const struct file\_operations * fops}}] \leavevmode
file operations associated with this devices

\end{description}

\textbf{Description}

If \textbf{major} == 0 this functions will dynamically allocate a major and return
its number.

If \textbf{major} \textgreater{} 0 this function will attempt to reserve a device with the given
major number and will return zero on success.

Returns a -ve errno on failure.

The name of this device has nothing to do with the name of the device in
/dev. It only helps to keep track of the different owners of devices. If
your module name has only one type of devices it's ok to use e.g. the name
of the module here.
\index{unregister\_chrdev\_region (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.unregister_chrdev_region}\pysiglinewithargsret{void \bfcode{unregister\_chrdev\_region}}{dev\_t\emph{ from}, unsigned\emph{ count}}{}
unregister a range of device numbers

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{dev\_t from}}] \leavevmode
the first in the range of numbers to unregister

\item[{\code{unsigned count}}] \leavevmode
the number of device numbers to unregister

\end{description}

\textbf{Description}

This function will unregister a range of \textbf{count} device numbers,
starting with \textbf{from}.  The caller should normally be the one who
allocated those numbers in the first place...
\index{\_\_unregister\_chrdev (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__unregister_chrdev}\pysiglinewithargsret{void \bfcode{\_\_unregister\_chrdev}}{unsigned int\emph{ major}, unsigned int\emph{ baseminor}, unsigned int\emph{ count}, const char *\emph{ name}}{}
unregister and destroy a cdev

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int major}}] \leavevmode
major device number

\item[{\code{unsigned int baseminor}}] \leavevmode
first of the range of minor numbers

\item[{\code{unsigned int count}}] \leavevmode
the number of minor numbers this cdev is occupying

\item[{\code{const char * name}}] \leavevmode
name of this range of devices

\end{description}

\textbf{Description}

Unregister and destroy the cdev occupying the region described by
\textbf{major}, \textbf{baseminor} and \textbf{count}.  This function undoes what
{\hyperref[core\string-api/kernel\string-api:c.__register_chrdev]{\emph{\code{\_\_register\_chrdev()}}}} did.
\index{cdev\_add (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.cdev_add}\pysiglinewithargsret{int \bfcode{cdev\_add}}{struct cdev *\emph{ p}, dev\_t\emph{ dev}, unsigned\emph{ count}}{}
add a char device to the system

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct cdev * p}}] \leavevmode
the cdev structure for the device

\item[{\code{dev\_t dev}}] \leavevmode
the first device number for which this device is responsible

\item[{\code{unsigned count}}] \leavevmode
the number of consecutive minor numbers corresponding to this
device

\end{description}

\textbf{Description}

{\hyperref[core\string-api/kernel\string-api:c.cdev_add]{\emph{\code{cdev\_add()}}}} adds the device represented by \textbf{p} to the system, making it
live immediately.  A negative error code is returned on failure.
\index{cdev\_set\_parent (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.cdev_set_parent}\pysiglinewithargsret{void \bfcode{cdev\_set\_parent}}{struct cdev *\emph{ p}, struct kobject *\emph{ kobj}}{}
set the parent kobject for a char device

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct cdev * p}}] \leavevmode
the cdev structure

\item[{\code{struct kobject * kobj}}] \leavevmode
the kobject to take a reference to

\end{description}

\textbf{Description}

{\hyperref[core\string-api/kernel\string-api:c.cdev_set_parent]{\emph{\code{cdev\_set\_parent()}}}} sets a parent kobject which will be referenced
appropriately so the parent is not freed before the cdev. This
should be called before cdev\_add.
\index{cdev\_device\_add (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.cdev_device_add}\pysiglinewithargsret{int \bfcode{cdev\_device\_add}}{struct cdev *\emph{ cdev}, struct device *\emph{ dev}}{}
add a char device and it's corresponding struct device, linkink

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct cdev * cdev}}] \leavevmode
the cdev structure

\item[{\code{struct device * dev}}] \leavevmode
the device structure

\end{description}

\textbf{Description}

{\hyperref[core\string-api/kernel\string-api:c.cdev_device_add]{\emph{\code{cdev\_device\_add()}}}} adds the char device represented by \textbf{cdev} to the system,
just as cdev\_add does. It then adds \textbf{dev} to the system using device\_add
The dev\_t for the char device will be taken from the struct device which
needs to be initialized first. This helper function correctly takes a
reference to the parent device so the parent will not get released until
all references to the cdev are released.

This helper uses dev-\textgreater{}devt for the device number. If it is not set
it will not add the cdev and it will be equivalent to device\_add.

This function should be used whenever the struct cdev and the
struct device are members of the same structure whose lifetime is
managed by the struct device.

\textbf{NOTE}

Callers must assume that userspace was able to open the cdev and
can call cdev fops callbacks at any time, even if this function fails.
\index{cdev\_device\_del (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.cdev_device_del}\pysiglinewithargsret{void \bfcode{cdev\_device\_del}}{struct cdev *\emph{ cdev}, struct device *\emph{ dev}}{}
inverse of cdev\_device\_add

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct cdev * cdev}}] \leavevmode
the cdev structure

\item[{\code{struct device * dev}}] \leavevmode
the device structure

\end{description}

\textbf{Description}

{\hyperref[core\string-api/kernel\string-api:c.cdev_device_del]{\emph{\code{cdev\_device\_del()}}}} is a helper function to call cdev\_del and device\_del.
It should be used whenever cdev\_device\_add is used.

If dev-\textgreater{}devt is not set it will not remove the cdev and will be equivalent
to device\_del.

\textbf{NOTE}

This guarantees that associated sysfs callbacks are not running
or runnable, however any cdevs already open will remain and their fops
will still be callable even after this function returns.
\index{cdev\_del (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.cdev_del}\pysiglinewithargsret{void \bfcode{cdev\_del}}{struct cdev *\emph{ p}}{}
remove a cdev from the system

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct cdev * p}}] \leavevmode
the cdev structure to be removed

\end{description}

\textbf{Description}

{\hyperref[core\string-api/kernel\string-api:c.cdev_del]{\emph{\code{cdev\_del()}}}} removes \textbf{p} from the system, possibly freeing the structure
itself.

\textbf{NOTE}

This guarantees that cdev device will no longer be able to be
opened, however any cdevs already open will remain and their fops will
still be callable even after cdev\_del returns.
\index{cdev\_alloc (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.cdev_alloc}\pysiglinewithargsret{struct cdev * \bfcode{cdev\_alloc}}{void}{}
allocate a cdev structure

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

Allocates and returns a cdev structure, or NULL on failure.
\index{cdev\_init (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.cdev_init}\pysiglinewithargsret{void \bfcode{cdev\_init}}{struct cdev *\emph{ cdev}, const struct file\_operations *\emph{ fops}}{}
initialize a cdev structure

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct cdev * cdev}}] \leavevmode
the structure to initialize

\item[{\code{const struct file\_operations * fops}}] \leavevmode
the file\_operations for this device

\end{description}

\textbf{Description}

Initializes \textbf{cdev}, remembering \textbf{fops}, making it ready to add to the
system with {\hyperref[core\string-api/kernel\string-api:c.cdev_add]{\emph{\code{cdev\_add()}}}}.


\subsection{Clock Framework}
\label{core-api/kernel-api:clock-framework}
The clock framework defines programming interfaces to support software
management of the system clock tree. This framework is widely used with
System-On-Chip (SOC) platforms to support power management and various
devices which may need custom clock rates. Note that these ``clocks''
don't relate to timekeeping or real time clocks (RTCs), each of which
have separate frameworks. These \code{struct clk}
instances may be used to manage for example a 96 MHz signal that is used
to shift bits into and out of peripherals or busses, or otherwise
trigger synchronous state machine transitions in system hardware.

Power management is supported by explicit software clock gating: unused
clocks are disabled, so the system doesn't waste power changing the
state of transistors that aren't in active use. On some systems this may
be backed by hardware clock gating, where clocks are gated without being
disabled in software. Sections of chips that are powered but not clocked
may be able to retain their last state. This low power state is often
called a \emph{retention mode}. This mode still incurs leakage currents,
especially with finer circuit geometries, but for CMOS circuits power is
mostly used by clocked state changes.

Power-aware drivers only enable their clocks when the device they manage
is in active use. Also, system sleep states often differ according to
which clock domains are active: while a ``standby'' state may allow wakeup
from several active domains, a ``mem'' (suspend-to-RAM) state may require
a more wholesale shutdown of clocks derived from higher speed PLLs and
oscillators, limiting the number of possible wakeup event sources. A
driver's suspend method may need to be aware of system-specific clock
constraints on the target sleep state.

Some platforms support programmable clock generators. These can be used
by external chips of various kinds, such as other CPUs, multimedia
codecs, and devices with strict requirements for interface clocking.
\index{clk\_notifier (C type)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.clk_notifier}\pysigline{struct \bfcode{clk\_notifier}}
associate a clk with a notifier

\end{fulllineitems}


\textbf{Definition}

\begin{Verbatim}[commandchars=\\\{\}]
struct clk\PYGZus{}notifier \PYGZob{}
  struct clk                      *clk;
  struct srcu\PYGZus{}notifier\PYGZus{}head       notifier\PYGZus{}head;
  struct list\PYGZus{}head                node;
\PYGZcb{};
\end{Verbatim}

\textbf{Members}
\begin{description}
\item[{\code{clk}}] \leavevmode
struct clk * to associate the notifier with

\item[{\code{notifier\_head}}] \leavevmode
a blocking\_notifier\_head for this clk

\item[{\code{node}}] \leavevmode
linked list pointers

\end{description}

\textbf{Description}

A list of struct clk\_notifier is maintained by the notifier code.
An entry is created whenever code registers the first notifier on a
particular \textbf{clk}.  Future notifiers on that \textbf{clk} are added to the
\textbf{notifier\_head}.
\index{clk\_notifier\_data (C type)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.clk_notifier_data}\pysigline{struct \bfcode{clk\_notifier\_data}}
rate data to pass to the notifier callback

\end{fulllineitems}


\textbf{Definition}

\begin{Verbatim}[commandchars=\\\{\}]
struct clk\PYGZus{}notifier\PYGZus{}data \PYGZob{}
  struct clk              *clk;
  unsigned long           old\PYGZus{}rate;
  unsigned long           new\PYGZus{}rate;
\PYGZcb{};
\end{Verbatim}

\textbf{Members}
\begin{description}
\item[{\code{clk}}] \leavevmode
struct clk * being changed

\item[{\code{old\_rate}}] \leavevmode
previous rate of this clk

\item[{\code{new\_rate}}] \leavevmode
new rate of this clk

\end{description}

\textbf{Description}

For a pre-notifier, old\_rate is the clk's rate before this rate
change, and new\_rate is what the rate will be in the future.  For a
post-notifier, old\_rate and new\_rate are both set to the clk's
current rate (this was done to optimize the implementation).
\index{clk\_bulk\_data (C type)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.clk_bulk_data}\pysigline{struct \bfcode{clk\_bulk\_data}}
Data used for bulk clk operations.

\end{fulllineitems}


\textbf{Definition}

\begin{Verbatim}[commandchars=\\\{\}]
struct clk\PYGZus{}bulk\PYGZus{}data \PYGZob{}
  const char              *id;
  struct clk              *clk;
\PYGZcb{};
\end{Verbatim}

\textbf{Members}
\begin{description}
\item[{\code{id}}] \leavevmode
clock consumer ID

\item[{\code{clk}}] \leavevmode
struct clk * to store the associated clock

\end{description}

\textbf{Description}

The CLK APIs provide a series of \code{clk\_bulk\_()} API calls as
a convenience to consumers which require multiple clks.  This
structure is used to manage data for these calls.
\index{clk\_notifier\_register (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.clk_notifier_register}\pysiglinewithargsret{int \bfcode{clk\_notifier\_register}}{struct clk *\emph{ clk}, struct notifier\_block *\emph{ nb}}{}
change notifier callback

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct clk * clk}}] \leavevmode
clock whose rate we are interested in

\item[{\code{struct notifier\_block * nb}}] \leavevmode
notifier block with callback function pointer

\end{description}

\textbf{Description}

ProTip: debugging across notifier chains can be frustrating. Make sure that
your notifier callback function prints a nice big warning in case of
failure.
\index{clk\_notifier\_unregister (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.clk_notifier_unregister}\pysiglinewithargsret{int \bfcode{clk\_notifier\_unregister}}{struct clk *\emph{ clk}, struct notifier\_block *\emph{ nb}}{}
change notifier callback

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct clk * clk}}] \leavevmode
clock whose rate we are no longer interested in

\item[{\code{struct notifier\_block * nb}}] \leavevmode
notifier block which will be unregistered

\end{description}
\index{clk\_get\_accuracy (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.clk_get_accuracy}\pysiglinewithargsret{long \bfcode{clk\_get\_accuracy}}{struct clk *\emph{ clk}}{}
obtain the clock accuracy in ppb (parts per billion) for a clock source.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct clk * clk}}] \leavevmode
clock source

\end{description}

\textbf{Description}

This gets the clock source accuracy expressed in ppb.
A perfect clock returns 0.
\index{clk\_set\_phase (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.clk_set_phase}\pysiglinewithargsret{int \bfcode{clk\_set\_phase}}{struct clk *\emph{ clk}, int\emph{ degrees}}{}
adjust the phase shift of a clock signal

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct clk * clk}}] \leavevmode
clock signal source

\item[{\code{int degrees}}] \leavevmode
number of degrees the signal is shifted

\end{description}

\textbf{Description}

Shifts the phase of a clock signal by the specified degrees. Returns 0 on
success, -EERROR otherwise.
\index{clk\_get\_phase (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.clk_get_phase}\pysiglinewithargsret{int \bfcode{clk\_get\_phase}}{struct clk *\emph{ clk}}{}
return the phase shift of a clock signal

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct clk * clk}}] \leavevmode
clock signal source

\end{description}

\textbf{Description}

Returns the phase shift of a clock node in degrees, otherwise returns
-EERROR.
\index{clk\_is\_match (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.clk_is_match}\pysiglinewithargsret{bool \bfcode{clk\_is\_match}}{const struct clk *\emph{ p}, const struct clk *\emph{ q}}{}
check if two clk's point to the same hardware clock

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const struct clk * p}}] \leavevmode
clk compared against q

\item[{\code{const struct clk * q}}] \leavevmode
clk compared against p

\end{description}

\textbf{Description}

Returns true if the two struct clk pointers both point to the same hardware
clock node. Put differently, returns true if \textbf{p} and \textbf{q}
share the same \code{struct clk\_core} object.

Returns false otherwise. Note that two NULL clks are treated as matching.
\index{clk\_prepare (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.clk_prepare}\pysiglinewithargsret{int \bfcode{clk\_prepare}}{struct clk *\emph{ clk}}{}
prepare a clock source

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct clk * clk}}] \leavevmode
clock source

\end{description}

\textbf{Description}

This prepares the clock source for use.

Must not be called from within atomic context.
\index{clk\_unprepare (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.clk_unprepare}\pysiglinewithargsret{void \bfcode{clk\_unprepare}}{struct clk *\emph{ clk}}{}
undo preparation of a clock source

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct clk * clk}}] \leavevmode
clock source

\end{description}

\textbf{Description}

This undoes a previously prepared clock.  The caller must balance
the number of prepare and unprepare calls.

Must not be called from within atomic context.
\index{clk\_get (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.clk_get}\pysiglinewithargsret{struct clk * \bfcode{clk\_get}}{struct device *\emph{ dev}, const char *\emph{ id}}{}
lookup and obtain a reference to a clock producer.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct device * dev}}] \leavevmode
device for clock ``consumer''

\item[{\code{const char * id}}] \leavevmode
clock consumer ID

\end{description}

\textbf{Description}

Returns a struct clk corresponding to the clock producer, or
valid \code{IS\_ERR()} condition containing errno.  The implementation
uses \textbf{dev} and \textbf{id} to determine the clock consumer, and thereby
the clock producer.  (IOW, \textbf{id} may be identical strings, but
clk\_get may return different clock producers depending on \textbf{dev}.)

Drivers must assume that the clock source is not enabled.

clk\_get should not be called from within interrupt context.
\index{clk\_bulk\_get (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.clk_bulk_get}\pysiglinewithargsret{int \bfcode{clk\_bulk\_get}}{struct device *\emph{ dev}, int\emph{ num\_clks}, struct {\hyperref[core\string-api/kernel\string-api:c.clk_bulk_data]{\emph{clk\_bulk\_data}}} *\emph{ clks}}{}
lookup and obtain a number of references to clock producer.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct device * dev}}] \leavevmode
device for clock ``consumer''

\item[{\code{int num\_clks}}] \leavevmode
the number of clk\_bulk\_data

\item[{\code{struct clk\_bulk\_data * clks}}] \leavevmode
the clk\_bulk\_data table of consumer

\end{description}

\textbf{Description}

This helper function allows drivers to get several clk consumers in one
operation. If any of the clk cannot be acquired then any clks
that were obtained will be freed before returning to the caller.

Returns 0 if all clocks specified in clk\_bulk\_data table are obtained
successfully, or valid \code{IS\_ERR()} condition containing errno.
The implementation uses \textbf{dev} and \textbf{clk\_bulk\_data.id} to determine the
clock consumer, and thereby the clock producer.
The clock returned is stored in each \textbf{clk\_bulk\_data.clk} field.

Drivers must assume that the clock source is not enabled.

clk\_bulk\_get should not be called from within interrupt context.
\index{devm\_clk\_bulk\_get (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.devm_clk_bulk_get}\pysiglinewithargsret{int \bfcode{devm\_clk\_bulk\_get}}{struct device *\emph{ dev}, int\emph{ num\_clks}, struct {\hyperref[core\string-api/kernel\string-api:c.clk_bulk_data]{\emph{clk\_bulk\_data}}} *\emph{ clks}}{}
managed get multiple clk consumers

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct device * dev}}] \leavevmode
device for clock ``consumer''

\item[{\code{int num\_clks}}] \leavevmode
the number of clk\_bulk\_data

\item[{\code{struct clk\_bulk\_data * clks}}] \leavevmode
the clk\_bulk\_data table of consumer

\end{description}

\textbf{Description}

Return 0 on success, an errno on failure.

This helper function allows drivers to get several clk
consumers in one operation with management, the clks will
automatically be freed when the device is unbound.
\index{devm\_clk\_get (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.devm_clk_get}\pysiglinewithargsret{struct clk * \bfcode{devm\_clk\_get}}{struct device *\emph{ dev}, const char *\emph{ id}}{}
lookup and obtain a managed reference to a clock producer.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct device * dev}}] \leavevmode
device for clock ``consumer''

\item[{\code{const char * id}}] \leavevmode
clock consumer ID

\end{description}

\textbf{Description}

Returns a struct clk corresponding to the clock producer, or
valid \code{IS\_ERR()} condition containing errno.  The implementation
uses \textbf{dev} and \textbf{id} to determine the clock consumer, and thereby
the clock producer.  (IOW, \textbf{id} may be identical strings, but
clk\_get may return different clock producers depending on \textbf{dev}.)

Drivers must assume that the clock source is not enabled.

devm\_clk\_get should not be called from within interrupt context.

The clock will automatically be freed when the device is unbound
from the bus.
\index{devm\_get\_clk\_from\_child (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.devm_get_clk_from_child}\pysiglinewithargsret{struct clk * \bfcode{devm\_get\_clk\_from\_child}}{struct device *\emph{ dev}, struct device\_node *\emph{ np}, const char *\emph{ con\_id}}{}
lookup and obtain a managed reference to a clock producer from child node.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct device * dev}}] \leavevmode
device for clock ``consumer''

\item[{\code{struct device\_node * np}}] \leavevmode
pointer to clock consumer node

\item[{\code{const char * con\_id}}] \leavevmode
clock consumer ID

\end{description}

\textbf{Description}

This function parses the clocks, and uses them to look up the
struct clk from the registered list of clock providers by using
\textbf{np} and \textbf{con\_id}

The clock will automatically be freed when the device is unbound
from the bus.
\index{clk\_rate\_exclusive\_get (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.clk_rate_exclusive_get}\pysiglinewithargsret{int \bfcode{clk\_rate\_exclusive\_get}}{struct clk *\emph{ clk}}{}
get exclusivity over the rate control of a producer

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct clk * clk}}] \leavevmode
clock source

\end{description}

\textbf{Description}

This function allows drivers to get exclusive control over the rate of a
provider. It prevents any other consumer to execute, even indirectly,
opereation which could alter the rate of the provider or cause glitches

If exlusivity is claimed more than once on clock, even by the same driver,
the rate effectively gets locked as exclusivity can't be preempted.

Must not be called from within atomic context.

Returns success (0) or negative errno.
\index{clk\_rate\_exclusive\_put (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.clk_rate_exclusive_put}\pysiglinewithargsret{void \bfcode{clk\_rate\_exclusive\_put}}{struct clk *\emph{ clk}}{}
release exclusivity over the rate control of a producer

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct clk * clk}}] \leavevmode
clock source

\end{description}

\textbf{Description}

This function allows drivers to release the exclusivity it previously got
from {\hyperref[core\string-api/kernel\string-api:c.clk_rate_exclusive_get]{\emph{\code{clk\_rate\_exclusive\_get()}}}}

The caller must balance the number of {\hyperref[core\string-api/kernel\string-api:c.clk_rate_exclusive_get]{\emph{\code{clk\_rate\_exclusive\_get()}}}} and
{\hyperref[core\string-api/kernel\string-api:c.clk_rate_exclusive_put]{\emph{\code{clk\_rate\_exclusive\_put()}}}} calls.

Must not be called from within atomic context.
\index{clk\_enable (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.clk_enable}\pysiglinewithargsret{int \bfcode{clk\_enable}}{struct clk *\emph{ clk}}{}
inform the system when the clock source should be running.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct clk * clk}}] \leavevmode
clock source

\end{description}

\textbf{Description}

If the clock can not be enabled/disabled, this should return success.

May be called from atomic contexts.

Returns success (0) or negative errno.
\index{clk\_bulk\_enable (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.clk_bulk_enable}\pysiglinewithargsret{int \bfcode{clk\_bulk\_enable}}{int\emph{ num\_clks}, const struct {\hyperref[core\string-api/kernel\string-api:c.clk_bulk_data]{\emph{clk\_bulk\_data}}} *\emph{ clks}}{}
inform the system when the set of clks should be running.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{int num\_clks}}] \leavevmode
the number of clk\_bulk\_data

\item[{\code{const struct clk\_bulk\_data * clks}}] \leavevmode
the clk\_bulk\_data table of consumer

\end{description}

\textbf{Description}

May be called from atomic contexts.

Returns success (0) or negative errno.
\index{clk\_disable (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.clk_disable}\pysiglinewithargsret{void \bfcode{clk\_disable}}{struct clk *\emph{ clk}}{}
inform the system when the clock source is no longer required.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct clk * clk}}] \leavevmode
clock source

\end{description}

\textbf{Description}

Inform the system that a clock source is no longer required by
a driver and may be shut down.

May be called from atomic contexts.

Implementation detail: if the clock source is shared between
multiple drivers, {\hyperref[core\string-api/kernel\string-api:c.clk_enable]{\emph{\code{clk\_enable()}}}} calls must be balanced by the
same number of {\hyperref[core\string-api/kernel\string-api:c.clk_disable]{\emph{\code{clk\_disable()}}}} calls for the clock source to be
disabled.
\index{clk\_bulk\_disable (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.clk_bulk_disable}\pysiglinewithargsret{void \bfcode{clk\_bulk\_disable}}{int\emph{ num\_clks}, const struct {\hyperref[core\string-api/kernel\string-api:c.clk_bulk_data]{\emph{clk\_bulk\_data}}} *\emph{ clks}}{}
inform the system when the set of clks is no longer required.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{int num\_clks}}] \leavevmode
the number of clk\_bulk\_data

\item[{\code{const struct clk\_bulk\_data * clks}}] \leavevmode
the clk\_bulk\_data table of consumer

\end{description}

\textbf{Description}

Inform the system that a set of clks is no longer required by
a driver and may be shut down.

May be called from atomic contexts.

Implementation detail: if the set of clks is shared between
multiple drivers, {\hyperref[core\string-api/kernel\string-api:c.clk_bulk_enable]{\emph{\code{clk\_bulk\_enable()}}}} calls must be balanced by the
same number of {\hyperref[core\string-api/kernel\string-api:c.clk_bulk_disable]{\emph{\code{clk\_bulk\_disable()}}}} calls for the clock source to be
disabled.
\index{clk\_get\_rate (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.clk_get_rate}\pysiglinewithargsret{unsigned long \bfcode{clk\_get\_rate}}{struct clk *\emph{ clk}}{}
obtain the current clock rate (in Hz) for a clock source. This is only valid once the clock source has been enabled.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct clk * clk}}] \leavevmode
clock source

\end{description}
\index{clk\_put (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.clk_put}\pysiglinewithargsret{void \bfcode{clk\_put}}{struct clk *\emph{ clk}}{}
``free'' the clock source

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct clk * clk}}] \leavevmode
clock source

\end{description}

\textbf{Note}

drivers must ensure that all clk\_enable calls made on this
clock source are balanced by clk\_disable calls prior to calling
this function.

clk\_put should not be called from within interrupt context.
\index{clk\_bulk\_put (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.clk_bulk_put}\pysiglinewithargsret{void \bfcode{clk\_bulk\_put}}{int\emph{ num\_clks}, struct {\hyperref[core\string-api/kernel\string-api:c.clk_bulk_data]{\emph{clk\_bulk\_data}}} *\emph{ clks}}{}
``free'' the clock source

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{int num\_clks}}] \leavevmode
the number of clk\_bulk\_data

\item[{\code{struct clk\_bulk\_data * clks}}] \leavevmode
the clk\_bulk\_data table of consumer

\end{description}

\textbf{Note}

drivers must ensure that all clk\_bulk\_enable calls made on this
clock source are balanced by clk\_bulk\_disable calls prior to calling
this function.

clk\_bulk\_put should not be called from within interrupt context.
\index{devm\_clk\_put (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.devm_clk_put}\pysiglinewithargsret{void \bfcode{devm\_clk\_put}}{struct device *\emph{ dev}, struct clk *\emph{ clk}}{}
``free'' a managed clock source

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct device * dev}}] \leavevmode
device used to acquire the clock

\item[{\code{struct clk * clk}}] \leavevmode
clock source acquired with {\hyperref[core\string-api/kernel\string-api:c.devm_clk_get]{\emph{\code{devm\_clk\_get()}}}}

\end{description}

\textbf{Note}

drivers must ensure that all clk\_enable calls made on this
clock source are balanced by clk\_disable calls prior to calling
this function.

clk\_put should not be called from within interrupt context.
\index{clk\_round\_rate (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.clk_round_rate}\pysiglinewithargsret{long \bfcode{clk\_round\_rate}}{struct clk *\emph{ clk}, unsigned long\emph{ rate}}{}
adjust a rate to the exact rate a clock can provide

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct clk * clk}}] \leavevmode
clock source

\item[{\code{unsigned long rate}}] \leavevmode
desired clock rate in Hz

\end{description}

\textbf{Description}

This answers the question ``if I were to pass \textbf{rate} to {\hyperref[core\string-api/kernel\string-api:c.clk_set_rate]{\emph{\code{clk\_set\_rate()}}}},
what clock rate would I end up with?'' without changing the hardware
in any way.  In other words:
\begin{quote}

rate = clk\_round\_rate(clk, r);
\end{quote}

and:
\begin{quote}

clk\_set\_rate(clk, r);
rate = clk\_get\_rate(clk);
\end{quote}

are equivalent except the former does not modify the clock hardware
in any way.

Returns rounded clock rate in Hz, or negative errno.
\index{clk\_set\_rate (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.clk_set_rate}\pysiglinewithargsret{int \bfcode{clk\_set\_rate}}{struct clk *\emph{ clk}, unsigned long\emph{ rate}}{}
set the clock rate for a clock source

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct clk * clk}}] \leavevmode
clock source

\item[{\code{unsigned long rate}}] \leavevmode
desired clock rate in Hz

\end{description}

\textbf{Description}

Returns success (0) or negative errno.
\index{clk\_set\_rate\_exclusive (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.clk_set_rate_exclusive}\pysiglinewithargsret{int \bfcode{clk\_set\_rate\_exclusive}}{struct clk *\emph{ clk}, unsigned long\emph{ rate}}{}
set the clock rate and claim exclusivity over clock source

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct clk * clk}}] \leavevmode
clock source

\item[{\code{unsigned long rate}}] \leavevmode
desired clock rate in Hz

\end{description}

\textbf{Description}

This helper function allows drivers to atomically set the rate of a producer
and claim exclusivity over the rate control of the producer.

It is essentially a combination of {\hyperref[core\string-api/kernel\string-api:c.clk_set_rate]{\emph{\code{clk\_set\_rate()}}}} and
\code{clk\_rate\_exclusite\_get()}. Caller must balance this call with a call to
{\hyperref[core\string-api/kernel\string-api:c.clk_rate_exclusive_put]{\emph{\code{clk\_rate\_exclusive\_put()}}}}

Returns success (0) or negative errno.
\index{clk\_has\_parent (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.clk_has_parent}\pysiglinewithargsret{bool \bfcode{clk\_has\_parent}}{struct clk *\emph{ clk}, struct clk *\emph{ parent}}{}
check if a clock is a possible parent for another

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct clk * clk}}] \leavevmode
clock source

\item[{\code{struct clk * parent}}] \leavevmode
parent clock source

\end{description}

\textbf{Description}

This function can be used in drivers that need to check that a clock can be
the parent of another without actually changing the parent.

Returns true if \textbf{parent} is a possible parent for \textbf{clk}, false otherwise.
\index{clk\_set\_rate\_range (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.clk_set_rate_range}\pysiglinewithargsret{int \bfcode{clk\_set\_rate\_range}}{struct clk *\emph{ clk}, unsigned long\emph{ min}, unsigned long\emph{ max}}{}
set a rate range for a clock source

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct clk * clk}}] \leavevmode
clock source

\item[{\code{unsigned long min}}] \leavevmode
desired minimum clock rate in Hz, inclusive

\item[{\code{unsigned long max}}] \leavevmode
desired maximum clock rate in Hz, inclusive

\end{description}

\textbf{Description}

Returns success (0) or negative errno.
\index{clk\_set\_min\_rate (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.clk_set_min_rate}\pysiglinewithargsret{int \bfcode{clk\_set\_min\_rate}}{struct clk *\emph{ clk}, unsigned long\emph{ rate}}{}
set a minimum clock rate for a clock source

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct clk * clk}}] \leavevmode
clock source

\item[{\code{unsigned long rate}}] \leavevmode
desired minimum clock rate in Hz, inclusive

\end{description}

\textbf{Description}

Returns success (0) or negative errno.
\index{clk\_set\_max\_rate (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.clk_set_max_rate}\pysiglinewithargsret{int \bfcode{clk\_set\_max\_rate}}{struct clk *\emph{ clk}, unsigned long\emph{ rate}}{}
set a maximum clock rate for a clock source

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct clk * clk}}] \leavevmode
clock source

\item[{\code{unsigned long rate}}] \leavevmode
desired maximum clock rate in Hz, inclusive

\end{description}

\textbf{Description}

Returns success (0) or negative errno.
\index{clk\_set\_parent (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.clk_set_parent}\pysiglinewithargsret{int \bfcode{clk\_set\_parent}}{struct clk *\emph{ clk}, struct clk *\emph{ parent}}{}
set the parent clock source for this clock

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct clk * clk}}] \leavevmode
clock source

\item[{\code{struct clk * parent}}] \leavevmode
parent clock source

\end{description}

\textbf{Description}

Returns success (0) or negative errno.
\index{clk\_get\_parent (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.clk_get_parent}\pysiglinewithargsret{struct clk * \bfcode{clk\_get\_parent}}{struct clk *\emph{ clk}}{}
get the parent clock source for this clock

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct clk * clk}}] \leavevmode
clock source

\end{description}

\textbf{Description}

Returns struct clk corresponding to parent clock source, or
valid \code{IS\_ERR()} condition containing errno.
\index{clk\_get\_sys (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.clk_get_sys}\pysiglinewithargsret{struct clk * \bfcode{clk\_get\_sys}}{const char *\emph{ dev\_id}, const char *\emph{ con\_id}}{}
get a clock based upon the device name

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * dev\_id}}] \leavevmode
device name

\item[{\code{const char * con\_id}}] \leavevmode
connection ID

\end{description}

\textbf{Description}

Returns a struct clk corresponding to the clock producer, or
valid \code{IS\_ERR()} condition containing errno.  The implementation
uses \textbf{dev\_id} and \textbf{con\_id} to determine the clock consumer, and
thereby the clock producer. In contrast to {\hyperref[core\string-api/kernel\string-api:c.clk_get]{\emph{\code{clk\_get()}}}} this function
takes the device name instead of the device itself for identification.

Drivers must assume that the clock source is not enabled.

clk\_get\_sys should not be called from within interrupt context.


\subsection{Synchronization Primitives}
\label{core-api/kernel-api:synchronization-primitives}

\subsubsection{Read-Copy Update (RCU)}
\label{core-api/kernel-api:read-copy-update-rcu}\index{RCU\_NONIDLE (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.RCU_NONIDLE}\pysiglinewithargsret{\bfcode{RCU\_NONIDLE}}{\emph{a}}{}
Indicate idle-loop code that needs RCU readers

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{a}}] \leavevmode
Code that RCU needs to pay attention to.

\end{description}

\textbf{Description}

RCU, RCU-bh, and RCU-sched read-side critical sections are forbidden
in the inner idle loop, that is, between the {\hyperref[core\string-api/kernel\string-api:c.rcu_idle_enter]{\emph{\code{rcu\_idle\_enter()}}}} and
the {\hyperref[core\string-api/kernel\string-api:c.rcu_idle_exit]{\emph{\code{rcu\_idle\_exit()}}}} -- RCU will happily ignore any such read-side
critical sections.  However, things like powertop need tracepoints
in the inner idle loop.

This macro provides the way out:  RCU\_NONIDLE(\code{do\_something\_with\_RCU()})
will tell RCU that it needs to pay attention, invoke its argument
(in this example, calling the \code{do\_something\_with\_RCU()} function),
and then tell RCU to go back to ignoring this CPU.  It is permissible
to nest {\hyperref[core\string-api/kernel\string-api:c.RCU_NONIDLE]{\emph{\code{RCU\_NONIDLE()}}}} wrappers, but not indefinitely (but the limit is
on the order of a million or so, even on 32-bit systems).  It is
not legal to block within {\hyperref[core\string-api/kernel\string-api:c.RCU_NONIDLE]{\emph{\code{RCU\_NONIDLE()}}}}, nor is it permissible to
transfer control either into or out of {\hyperref[core\string-api/kernel\string-api:c.RCU_NONIDLE]{\emph{\code{RCU\_NONIDLE()}}}}`s statement.
\index{cond\_resched\_rcu\_qs (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.cond_resched_rcu_qs}\pysiglinewithargsret{\bfcode{cond\_resched\_rcu\_qs}}{}{}
Report potential quiescent states to RCU

\end{fulllineitems}


\textbf{Parameters}

\textbf{Description}

This macro resembles \code{cond\_resched()}, except that it is defined to
report potential quiescent states to RCU-tasks even if the \code{cond\_resched()}
machinery were to be shut off, as some advocate for PREEMPT kernels.
\index{RCU\_LOCKDEP\_WARN (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.RCU_LOCKDEP_WARN}\pysiglinewithargsret{\bfcode{RCU\_LOCKDEP\_WARN}}{\emph{c}, \emph{s}}{}
emit lockdep splat if specified condition is met

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{c}}] \leavevmode
condition to check

\item[{\code{s}}] \leavevmode
informative message

\end{description}
\index{RCU\_INITIALIZER (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.RCU_INITIALIZER}\pysiglinewithargsret{\bfcode{RCU\_INITIALIZER}}{\emph{v}}{}
statically initialize an RCU-protected global variable

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{v}}] \leavevmode
The value to statically initialize with.

\end{description}
\index{rcu\_assign\_pointer (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_assign_pointer}\pysiglinewithargsret{\bfcode{rcu\_assign\_pointer}}{\emph{p}, \emph{v}}{}
assign to RCU-protected pointer

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{p}}] \leavevmode
pointer to assign to

\item[{\code{v}}] \leavevmode
value to assign (publish)

\end{description}

\textbf{Description}

Assigns the specified value to the specified RCU-protected
pointer, ensuring that any concurrent RCU readers will see
any prior initialization.

Inserts memory barriers on architectures that require them
(which is most of them), and also prevents the compiler from
reordering the code that initializes the structure after the pointer
assignment.  More importantly, this call documents which pointers
will be dereferenced by RCU read-side code.

In some special cases, you may use {\hyperref[core\string-api/kernel\string-api:c.RCU_INIT_POINTER]{\emph{\code{RCU\_INIT\_POINTER()}}}} instead
of {\hyperref[core\string-api/kernel\string-api:c.rcu_assign_pointer]{\emph{\code{rcu\_assign\_pointer()}}}}.  {\hyperref[core\string-api/kernel\string-api:c.RCU_INIT_POINTER]{\emph{\code{RCU\_INIT\_POINTER()}}}} is a bit faster due
to the fact that it does not constrain either the CPU or the compiler.
That said, using {\hyperref[core\string-api/kernel\string-api:c.RCU_INIT_POINTER]{\emph{\code{RCU\_INIT\_POINTER()}}}} when you should have used
{\hyperref[core\string-api/kernel\string-api:c.rcu_assign_pointer]{\emph{\code{rcu\_assign\_pointer()}}}} is a very bad thing that results in
impossible-to-diagnose memory corruption.  So please be careful.
See the {\hyperref[core\string-api/kernel\string-api:c.RCU_INIT_POINTER]{\emph{\code{RCU\_INIT\_POINTER()}}}} comment header for details.

Note that {\hyperref[core\string-api/kernel\string-api:c.rcu_assign_pointer]{\emph{\code{rcu\_assign\_pointer()}}}} evaluates each of its arguments only
once, appearances notwithstanding.  One of the ``extra'' evaluations
is in \code{typeof()} and the other visible only to sparse (\_\_CHECKER\_\_),
neither of which actually execute the argument.  As with most cpp
macros, this execute-arguments-only-once property is important, so
please be careful when making changes to {\hyperref[core\string-api/kernel\string-api:c.rcu_assign_pointer]{\emph{\code{rcu\_assign\_pointer()}}}} and the
other macros that it invokes.
\index{rcu\_swap\_protected (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_swap_protected}\pysiglinewithargsret{\bfcode{rcu\_swap\_protected}}{\emph{rcu\_ptr}, \emph{ptr}, \emph{c}}{}
swap an RCU and a regular pointer

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{rcu\_ptr}}] \leavevmode
RCU pointer

\item[{\code{ptr}}] \leavevmode
regular pointer

\item[{\code{c}}] \leavevmode
the conditions under which the dereference will take place

\end{description}

\textbf{Description}

Perform swap(\textbf{rcu\_ptr}, \textbf{ptr}) where \textbf{rcu\_ptr} is an RCU-annotated pointer and
\textbf{c} is the argument that is passed to the {\hyperref[core\string-api/kernel\string-api:c.rcu_dereference_protected]{\emph{\code{rcu\_dereference\_protected()}}}} call
used to read that pointer.
\index{rcu\_access\_pointer (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_access_pointer}\pysiglinewithargsret{\bfcode{rcu\_access\_pointer}}{\emph{p}}{}
fetch RCU pointer with no dereferencing

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{p}}] \leavevmode
The pointer to read

\end{description}

\textbf{Description}

Return the value of the specified RCU-protected pointer, but omit the
lockdep checks for being in an RCU read-side critical section.  This is
useful when the value of this pointer is accessed, but the pointer is
not dereferenced, for example, when testing an RCU-protected pointer
against NULL.  Although {\hyperref[core\string-api/kernel\string-api:c.rcu_access_pointer]{\emph{\code{rcu\_access\_pointer()}}}} may also be used in cases
where update-side locks prevent the value of the pointer from changing,
you should instead use {\hyperref[core\string-api/kernel\string-api:c.rcu_dereference_protected]{\emph{\code{rcu\_dereference\_protected()}}}} for this use case.

It is also permissible to use {\hyperref[core\string-api/kernel\string-api:c.rcu_access_pointer]{\emph{\code{rcu\_access\_pointer()}}}} when read-side
access to the pointer was removed at least one grace period ago, as
is the case in the context of the RCU callback that is freeing up
the data, or after a \code{synchronize\_rcu()} returns.  This can be useful
when tearing down multi-linked structures after a grace period
has elapsed.
\index{rcu\_dereference\_check (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_dereference_check}\pysiglinewithargsret{\bfcode{rcu\_dereference\_check}}{\emph{p}, \emph{c}}{}
rcu\_dereference with debug checking

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{p}}] \leavevmode
The pointer to read, prior to dereferencing

\item[{\code{c}}] \leavevmode
The conditions under which the dereference will take place

\end{description}

\textbf{Description}

Do an {\hyperref[core\string-api/kernel\string-api:c.rcu_dereference]{\emph{\code{rcu\_dereference()}}}}, but check that the conditions under which the
dereference will take place are correct.  Typically the conditions
indicate the various locking conditions that should be held at that
point.  The check should return true if the conditions are satisfied.
An implicit check for being in an RCU read-side critical section
({\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock]{\emph{\code{rcu\_read\_lock()}}}}) is included.

For example:
\begin{quote}

bar = rcu\_dereference\_check(foo-\textgreater{}bar, lockdep\_is\_held(\code{foo-\textgreater{}lock}));
\end{quote}

could be used to indicate to lockdep that foo-\textgreater{}bar may only be dereferenced
if either {\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock]{\emph{\code{rcu\_read\_lock()}}}} is held, or that the lock required to replace
the bar struct at foo-\textgreater{}bar is held.

Note that the list of conditions may also include indications of when a lock
need not be held, for example during initialisation or destruction of the
target struct:
\begin{quote}
\begin{description}
\item[{bar = rcu\_dereference\_check(foo-\textgreater{}bar, lockdep\_is\_held(\code{foo-\textgreater{}lock}) \textbar{}\textbar{}}] \leavevmode
atomic\_read(\code{foo-\textgreater{}usage}) == 0);

\end{description}
\end{quote}

Inserts memory barriers on architectures that require them
(currently only the Alpha), prevents the compiler from refetching
(and from merging fetches), and, more importantly, documents exactly
which pointers are protected by RCU and checks that the pointer is
annotated as \_\_rcu.
\index{rcu\_dereference\_bh\_check (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_dereference_bh_check}\pysiglinewithargsret{\bfcode{rcu\_dereference\_bh\_check}}{\emph{p}, \emph{c}}{}
rcu\_dereference\_bh with debug checking

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{p}}] \leavevmode
The pointer to read, prior to dereferencing

\item[{\code{c}}] \leavevmode
The conditions under which the dereference will take place

\end{description}

\textbf{Description}

This is the RCU-bh counterpart to {\hyperref[core\string-api/kernel\string-api:c.rcu_dereference_check]{\emph{\code{rcu\_dereference\_check()}}}}.
\index{rcu\_dereference\_sched\_check (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_dereference_sched_check}\pysiglinewithargsret{\bfcode{rcu\_dereference\_sched\_check}}{\emph{p}, \emph{c}}{}
rcu\_dereference\_sched with debug checking

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{p}}] \leavevmode
The pointer to read, prior to dereferencing

\item[{\code{c}}] \leavevmode
The conditions under which the dereference will take place

\end{description}

\textbf{Description}

This is the RCU-sched counterpart to {\hyperref[core\string-api/kernel\string-api:c.rcu_dereference_check]{\emph{\code{rcu\_dereference\_check()}}}}.
\index{rcu\_dereference\_protected (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_dereference_protected}\pysiglinewithargsret{\bfcode{rcu\_dereference\_protected}}{\emph{p}, \emph{c}}{}
fetch RCU pointer when updates prevented

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{p}}] \leavevmode
The pointer to read, prior to dereferencing

\item[{\code{c}}] \leavevmode
The conditions under which the dereference will take place

\end{description}

\textbf{Description}

Return the value of the specified RCU-protected pointer, but omit
the \code{READ\_ONCE()}.  This is useful in cases where update-side locks
prevent the value of the pointer from changing.  Please note that this
primitive does \emph{not} prevent the compiler from repeating this reference
or combining it with other references, so it should not be used without
protection of appropriate locks.

This function is only for update-side use.  Using this function
when protected only by {\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock]{\emph{\code{rcu\_read\_lock()}}}} will result in infrequent
but very ugly failures.
\index{rcu\_dereference (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_dereference}\pysiglinewithargsret{\bfcode{rcu\_dereference}}{\emph{p}}{}
fetch RCU-protected pointer for dereferencing

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{p}}] \leavevmode
The pointer to read, prior to dereferencing

\end{description}

\textbf{Description}

This is a simple wrapper around {\hyperref[core\string-api/kernel\string-api:c.rcu_dereference_check]{\emph{\code{rcu\_dereference\_check()}}}}.
\index{rcu\_dereference\_bh (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_dereference_bh}\pysiglinewithargsret{\bfcode{rcu\_dereference\_bh}}{\emph{p}}{}
fetch an RCU-bh-protected pointer for dereferencing

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{p}}] \leavevmode
The pointer to read, prior to dereferencing

\end{description}

\textbf{Description}

Makes {\hyperref[core\string-api/kernel\string-api:c.rcu_dereference_check]{\emph{\code{rcu\_dereference\_check()}}}} do the dirty work.
\index{rcu\_dereference\_sched (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_dereference_sched}\pysiglinewithargsret{\bfcode{rcu\_dereference\_sched}}{\emph{p}}{}
fetch RCU-sched-protected pointer for dereferencing

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{p}}] \leavevmode
The pointer to read, prior to dereferencing

\end{description}

\textbf{Description}

Makes {\hyperref[core\string-api/kernel\string-api:c.rcu_dereference_check]{\emph{\code{rcu\_dereference\_check()}}}} do the dirty work.
\index{rcu\_pointer\_handoff (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_pointer_handoff}\pysiglinewithargsret{\bfcode{rcu\_pointer\_handoff}}{\emph{p}}{}
Hand off a pointer from RCU to other mechanism

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{p}}] \leavevmode
The pointer to hand off

\end{description}

\textbf{Description}

This is simply an identity function, but it documents where a pointer
is handed off from RCU to some other synchronization mechanism, for
example, reference counting or locking.  In C11, it would map to
\code{kill\_dependency()}.  It could be used as follows:
{}`{}`
\begin{quote}

{\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock]{\emph{\code{rcu\_read\_lock()}}}};
p = rcu\_dereference(gp);
long\_lived = is\_long\_lived(p);
if (long\_lived) \{
\begin{quote}
\begin{description}
\item[{if (!atomic\_inc\_not\_zero(p-\textgreater{}refcnt))}] \leavevmode
long\_lived = false;

\item[{else}] \leavevmode
p = rcu\_pointer\_handoff(p);

\end{description}
\end{quote}

\}
{\hyperref[core\string-api/kernel\string-api:c.rcu_read_unlock]{\emph{\code{rcu\_read\_unlock()}}}};
\end{quote}

{\color{red}\bfseries{}{}`{}`}
\index{rcu\_read\_lock (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_read_lock}\pysiglinewithargsret{void \bfcode{rcu\_read\_lock}}{void}{}
mark the beginning of an RCU read-side critical section

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

When \code{synchronize\_rcu()} is invoked on one CPU while other CPUs
are within RCU read-side critical sections, then the
\code{synchronize\_rcu()} is guaranteed to block until after all the other
CPUs exit their critical sections.  Similarly, if \code{call\_rcu()} is invoked
on one CPU while other CPUs are within RCU read-side critical
sections, invocation of the corresponding RCU callback is deferred
until after the all the other CPUs exit their critical sections.

Note, however, that RCU callbacks are permitted to run concurrently
with new RCU read-side critical sections.  One way that this can happen
is via the following sequence of events: (1) CPU 0 enters an RCU
read-side critical section, (2) CPU 1 invokes \code{call\_rcu()} to register
an RCU callback, (3) CPU 0 exits the RCU read-side critical section,
(4) CPU 2 enters a RCU read-side critical section, (5) the RCU
callback is invoked.  This is legal, because the RCU read-side critical
section that was running concurrently with the \code{call\_rcu()} (and which
therefore might be referencing something that the corresponding RCU
callback would free up) has completed before the corresponding
RCU callback is invoked.

RCU read-side critical sections may be nested.  Any deferred actions
will be deferred until the outermost RCU read-side critical section
completes.

You can avoid reading and understanding the next paragraph by
following this rule: don't put anything in an {\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock]{\emph{\code{rcu\_read\_lock()}}}} RCU
read-side critical section that would block in a !PREEMPT kernel.
But if you want the full story, read on!

In non-preemptible RCU implementations (TREE\_RCU and TINY\_RCU),
it is illegal to block while in an RCU read-side critical section.
In preemptible RCU implementations (PREEMPT\_RCU) in CONFIG\_PREEMPT
kernel builds, RCU read-side critical sections may be preempted,
but explicit blocking is illegal.  Finally, in preemptible RCU
implementations in real-time (with -rt patchset) kernel builds, RCU
read-side critical sections may be preempted and they may also block, but
only when acquiring spinlocks that are subject to priority inheritance.
\index{rcu\_read\_unlock (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_read_unlock}\pysiglinewithargsret{void \bfcode{rcu\_read\_unlock}}{void}{}
marks the end of an RCU read-side critical section.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

In most situations, {\hyperref[core\string-api/kernel\string-api:c.rcu_read_unlock]{\emph{\code{rcu\_read\_unlock()}}}} is immune from deadlock.
However, in kernels built with CONFIG\_RCU\_BOOST, {\hyperref[core\string-api/kernel\string-api:c.rcu_read_unlock]{\emph{\code{rcu\_read\_unlock()}}}}
is responsible for deboosting, which it does via \code{rt\_mutex\_unlock()}.
Unfortunately, this function acquires the scheduler's runqueue and
priority-inheritance spinlocks.  This means that deadlock could result
if the caller of {\hyperref[core\string-api/kernel\string-api:c.rcu_read_unlock]{\emph{\code{rcu\_read\_unlock()}}}} already holds one of these locks or
any lock that is ever acquired while holding them; or any lock which
can be taken from interrupt context because \code{rcu\_boost()}-\textgreater{}:c:func:\emph{rt\_mutex\_lock()}
does not disable irqs while taking -\textgreater{}wait\_lock.

That said, RCU readers are never priority boosted unless they were
preempted.  Therefore, one way to avoid deadlock is to make sure
that preemption never happens within any RCU read-side critical
section whose outermost {\hyperref[core\string-api/kernel\string-api:c.rcu_read_unlock]{\emph{\code{rcu\_read\_unlock()}}}} is called with one of
\code{rt\_mutex\_unlock()}`s locks held.  Such preemption can be avoided in
a number of ways, for example, by invoking \code{preempt\_disable()} before
critical section's outermost {\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock]{\emph{\code{rcu\_read\_lock()}}}}.

Given that the set of locks acquired by \code{rt\_mutex\_unlock()} might change
at any time, a somewhat more future-proofed approach is to make sure
that that preemption never happens within any RCU read-side critical
section whose outermost {\hyperref[core\string-api/kernel\string-api:c.rcu_read_unlock]{\emph{\code{rcu\_read\_unlock()}}}} is called with irqs disabled.
This approach relies on the fact that \code{rt\_mutex\_unlock()} currently only
acquires irq-disabled locks.

The second of these two approaches is best in most situations,
however, the first approach can also be useful, at least to those
developers willing to keep abreast of the set of locks acquired by
\code{rt\_mutex\_unlock()}.

See {\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock]{\emph{\code{rcu\_read\_lock()}}}} for more information.
\index{rcu\_read\_lock\_bh (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_read_lock_bh}\pysiglinewithargsret{void \bfcode{rcu\_read\_lock\_bh}}{void}{}
mark the beginning of an RCU-bh critical section

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

This is equivalent of {\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock]{\emph{\code{rcu\_read\_lock()}}}}, but to be used when updates
are being done using \code{call\_rcu\_bh()} or \code{synchronize\_rcu\_bh()}. Since
both \code{call\_rcu\_bh()} and \code{synchronize\_rcu\_bh()} consider completion of a
softirq handler to be a quiescent state, a process in RCU read-side
critical section must be protected by disabling softirqs. Read-side
critical sections in interrupt context can use just {\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock]{\emph{\code{rcu\_read\_lock()}}}},
though this should at least be commented to avoid confusing people
reading the code.

Note that {\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock_bh]{\emph{\code{rcu\_read\_lock\_bh()}}}} and the matching \code{rcu\_read\_unlock\_bh()}
must occur in the same context, for example, it is illegal to invoke
\code{rcu\_read\_unlock\_bh()} from one task if the matching {\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock_bh]{\emph{\code{rcu\_read\_lock\_bh()}}}}
was invoked from some other task.
\index{rcu\_read\_lock\_sched (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_read_lock_sched}\pysiglinewithargsret{void \bfcode{rcu\_read\_lock\_sched}}{void}{}
mark the beginning of a RCU-sched critical section

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

This is equivalent of {\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock]{\emph{\code{rcu\_read\_lock()}}}}, but to be used when updates
are being done using \code{call\_rcu\_sched()} or \code{synchronize\_rcu\_sched()}.
Read-side critical sections can also be introduced by anything that
disables preemption, including \code{local\_irq\_disable()} and friends.

Note that {\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock_sched]{\emph{\code{rcu\_read\_lock\_sched()}}}} and the matching \code{rcu\_read\_unlock\_sched()}
must occur in the same context, for example, it is illegal to invoke
\code{rcu\_read\_unlock\_sched()} from process context if the matching
{\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock_sched]{\emph{\code{rcu\_read\_lock\_sched()}}}} was invoked from an NMI handler.
\index{RCU\_INIT\_POINTER (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.RCU_INIT_POINTER}\pysiglinewithargsret{\bfcode{RCU\_INIT\_POINTER}}{\emph{p}, \emph{v}}{}
initialize an RCU protected pointer

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{p}}] \leavevmode
The pointer to be initialized.

\item[{\code{v}}] \leavevmode
The value to initialized the pointer to.

\end{description}

\textbf{Description}

Initialize an RCU-protected pointer in special cases where readers
do not need ordering constraints on the CPU or the compiler.  These
special cases are:
\begin{enumerate}
\item {} 
This use of {\hyperref[core\string-api/kernel\string-api:c.RCU_INIT_POINTER]{\emph{\code{RCU\_INIT\_POINTER()}}}} is NULLing out the pointer \emph{or}

\item {} 
The caller has taken whatever steps are required to prevent
RCU readers from concurrently accessing this pointer \emph{or}

\item {} 
The referenced data structure has already been exposed to
readers either at compile time or via {\hyperref[core\string-api/kernel\string-api:c.rcu_assign_pointer]{\emph{\code{rcu\_assign\_pointer()}}}} \emph{and}
\begin{enumerate}
\item {} 
You have not made \emph{any} reader-visible changes to
this structure since then \emph{or}

\item {} 
It is OK for readers accessing this structure from its
new location to see the old state of the structure.  (For
example, the changes were to statistical counters or to
other state where exact synchronization is not required.)

\end{enumerate}

\end{enumerate}

Failure to follow these rules governing use of {\hyperref[core\string-api/kernel\string-api:c.RCU_INIT_POINTER]{\emph{\code{RCU\_INIT\_POINTER()}}}} will
result in impossible-to-diagnose memory corruption.  As in the structures
will look OK in crash dumps, but any concurrent RCU readers might
see pre-initialized values of the referenced data structure.  So
please be very careful how you use {\hyperref[core\string-api/kernel\string-api:c.RCU_INIT_POINTER]{\emph{\code{RCU\_INIT\_POINTER()}}}}!!!

If you are creating an RCU-protected linked structure that is accessed
by a single external-to-structure RCU-protected pointer, then you may
use {\hyperref[core\string-api/kernel\string-api:c.RCU_INIT_POINTER]{\emph{\code{RCU\_INIT\_POINTER()}}}} to initialize the internal RCU-protected
pointers, but you must use {\hyperref[core\string-api/kernel\string-api:c.rcu_assign_pointer]{\emph{\code{rcu\_assign\_pointer()}}}} to initialize the
external-to-structure pointer \emph{after} you have completely initialized
the reader-accessible portions of the linked structure.

Note that unlike {\hyperref[core\string-api/kernel\string-api:c.rcu_assign_pointer]{\emph{\code{rcu\_assign\_pointer()}}}}, {\hyperref[core\string-api/kernel\string-api:c.RCU_INIT_POINTER]{\emph{\code{RCU\_INIT\_POINTER()}}}} provides no
ordering guarantees for either the CPU or the compiler.
\index{RCU\_POINTER\_INITIALIZER (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.RCU_POINTER_INITIALIZER}\pysiglinewithargsret{\bfcode{RCU\_POINTER\_INITIALIZER}}{\emph{p}, \emph{v}}{}
statically initialize an RCU protected pointer

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{p}}] \leavevmode
The pointer to be initialized.

\item[{\code{v}}] \leavevmode
The value to initialized the pointer to.

\end{description}

\textbf{Description}

GCC-style initialization for an RCU-protected pointer in a structure field.
\index{kfree\_rcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.kfree_rcu}\pysiglinewithargsret{\bfcode{kfree\_rcu}}{\emph{ptr}, \emph{rcu\_head}}{}
kfree an object after a grace period.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{ptr}}] \leavevmode
pointer to kfree

\item[{\code{rcu\_head}}] \leavevmode
the name of the struct rcu\_head within the type of \textbf{ptr}.

\end{description}

\textbf{Description}

Many rcu callbacks functions just call {\hyperref[core\string-api/kernel\string-api:c.kfree]{\emph{\code{kfree()}}}} on the base structure.
These functions are trivial, but their size adds up, and furthermore
when they are used in a kernel module, that module must invoke the
high-latency \code{rcu\_barrier()} function at module-unload time.

The {\hyperref[core\string-api/kernel\string-api:c.kfree_rcu]{\emph{\code{kfree\_rcu()}}}} function handles this issue.  Rather than encoding a
function address in the embedded rcu\_head structure, {\hyperref[core\string-api/kernel\string-api:c.kfree_rcu]{\emph{\code{kfree\_rcu()}}}} instead
encodes the offset of the rcu\_head structure within the base structure.
Because the functions are not allowed in the low-order 4096 bytes of
kernel virtual memory, offsets up to 4095 bytes can be accommodated.
If the offset is larger than 4095 bytes, a compile-time error will
be generated in \code{\_\_kfree\_rcu()}.  If this error is triggered, you can
either fall back to use of \code{call\_rcu()} or rearrange the structure to
position the rcu\_head structure into the first 4096 bytes.

Note that the allowable offset might decrease in the future, for example,
to allow something like \code{kmem\_cache\_free\_rcu()}.

The BUILD\_BUG\_ON check must not involve any function calls, hence the
checks are done in macros here.
\index{synchronize\_rcu\_mult (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.synchronize_rcu_mult}\pysiglinewithargsret{\bfcode{synchronize\_rcu\_mult}}{\emph{...}}{}
Wait concurrently for multiple grace periods

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{...}}] \leavevmode
List of \code{call\_rcu()} functions for the flavors to wait on.

\end{description}

\textbf{Description}

This macro waits concurrently for multiple flavors of RCU grace periods.
For example, synchronize\_rcu\_mult(call\_rcu, call\_rcu\_bh) would wait
on concurrent RCU and RCU-bh grace periods.  Waiting on a give SRCU
domain requires you to write a wrapper function for that SRCU domain's
{\hyperref[core\string-api/kernel\string-api:c.call_srcu]{\emph{\code{call\_srcu()}}}} function, supplying the corresponding srcu\_struct.

If Tiny RCU, tell \code{\_wait\_rcu\_gp()} not to bother waiting for RCU
or RCU-bh, given that anywhere {\hyperref[core\string-api/kernel\string-api:c.synchronize_rcu_mult]{\emph{\code{synchronize\_rcu\_mult()}}}} can be called
is automatically a grace period.
\index{synchronize\_rcu\_bh\_expedited (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.synchronize_rcu_bh_expedited}\pysiglinewithargsret{void \bfcode{synchronize\_rcu\_bh\_expedited}}{void}{}
Brute-force RCU-bh grace period

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

Wait for an RCU-bh grace period to elapse, but use a ``big hammer''
approach to force the grace period to end quickly.  This consumes
significant time on all CPUs and is unfriendly to real-time workloads,
so is thus not recommended for any sort of common-case code.  In fact,
if you are using {\hyperref[core\string-api/kernel\string-api:c.synchronize_rcu_bh_expedited]{\emph{\code{synchronize\_rcu\_bh\_expedited()}}}} in a loop, please
restructure your code to batch your updates, and then use a single
\code{synchronize\_rcu\_bh()} instead.

Note that it is illegal to call this function while holding any lock
that is acquired by a CPU-hotplug notifier.  And yes, it is also illegal
to call this function from a CPU-hotplug notifier.  Failing to observe
these restriction will result in deadlock.
\index{rcu\_idle\_enter (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_idle_enter}\pysiglinewithargsret{void \bfcode{rcu\_idle\_enter}}{void}{}
inform RCU that current CPU is entering idle

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

Enter idle mode, in other words, -leave- the mode in which RCU
read-side critical sections can occur.  (Though RCU read-side
critical sections can occur in irq handlers in idle, a possibility
handled by \code{irq\_enter()} and \code{irq\_exit()}.)

If you add or remove a call to {\hyperref[core\string-api/kernel\string-api:c.rcu_idle_enter]{\emph{\code{rcu\_idle\_enter()}}}}, be sure to test with
CONFIG\_RCU\_EQS\_DEBUG=y.
\index{rcu\_user\_enter (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_user_enter}\pysiglinewithargsret{void \bfcode{rcu\_user\_enter}}{void}{}
inform RCU that we are resuming userspace.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

Enter RCU idle mode right before resuming userspace.  No use of RCU
is permitted between this call and {\hyperref[core\string-api/kernel\string-api:c.rcu_user_exit]{\emph{\code{rcu\_user\_exit()}}}}. This way the
CPU doesn't need to maintain the tick for RCU maintenance purposes
when the CPU runs in userspace.

If you add or remove a call to {\hyperref[core\string-api/kernel\string-api:c.rcu_user_enter]{\emph{\code{rcu\_user\_enter()}}}}, be sure to test with
CONFIG\_RCU\_EQS\_DEBUG=y.
\index{rcu\_nmi\_exit (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_nmi_exit}\pysiglinewithargsret{void \bfcode{rcu\_nmi\_exit}}{void}{}
inform RCU of exit from NMI context

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

If we are returning from the outermost NMI handler that interrupted an
RCU-idle period, update rdtp-\textgreater{}dynticks and rdtp-\textgreater{}dynticks\_nmi\_nesting
to let the RCU grace-period handling know that the CPU is back to
being RCU-idle.

If you add or remove a call to {\hyperref[core\string-api/kernel\string-api:c.rcu_nmi_exit]{\emph{\code{rcu\_nmi\_exit()}}}}, be sure to test
with CONFIG\_RCU\_EQS\_DEBUG=y.
\index{rcu\_irq\_exit (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_irq_exit}\pysiglinewithargsret{void \bfcode{rcu\_irq\_exit}}{void}{}
inform RCU that current CPU is exiting irq towards idle

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

Exit from an interrupt handler, which might possibly result in entering
idle mode, in other words, leaving the mode in which read-side critical
sections can occur.  The caller must have disabled interrupts.

This code assumes that the idle loop never does anything that might
result in unbalanced calls to \code{irq\_enter()} and \code{irq\_exit()}.  If your
architecture's idle loop violates this assumption, RCU will give you what
you deserve, good and hard.  But very infrequently and irreproducibly.

Use things like work queues to work around this limitation.

You have been warned.

If you add or remove a call to {\hyperref[core\string-api/kernel\string-api:c.rcu_irq_exit]{\emph{\code{rcu\_irq\_exit()}}}}, be sure to test with
CONFIG\_RCU\_EQS\_DEBUG=y.
\index{rcu\_idle\_exit (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_idle_exit}\pysiglinewithargsret{void \bfcode{rcu\_idle\_exit}}{void}{}
inform RCU that current CPU is leaving idle

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

Exit idle mode, in other words, -enter- the mode in which RCU
read-side critical sections can occur.

If you add or remove a call to {\hyperref[core\string-api/kernel\string-api:c.rcu_idle_exit]{\emph{\code{rcu\_idle\_exit()}}}}, be sure to test with
CONFIG\_RCU\_EQS\_DEBUG=y.
\index{rcu\_user\_exit (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_user_exit}\pysiglinewithargsret{void \bfcode{rcu\_user\_exit}}{void}{}
inform RCU that we are exiting userspace.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

Exit RCU idle mode while entering the kernel because it can
run a RCU read side critical section anytime.

If you add or remove a call to {\hyperref[core\string-api/kernel\string-api:c.rcu_user_exit]{\emph{\code{rcu\_user\_exit()}}}}, be sure to test with
CONFIG\_RCU\_EQS\_DEBUG=y.
\index{rcu\_nmi\_enter (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_nmi_enter}\pysiglinewithargsret{void \bfcode{rcu\_nmi\_enter}}{void}{}
inform RCU of entry to NMI context

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

If the CPU was idle from RCU's viewpoint, update rdtp-\textgreater{}dynticks and
rdtp-\textgreater{}dynticks\_nmi\_nesting to let the RCU grace-period handling know
that the CPU is active.  This implementation permits nested NMIs, as
long as the nesting level does not overflow an int.  (You will probably
run out of stack space first.)

If you add or remove a call to {\hyperref[core\string-api/kernel\string-api:c.rcu_nmi_enter]{\emph{\code{rcu\_nmi\_enter()}}}}, be sure to test
with CONFIG\_RCU\_EQS\_DEBUG=y.
\index{rcu\_irq\_enter (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_irq_enter}\pysiglinewithargsret{void \bfcode{rcu\_irq\_enter}}{void}{}
inform RCU that current CPU is entering irq away from idle

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

Enter an interrupt handler, which might possibly result in exiting
idle mode, in other words, entering the mode in which read-side critical
sections can occur.  The caller must have disabled interrupts.

Note that the Linux kernel is fully capable of entering an interrupt
handler that it never exits, for example when doing upcalls to user mode!
This code assumes that the idle loop never does upcalls to user mode.
If your architecture's idle loop does do upcalls to user mode (or does
anything else that results in unbalanced calls to the \code{irq\_enter()} and
\code{irq\_exit()} functions), RCU will give you what you deserve, good and hard.
But very infrequently and irreproducibly.

Use things like work queues to work around this limitation.

You have been warned.

If you add or remove a call to {\hyperref[core\string-api/kernel\string-api:c.rcu_irq_enter]{\emph{\code{rcu\_irq\_enter()}}}}, be sure to test with
CONFIG\_RCU\_EQS\_DEBUG=y.
\index{rcu\_is\_watching (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_is_watching}\pysiglinewithargsret{bool notrace \bfcode{rcu\_is\_watching}}{void}{}
see if RCU thinks that the current CPU is idle

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

Return true if RCU is watching the running CPU, which means that this
CPU can safely enter RCU read-side critical sections.  In other words,
if the current CPU is in its idle loop and is neither in an interrupt
or NMI handler, return true.
\index{rcu\_is\_cpu\_rrupt\_from\_idle (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_is_cpu_rrupt_from_idle}\pysiglinewithargsret{int \bfcode{rcu\_is\_cpu\_rrupt\_from\_idle}}{void}{}
see if idle or immediately interrupted from idle

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

If the current CPU is idle or running at a first-level (not nested)
interrupt from idle, return true.  The caller must have at least
disabled preemption.
\index{rcu\_cpu\_stall\_reset (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_cpu_stall_reset}\pysiglinewithargsret{void \bfcode{rcu\_cpu\_stall\_reset}}{void}{}
prevent further stall warnings in current grace period

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

Set the stall-warning timeout way off into the future, thus preventing
any RCU CPU stall-warning messages from appearing in the current set of
RCU grace periods.

The caller must disable hard irqs.
\index{call\_rcu\_sched (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.call_rcu_sched}\pysiglinewithargsret{void \bfcode{call\_rcu\_sched}}{struct rcu\_head *\emph{ head}, rcu\_callback\_t\emph{ func}}{}
Queue an RCU for invocation after sched grace period.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct rcu\_head * head}}] \leavevmode
structure to be used for queueing the RCU updates.

\item[{\code{rcu\_callback\_t func}}] \leavevmode
actual callback function to be invoked after the grace period

\end{description}

\textbf{Description}

The callback function will be invoked some time after a full grace
period elapses, in other words after all currently executing RCU
read-side critical sections have completed. \code{call\_rcu\_sched()} assumes
that the read-side critical sections end on enabling of preemption
or on voluntary preemption.
RCU read-side critical sections are delimited by:
\begin{itemize}
\item {} 
{\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock_sched]{\emph{\code{rcu\_read\_lock\_sched()}}}} and \code{rcu\_read\_unlock\_sched()}, OR

\item {} 
anything that disables preemption.

\end{itemize}
\begin{quote}

These may be nested.
\end{quote}

See the description of \code{call\_rcu()} for more detailed information on
memory ordering guarantees.
\index{call\_rcu\_bh (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.call_rcu_bh}\pysiglinewithargsret{void \bfcode{call\_rcu\_bh}}{struct rcu\_head *\emph{ head}, rcu\_callback\_t\emph{ func}}{}
Queue an RCU for invocation after a quicker grace period.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct rcu\_head * head}}] \leavevmode
structure to be used for queueing the RCU updates.

\item[{\code{rcu\_callback\_t func}}] \leavevmode
actual callback function to be invoked after the grace period

\end{description}

\textbf{Description}

The callback function will be invoked some time after a full grace
period elapses, in other words after all currently executing RCU
read-side critical sections have completed. \code{call\_rcu\_bh()} assumes
that the read-side critical sections end on completion of a softirq
handler. This means that read-side critical sections in process
context must not be interrupted by softirqs. This interface is to be
used when most of the read-side critical sections are in softirq context.
RCU read-side critical sections are delimited by:
\begin{itemize}
\item {} 
{\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock]{\emph{\code{rcu\_read\_lock()}}}} and  {\hyperref[core\string-api/kernel\string-api:c.rcu_read_unlock]{\emph{\code{rcu\_read\_unlock()}}}}, if in interrupt context, OR

\item {} 
{\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock_bh]{\emph{\code{rcu\_read\_lock\_bh()}}}} and \code{rcu\_read\_unlock\_bh()}, if in process context.

\end{itemize}

These may be nested.

See the description of \code{call\_rcu()} for more detailed information on
memory ordering guarantees.
\index{synchronize\_sched (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.synchronize_sched}\pysiglinewithargsret{void \bfcode{synchronize\_sched}}{void}{}
wait until an rcu-sched grace period has elapsed.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

Control will return to the caller some time after a full rcu-sched
grace period has elapsed, in other words after all currently executing
rcu-sched read-side critical sections have completed.   These read-side
critical sections are delimited by {\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock_sched]{\emph{\code{rcu\_read\_lock\_sched()}}}} and
\code{rcu\_read\_unlock\_sched()}, and may be nested.  Note that \code{preempt\_disable()},
\code{local\_irq\_disable()}, and so on may be used in place of
{\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock_sched]{\emph{\code{rcu\_read\_lock\_sched()}}}}.

This means that all preempt\_disable code sequences, including NMI and
non-threaded hardware-interrupt handlers, in progress on entry will
have completed before this primitive returns.  However, this does not
guarantee that softirq handlers will have completed, since in some
kernels, these handlers can run in process context, and can block.

Note that this guarantee implies further memory-ordering guarantees.
On systems with more than one CPU, when \code{synchronize\_sched()} returns,
each CPU is guaranteed to have executed a full memory barrier since the
end of its last RCU-sched read-side critical section whose beginning
preceded the call to \code{synchronize\_sched()}.  In addition, each CPU having
an RCU read-side critical section that extends beyond the return from
\code{synchronize\_sched()} is guaranteed to have executed a full memory barrier
after the beginning of \code{synchronize\_sched()} and before the beginning of
that RCU read-side critical section.  Note that these guarantees include
CPUs that are offline, idle, or executing in user mode, as well as CPUs
that are executing in the kernel.

Furthermore, if CPU A invoked \code{synchronize\_sched()}, which returned
to its caller on CPU B, then both CPU A and CPU B are guaranteed
to have executed a full memory barrier during the execution of
\code{synchronize\_sched()} -- even if CPU A and CPU B are the same CPU (but
again only if the system has more than one CPU).
\index{synchronize\_rcu\_bh (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.synchronize_rcu_bh}\pysiglinewithargsret{void \bfcode{synchronize\_rcu\_bh}}{void}{}
wait until an rcu\_bh grace period has elapsed.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

Control will return to the caller some time after a full rcu\_bh grace
period has elapsed, in other words after all currently executing rcu\_bh
read-side critical sections have completed.  RCU read-side critical
sections are delimited by {\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock_bh]{\emph{\code{rcu\_read\_lock\_bh()}}}} and \code{rcu\_read\_unlock\_bh()},
and may be nested.

See the description of \code{synchronize\_sched()} for more detailed information
on memory ordering guarantees.
\index{get\_state\_synchronize\_rcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.get_state_synchronize_rcu}\pysiglinewithargsret{unsigned long \bfcode{get\_state\_synchronize\_rcu}}{void}{}
Snapshot current RCU state

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

Returns a cookie that is used by a later call to \code{cond\_synchronize\_rcu()}
to determine whether or not a full grace period has elapsed in the
meantime.
\index{cond\_synchronize\_rcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.cond_synchronize_rcu}\pysiglinewithargsret{void \bfcode{cond\_synchronize\_rcu}}{unsigned long\emph{ oldstate}}{}
Conditionally wait for an RCU grace period

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned long oldstate}}] \leavevmode
return value from earlier call to \code{get\_state\_synchronize\_rcu()}

\end{description}

\textbf{Description}

If a full RCU grace period has elapsed since the earlier call to
\code{get\_state\_synchronize\_rcu()}, just return.  Otherwise, invoke
\code{synchronize\_rcu()} to wait for a full grace period.

Yes, this function does not take counter wrap into account.  But
counter wrap is harmless.  If the counter wraps, we have waited for
more than 2 billion grace periods (and way more on a 64-bit system!),
so waiting for one additional grace period should be just fine.
\index{get\_state\_synchronize\_sched (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.get_state_synchronize_sched}\pysiglinewithargsret{unsigned long \bfcode{get\_state\_synchronize\_sched}}{void}{}
Snapshot current RCU-sched state

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

Returns a cookie that is used by a later call to \code{cond\_synchronize\_sched()}
to determine whether or not a full grace period has elapsed in the
meantime.
\index{cond\_synchronize\_sched (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.cond_synchronize_sched}\pysiglinewithargsret{void \bfcode{cond\_synchronize\_sched}}{unsigned long\emph{ oldstate}}{}
Conditionally wait for an RCU-sched grace period

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned long oldstate}}] \leavevmode
return value from earlier call to \code{get\_state\_synchronize\_sched()}

\end{description}

\textbf{Description}

If a full RCU-sched grace period has elapsed since the earlier call to
\code{get\_state\_synchronize\_sched()}, just return.  Otherwise, invoke
\code{synchronize\_sched()} to wait for a full grace period.

Yes, this function does not take counter wrap into account.  But
counter wrap is harmless.  If the counter wraps, we have waited for
more than 2 billion grace periods (and way more on a 64-bit system!),
so waiting for one additional grace period should be just fine.
\index{rcu\_barrier\_bh (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_barrier_bh}\pysiglinewithargsret{void \bfcode{rcu\_barrier\_bh}}{void}{}
Wait until all in-flight \code{call\_rcu\_bh()} callbacks complete.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}
\index{rcu\_barrier\_sched (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_barrier_sched}\pysiglinewithargsret{void \bfcode{rcu\_barrier\_sched}}{void}{}
Wait for in-flight \code{call\_rcu\_sched()} callbacks.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}
\index{call\_rcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.call_rcu}\pysiglinewithargsret{void \bfcode{call\_rcu}}{struct rcu\_head *\emph{ head}, rcu\_callback\_t\emph{ func}}{}
Queue an RCU callback for invocation after a grace period.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct rcu\_head * head}}] \leavevmode
structure to be used for queueing the RCU updates.

\item[{\code{rcu\_callback\_t func}}] \leavevmode
actual callback function to be invoked after the grace period

\end{description}

\textbf{Description}

The callback function will be invoked some time after a full grace
period elapses, in other words after all pre-existing RCU read-side
critical sections have completed.  However, the callback function
might well execute concurrently with RCU read-side critical sections
that started after \code{call\_rcu()} was invoked.  RCU read-side critical
sections are delimited by {\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock]{\emph{\code{rcu\_read\_lock()}}}} and {\hyperref[core\string-api/kernel\string-api:c.rcu_read_unlock]{\emph{\code{rcu\_read\_unlock()}}}},
and may be nested.

Note that all CPUs must agree that the grace period extended beyond
all pre-existing RCU read-side critical section.  On systems with more
than one CPU, this means that when ``\code{func()}'' is invoked, each CPU is
guaranteed to have executed a full memory barrier since the end of its
last RCU read-side critical section whose beginning preceded the call
to \code{call\_rcu()}.  It also means that each CPU executing an RCU read-side
critical section that continues beyond the start of ``\code{func()}'' must have
executed a memory barrier after the \code{call\_rcu()} but before the beginning
of that RCU read-side critical section.  Note that these guarantees
include CPUs that are offline, idle, or executing in user mode, as
well as CPUs that are executing in the kernel.

Furthermore, if CPU A invoked \code{call\_rcu()} and CPU B invoked the
resulting RCU callback function ``\code{func()}'', then both CPU A and CPU B are
guaranteed to execute a full memory barrier during the time interval
between the call to \code{call\_rcu()} and the invocation of ``\code{func()}'' -- even
if CPU A and CPU B are the same CPU (but again only if the system has
more than one CPU).
\index{synchronize\_rcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.synchronize_rcu}\pysiglinewithargsret{void \bfcode{synchronize\_rcu}}{void}{}
wait until a grace period has elapsed.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

Control will return to the caller some time after a full grace
period has elapsed, in other words after all currently executing RCU
read-side critical sections have completed.  Note, however, that
upon return from \code{synchronize\_rcu()}, the caller might well be executing
concurrently with new RCU read-side critical sections that began while
\code{synchronize\_rcu()} was waiting.  RCU read-side critical sections are
delimited by {\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock]{\emph{\code{rcu\_read\_lock()}}}} and {\hyperref[core\string-api/kernel\string-api:c.rcu_read_unlock]{\emph{\code{rcu\_read\_unlock()}}}}, and may be nested.

See the description of \code{synchronize\_sched()} for more detailed
information on memory-ordering guarantees.  However, please note
that -only- the memory-ordering guarantees apply.  For example,
\code{synchronize\_rcu()} is -not- guaranteed to wait on things like code
protected by \code{preempt\_disable()}, instead, \code{synchronize\_rcu()} is -only-
guaranteed to wait on RCU read-side critical sections, that is, sections
of code protected by {\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock]{\emph{\code{rcu\_read\_lock()}}}}.
\index{rcu\_barrier (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_barrier}\pysiglinewithargsret{void \bfcode{rcu\_barrier}}{void}{}
Wait until all in-flight \code{call\_rcu()} callbacks complete.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

Note that this primitive does not necessarily wait for an RCU grace period
to complete.  For example, if there are no RCU callbacks queued anywhere
in the system, then \code{rcu\_barrier()} is within its rights to return
immediately, without waiting for anything, much less an RCU grace period.
\index{synchronize\_sched\_expedited (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.synchronize_sched_expedited}\pysiglinewithargsret{void \bfcode{synchronize\_sched\_expedited}}{void}{}
Brute-force RCU-sched grace period

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

Wait for an RCU-sched grace period to elapse, but use a ``big hammer''
approach to force the grace period to end quickly.  This consumes
significant time on all CPUs and is unfriendly to real-time workloads,
so is thus not recommended for any sort of common-case code.  In fact,
if you are using {\hyperref[core\string-api/kernel\string-api:c.synchronize_sched_expedited]{\emph{\code{synchronize\_sched\_expedited()}}}} in a loop, please
restructure your code to batch your updates, and then use a single
\code{synchronize\_sched()} instead.

This implementation can be thought of as an application of sequence
locking to expedited grace periods, but using the sequence counter to
determine when someone else has already done the work instead of for
retrying readers.
\index{synchronize\_rcu\_expedited (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.synchronize_rcu_expedited}\pysiglinewithargsret{void \bfcode{synchronize\_rcu\_expedited}}{void}{}
Brute-force RCU grace period

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

Wait for an RCU-preempt grace period, but expedite it.  The basic
idea is to IPI all non-idle non-nohz online CPUs.  The IPI handler
checks whether the CPU is in an RCU-preempt critical section, and
if so, it sets a flag that causes the outermost {\hyperref[core\string-api/kernel\string-api:c.rcu_read_unlock]{\emph{\code{rcu\_read\_unlock()}}}}
to report the quiescent state.  On the other hand, if the CPU is
not in an RCU read-side critical section, the IPI handler reports
the quiescent state immediately.

Although this is a greate improvement over previous expedited
implementations, it is still unfriendly to real-time workloads, so is
thus not recommended for any sort of common-case code.  In fact, if
you are using {\hyperref[core\string-api/kernel\string-api:c.synchronize_rcu_expedited]{\emph{\code{synchronize\_rcu\_expedited()}}}} in a loop, please restructure
your code to batch your updates, and then Use a single \code{synchronize\_rcu()}
instead.
\index{rcu\_read\_lock\_sched\_held (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_read_lock_sched_held}\pysiglinewithargsret{int \bfcode{rcu\_read\_lock\_sched\_held}}{void}{}
might we be in RCU-sched read-side critical section?

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

If CONFIG\_DEBUG\_LOCK\_ALLOC is selected, returns nonzero iff in an
RCU-sched read-side critical section.  In absence of
CONFIG\_DEBUG\_LOCK\_ALLOC, this assumes we are in an RCU-sched read-side
critical section unless it can prove otherwise.  Note that disabling
of preemption (including disabling irqs) counts as an RCU-sched
read-side critical section.  This is useful for debug checks in functions
that required that they be called within an RCU-sched read-side
critical section.

Check \code{debug\_lockdep\_rcu\_enabled()} to prevent false positives during boot
and while lockdep is disabled.

Note that if the CPU is in the idle loop from an RCU point of
view (ie: that we are in the section between {\hyperref[core\string-api/kernel\string-api:c.rcu_idle_enter]{\emph{\code{rcu\_idle\_enter()}}}} and
{\hyperref[core\string-api/kernel\string-api:c.rcu_idle_exit]{\emph{\code{rcu\_idle\_exit()}}}}) then \code{rcu\_read\_lock\_held()} returns false even if the CPU
did an {\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock]{\emph{\code{rcu\_read\_lock()}}}}.  The reason for this is that RCU ignores CPUs
that are in such a section, considering these as in extended quiescent
state, so such a CPU is effectively never in an RCU read-side critical
section regardless of what RCU primitives it invokes.  This state of
affairs is required --- we need to keep an RCU-free window in idle
where the CPU may possibly enter into low power mode. This way we can
notice an extended quiescent state to other CPUs that started a grace
period. Otherwise we would delay any grace period as long as we run in
the idle task.

Similarly, we avoid claiming an SRCU read lock held if the current
CPU is offline.
\index{rcu\_expedite\_gp (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_expedite_gp}\pysiglinewithargsret{void \bfcode{rcu\_expedite\_gp}}{void}{}
Expedite future RCU grace periods

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

After a call to this function, future calls to \code{synchronize\_rcu()} and
friends act as the corresponding {\hyperref[core\string-api/kernel\string-api:c.synchronize_rcu_expedited]{\emph{\code{synchronize\_rcu\_expedited()}}}} function
had instead been called.
\index{rcu\_unexpedite\_gp (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_unexpedite_gp}\pysiglinewithargsret{void \bfcode{rcu\_unexpedite\_gp}}{void}{}
Cancel prior \code{rcu\_expedite\_gp()} invocation

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

Undo a prior call to \code{rcu\_expedite\_gp()}.  If all prior calls to
\code{rcu\_expedite\_gp()} are undone by a subsequent call to \code{rcu\_unexpedite\_gp()},
and if the rcu\_expedited sysfs/boot parameter is not set, then all
subsequent calls to \code{synchronize\_rcu()} and friends will return to
their normal non-expedited behavior.
\index{rcu\_read\_lock\_held (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_read_lock_held}\pysiglinewithargsret{int \bfcode{rcu\_read\_lock\_held}}{void}{}
might we be in RCU read-side critical section?

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

If CONFIG\_DEBUG\_LOCK\_ALLOC is selected, returns nonzero iff in an RCU
read-side critical section.  In absence of CONFIG\_DEBUG\_LOCK\_ALLOC,
this assumes we are in an RCU read-side critical section unless it can
prove otherwise.  This is useful for debug checks in functions that
require that they be called within an RCU read-side critical section.

Checks \code{debug\_lockdep\_rcu\_enabled()} to prevent false positives during boot
and while lockdep is disabled.

Note that {\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock]{\emph{\code{rcu\_read\_lock()}}}} and the matching {\hyperref[core\string-api/kernel\string-api:c.rcu_read_unlock]{\emph{\code{rcu\_read\_unlock()}}}} must
occur in the same context, for example, it is illegal to invoke
{\hyperref[core\string-api/kernel\string-api:c.rcu_read_unlock]{\emph{\code{rcu\_read\_unlock()}}}} in process context if the matching {\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock]{\emph{\code{rcu\_read\_lock()}}}}
was invoked from within an irq handler.

Note that {\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock]{\emph{\code{rcu\_read\_lock()}}}} is disallowed if the CPU is either idle or
offline from an RCU perspective, so check for those as well.
\index{rcu\_read\_lock\_bh\_held (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_read_lock_bh_held}\pysiglinewithargsret{int \bfcode{rcu\_read\_lock\_bh\_held}}{void}{}
might we be in RCU-bh read-side critical section?

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

Check for bottom half being disabled, which covers both the
CONFIG\_PROVE\_RCU and not cases.  Note that if someone uses
{\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock_bh]{\emph{\code{rcu\_read\_lock\_bh()}}}}, but then later enables BH, lockdep (if enabled)
will show the situation.  This is useful for debug checks in functions
that require that they be called within an RCU read-side critical
section.

Check \code{debug\_lockdep\_rcu\_enabled()} to prevent false positives during boot.

Note that {\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock]{\emph{\code{rcu\_read\_lock()}}}} is disallowed if the CPU is either idle or
offline from an RCU perspective, so check for those as well.
\index{wakeme\_after\_rcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.wakeme_after_rcu}\pysiglinewithargsret{void \bfcode{wakeme\_after\_rcu}}{struct rcu\_head *\emph{ head}}{}
Callback function to awaken a task after grace period

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct rcu\_head * head}}] \leavevmode
Pointer to rcu\_head member within rcu\_synchronize structure

\end{description}

\textbf{Description}

Awaken the corresponding task now that a grace period has elapsed.
\index{init\_rcu\_head\_on\_stack (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.init_rcu_head_on_stack}\pysiglinewithargsret{void \bfcode{init\_rcu\_head\_on\_stack}}{struct rcu\_head *\emph{ head}}{}
initialize on-stack rcu\_head for debugobjects

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct rcu\_head * head}}] \leavevmode
pointer to rcu\_head structure to be initialized

\end{description}

\textbf{Description}

This function informs debugobjects of a new rcu\_head structure that
has been allocated as an auto variable on the stack.  This function
is not required for rcu\_head structures that are statically defined or
that are dynamically allocated on the heap.  This function has no
effect for !CONFIG\_DEBUG\_OBJECTS\_RCU\_HEAD kernel builds.
\index{destroy\_rcu\_head\_on\_stack (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.destroy_rcu_head_on_stack}\pysiglinewithargsret{void \bfcode{destroy\_rcu\_head\_on\_stack}}{struct rcu\_head *\emph{ head}}{}
destroy on-stack rcu\_head for debugobjects

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct rcu\_head * head}}] \leavevmode
pointer to rcu\_head structure to be initialized

\end{description}

\textbf{Description}

This function informs debugobjects that an on-stack rcu\_head structure
is about to go out of scope.  As with \code{init\_rcu\_head\_on\_stack()}, this
function is not required for rcu\_head structures that are statically
defined or that are dynamically allocated on the heap.  Also as with
\code{init\_rcu\_head\_on\_stack()}, this function has no effect for
!CONFIG\_DEBUG\_OBJECTS\_RCU\_HEAD kernel builds.
\index{call\_rcu\_tasks (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.call_rcu_tasks}\pysiglinewithargsret{void \bfcode{call\_rcu\_tasks}}{struct rcu\_head *\emph{ rhp}, rcu\_callback\_t\emph{ func}}{}
Queue an RCU for invocation task-based grace period

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct rcu\_head * rhp}}] \leavevmode
structure to be used for queueing the RCU updates.

\item[{\code{rcu\_callback\_t func}}] \leavevmode
actual callback function to be invoked after the grace period

\end{description}

\textbf{Description}

The callback function will be invoked some time after a full grace
period elapses, in other words after all currently executing RCU
read-side critical sections have completed. \code{call\_rcu\_tasks()} assumes
that the read-side critical sections end at a voluntary context
switch (not a preemption!), entry into idle, or transition to usermode
execution.  As such, there are no read-side primitives analogous to
{\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock]{\emph{\code{rcu\_read\_lock()}}}} and {\hyperref[core\string-api/kernel\string-api:c.rcu_read_unlock]{\emph{\code{rcu\_read\_unlock()}}}} because this primitive is intended
to determine that all tasks have passed through a safe state, not so
much for data-strcuture synchronization.

See the description of \code{call\_rcu()} for more detailed information on
memory ordering guarantees.
\index{synchronize\_rcu\_tasks (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.synchronize_rcu_tasks}\pysiglinewithargsret{void \bfcode{synchronize\_rcu\_tasks}}{void}{}
wait until an rcu-tasks grace period has elapsed.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

Control will return to the caller some time after a full rcu-tasks
grace period has elapsed, in other words after all currently
executing rcu-tasks read-side critical sections have elapsed.  These
read-side critical sections are delimited by calls to \code{schedule()},
{\hyperref[core\string-api/kernel\string-api:c.cond_resched_rcu_qs]{\emph{\code{cond\_resched\_rcu\_qs()}}}}, idle execution, userspace execution, calls
to \code{synchronize\_rcu\_tasks()}, and (in theory, anyway) \code{cond\_resched()}.

This is a very specialized primitive, intended only for a few uses in
tracing and other situations requiring manipulation of function
preambles and profiling hooks.  The \code{synchronize\_rcu\_tasks()} function
is not (yet) intended for heavy use from multiple CPUs.

Note that this guarantee implies further memory-ordering guarantees.
On systems with more than one CPU, when \code{synchronize\_rcu\_tasks()} returns,
each CPU is guaranteed to have executed a full memory barrier since the
end of its last RCU-tasks read-side critical section whose beginning
preceded the call to \code{synchronize\_rcu\_tasks()}.  In addition, each CPU
having an RCU-tasks read-side critical section that extends beyond
the return from \code{synchronize\_rcu\_tasks()} is guaranteed to have executed
a full memory barrier after the beginning of \code{synchronize\_rcu\_tasks()}
and before the beginning of that RCU-tasks read-side critical section.
Note that these guarantees include CPUs that are offline, idle, or
executing in user mode, as well as CPUs that are executing in the kernel.

Furthermore, if CPU A invoked \code{synchronize\_rcu\_tasks()}, which returned
to its caller on CPU B, then both CPU A and CPU B are guaranteed
to have executed a full memory barrier during the execution of
\code{synchronize\_rcu\_tasks()} -- even if CPU A and CPU B are the same CPU
(but again only if the system has more than one CPU).
\index{rcu\_barrier\_tasks (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_barrier_tasks}\pysiglinewithargsret{void \bfcode{rcu\_barrier\_tasks}}{void}{}
Wait for in-flight \code{call\_rcu\_tasks()} callbacks.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

Although the current implementation is guaranteed to wait, it is not
obligated to, for example, if there are no pending callbacks.
\index{srcu\_read\_lock\_held (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.srcu_read_lock_held}\pysiglinewithargsret{int \bfcode{srcu\_read\_lock\_held}}{const struct srcu\_struct *\emph{ sp}}{}
might we be in SRCU read-side critical section?

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const struct srcu\_struct * sp}}] \leavevmode
The srcu\_struct structure to check

\end{description}

\textbf{Description}

If CONFIG\_DEBUG\_LOCK\_ALLOC is selected, returns nonzero iff in an SRCU
read-side critical section.  In absence of CONFIG\_DEBUG\_LOCK\_ALLOC,
this assumes we are in an SRCU read-side critical section unless it can
prove otherwise.

Checks \code{debug\_lockdep\_rcu\_enabled()} to prevent false positives during boot
and while lockdep is disabled.

Note that SRCU is based on its own statemachine and it doesn't
relies on normal RCU, it can be called from the CPU which
is in the idle loop from an RCU point of view or offline.
\index{srcu\_dereference\_check (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.srcu_dereference_check}\pysiglinewithargsret{\bfcode{srcu\_dereference\_check}}{\emph{p}, \emph{sp}, \emph{c}}{}
fetch SRCU-protected pointer for later dereferencing

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{p}}] \leavevmode
the pointer to fetch and protect for later dereferencing

\item[{\code{sp}}] \leavevmode
pointer to the srcu\_struct, which is used to check that we
really are in an SRCU read-side critical section.

\item[{\code{c}}] \leavevmode
condition to check for update-side use

\end{description}

\textbf{Description}

If PROVE\_RCU is enabled, invoking this outside of an RCU read-side
critical section will result in an RCU-lockdep splat, unless \textbf{c} evaluates
to 1.  The \textbf{c} argument will normally be a logical expression containing
\code{lockdep\_is\_held()} calls.
\index{srcu\_dereference (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.srcu_dereference}\pysiglinewithargsret{\bfcode{srcu\_dereference}}{\emph{p}, \emph{sp}}{}
fetch SRCU-protected pointer for later dereferencing

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{p}}] \leavevmode
the pointer to fetch and protect for later dereferencing

\item[{\code{sp}}] \leavevmode
pointer to the srcu\_struct, which is used to check that we
really are in an SRCU read-side critical section.

\end{description}

\textbf{Description}

Makes {\hyperref[core\string-api/kernel\string-api:c.rcu_dereference_check]{\emph{\code{rcu\_dereference\_check()}}}} do the dirty work.  If PROVE\_RCU
is enabled, invoking this outside of an RCU read-side critical
section will result in an RCU-lockdep splat.
\index{srcu\_read\_lock (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.srcu_read_lock}\pysiglinewithargsret{int \bfcode{srcu\_read\_lock}}{struct srcu\_struct *\emph{ sp}}{}
register a new reader for an SRCU-protected structure.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct srcu\_struct * sp}}] \leavevmode
srcu\_struct in which to register the new reader.

\end{description}

\textbf{Description}

Enter an SRCU read-side critical section.  Note that SRCU read-side
critical sections may be nested.  However, it is illegal to
call anything that waits on an SRCU grace period for the same
srcu\_struct, whether directly or indirectly.  Please note that
one way to indirectly wait on an SRCU grace period is to acquire
a mutex that is held elsewhere while calling {\hyperref[core\string-api/kernel\string-api:c.synchronize_srcu]{\emph{\code{synchronize\_srcu()}}}} or
{\hyperref[core\string-api/kernel\string-api:c.synchronize_srcu_expedited]{\emph{\code{synchronize\_srcu\_expedited()}}}}.

Note that {\hyperref[core\string-api/kernel\string-api:c.srcu_read_lock]{\emph{\code{srcu\_read\_lock()}}}} and the matching {\hyperref[core\string-api/kernel\string-api:c.srcu_read_unlock]{\emph{\code{srcu\_read\_unlock()}}}} must
occur in the same context, for example, it is illegal to invoke
{\hyperref[core\string-api/kernel\string-api:c.srcu_read_unlock]{\emph{\code{srcu\_read\_unlock()}}}} in an irq handler if the matching {\hyperref[core\string-api/kernel\string-api:c.srcu_read_lock]{\emph{\code{srcu\_read\_lock()}}}}
was invoked in process context.
\index{srcu\_read\_unlock (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.srcu_read_unlock}\pysiglinewithargsret{void \bfcode{srcu\_read\_unlock}}{struct srcu\_struct *\emph{ sp}, int\emph{ idx}}{}
unregister a old reader from an SRCU-protected structure.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct srcu\_struct * sp}}] \leavevmode
srcu\_struct in which to unregister the old reader.

\item[{\code{int idx}}] \leavevmode
return value from corresponding {\hyperref[core\string-api/kernel\string-api:c.srcu_read_lock]{\emph{\code{srcu\_read\_lock()}}}}.

\end{description}

\textbf{Description}

Exit an SRCU read-side critical section.
\index{smp\_mb\_\_after\_srcu\_read\_unlock (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.smp_mb__after_srcu_read_unlock}\pysiglinewithargsret{void \bfcode{smp\_mb\_\_after\_srcu\_read\_unlock}}{void}{}
ensure full ordering after srcu\_read\_unlock

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

Converts the preceding srcu\_read\_unlock into a two-way memory barrier.

Call this after srcu\_read\_unlock, to guarantee that all memory operations
that occur after smp\_mb\_\_after\_srcu\_read\_unlock will appear to happen after
the preceding srcu\_read\_unlock.
\index{init\_srcu\_struct (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.init_srcu_struct}\pysiglinewithargsret{int \bfcode{init\_srcu\_struct}}{struct srcu\_struct *\emph{ sp}}{}
initialize a sleep-RCU structure

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct srcu\_struct * sp}}] \leavevmode
structure to initialize.

\end{description}

\textbf{Description}

Must invoke this on a given srcu\_struct before passing that srcu\_struct
to any other function.  Each srcu\_struct represents a separate domain
of SRCU protection.
\index{srcu\_readers\_active (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.srcu_readers_active}\pysiglinewithargsret{bool \bfcode{srcu\_readers\_active}}{struct srcu\_struct *\emph{ sp}}{}
returns true if there are readers. and false otherwise

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct srcu\_struct * sp}}] \leavevmode
which srcu\_struct to count active readers (holding srcu\_read\_lock).

\end{description}

\textbf{Description}

Note that this is not an atomic primitive, and can therefore suffer
severe errors when invoked on an active srcu\_struct.  That said, it
can be useful as an error check at cleanup time.
\index{cleanup\_srcu\_struct (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.cleanup_srcu_struct}\pysiglinewithargsret{void \bfcode{cleanup\_srcu\_struct}}{struct srcu\_struct *\emph{ sp}}{}
deconstruct a sleep-RCU structure

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct srcu\_struct * sp}}] \leavevmode
structure to clean up.

\end{description}

\textbf{Description}

Must invoke this after you are finished using a given srcu\_struct that
was initialized via {\hyperref[core\string-api/kernel\string-api:c.init_srcu_struct]{\emph{\code{init\_srcu\_struct()}}}}, else you leak memory.
\index{call\_srcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.call_srcu}\pysiglinewithargsret{void \bfcode{call\_srcu}}{struct srcu\_struct *\emph{ sp}, struct rcu\_head *\emph{ rhp}, rcu\_callback\_t\emph{ func}}{}
Queue a callback for invocation after an SRCU grace period

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct srcu\_struct * sp}}] \leavevmode
srcu\_struct in queue the callback

\item[{\code{struct rcu\_head * rhp}}] \leavevmode
structure to be used for queueing the SRCU callback.

\item[{\code{rcu\_callback\_t func}}] \leavevmode
function to be invoked after the SRCU grace period

\end{description}

\textbf{Description}

The callback function will be invoked some time after a full SRCU
grace period elapses, in other words after all pre-existing SRCU
read-side critical sections have completed.  However, the callback
function might well execute concurrently with other SRCU read-side
critical sections that started after {\hyperref[core\string-api/kernel\string-api:c.call_srcu]{\emph{\code{call\_srcu()}}}} was invoked.  SRCU
read-side critical sections are delimited by {\hyperref[core\string-api/kernel\string-api:c.srcu_read_lock]{\emph{\code{srcu\_read\_lock()}}}} and
{\hyperref[core\string-api/kernel\string-api:c.srcu_read_unlock]{\emph{\code{srcu\_read\_unlock()}}}}, and may be nested.

The callback will be invoked from process context, but must nevertheless
be fast and must not block.
\index{synchronize\_srcu\_expedited (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.synchronize_srcu_expedited}\pysiglinewithargsret{void \bfcode{synchronize\_srcu\_expedited}}{struct srcu\_struct *\emph{ sp}}{}
Brute-force SRCU grace period

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct srcu\_struct * sp}}] \leavevmode
srcu\_struct with which to synchronize.

\end{description}

\textbf{Description}

Wait for an SRCU grace period to elapse, but be more aggressive about
spinning rather than blocking when waiting.

Note that {\hyperref[core\string-api/kernel\string-api:c.synchronize_srcu_expedited]{\emph{\code{synchronize\_srcu\_expedited()}}}} has the same deadlock and
memory-ordering properties as does {\hyperref[core\string-api/kernel\string-api:c.synchronize_srcu]{\emph{\code{synchronize\_srcu()}}}}.
\index{synchronize\_srcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.synchronize_srcu}\pysiglinewithargsret{void \bfcode{synchronize\_srcu}}{struct srcu\_struct *\emph{ sp}}{}
wait for prior SRCU read-side critical-section completion

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct srcu\_struct * sp}}] \leavevmode
srcu\_struct with which to synchronize.

\end{description}

\textbf{Description}

Wait for the count to drain to zero of both indexes. To avoid the
possible starvation of {\hyperref[core\string-api/kernel\string-api:c.synchronize_srcu]{\emph{\code{synchronize\_srcu()}}}}, it waits for the count of
the index=((-\textgreater{}srcu\_idx \& 1) \textasciicircum{} 1) to drain to zero at first,
and then flip the srcu\_idx and wait for the count of the other index.

Can block; must be called from process context.

Note that it is illegal to call {\hyperref[core\string-api/kernel\string-api:c.synchronize_srcu]{\emph{\code{synchronize\_srcu()}}}} from the corresponding
SRCU read-side critical section; doing so will result in deadlock.
However, it is perfectly legal to call {\hyperref[core\string-api/kernel\string-api:c.synchronize_srcu]{\emph{\code{synchronize\_srcu()}}}} on one
srcu\_struct from some other srcu\_struct's read-side critical section,
as long as the resulting graph of srcu\_structs is acyclic.

There are memory-ordering constraints implied by {\hyperref[core\string-api/kernel\string-api:c.synchronize_srcu]{\emph{\code{synchronize\_srcu()}}}}.
On systems with more than one CPU, when {\hyperref[core\string-api/kernel\string-api:c.synchronize_srcu]{\emph{\code{synchronize\_srcu()}}}} returns,
each CPU is guaranteed to have executed a full memory barrier since
the end of its last corresponding SRCU-sched read-side critical section
whose beginning preceded the call to {\hyperref[core\string-api/kernel\string-api:c.synchronize_srcu]{\emph{\code{synchronize\_srcu()}}}}.  In addition,
each CPU having an SRCU read-side critical section that extends beyond
the return from {\hyperref[core\string-api/kernel\string-api:c.synchronize_srcu]{\emph{\code{synchronize\_srcu()}}}} is guaranteed to have executed a
full memory barrier after the beginning of {\hyperref[core\string-api/kernel\string-api:c.synchronize_srcu]{\emph{\code{synchronize\_srcu()}}}} and before
the beginning of that SRCU read-side critical section.  Note that these
guarantees include CPUs that are offline, idle, or executing in user mode,
as well as CPUs that are executing in the kernel.

Furthermore, if CPU A invoked {\hyperref[core\string-api/kernel\string-api:c.synchronize_srcu]{\emph{\code{synchronize\_srcu()}}}}, which returned
to its caller on CPU B, then both CPU A and CPU B are guaranteed
to have executed a full memory barrier during the execution of
{\hyperref[core\string-api/kernel\string-api:c.synchronize_srcu]{\emph{\code{synchronize\_srcu()}}}}.  This guarantee applies even if CPU A and CPU B
are the same CPU, but again only if the system has more than one CPU.

Of course, these memory-ordering guarantees apply only when
{\hyperref[core\string-api/kernel\string-api:c.synchronize_srcu]{\emph{\code{synchronize\_srcu()}}}}, {\hyperref[core\string-api/kernel\string-api:c.srcu_read_lock]{\emph{\code{srcu\_read\_lock()}}}}, and {\hyperref[core\string-api/kernel\string-api:c.srcu_read_unlock]{\emph{\code{srcu\_read\_unlock()}}}} are
passed the same srcu\_struct structure.

If SRCU is likely idle, expedite the first request.  This semantic
was provided by Classic SRCU, and is relied upon by its users, so TREE
SRCU must also provide it.  Note that detecting idleness is heuristic
and subject to both false positives and negatives.
\index{srcu\_barrier (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.srcu_barrier}\pysiglinewithargsret{void \bfcode{srcu\_barrier}}{struct srcu\_struct *\emph{ sp}}{}
Wait until all in-flight {\hyperref[core\string-api/kernel\string-api:c.call_srcu]{\emph{\code{call\_srcu()}}}} callbacks complete.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct srcu\_struct * sp}}] \leavevmode
srcu\_struct on which to wait for in-flight callbacks.

\end{description}
\index{srcu\_batches\_completed (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.srcu_batches_completed}\pysiglinewithargsret{unsigned long \bfcode{srcu\_batches\_completed}}{struct srcu\_struct *\emph{ sp}}{}
return batches completed.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct srcu\_struct * sp}}] \leavevmode
srcu\_struct on which to report batch completion.

\end{description}

\textbf{Description}

Report the number of batches, correlated with, but not necessarily
precisely the same as, the number of grace periods that have elapsed.
\index{hlist\_bl\_del\_init\_rcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.hlist_bl_del_init_rcu}\pysiglinewithargsret{void \bfcode{hlist\_bl\_del\_init\_rcu}}{struct hlist\_bl\_node *\emph{ n}}{}
deletes entry from hash list with re-initialization

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct hlist\_bl\_node * n}}] \leavevmode
the element to delete from the hash list.

\end{description}

\textbf{Note}

\code{hlist\_bl\_unhashed()} on the node returns true after this. It is
useful for RCU based read lockfree traversal if the writer side
must know if the list entry is still hashed or already unhashed.

In particular, it means that we can not poison the forward pointers
that may still be used for walking the hash list and we can only
zero the pprev pointer so \code{list\_unhashed()} will return true after
this.

The caller must take whatever precautions are necessary (such as
holding appropriate locks) to avoid racing with another
list-mutation primitive, such as {\hyperref[core\string-api/kernel\string-api:c.hlist_bl_add_head_rcu]{\emph{\code{hlist\_bl\_add\_head\_rcu()}}}} or
{\hyperref[core\string-api/kernel\string-api:c.hlist_bl_del_rcu]{\emph{\code{hlist\_bl\_del\_rcu()}}}}, running on this same list.  However, it is
perfectly legal to run concurrently with the \_rcu list-traversal
primitives, such as {\hyperref[core\string-api/kernel\string-api:c.hlist_bl_for_each_entry_rcu]{\emph{\code{hlist\_bl\_for\_each\_entry\_rcu()}}}}.
\index{hlist\_bl\_del\_rcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.hlist_bl_del_rcu}\pysiglinewithargsret{void \bfcode{hlist\_bl\_del\_rcu}}{struct hlist\_bl\_node *\emph{ n}}{}
deletes entry from hash list without re-initialization

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct hlist\_bl\_node * n}}] \leavevmode
the element to delete from the hash list.

\end{description}

\textbf{Note}

\code{hlist\_bl\_unhashed()} on entry does not return true after this,
the entry is in an undefined state. It is useful for RCU based
lockfree traversal.

In particular, it means that we can not poison the forward
pointers that may still be used for walking the hash list.

The caller must take whatever precautions are necessary
(such as holding appropriate locks) to avoid racing
with another list-mutation primitive, such as {\hyperref[core\string-api/kernel\string-api:c.hlist_bl_add_head_rcu]{\emph{\code{hlist\_bl\_add\_head\_rcu()}}}}
or {\hyperref[core\string-api/kernel\string-api:c.hlist_bl_del_rcu]{\emph{\code{hlist\_bl\_del\_rcu()}}}}, running on this same list.
However, it is perfectly legal to run concurrently with
the \_rcu list-traversal primitives, such as
\code{hlist\_bl\_for\_each\_entry()}.
\index{hlist\_bl\_add\_head\_rcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.hlist_bl_add_head_rcu}\pysiglinewithargsret{void \bfcode{hlist\_bl\_add\_head\_rcu}}{struct hlist\_bl\_node *\emph{ n}, struct hlist\_bl\_head *\emph{ h}}{}
\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct hlist\_bl\_node * n}}] \leavevmode
the element to add to the hash list.

\item[{\code{struct hlist\_bl\_head * h}}] \leavevmode
the list to add to.

\end{description}

\textbf{Description}

Adds the specified element to the specified hlist\_bl,
while permitting racing traversals.

The caller must take whatever precautions are necessary
(such as holding appropriate locks) to avoid racing
with another list-mutation primitive, such as {\hyperref[core\string-api/kernel\string-api:c.hlist_bl_add_head_rcu]{\emph{\code{hlist\_bl\_add\_head\_rcu()}}}}
or {\hyperref[core\string-api/kernel\string-api:c.hlist_bl_del_rcu]{\emph{\code{hlist\_bl\_del\_rcu()}}}}, running on this same list.
However, it is perfectly legal to run concurrently with
the \_rcu list-traversal primitives, such as
{\hyperref[core\string-api/kernel\string-api:c.hlist_bl_for_each_entry_rcu]{\emph{\code{hlist\_bl\_for\_each\_entry\_rcu()}}}}, used to prevent memory-consistency
problems on Alpha CPUs.  Regardless of the type of CPU, the
list-traversal primitive must be guarded by {\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock]{\emph{\code{rcu\_read\_lock()}}}}.
\index{hlist\_bl\_for\_each\_entry\_rcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.hlist_bl_for_each_entry_rcu}\pysiglinewithargsret{\bfcode{hlist\_bl\_for\_each\_entry\_rcu}}{\emph{tpos}, \emph{pos}, \emph{head}, \emph{member}}{}
iterate over rcu list of given type

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{tpos}}] \leavevmode
the type * to use as a loop cursor.

\item[{\code{pos}}] \leavevmode
the \code{struct hlist\_bl\_node} to use as a loop cursor.

\item[{\code{head}}] \leavevmode
the head for your list.

\item[{\code{member}}] \leavevmode
the name of the hlist\_bl\_node within the struct.

\end{description}
\index{list\_add\_rcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_add_rcu}\pysiglinewithargsret{void \bfcode{list\_add\_rcu}}{struct list\_head *\emph{ new}, struct list\_head *\emph{ head}}{}
add a new entry to rcu-protected list

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct list\_head * new}}] \leavevmode
new entry to be added

\item[{\code{struct list\_head * head}}] \leavevmode
list head to add it after

\end{description}

\textbf{Description}

Insert a new entry after the specified head.
This is good for implementing stacks.

The caller must take whatever precautions are necessary
(such as holding appropriate locks) to avoid racing
with another list-mutation primitive, such as {\hyperref[core\string-api/kernel\string-api:c.list_add_rcu]{\emph{\code{list\_add\_rcu()}}}}
or {\hyperref[core\string-api/kernel\string-api:c.list_del_rcu]{\emph{\code{list\_del\_rcu()}}}}, running on this same list.
However, it is perfectly legal to run concurrently with
the \_rcu list-traversal primitives, such as
{\hyperref[core\string-api/kernel\string-api:c.list_for_each_entry_rcu]{\emph{\code{list\_for\_each\_entry\_rcu()}}}}.
\index{list\_add\_tail\_rcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_add_tail_rcu}\pysiglinewithargsret{void \bfcode{list\_add\_tail\_rcu}}{struct list\_head *\emph{ new}, struct list\_head *\emph{ head}}{}
add a new entry to rcu-protected list

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct list\_head * new}}] \leavevmode
new entry to be added

\item[{\code{struct list\_head * head}}] \leavevmode
list head to add it before

\end{description}

\textbf{Description}

Insert a new entry before the specified head.
This is useful for implementing queues.

The caller must take whatever precautions are necessary
(such as holding appropriate locks) to avoid racing
with another list-mutation primitive, such as {\hyperref[core\string-api/kernel\string-api:c.list_add_tail_rcu]{\emph{\code{list\_add\_tail\_rcu()}}}}
or {\hyperref[core\string-api/kernel\string-api:c.list_del_rcu]{\emph{\code{list\_del\_rcu()}}}}, running on this same list.
However, it is perfectly legal to run concurrently with
the \_rcu list-traversal primitives, such as
{\hyperref[core\string-api/kernel\string-api:c.list_for_each_entry_rcu]{\emph{\code{list\_for\_each\_entry\_rcu()}}}}.
\index{list\_del\_rcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_del_rcu}\pysiglinewithargsret{void \bfcode{list\_del\_rcu}}{struct list\_head *\emph{ entry}}{}
deletes entry from list without re-initialization

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct list\_head * entry}}] \leavevmode
the element to delete from the list.

\end{description}

\textbf{Note}

{\hyperref[core\string-api/kernel\string-api:c.list_empty]{\emph{\code{list\_empty()}}}} on entry does not return true after this,
the entry is in an undefined state. It is useful for RCU based
lockfree traversal.

In particular, it means that we can not poison the forward
pointers that may still be used for walking the list.

The caller must take whatever precautions are necessary
(such as holding appropriate locks) to avoid racing
with another list-mutation primitive, such as {\hyperref[core\string-api/kernel\string-api:c.list_del_rcu]{\emph{\code{list\_del\_rcu()}}}}
or {\hyperref[core\string-api/kernel\string-api:c.list_add_rcu]{\emph{\code{list\_add\_rcu()}}}}, running on this same list.
However, it is perfectly legal to run concurrently with
the \_rcu list-traversal primitives, such as
{\hyperref[core\string-api/kernel\string-api:c.list_for_each_entry_rcu]{\emph{\code{list\_for\_each\_entry\_rcu()}}}}.

Note that the caller is not permitted to immediately free
the newly deleted entry.  Instead, either \code{synchronize\_rcu()}
or \code{call\_rcu()} must be used to defer freeing until an RCU
grace period has elapsed.
\index{hlist\_del\_init\_rcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.hlist_del_init_rcu}\pysiglinewithargsret{void \bfcode{hlist\_del\_init\_rcu}}{struct hlist\_node *\emph{ n}}{}
deletes entry from hash list with re-initialization

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct hlist\_node * n}}] \leavevmode
the element to delete from the hash list.

\end{description}

\textbf{Note}

\code{list\_unhashed()} on the node return true after this. It is
useful for RCU based read lockfree traversal if the writer side
must know if the list entry is still hashed or already unhashed.

In particular, it means that we can not poison the forward pointers
that may still be used for walking the hash list and we can only
zero the pprev pointer so \code{list\_unhashed()} will return true after
this.

The caller must take whatever precautions are necessary (such as
holding appropriate locks) to avoid racing with another
list-mutation primitive, such as {\hyperref[core\string-api/kernel\string-api:c.hlist_add_head_rcu]{\emph{\code{hlist\_add\_head\_rcu()}}}} or
{\hyperref[core\string-api/kernel\string-api:c.hlist_del_rcu]{\emph{\code{hlist\_del\_rcu()}}}}, running on this same list.  However, it is
perfectly legal to run concurrently with the \_rcu list-traversal
primitives, such as {\hyperref[core\string-api/kernel\string-api:c.hlist_for_each_entry_rcu]{\emph{\code{hlist\_for\_each\_entry\_rcu()}}}}.
\index{list\_replace\_rcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_replace_rcu}\pysiglinewithargsret{void \bfcode{list\_replace\_rcu}}{struct list\_head *\emph{ old}, struct list\_head *\emph{ new}}{}
replace old entry by new one

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct list\_head * old}}] \leavevmode
the element to be replaced

\item[{\code{struct list\_head * new}}] \leavevmode
the new element to insert

\end{description}

\textbf{Description}

The \textbf{old} entry will be replaced with the \textbf{new} entry atomically.

\textbf{Note}

\textbf{old} should not be empty.
\index{\_\_list\_splice\_init\_rcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.__list_splice_init_rcu}\pysiglinewithargsret{void \bfcode{\_\_list\_splice\_init\_rcu}}{struct list\_head *\emph{ list}, struct list\_head *\emph{ prev}, struct list\_head *\emph{ next}, void (*sync)\emph{ (void}}{}
join an RCU-protected list into an existing list.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct list\_head * list}}] \leavevmode
the RCU-protected list to splice

\item[{\code{struct list\_head * prev}}] \leavevmode
points to the last element of the existing list

\item[{\code{struct list\_head * next}}] \leavevmode
points to the first element of the existing list

\item[{\code{void (*)(void) sync}}] \leavevmode
function to sync: \code{synchronize\_rcu()}, \code{synchronize\_sched()}, ...

\end{description}

\textbf{Description}

The list pointed to by \textbf{prev} and \textbf{next} can be RCU-read traversed
concurrently with this function.

Note that this function blocks.

Important note: the caller must take whatever action is necessary to prevent
any other updates to the existing list.  In principle, it is possible to
modify the list as soon as \code{sync()} begins execution. If this sort of thing
becomes necessary, an alternative version based on \code{call\_rcu()} could be
created.  But only if -really- needed -- there is no shortage of RCU API
members.
\index{list\_splice\_init\_rcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_splice_init_rcu}\pysiglinewithargsret{void \bfcode{list\_splice\_init\_rcu}}{struct list\_head *\emph{ list}, struct list\_head *\emph{ head}, void (*sync)\emph{ (void}}{}
splice an RCU-protected list into an existing list, designed for stacks.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct list\_head * list}}] \leavevmode
the RCU-protected list to splice

\item[{\code{struct list\_head * head}}] \leavevmode
the place in the existing list to splice the first list into

\item[{\code{void (*)(void) sync}}] \leavevmode
function to sync: \code{synchronize\_rcu()}, \code{synchronize\_sched()}, ...

\end{description}
\index{list\_splice\_tail\_init\_rcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_splice_tail_init_rcu}\pysiglinewithargsret{void \bfcode{list\_splice\_tail\_init\_rcu}}{struct list\_head *\emph{ list}, struct list\_head *\emph{ head}, void (*sync)\emph{ (void}}{}
splice an RCU-protected list into an existing list, designed for queues.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct list\_head * list}}] \leavevmode
the RCU-protected list to splice

\item[{\code{struct list\_head * head}}] \leavevmode
the place in the existing list to splice the first list into

\item[{\code{void (*)(void) sync}}] \leavevmode
function to sync: \code{synchronize\_rcu()}, \code{synchronize\_sched()}, ...

\end{description}
\index{list\_entry\_rcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_entry_rcu}\pysiglinewithargsret{\bfcode{list\_entry\_rcu}}{\emph{ptr}, \emph{type}, \emph{member}}{}
get the struct for this entry

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{ptr}}] \leavevmode
the \code{struct list\_head} pointer.

\item[{\code{type}}] \leavevmode
the type of the struct this is embedded in.

\item[{\code{member}}] \leavevmode
the name of the list\_head within the struct.

\end{description}

\textbf{Description}

This primitive may safely run concurrently with the \_rcu list-mutation
primitives such as {\hyperref[core\string-api/kernel\string-api:c.list_add_rcu]{\emph{\code{list\_add\_rcu()}}}} as long as it's guarded by {\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock]{\emph{\code{rcu\_read\_lock()}}}}.
\index{list\_first\_or\_null\_rcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_first_or_null_rcu}\pysiglinewithargsret{\bfcode{list\_first\_or\_null\_rcu}}{\emph{ptr}, \emph{type}, \emph{member}}{}
get the first element from a list

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{ptr}}] \leavevmode
the list head to take the element from.

\item[{\code{type}}] \leavevmode
the type of the struct this is embedded in.

\item[{\code{member}}] \leavevmode
the name of the list\_head within the struct.

\end{description}

\textbf{Description}

Note that if the list is empty, it returns NULL.

This primitive may safely run concurrently with the \_rcu list-mutation
primitives such as {\hyperref[core\string-api/kernel\string-api:c.list_add_rcu]{\emph{\code{list\_add\_rcu()}}}} as long as it's guarded by {\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock]{\emph{\code{rcu\_read\_lock()}}}}.
\index{list\_next\_or\_null\_rcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_next_or_null_rcu}\pysiglinewithargsret{\bfcode{list\_next\_or\_null\_rcu}}{\emph{head}, \emph{ptr}, \emph{type}, \emph{member}}{}
get the first element from a list

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{head}}] \leavevmode
the head for the list.

\item[{\code{ptr}}] \leavevmode
the list head to take the next element from.

\item[{\code{type}}] \leavevmode
the type of the struct this is embedded in.

\item[{\code{member}}] \leavevmode
the name of the list\_head within the struct.

\end{description}

\textbf{Description}

Note that if the ptr is at the end of the list, NULL is returned.

This primitive may safely run concurrently with the \_rcu list-mutation
primitives such as {\hyperref[core\string-api/kernel\string-api:c.list_add_rcu]{\emph{\code{list\_add\_rcu()}}}} as long as it's guarded by {\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock]{\emph{\code{rcu\_read\_lock()}}}}.
\index{list\_for\_each\_entry\_rcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_for_each_entry_rcu}\pysiglinewithargsret{\bfcode{list\_for\_each\_entry\_rcu}}{\emph{pos}, \emph{head}, \emph{member}}{}
iterate over rcu list of given type

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{pos}}] \leavevmode
the type * to use as a loop cursor.

\item[{\code{head}}] \leavevmode
the head for your list.

\item[{\code{member}}] \leavevmode
the name of the list\_head within the struct.

\end{description}

\textbf{Description}

This list-traversal primitive may safely run concurrently with
the \_rcu list-mutation primitives such as {\hyperref[core\string-api/kernel\string-api:c.list_add_rcu]{\emph{\code{list\_add\_rcu()}}}}
as long as the traversal is guarded by {\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock]{\emph{\code{rcu\_read\_lock()}}}}.
\index{list\_entry\_lockless (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_entry_lockless}\pysiglinewithargsret{\bfcode{list\_entry\_lockless}}{\emph{ptr}, \emph{type}, \emph{member}}{}
get the struct for this entry

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{ptr}}] \leavevmode
the \code{struct list\_head} pointer.

\item[{\code{type}}] \leavevmode
the type of the struct this is embedded in.

\item[{\code{member}}] \leavevmode
the name of the list\_head within the struct.

\end{description}

\textbf{Description}

This primitive may safely run concurrently with the \_rcu list-mutation
primitives such as {\hyperref[core\string-api/kernel\string-api:c.list_add_rcu]{\emph{\code{list\_add\_rcu()}}}}, but requires some implicit RCU
read-side guarding.  One example is running within a special
exception-time environment where preemption is disabled and where
lockdep cannot be invoked (in which case updaters must use RCU-sched,
as in \code{synchronize\_sched()}, \code{call\_rcu\_sched()}, and friends).  Another
example is when items are added to the list, but never deleted.
\index{list\_for\_each\_entry\_lockless (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_for_each_entry_lockless}\pysiglinewithargsret{\bfcode{list\_for\_each\_entry\_lockless}}{\emph{pos}, \emph{head}, \emph{member}}{}
iterate over rcu list of given type

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{pos}}] \leavevmode
the type * to use as a loop cursor.

\item[{\code{head}}] \leavevmode
the head for your list.

\item[{\code{member}}] \leavevmode
the name of the list\_struct within the struct.

\end{description}

\textbf{Description}

This primitive may safely run concurrently with the \_rcu list-mutation
primitives such as {\hyperref[core\string-api/kernel\string-api:c.list_add_rcu]{\emph{\code{list\_add\_rcu()}}}}, but requires some implicit RCU
read-side guarding.  One example is running within a special
exception-time environment where preemption is disabled and where
lockdep cannot be invoked (in which case updaters must use RCU-sched,
as in \code{synchronize\_sched()}, \code{call\_rcu\_sched()}, and friends).  Another
example is when items are added to the list, but never deleted.
\index{list\_for\_each\_entry\_continue\_rcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.list_for_each_entry_continue_rcu}\pysiglinewithargsret{\bfcode{list\_for\_each\_entry\_continue\_rcu}}{\emph{pos}, \emph{head}, \emph{member}}{}
continue iteration over list of given type

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{pos}}] \leavevmode
the type * to use as a loop cursor.

\item[{\code{head}}] \leavevmode
the head for your list.

\item[{\code{member}}] \leavevmode
the name of the list\_head within the struct.

\end{description}

\textbf{Description}

Continue to iterate over list of given type, continuing after
the current position.
\index{hlist\_del\_rcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.hlist_del_rcu}\pysiglinewithargsret{void \bfcode{hlist\_del\_rcu}}{struct hlist\_node *\emph{ n}}{}
deletes entry from hash list without re-initialization

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct hlist\_node * n}}] \leavevmode
the element to delete from the hash list.

\end{description}

\textbf{Note}

\code{list\_unhashed()} on entry does not return true after this,
the entry is in an undefined state. It is useful for RCU based
lockfree traversal.

In particular, it means that we can not poison the forward
pointers that may still be used for walking the hash list.

The caller must take whatever precautions are necessary
(such as holding appropriate locks) to avoid racing
with another list-mutation primitive, such as {\hyperref[core\string-api/kernel\string-api:c.hlist_add_head_rcu]{\emph{\code{hlist\_add\_head\_rcu()}}}}
or {\hyperref[core\string-api/kernel\string-api:c.hlist_del_rcu]{\emph{\code{hlist\_del\_rcu()}}}}, running on this same list.
However, it is perfectly legal to run concurrently with
the \_rcu list-traversal primitives, such as
{\hyperref[core\string-api/kernel\string-api:c.hlist_for_each_entry]{\emph{\code{hlist\_for\_each\_entry()}}}}.
\index{hlist\_replace\_rcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.hlist_replace_rcu}\pysiglinewithargsret{void \bfcode{hlist\_replace\_rcu}}{struct hlist\_node *\emph{ old}, struct hlist\_node *\emph{ new}}{}
replace old entry by new one

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct hlist\_node * old}}] \leavevmode
the element to be replaced

\item[{\code{struct hlist\_node * new}}] \leavevmode
the new element to insert

\end{description}

\textbf{Description}

The \textbf{old} entry will be replaced with the \textbf{new} entry atomically.
\index{hlist\_add\_head\_rcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.hlist_add_head_rcu}\pysiglinewithargsret{void \bfcode{hlist\_add\_head\_rcu}}{struct hlist\_node *\emph{ n}, struct hlist\_head *\emph{ h}}{}
\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct hlist\_node * n}}] \leavevmode
the element to add to the hash list.

\item[{\code{struct hlist\_head * h}}] \leavevmode
the list to add to.

\end{description}

\textbf{Description}

Adds the specified element to the specified hlist,
while permitting racing traversals.

The caller must take whatever precautions are necessary
(such as holding appropriate locks) to avoid racing
with another list-mutation primitive, such as {\hyperref[core\string-api/kernel\string-api:c.hlist_add_head_rcu]{\emph{\code{hlist\_add\_head\_rcu()}}}}
or {\hyperref[core\string-api/kernel\string-api:c.hlist_del_rcu]{\emph{\code{hlist\_del\_rcu()}}}}, running on this same list.
However, it is perfectly legal to run concurrently with
the \_rcu list-traversal primitives, such as
{\hyperref[core\string-api/kernel\string-api:c.hlist_for_each_entry_rcu]{\emph{\code{hlist\_for\_each\_entry\_rcu()}}}}, used to prevent memory-consistency
problems on Alpha CPUs.  Regardless of the type of CPU, the
list-traversal primitive must be guarded by {\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock]{\emph{\code{rcu\_read\_lock()}}}}.
\index{hlist\_add\_tail\_rcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.hlist_add_tail_rcu}\pysiglinewithargsret{void \bfcode{hlist\_add\_tail\_rcu}}{struct hlist\_node *\emph{ n}, struct hlist\_head *\emph{ h}}{}
\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct hlist\_node * n}}] \leavevmode
the element to add to the hash list.

\item[{\code{struct hlist\_head * h}}] \leavevmode
the list to add to.

\end{description}

\textbf{Description}

Adds the specified element to the specified hlist,
while permitting racing traversals.

The caller must take whatever precautions are necessary
(such as holding appropriate locks) to avoid racing
with another list-mutation primitive, such as {\hyperref[core\string-api/kernel\string-api:c.hlist_add_head_rcu]{\emph{\code{hlist\_add\_head\_rcu()}}}}
or {\hyperref[core\string-api/kernel\string-api:c.hlist_del_rcu]{\emph{\code{hlist\_del\_rcu()}}}}, running on this same list.
However, it is perfectly legal to run concurrently with
the \_rcu list-traversal primitives, such as
{\hyperref[core\string-api/kernel\string-api:c.hlist_for_each_entry_rcu]{\emph{\code{hlist\_for\_each\_entry\_rcu()}}}}, used to prevent memory-consistency
problems on Alpha CPUs.  Regardless of the type of CPU, the
list-traversal primitive must be guarded by {\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock]{\emph{\code{rcu\_read\_lock()}}}}.
\index{hlist\_add\_before\_rcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.hlist_add_before_rcu}\pysiglinewithargsret{void \bfcode{hlist\_add\_before\_rcu}}{struct hlist\_node *\emph{ n}, struct hlist\_node *\emph{ next}}{}
\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct hlist\_node * n}}] \leavevmode
the new element to add to the hash list.

\item[{\code{struct hlist\_node * next}}] \leavevmode
the existing element to add the new element before.

\end{description}

\textbf{Description}

Adds the specified element to the specified hlist
before the specified node while permitting racing traversals.

The caller must take whatever precautions are necessary
(such as holding appropriate locks) to avoid racing
with another list-mutation primitive, such as {\hyperref[core\string-api/kernel\string-api:c.hlist_add_head_rcu]{\emph{\code{hlist\_add\_head\_rcu()}}}}
or {\hyperref[core\string-api/kernel\string-api:c.hlist_del_rcu]{\emph{\code{hlist\_del\_rcu()}}}}, running on this same list.
However, it is perfectly legal to run concurrently with
the \_rcu list-traversal primitives, such as
{\hyperref[core\string-api/kernel\string-api:c.hlist_for_each_entry_rcu]{\emph{\code{hlist\_for\_each\_entry\_rcu()}}}}, used to prevent memory-consistency
problems on Alpha CPUs.
\index{hlist\_add\_behind\_rcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.hlist_add_behind_rcu}\pysiglinewithargsret{void \bfcode{hlist\_add\_behind\_rcu}}{struct hlist\_node *\emph{ n}, struct hlist\_node *\emph{ prev}}{}
\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct hlist\_node * n}}] \leavevmode
the new element to add to the hash list.

\item[{\code{struct hlist\_node * prev}}] \leavevmode
the existing element to add the new element after.

\end{description}

\textbf{Description}

Adds the specified element to the specified hlist
after the specified node while permitting racing traversals.

The caller must take whatever precautions are necessary
(such as holding appropriate locks) to avoid racing
with another list-mutation primitive, such as {\hyperref[core\string-api/kernel\string-api:c.hlist_add_head_rcu]{\emph{\code{hlist\_add\_head\_rcu()}}}}
or {\hyperref[core\string-api/kernel\string-api:c.hlist_del_rcu]{\emph{\code{hlist\_del\_rcu()}}}}, running on this same list.
However, it is perfectly legal to run concurrently with
the \_rcu list-traversal primitives, such as
{\hyperref[core\string-api/kernel\string-api:c.hlist_for_each_entry_rcu]{\emph{\code{hlist\_for\_each\_entry\_rcu()}}}}, used to prevent memory-consistency
problems on Alpha CPUs.
\index{hlist\_for\_each\_entry\_rcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.hlist_for_each_entry_rcu}\pysiglinewithargsret{\bfcode{hlist\_for\_each\_entry\_rcu}}{\emph{pos}, \emph{head}, \emph{member}}{}
iterate over rcu list of given type

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{pos}}] \leavevmode
the type * to use as a loop cursor.

\item[{\code{head}}] \leavevmode
the head for your list.

\item[{\code{member}}] \leavevmode
the name of the hlist\_node within the struct.

\end{description}

\textbf{Description}

This list-traversal primitive may safely run concurrently with
the \_rcu list-mutation primitives such as {\hyperref[core\string-api/kernel\string-api:c.hlist_add_head_rcu]{\emph{\code{hlist\_add\_head\_rcu()}}}}
as long as the traversal is guarded by {\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock]{\emph{\code{rcu\_read\_lock()}}}}.
\index{hlist\_for\_each\_entry\_rcu\_notrace (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.hlist_for_each_entry_rcu_notrace}\pysiglinewithargsret{\bfcode{hlist\_for\_each\_entry\_rcu\_notrace}}{\emph{pos}, \emph{head}, \emph{member}}{}
iterate over rcu list of given type (for tracing)

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{pos}}] \leavevmode
the type * to use as a loop cursor.

\item[{\code{head}}] \leavevmode
the head for your list.

\item[{\code{member}}] \leavevmode
the name of the hlist\_node within the struct.

\end{description}

\textbf{Description}

This list-traversal primitive may safely run concurrently with
the \_rcu list-mutation primitives such as {\hyperref[core\string-api/kernel\string-api:c.hlist_add_head_rcu]{\emph{\code{hlist\_add\_head\_rcu()}}}}
as long as the traversal is guarded by {\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock]{\emph{\code{rcu\_read\_lock()}}}}.

This is the same as {\hyperref[core\string-api/kernel\string-api:c.hlist_for_each_entry_rcu]{\emph{\code{hlist\_for\_each\_entry\_rcu()}}}} except that it does
not do any RCU debugging or tracing.
\index{hlist\_for\_each\_entry\_rcu\_bh (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.hlist_for_each_entry_rcu_bh}\pysiglinewithargsret{\bfcode{hlist\_for\_each\_entry\_rcu\_bh}}{\emph{pos}, \emph{head}, \emph{member}}{}
iterate over rcu list of given type

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{pos}}] \leavevmode
the type * to use as a loop cursor.

\item[{\code{head}}] \leavevmode
the head for your list.

\item[{\code{member}}] \leavevmode
the name of the hlist\_node within the struct.

\end{description}

\textbf{Description}

This list-traversal primitive may safely run concurrently with
the \_rcu list-mutation primitives such as {\hyperref[core\string-api/kernel\string-api:c.hlist_add_head_rcu]{\emph{\code{hlist\_add\_head\_rcu()}}}}
as long as the traversal is guarded by {\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock]{\emph{\code{rcu\_read\_lock()}}}}.
\index{hlist\_for\_each\_entry\_continue\_rcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.hlist_for_each_entry_continue_rcu}\pysiglinewithargsret{\bfcode{hlist\_for\_each\_entry\_continue\_rcu}}{\emph{pos}, \emph{member}}{}
iterate over a hlist continuing after current point

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{pos}}] \leavevmode
the type * to use as a loop cursor.

\item[{\code{member}}] \leavevmode
the name of the hlist\_node within the struct.

\end{description}
\index{hlist\_for\_each\_entry\_continue\_rcu\_bh (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.hlist_for_each_entry_continue_rcu_bh}\pysiglinewithargsret{\bfcode{hlist\_for\_each\_entry\_continue\_rcu\_bh}}{\emph{pos}, \emph{member}}{}
iterate over a hlist continuing after current point

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{pos}}] \leavevmode
the type * to use as a loop cursor.

\item[{\code{member}}] \leavevmode
the name of the hlist\_node within the struct.

\end{description}
\index{hlist\_for\_each\_entry\_from\_rcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.hlist_for_each_entry_from_rcu}\pysiglinewithargsret{\bfcode{hlist\_for\_each\_entry\_from\_rcu}}{\emph{pos}, \emph{member}}{}
iterate over a hlist continuing from current point

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{pos}}] \leavevmode
the type * to use as a loop cursor.

\item[{\code{member}}] \leavevmode
the name of the hlist\_node within the struct.

\end{description}
\index{hlist\_nulls\_del\_init\_rcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.hlist_nulls_del_init_rcu}\pysiglinewithargsret{void \bfcode{hlist\_nulls\_del\_init\_rcu}}{struct hlist\_nulls\_node *\emph{ n}}{}
deletes entry from hash list with re-initialization

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct hlist\_nulls\_node * n}}] \leavevmode
the element to delete from the hash list.

\end{description}

\textbf{Note}

\code{hlist\_nulls\_unhashed()} on the node return true after this. It is
useful for RCU based read lockfree traversal if the writer side
must know if the list entry is still hashed or already unhashed.

In particular, it means that we can not poison the forward pointers
that may still be used for walking the hash list and we can only
zero the pprev pointer so \code{list\_unhashed()} will return true after
this.

The caller must take whatever precautions are necessary (such as
holding appropriate locks) to avoid racing with another
list-mutation primitive, such as {\hyperref[core\string-api/kernel\string-api:c.hlist_nulls_add_head_rcu]{\emph{\code{hlist\_nulls\_add\_head\_rcu()}}}} or
{\hyperref[core\string-api/kernel\string-api:c.hlist_nulls_del_rcu]{\emph{\code{hlist\_nulls\_del\_rcu()}}}}, running on this same list.  However, it is
perfectly legal to run concurrently with the \_rcu list-traversal
primitives, such as {\hyperref[core\string-api/kernel\string-api:c.hlist_nulls_for_each_entry_rcu]{\emph{\code{hlist\_nulls\_for\_each\_entry\_rcu()}}}}.
\index{hlist\_nulls\_del\_rcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.hlist_nulls_del_rcu}\pysiglinewithargsret{void \bfcode{hlist\_nulls\_del\_rcu}}{struct hlist\_nulls\_node *\emph{ n}}{}
deletes entry from hash list without re-initialization

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct hlist\_nulls\_node * n}}] \leavevmode
the element to delete from the hash list.

\end{description}

\textbf{Note}

\code{hlist\_nulls\_unhashed()} on entry does not return true after this,
the entry is in an undefined state. It is useful for RCU based
lockfree traversal.

In particular, it means that we can not poison the forward
pointers that may still be used for walking the hash list.

The caller must take whatever precautions are necessary
(such as holding appropriate locks) to avoid racing
with another list-mutation primitive, such as {\hyperref[core\string-api/kernel\string-api:c.hlist_nulls_add_head_rcu]{\emph{\code{hlist\_nulls\_add\_head\_rcu()}}}}
or {\hyperref[core\string-api/kernel\string-api:c.hlist_nulls_del_rcu]{\emph{\code{hlist\_nulls\_del\_rcu()}}}}, running on this same list.
However, it is perfectly legal to run concurrently with
the \_rcu list-traversal primitives, such as
\code{hlist\_nulls\_for\_each\_entry()}.
\index{hlist\_nulls\_add\_head\_rcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.hlist_nulls_add_head_rcu}\pysiglinewithargsret{void \bfcode{hlist\_nulls\_add\_head\_rcu}}{struct hlist\_nulls\_node *\emph{ n}, struct hlist\_nulls\_head *\emph{ h}}{}
\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct hlist\_nulls\_node * n}}] \leavevmode
the element to add to the hash list.

\item[{\code{struct hlist\_nulls\_head * h}}] \leavevmode
the list to add to.

\end{description}

\textbf{Description}

Adds the specified element to the specified hlist\_nulls,
while permitting racing traversals.

The caller must take whatever precautions are necessary
(such as holding appropriate locks) to avoid racing
with another list-mutation primitive, such as {\hyperref[core\string-api/kernel\string-api:c.hlist_nulls_add_head_rcu]{\emph{\code{hlist\_nulls\_add\_head\_rcu()}}}}
or {\hyperref[core\string-api/kernel\string-api:c.hlist_nulls_del_rcu]{\emph{\code{hlist\_nulls\_del\_rcu()}}}}, running on this same list.
However, it is perfectly legal to run concurrently with
the \_rcu list-traversal primitives, such as
{\hyperref[core\string-api/kernel\string-api:c.hlist_nulls_for_each_entry_rcu]{\emph{\code{hlist\_nulls\_for\_each\_entry\_rcu()}}}}, used to prevent memory-consistency
problems on Alpha CPUs.  Regardless of the type of CPU, the
list-traversal primitive must be guarded by {\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock]{\emph{\code{rcu\_read\_lock()}}}}.
\index{hlist\_nulls\_for\_each\_entry\_rcu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.hlist_nulls_for_each_entry_rcu}\pysiglinewithargsret{\bfcode{hlist\_nulls\_for\_each\_entry\_rcu}}{\emph{tpos}, \emph{pos}, \emph{head}, \emph{member}}{}
iterate over rcu list of given type

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{tpos}}] \leavevmode
the type * to use as a loop cursor.

\item[{\code{pos}}] \leavevmode
the \code{struct hlist\_nulls\_node} to use as a loop cursor.

\item[{\code{head}}] \leavevmode
the head for your list.

\item[{\code{member}}] \leavevmode
the name of the hlist\_nulls\_node within the struct.

\end{description}

\textbf{Description}

The \code{barrier()} is needed to make sure compiler doesn't cache first element {[}1{]},
as this loop can be restarted {[}2{]}
{[}1{]} Documentation/atomic\_ops.txt around line 114
{[}2{]} Documentation/RCU/rculist\_nulls.txt around line 146
\index{hlist\_nulls\_for\_each\_entry\_safe (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.hlist_nulls_for_each_entry_safe}\pysiglinewithargsret{\bfcode{hlist\_nulls\_for\_each\_entry\_safe}}{\emph{tpos}, \emph{pos}, \emph{head}, \emph{member}}{}
iterate over list of given type safe against removal of list entry

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{tpos}}] \leavevmode
the type * to use as a loop cursor.

\item[{\code{pos}}] \leavevmode
the \code{struct hlist\_nulls\_node} to use as a loop cursor.

\item[{\code{head}}] \leavevmode
the head for your list.

\item[{\code{member}}] \leavevmode
the name of the hlist\_nulls\_node within the struct.

\end{description}
\index{rcu\_sync\_is\_idle (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_sync_is_idle}\pysiglinewithargsret{bool \bfcode{rcu\_sync\_is\_idle}}{struct rcu\_sync *\emph{ rsp}}{}
Are readers permitted to use their fastpaths?

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct rcu\_sync * rsp}}] \leavevmode
Pointer to rcu\_sync structure to use for synchronization

\end{description}

\textbf{Description}

Returns true if readers are permitted to use their fastpaths.
Must be invoked within an RCU read-side critical section whose
flavor matches that of the rcu\_sync struture.
\index{rcu\_sync\_init (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_sync_init}\pysiglinewithargsret{void \bfcode{rcu\_sync\_init}}{struct rcu\_sync *\emph{ rsp}, enum rcu\_sync\_type\emph{ type}}{}
Initialize an rcu\_sync structure

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct rcu\_sync * rsp}}] \leavevmode
Pointer to rcu\_sync structure to be initialized

\item[{\code{enum rcu\_sync\_type type}}] \leavevmode
Flavor of RCU with which to synchronize rcu\_sync structure

\end{description}
\index{rcu\_sync\_enter\_start (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_sync_enter_start}\pysiglinewithargsret{void \bfcode{rcu\_sync\_enter\_start}}{struct rcu\_sync *\emph{ rsp}}{}
Force readers onto slow path for multiple updates

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct rcu\_sync * rsp}}] \leavevmode
Pointer to rcu\_sync structure to use for synchronization

\end{description}

\textbf{Description}

Must be called after {\hyperref[core\string-api/kernel\string-api:c.rcu_sync_init]{\emph{\code{rcu\_sync\_init()}}}} and before first use.

Ensures {\hyperref[core\string-api/kernel\string-api:c.rcu_sync_is_idle]{\emph{\code{rcu\_sync\_is\_idle()}}}} returns false and rcu\_sync\_\{enter,exit\}()
pairs turn into NO-OPs.
\index{rcu\_sync\_enter (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_sync_enter}\pysiglinewithargsret{void \bfcode{rcu\_sync\_enter}}{struct rcu\_sync *\emph{ rsp}}{}
Force readers onto slowpath

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct rcu\_sync * rsp}}] \leavevmode
Pointer to rcu\_sync structure to use for synchronization

\end{description}

\textbf{Description}

This function is used by updaters who need readers to make use of
a slowpath during the update.  After this function returns, all
subsequent calls to {\hyperref[core\string-api/kernel\string-api:c.rcu_sync_is_idle]{\emph{\code{rcu\_sync\_is\_idle()}}}} will return false, which
tells readers to stay off their fastpaths.  A later call to
{\hyperref[core\string-api/kernel\string-api:c.rcu_sync_exit]{\emph{\code{rcu\_sync\_exit()}}}} re-enables reader slowpaths.

When called in isolation, {\hyperref[core\string-api/kernel\string-api:c.rcu_sync_enter]{\emph{\code{rcu\_sync\_enter()}}}} must wait for a grace
period, however, closely spaced calls to {\hyperref[core\string-api/kernel\string-api:c.rcu_sync_enter]{\emph{\code{rcu\_sync\_enter()}}}} can
optimize away the grace-period wait via a state machine implemented
by {\hyperref[core\string-api/kernel\string-api:c.rcu_sync_enter]{\emph{\code{rcu\_sync\_enter()}}}}, {\hyperref[core\string-api/kernel\string-api:c.rcu_sync_exit]{\emph{\code{rcu\_sync\_exit()}}}}, and {\hyperref[core\string-api/kernel\string-api:c.rcu_sync_func]{\emph{\code{rcu\_sync\_func()}}}}.
\index{rcu\_sync\_func (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_sync_func}\pysiglinewithargsret{void \bfcode{rcu\_sync\_func}}{struct rcu\_head *\emph{ rhp}}{}
Callback function managing reader access to fastpath

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct rcu\_head * rhp}}] \leavevmode
Pointer to rcu\_head in rcu\_sync structure to use for synchronization

\end{description}

\textbf{Description}

This function is passed to one of the \code{call\_rcu()} functions by
{\hyperref[core\string-api/kernel\string-api:c.rcu_sync_exit]{\emph{\code{rcu\_sync\_exit()}}}}, so that it is invoked after a grace period following the
that invocation of {\hyperref[core\string-api/kernel\string-api:c.rcu_sync_exit]{\emph{\code{rcu\_sync\_exit()}}}}.  It takes action based on events that
have taken place in the meantime, so that closely spaced {\hyperref[core\string-api/kernel\string-api:c.rcu_sync_enter]{\emph{\code{rcu\_sync\_enter()}}}}
and {\hyperref[core\string-api/kernel\string-api:c.rcu_sync_exit]{\emph{\code{rcu\_sync\_exit()}}}} pairs need not wait for a grace period.

If another {\hyperref[core\string-api/kernel\string-api:c.rcu_sync_enter]{\emph{\code{rcu\_sync\_enter()}}}} is invoked before the grace period
ended, reset state to allow the next {\hyperref[core\string-api/kernel\string-api:c.rcu_sync_exit]{\emph{\code{rcu\_sync\_exit()}}}} to let the
readers back onto their fastpaths (after a grace period).  If both
another {\hyperref[core\string-api/kernel\string-api:c.rcu_sync_enter]{\emph{\code{rcu\_sync\_enter()}}}} and its matching {\hyperref[core\string-api/kernel\string-api:c.rcu_sync_exit]{\emph{\code{rcu\_sync\_exit()}}}} are invoked
before the grace period ended, re-invoke \code{call\_rcu()} on behalf of that
{\hyperref[core\string-api/kernel\string-api:c.rcu_sync_exit]{\emph{\code{rcu\_sync\_exit()}}}}.  Otherwise, set all state back to idle so that readers
can again use their fastpaths.
\index{rcu\_sync\_exit (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_sync_exit}\pysiglinewithargsret{void \bfcode{rcu\_sync\_exit}}{struct rcu\_sync *\emph{ rsp}}{}
Allow readers back onto fast patch after grace period

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct rcu\_sync * rsp}}] \leavevmode
Pointer to rcu\_sync structure to use for synchronization

\end{description}

\textbf{Description}

This function is used by updaters who have completed, and can therefore
now allow readers to make use of their fastpaths after a grace period
has elapsed.  After this grace period has completed, all subsequent
calls to {\hyperref[core\string-api/kernel\string-api:c.rcu_sync_is_idle]{\emph{\code{rcu\_sync\_is\_idle()}}}} will return true, which tells readers that
they can once again use their fastpaths.
\index{rcu\_sync\_dtor (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/kernel-api:c.rcu_sync_dtor}\pysiglinewithargsret{void \bfcode{rcu\_sync\_dtor}}{struct rcu\_sync *\emph{ rsp}}{}
Clean up an rcu\_sync structure

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct rcu\_sync * rsp}}] \leavevmode
Pointer to rcu\_sync structure to be cleaned up

\end{description}


\section{Generic Associative Array Implementation}
\label{core-api/assoc_array:generic-associative-array-implementation}\label{core-api/assoc_array::doc}

\subsection{Overview}
\label{core-api/assoc_array:overview}
This associative array implementation is an object container with the following
properties:
\begin{enumerate}
\item {} 
Objects are opaque pointers.  The implementation does not care where they
point (if anywhere) or what they point to (if anything).

\begin{notice}{note}{Note:}
Pointers to objects \_must\_ be zero in the least significant bit.
\end{notice}

\item {} 
Objects do not need to contain linkage blocks for use by the array.  This
permits an object to be located in multiple arrays simultaneously.
Rather, the array is made up of metadata blocks that point to objects.

\item {} 
Objects require index keys to locate them within the array.

\item {} 
Index keys must be unique.  Inserting an object with the same key as one
already in the array will replace the old object.

\item {} 
Index keys can be of any length and can be of different lengths.

\item {} 
Index keys should encode the length early on, before any variation due to
length is seen.

\item {} 
Index keys can include a hash to scatter objects throughout the array.

\item {} 
The array can iterated over.  The objects will not necessarily come out in
key order.

\item {} 
The array can be iterated over whilst it is being modified, provided the
RCU readlock is being held by the iterator.  Note, however, under these
circumstances, some objects may be seen more than once.  If this is a
problem, the iterator should lock against modification.  Objects will not
be missed, however, unless deleted.

\item {} 
Objects in the array can be looked up by means of their index key.

\item {} 
Objects can be looked up whilst the array is being modified, provided the
RCU readlock is being held by the thread doing the look up.

\end{enumerate}

The implementation uses a tree of 16-pointer nodes internally that are indexed
on each level by nibbles from the index key in the same manner as in a radix
tree.  To improve memory efficiency, shortcuts can be emplaced to skip over
what would otherwise be a series of single-occupancy nodes.  Further, nodes
pack leaf object pointers into spare space in the node rather than making an
extra branch until as such time an object needs to be added to a full node.


\subsection{The Public API}
\label{core-api/assoc_array:the-public-api}
The public API can be found in \code{\textless{}linux/assoc\_array.h\textgreater{}}.  The associative
array is rooted on the following structure:

\begin{Verbatim}[commandchars=\\\{\}]
struct assoc\PYGZus{}array \PYGZob{}
        ...
\PYGZcb{};
\end{Verbatim}

The code is selected by enabling \code{CONFIG\_ASSOCIATIVE\_ARRAY} with:

\begin{Verbatim}[commandchars=\\\{\}]
./script/config \PYGZhy{}e ASSOCIATIVE\PYGZus{}ARRAY
\end{Verbatim}


\subsubsection{Edit Script}
\label{core-api/assoc_array:edit-script}
The insertion and deletion functions produce an `edit script' that can later be
applied to effect the changes without risking \code{ENOMEM}. This retains the
preallocated metadata blocks that will be installed in the internal tree and
keeps track of the metadata blocks that will be removed from the tree when the
script is applied.

This is also used to keep track of dead blocks and dead objects after the
script has been applied so that they can be freed later.  The freeing is done
after an RCU grace period has passed - thus allowing access functions to
proceed under the RCU read lock.

The script appears as outside of the API as a pointer of the type:

\begin{Verbatim}[commandchars=\\\{\}]
struct assoc\PYGZus{}array\PYGZus{}edit;
\end{Verbatim}

There are two functions for dealing with the script:
\begin{enumerate}
\item {} 
Apply an edit script:

\begin{Verbatim}[commandchars=\\\{\}]
void assoc\PYGZus{}array\PYGZus{}apply\PYGZus{}edit(struct assoc\PYGZus{}array\PYGZus{}edit *edit);
\end{Verbatim}

\end{enumerate}

This will perform the edit functions, interpolating various write barriers
to permit accesses under the RCU read lock to continue.  The edit script
will then be passed to \code{call\_rcu()} to free it and any dead stuff it points
to.
\begin{enumerate}
\setcounter{enumi}{1}
\item {} 
Cancel an edit script:

\begin{Verbatim}[commandchars=\\\{\}]
void assoc\PYGZus{}array\PYGZus{}cancel\PYGZus{}edit(struct assoc\PYGZus{}array\PYGZus{}edit *edit);
\end{Verbatim}

\end{enumerate}

This frees the edit script and all preallocated memory immediately. If
this was for insertion, the new object is \_not\_ released by this function,
but must rather be released by the caller.

These functions are guaranteed not to fail.


\subsubsection{Operations Table}
\label{core-api/assoc_array:operations-table}
Various functions take a table of operations:

\begin{Verbatim}[commandchars=\\\{\}]
struct assoc\PYGZus{}array\PYGZus{}ops \PYGZob{}
        ...
\PYGZcb{};
\end{Verbatim}

This points to a number of methods, all of which need to be provided:
\begin{enumerate}
\item {} 
Get a chunk of index key from caller data:

\begin{Verbatim}[commandchars=\\\{\}]
unsigned long (*get\PYGZus{}key\PYGZus{}chunk)(const void *index\PYGZus{}key, int level);
\end{Verbatim}

\end{enumerate}

This should return a chunk of caller-supplied index key starting at the
\emph{bit} position given by the level argument.  The level argument will be a
multiple of \code{ASSOC\_ARRAY\_KEY\_CHUNK\_SIZE} and the function should return
\code{ASSOC\_ARRAY\_KEY\_CHUNK\_SIZE bits}.  No error is possible.
\begin{enumerate}
\setcounter{enumi}{1}
\item {} 
Get a chunk of an object's index key:

\begin{Verbatim}[commandchars=\\\{\}]
unsigned long (*get\PYGZus{}object\PYGZus{}key\PYGZus{}chunk)(const void *object, int level);
\end{Verbatim}

\end{enumerate}

As the previous function, but gets its data from an object in the array
rather than from a caller-supplied index key.
\begin{enumerate}
\setcounter{enumi}{2}
\item {} 
See if this is the object we're looking for:

\begin{Verbatim}[commandchars=\\\{\}]
bool (*compare\PYGZus{}object)(const void *object, const void *index\PYGZus{}key);
\end{Verbatim}

\end{enumerate}

Compare the object against an index key and return \code{true} if it matches and
\code{false} if it doesn't.
\begin{enumerate}
\setcounter{enumi}{3}
\item {} 
Diff the index keys of two objects:

\begin{Verbatim}[commandchars=\\\{\}]
int (*diff\PYGZus{}objects)(const void *object, const void *index\PYGZus{}key);
\end{Verbatim}

\end{enumerate}

Return the bit position at which the index key of the specified object
differs from the given index key or -1 if they are the same.
\begin{enumerate}
\setcounter{enumi}{4}
\item {} 
Free an object:

\begin{Verbatim}[commandchars=\\\{\}]
void (*free\PYGZus{}object)(void *object);
\end{Verbatim}

\end{enumerate}

Free the specified object.  Note that this may be called an RCU grace period
after \code{assoc\_array\_apply\_edit()} was called, so \code{synchronize\_rcu()} may be
necessary on module unloading.


\subsubsection{Manipulation Functions}
\label{core-api/assoc_array:manipulation-functions}
There are a number of functions for manipulating an associative array:
\begin{enumerate}
\item {} 
Initialise an associative array:

\begin{Verbatim}[commandchars=\\\{\}]
void assoc\PYGZus{}array\PYGZus{}init(struct assoc\PYGZus{}array *array);
\end{Verbatim}

\end{enumerate}

This initialises the base structure for an associative array.  It can't fail.
\begin{enumerate}
\setcounter{enumi}{1}
\item {} 
Insert/replace an object in an associative array:

\begin{Verbatim}[commandchars=\\\{\}]
struct assoc\PYGZus{}array\PYGZus{}edit *
assoc\PYGZus{}array\PYGZus{}insert(struct assoc\PYGZus{}array *array,
                   const struct assoc\PYGZus{}array\PYGZus{}ops *ops,
                   const void *index\PYGZus{}key,
                   void *object);
\end{Verbatim}

\end{enumerate}

This inserts the given object into the array.  Note that the least
significant bit of the pointer must be zero as it's used to type-mark
pointers internally.

If an object already exists for that key then it will be replaced with the
new object and the old one will be freed automatically.

The \code{index\_key} argument should hold index key information and is
passed to the methods in the ops table when they are called.

This function makes no alteration to the array itself, but rather returns
an edit script that must be applied.  \code{-ENOMEM} is returned in the case of
an out-of-memory error.

The caller should lock exclusively against other modifiers of the array.
\begin{enumerate}
\setcounter{enumi}{2}
\item {} 
Delete an object from an associative array:

\begin{Verbatim}[commandchars=\\\{\}]
struct assoc\PYGZus{}array\PYGZus{}edit *
assoc\PYGZus{}array\PYGZus{}delete(struct assoc\PYGZus{}array *array,
                   const struct assoc\PYGZus{}array\PYGZus{}ops *ops,
                   const void *index\PYGZus{}key);
\end{Verbatim}

\end{enumerate}

This deletes an object that matches the specified data from the array.

The \code{index\_key} argument should hold index key information and is
passed to the methods in the ops table when they are called.

This function makes no alteration to the array itself, but rather returns
an edit script that must be applied.  \code{-ENOMEM} is returned in the case of
an out-of-memory error.  \code{NULL} will be returned if the specified object is
not found within the array.

The caller should lock exclusively against other modifiers of the array.
\begin{enumerate}
\setcounter{enumi}{3}
\item {} 
Delete all objects from an associative array:

\begin{Verbatim}[commandchars=\\\{\}]
struct assoc\PYGZus{}array\PYGZus{}edit *
assoc\PYGZus{}array\PYGZus{}clear(struct assoc\PYGZus{}array *array,
                  const struct assoc\PYGZus{}array\PYGZus{}ops *ops);
\end{Verbatim}

\end{enumerate}

This deletes all the objects from an associative array and leaves it
completely empty.

This function makes no alteration to the array itself, but rather returns
an edit script that must be applied.  \code{-ENOMEM} is returned in the case of
an out-of-memory error.

The caller should lock exclusively against other modifiers of the array.
\begin{enumerate}
\setcounter{enumi}{4}
\item {} 
Destroy an associative array, deleting all objects:

\begin{Verbatim}[commandchars=\\\{\}]
void assoc\PYGZus{}array\PYGZus{}destroy(struct assoc\PYGZus{}array *array,
                         const struct assoc\PYGZus{}array\PYGZus{}ops *ops);
\end{Verbatim}

\end{enumerate}

This destroys the contents of the associative array and leaves it
completely empty.  It is not permitted for another thread to be traversing
the array under the RCU read lock at the same time as this function is
destroying it as no RCU deferral is performed on memory release -
something that would require memory to be allocated.

The caller should lock exclusively against other modifiers and accessors
of the array.
\begin{enumerate}
\setcounter{enumi}{5}
\item {} 
Garbage collect an associative array:

\begin{Verbatim}[commandchars=\\\{\}]
int assoc\PYGZus{}array\PYGZus{}gc(struct assoc\PYGZus{}array *array,
                   const struct assoc\PYGZus{}array\PYGZus{}ops *ops,
                   bool (*iterator)(void *object, void *iterator\PYGZus{}data),
                   void *iterator\PYGZus{}data);
\end{Verbatim}

\end{enumerate}

This iterates over the objects in an associative array and passes each one to
\code{iterator()}.  If \code{iterator()} returns \code{true}, the object is kept.  If it
returns \code{false}, the object will be freed.  If the \code{iterator()} function
returns \code{true}, it must perform any appropriate refcount incrementing on the
object before returning.

The internal tree will be packed down if possible as part of the iteration
to reduce the number of nodes in it.

The \code{iterator\_data} is passed directly to \code{iterator()} and is otherwise
ignored by the function.

The function will return \code{0} if successful and \code{-ENOMEM} if there wasn't
enough memory.

It is possible for other threads to iterate over or search the array under
the RCU read lock whilst this function is in progress.  The caller should
lock exclusively against other modifiers of the array.


\subsubsection{Access Functions}
\label{core-api/assoc_array:access-functions}
There are two functions for accessing an associative array:
\begin{enumerate}
\item {} 
Iterate over all the objects in an associative array:

\begin{Verbatim}[commandchars=\\\{\}]
int assoc\PYGZus{}array\PYGZus{}iterate(const struct assoc\PYGZus{}array *array,
                        int (*iterator)(const void *object,
                                        void *iterator\PYGZus{}data),
                        void *iterator\PYGZus{}data);
\end{Verbatim}

\end{enumerate}

This passes each object in the array to the iterator callback function.
\code{iterator\_data} is private data for that function.

This may be used on an array at the same time as the array is being
modified, provided the RCU read lock is held.  Under such circumstances,
it is possible for the iteration function to see some objects twice.  If
this is a problem, then modification should be locked against.  The
iteration algorithm should not, however, miss any objects.

The function will return \code{0} if no objects were in the array or else it will
return the result of the last iterator function called.  Iteration stops
immediately if any call to the iteration function results in a non-zero
return.
\begin{enumerate}
\setcounter{enumi}{1}
\item {} 
Find an object in an associative array:

\begin{Verbatim}[commandchars=\\\{\}]
void *assoc\PYGZus{}array\PYGZus{}find(const struct assoc\PYGZus{}array *array,
                       const struct assoc\PYGZus{}array\PYGZus{}ops *ops,
                       const void *index\PYGZus{}key);
\end{Verbatim}

\end{enumerate}

This walks through the array's internal tree directly to the object
specified by the index key..

This may be used on an array at the same time as the array is being
modified, provided the RCU read lock is held.

The function will return the object if found (and set \code{*\_type} to the object
type) or will return \code{NULL} if the object was not found.


\subsubsection{Index Key Form}
\label{core-api/assoc_array:index-key-form}
The index key can be of any form, but since the algorithms aren't told how long
the key is, it is strongly recommended that the index key includes its length
very early on before any variation due to the length would have an effect on
comparisons.

This will cause leaves with different length keys to scatter away from each
other - and those with the same length keys to cluster together.

It is also recommended that the index key begin with a hash of the rest of the
key to maximise scattering throughout keyspace.

The better the scattering, the wider and lower the internal tree will be.

Poor scattering isn't too much of a problem as there are shortcuts and nodes
can contain mixtures of leaves and metadata pointers.

The index key is read in chunks of machine word.  Each chunk is subdivided into
one nibble (4 bits) per level, so on a 32-bit CPU this is good for 8 levels and
on a 64-bit CPU, 16 levels.  Unless the scattering is really poor, it is
unlikely that more than one word of any particular index key will have to be
used.


\subsection{Internal Workings}
\label{core-api/assoc_array:internal-workings}
The associative array data structure has an internal tree.  This tree is
constructed of two types of metadata blocks: nodes and shortcuts.

A node is an array of slots.  Each slot can contain one of four things:
\begin{itemize}
\item {} 
A NULL pointer, indicating that the slot is empty.

\item {} 
A pointer to an object (a leaf).

\item {} 
A pointer to a node at the next level.

\item {} 
A pointer to a shortcut.

\end{itemize}


\subsubsection{Basic Internal Tree Layout}
\label{core-api/assoc_array:basic-internal-tree-layout}
Ignoring shortcuts for the moment, the nodes form a multilevel tree.  The index
key space is strictly subdivided by the nodes in the tree and nodes occur on
fixed levels.  For example:

\begin{Verbatim}[commandchars=\\\{\}]
Level: 0               1               2               3
       =============== =============== =============== ===============
                                                       NODE D
                       NODE B          NODE C  +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{}+\PYGZhy{}\PYGZhy{}\PYGZhy{}+
               +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{}+\PYGZhy{}\PYGZhy{}\PYGZhy{}+   +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{}+\PYGZhy{}\PYGZhy{}\PYGZhy{}+   \textbar{}       \textbar{} 0 \textbar{}
       NODE A  \textbar{}       \textbar{} 0 \textbar{}   \textbar{}       \textbar{} 0 \textbar{}   \textbar{}       +\PYGZhy{}\PYGZhy{}\PYGZhy{}+
       +\PYGZhy{}\PYGZhy{}\PYGZhy{}+   \textbar{}       +\PYGZhy{}\PYGZhy{}\PYGZhy{}+   \textbar{}       +\PYGZhy{}\PYGZhy{}\PYGZhy{}+   \textbar{}       :   :
       \textbar{} 0 \textbar{}   \textbar{}       :   :   \textbar{}       :   :   \textbar{}       +\PYGZhy{}\PYGZhy{}\PYGZhy{}+
       +\PYGZhy{}\PYGZhy{}\PYGZhy{}+   \textbar{}       +\PYGZhy{}\PYGZhy{}\PYGZhy{}+   \textbar{}       +\PYGZhy{}\PYGZhy{}\PYGZhy{}+   \textbar{}       \textbar{} f \textbar{}
       \textbar{} 1 \textbar{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+       \textbar{} 3 \textbar{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+       \textbar{} 7 \textbar{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+       +\PYGZhy{}\PYGZhy{}\PYGZhy{}+
       +\PYGZhy{}\PYGZhy{}\PYGZhy{}+           +\PYGZhy{}\PYGZhy{}\PYGZhy{}+           +\PYGZhy{}\PYGZhy{}\PYGZhy{}+
       :   :           :   :           \textbar{} 8 \textbar{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+
       +\PYGZhy{}\PYGZhy{}\PYGZhy{}+           +\PYGZhy{}\PYGZhy{}\PYGZhy{}+           +\PYGZhy{}\PYGZhy{}\PYGZhy{}+   \textbar{}       NODE E
       \textbar{} e \textbar{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+       \textbar{} f \textbar{}           :   :   +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{}+\PYGZhy{}\PYGZhy{}\PYGZhy{}+
       +\PYGZhy{}\PYGZhy{}\PYGZhy{}+   \textbar{}       +\PYGZhy{}\PYGZhy{}\PYGZhy{}+           +\PYGZhy{}\PYGZhy{}\PYGZhy{}+           \textbar{} 0 \textbar{}
       \textbar{} f \textbar{}   \textbar{}                       \textbar{} f \textbar{}           +\PYGZhy{}\PYGZhy{}\PYGZhy{}+
       +\PYGZhy{}\PYGZhy{}\PYGZhy{}+   \textbar{}                       +\PYGZhy{}\PYGZhy{}\PYGZhy{}+           :   :
               \textbar{}       NODE F                          +\PYGZhy{}\PYGZhy{}\PYGZhy{}+
               +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{}+\PYGZhy{}\PYGZhy{}\PYGZhy{}+                           \textbar{} f \textbar{}
                       \textbar{} 0 \textbar{}           NODE G          +\PYGZhy{}\PYGZhy{}\PYGZhy{}+
                       +\PYGZhy{}\PYGZhy{}\PYGZhy{}+   +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{}+\PYGZhy{}\PYGZhy{}\PYGZhy{}+
                       :   :   \textbar{}       \textbar{} 0 \textbar{}
                       +\PYGZhy{}\PYGZhy{}\PYGZhy{}+   \textbar{}       +\PYGZhy{}\PYGZhy{}\PYGZhy{}+
                       \textbar{} 6 \textbar{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+       :   :
                       +\PYGZhy{}\PYGZhy{}\PYGZhy{}+           +\PYGZhy{}\PYGZhy{}\PYGZhy{}+
                       :   :           \textbar{} f \textbar{}
                       +\PYGZhy{}\PYGZhy{}\PYGZhy{}+           +\PYGZhy{}\PYGZhy{}\PYGZhy{}+
                       \textbar{} f \textbar{}
                       +\PYGZhy{}\PYGZhy{}\PYGZhy{}+
\end{Verbatim}

In the above example, there are 7 nodes (A-G), each with 16 slots (0-f).
Assuming no other meta data nodes in the tree, the key space is divided
thusly:

\begin{Verbatim}[commandchars=\\\{\}]
KEY PREFIX      NODE
==========      ====
137*            D
138*            E
13[0\PYGZhy{}69\PYGZhy{}f]*     C
1[0\PYGZhy{}24\PYGZhy{}f]*      B
e6*             G
e[0\PYGZhy{}57\PYGZhy{}f]*      F
[02\PYGZhy{}df]*        A
\end{Verbatim}

So, for instance, keys with the following example index keys will be found in
the appropriate nodes:

\begin{Verbatim}[commandchars=\\\{\}]
INDEX KEY       PREFIX  NODE
=============== ======= ====
13694892892489  13      C
13795289025897  137     D
13889dde88793   138     E
138bbb89003093  138     E
1394879524789   12      C
1458952489      1       B
9431809de993ba  \PYGZhy{}       A
b4542910809cd   \PYGZhy{}       A
e5284310def98   e       F
e68428974237    e6      G
e7fffcbd443     e       F
f3842239082     \PYGZhy{}       A
\end{Verbatim}

To save memory, if a node can hold all the leaves in its portion of keyspace,
then the node will have all those leaves in it and will not have any metadata
pointers - even if some of those leaves would like to be in the same slot.

A node can contain a heterogeneous mix of leaves and metadata pointers.
Metadata pointers must be in the slots that match their subdivisions of key
space.  The leaves can be in any slot not occupied by a metadata pointer.  It
is guaranteed that none of the leaves in a node will match a slot occupied by a
metadata pointer.  If the metadata pointer is there, any leaf whose key matches
the metadata key prefix must be in the subtree that the metadata pointer points
to.

In the above example list of index keys, node A will contain:

\begin{Verbatim}[commandchars=\\\{\}]
SLOT    CONTENT         INDEX KEY (PREFIX)
====    =============== ==================
1       PTR TO NODE B   1*
any     LEAF            9431809de993ba
any     LEAF            b4542910809cd
e       PTR TO NODE F   e*
any     LEAF            f3842239082
\end{Verbatim}

and node B:

\begin{Verbatim}[commandchars=\\\{\}]
3   PTR TO NODE C   13*
any LEAF            1458952489
\end{Verbatim}


\subsubsection{Shortcuts}
\label{core-api/assoc_array:shortcuts}
Shortcuts are metadata records that jump over a piece of keyspace.  A shortcut
is a replacement for a series of single-occupancy nodes ascending through the
levels.  Shortcuts exist to save memory and to speed up traversal.

It is possible for the root of the tree to be a shortcut - say, for example,
the tree contains at least 17 nodes all with key prefix \code{1111}.  The
insertion algorithm will insert a shortcut to skip over the \code{1111} keyspace
in a single bound and get to the fourth level where these actually become
different.


\subsubsection{Splitting And Collapsing Nodes}
\label{core-api/assoc_array:splitting-and-collapsing-nodes}
Each node has a maximum capacity of 16 leaves and metadata pointers.  If the
insertion algorithm finds that it is trying to insert a 17th object into a
node, that node will be split such that at least two leaves that have a common
key segment at that level end up in a separate node rooted on that slot for
that common key segment.

If the leaves in a full node and the leaf that is being inserted are
sufficiently similar, then a shortcut will be inserted into the tree.

When the number of objects in the subtree rooted at a node falls to 16 or
fewer, then the subtree will be collapsed down to a single node - and this will
ripple towards the root if possible.


\subsubsection{Non-Recursive Iteration}
\label{core-api/assoc_array:non-recursive-iteration}
Each node and shortcut contains a back pointer to its parent and the number of
slot in that parent that points to it.  None-recursive iteration uses these to
proceed rootwards through the tree, going to the parent node, slot N + 1 to
make sure progress is made without the need for a stack.

The backpointers, however, make simultaneous alteration and iteration tricky.


\subsubsection{Simultaneous Alteration And Iteration}
\label{core-api/assoc_array:simultaneous-alteration-and-iteration}
There are a number of cases to consider:
\begin{enumerate}
\item {} 
Simple insert/replace.  This involves simply replacing a NULL or old
matching leaf pointer with the pointer to the new leaf after a barrier.
The metadata blocks don't change otherwise.  An old leaf won't be freed
until after the RCU grace period.

\item {} 
Simple delete.  This involves just clearing an old matching leaf.  The
metadata blocks don't change otherwise.  The old leaf won't be freed until
after the RCU grace period.

\item {} 
Insertion replacing part of a subtree that we haven't yet entered.  This
may involve replacement of part of that subtree - but that won't affect
the iteration as we won't have reached the pointer to it yet and the
ancestry blocks are not replaced (the layout of those does not change).

\item {} 
Insertion replacing nodes that we're actively processing.  This isn't a
problem as we've passed the anchoring pointer and won't switch onto the
new layout until we follow the back pointers - at which point we've
already examined the leaves in the replaced node (we iterate over all the
leaves in a node before following any of its metadata pointers).

We might, however, re-see some leaves that have been split out into a new
branch that's in a slot further along than we were at.

\item {} 
Insertion replacing nodes that we're processing a dependent branch of.
This won't affect us until we follow the back pointers.  Similar to (4).

\item {} 
Deletion collapsing a branch under us.  This doesn't affect us because the
back pointers will get us back to the parent of the new node before we
could see the new node.  The entire collapsed subtree is thrown away
unchanged - and will still be rooted on the same slot, so we shouldn't
process it a second time as we'll go back to slot + 1.

\end{enumerate}

\begin{notice}{note}{Note:}
Under some circumstances, we need to simultaneously change the parent
pointer and the parent slot pointer on a node (say, for example, we
inserted another node before it and moved it up a level).  We cannot do
this without locking against a read - so we have to replace that node too.

However, when we're changing a shortcut into a node this isn't a problem
as shortcuts only have one slot and so the parent slot number isn't used
when traversing backwards over one.  This means that it's okay to change
the slot number first - provided suitable barriers are used to make sure
the parent slot number is read after the back pointer.
\end{notice}

Obsolete blocks and leaves are freed up after an RCU grace period has passed,
so as long as anyone doing walking or iteration holds the RCU read lock, the
old superstructure should not go away on them.


\section{Semantics and Behavior of Atomic and Bitmask Operations}
\label{core-api/atomic_ops:semantics-and-behavior-of-atomic-and-bitmask-operations}\label{core-api/atomic_ops::doc}\begin{quote}\begin{description}
\item[{Author}] \leavevmode
David S. Miller

\end{description}\end{quote}

This document is intended to serve as a guide to Linux port
maintainers on how to implement atomic counter, bitops, and spinlock
interfaces properly.


\subsection{Atomic Type And Operations}
\label{core-api/atomic_ops:atomic-type-and-operations}
The atomic\_t type should be defined as a signed integer and
the atomic\_long\_t type as a signed long integer.  Also, they should
be made opaque such that any kind of cast to a normal C integer type
will fail.  Something like the following should suffice:

\begin{Verbatim}[commandchars=\\\{\}]
typedef struct \PYGZob{} int counter; \PYGZcb{} atomic\PYGZus{}t;
typedef struct \PYGZob{} long counter; \PYGZcb{} atomic\PYGZus{}long\PYGZus{}t;
\end{Verbatim}

Historically, counter has been declared volatile.  This is now discouraged.
See \DUspan{xref,std,std-ref}{Documentation/process/volatile-considered-harmful.rst} for the complete rationale.

local\_t is very similar to atomic\_t. If the counter is per CPU and only
updated by one CPU, local\_t is probably more appropriate. Please see
{\hyperref[core\string-api/local_ops:local\string-ops]{\emph{Documentation/core-api/local\_ops.rst}}} for the semantics of
local\_t.

The first operations to implement for atomic\_t's are the initializers and
plain reads.

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZsh{}define ATOMIC\PYGZus{}INIT(i)          \PYGZob{} (i) \PYGZcb{}
\PYGZsh{}define atomic\PYGZus{}set(v, i)        ((v)\PYGZhy{}\PYGZgt{}counter = (i))
\end{Verbatim}

The first macro is used in definitions, such as:

\begin{Verbatim}[commandchars=\\\{\}]
static atomic\PYGZus{}t my\PYGZus{}counter = ATOMIC\PYGZus{}INIT(1);
\end{Verbatim}

The initializer is atomic in that the return values of the atomic operations
are guaranteed to be correct reflecting the initialized value if the
initializer is used before runtime.  If the initializer is used at runtime, a
proper implicit or explicit read memory barrier is needed before reading the
value with atomic\_read from another thread.

As with all of the \code{atomic\_} interfaces, replace the leading \code{atomic\_}
with \code{atomic\_long\_} to operate on atomic\_long\_t.

The second interface can be used at runtime, as in:

\begin{Verbatim}[commandchars=\\\{\}]
struct foo \PYGZob{} atomic\PYGZus{}t counter; \PYGZcb{};
...

struct foo *k;

k = kmalloc(sizeof(*k), GFP\PYGZus{}KERNEL);
if (!k)
        return \PYGZhy{}ENOMEM;
atomic\PYGZus{}set(\PYGZam{}k\PYGZhy{}\PYGZgt{}counter, 0);
\end{Verbatim}

The setting is atomic in that the return values of the atomic operations by
all threads are guaranteed to be correct reflecting either the value that has
been set with this operation or set with another operation.  A proper implicit
or explicit memory barrier is needed before the value set with the operation
is guaranteed to be readable with atomic\_read from another thread.

Next, we have:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZsh{}define atomic\PYGZus{}read(v)  ((v)\PYGZhy{}\PYGZgt{}counter)
\end{Verbatim}

which simply reads the counter value currently visible to the calling thread.
The read is atomic in that the return value is guaranteed to be one of the
values initialized or modified with the interface operations if a proper
implicit or explicit memory barrier is used after possible runtime
initialization by any other thread and the value is modified only with the
interface operations.  atomic\_read does not guarantee that the runtime
initialization by any other thread is visible yet, so the user of the
interface must take care of that with a proper implicit or explicit memory
barrier.

\begin{notice}{warning}{Warning:}
\code{atomic\_read()} and \code{atomic\_set()} DO NOT IMPLY BARRIERS!

Some architectures may choose to use the volatile keyword, barriers, or
inline assembly to guarantee some degree of immediacy for atomic\_read()
and atomic\_set().  This is not uniformly guaranteed, and may change in
the future, so all users of atomic\_t should treat atomic\_read() and
atomic\_set() as simple C statements that may be reordered or optimized
away entirely by the compiler or processor, and explicitly invoke the
appropriate compiler and/or memory barrier for each use case.  Failure
to do so will result in code that may suddenly break when used with
different architectures or compiler optimizations, or even changes in
unrelated code which changes how the compiler optimizes the section
accessing atomic\_t variables.
\end{notice}

Properly aligned pointers, longs, ints, and chars (and unsigned
equivalents) may be atomically loaded from and stored to in the same
sense as described for atomic\_read() and atomic\_set().  The READ\_ONCE()
and WRITE\_ONCE() macros should be used to prevent the compiler from using
optimizations that might otherwise optimize accesses out of existence on
the one hand, or that might create unsolicited accesses on the other.

For example consider the following code:

\begin{Verbatim}[commandchars=\\\{\}]
while (a \PYGZgt{} 0)
        do\PYGZus{}something();
\end{Verbatim}

If the compiler can prove that do\_something() does not store to the
variable a, then the compiler is within its rights transforming this to
the following:

\begin{Verbatim}[commandchars=\\\{\}]
tmp = a;
if (a \PYGZgt{} 0)
        for (;;)
                do\PYGZus{}something();
\end{Verbatim}

If you don't want the compiler to do this (and you probably don't), then
you should use something like the following:

\begin{Verbatim}[commandchars=\\\{\}]
while (READ\PYGZus{}ONCE(a) \PYGZlt{} 0)
        do\PYGZus{}something();
\end{Verbatim}

Alternatively, you could place a barrier() call in the loop.

For another example, consider the following code:

\begin{Verbatim}[commandchars=\\\{\}]
tmp\PYGZus{}a = a;
do\PYGZus{}something\PYGZus{}with(tmp\PYGZus{}a);
do\PYGZus{}something\PYGZus{}else\PYGZus{}with(tmp\PYGZus{}a);
\end{Verbatim}

If the compiler can prove that do\_something\_with() does not store to the
variable a, then the compiler is within its rights to manufacture an
additional load as follows:

\begin{Verbatim}[commandchars=\\\{\}]
tmp\PYGZus{}a = a;
do\PYGZus{}something\PYGZus{}with(tmp\PYGZus{}a);
tmp\PYGZus{}a = a;
do\PYGZus{}something\PYGZus{}else\PYGZus{}with(tmp\PYGZus{}a);
\end{Verbatim}

This could fatally confuse your code if it expected the same value
to be passed to do\_something\_with() and do\_something\_else\_with().

The compiler would be likely to manufacture this additional load if
do\_something\_with() was an inline function that made very heavy use
of registers: reloading from variable a could save a flush to the
stack and later reload.  To prevent the compiler from attacking your
code in this manner, write the following:

\begin{Verbatim}[commandchars=\\\{\}]
tmp\PYGZus{}a = READ\PYGZus{}ONCE(a);
do\PYGZus{}something\PYGZus{}with(tmp\PYGZus{}a);
do\PYGZus{}something\PYGZus{}else\PYGZus{}with(tmp\PYGZus{}a);
\end{Verbatim}

For a final example, consider the following code, assuming that the
variable a is set at boot time before the second CPU is brought online
and never changed later, so that memory barriers are not needed:

\begin{Verbatim}[commandchars=\\\{\}]
if (a)
        b = 9;
else
        b = 42;
\end{Verbatim}

The compiler is within its rights to manufacture an additional store
by transforming the above code into the following:

\begin{Verbatim}[commandchars=\\\{\}]
b = 42;
if (a)
        b = 9;
\end{Verbatim}

This could come as a fatal surprise to other code running concurrently
that expected b to never have the value 42 if a was zero.  To prevent
the compiler from doing this, write something like:

\begin{Verbatim}[commandchars=\\\{\}]
if (a)
        WRITE\PYGZus{}ONCE(b, 9);
else
        WRITE\PYGZus{}ONCE(b, 42);
\end{Verbatim}

Don't even -think- about doing this without proper use of memory barriers,
locks, or atomic operations if variable a can change at runtime!

\begin{notice}{warning}{Warning:}
\code{READ\_ONCE()} OR \code{WRITE\_ONCE()} DO NOT IMPLY A BARRIER!
\end{notice}

Now, we move onto the atomic operation interfaces typically implemented with
the help of assembly code.

\begin{Verbatim}[commandchars=\\\{\}]
void atomic\PYGZus{}add(int i, atomic\PYGZus{}t *v);
void atomic\PYGZus{}sub(int i, atomic\PYGZus{}t *v);
void atomic\PYGZus{}inc(atomic\PYGZus{}t *v);
void atomic\PYGZus{}dec(atomic\PYGZus{}t *v);
\end{Verbatim}

These four routines add and subtract integral values to/from the given
atomic\_t value.  The first two routines pass explicit integers by
which to make the adjustment, whereas the latter two use an implicit
adjustment value of ``1''.

One very important aspect of these two routines is that they DO NOT
require any explicit memory barriers.  They need only perform the
atomic\_t counter update in an SMP safe manner.

Next, we have:

\begin{Verbatim}[commandchars=\\\{\}]
int atomic\PYGZus{}inc\PYGZus{}return(atomic\PYGZus{}t *v);
int atomic\PYGZus{}dec\PYGZus{}return(atomic\PYGZus{}t *v);
\end{Verbatim}

These routines add 1 and subtract 1, respectively, from the given
atomic\_t and return the new counter value after the operation is
performed.

Unlike the above routines, it is required that these primitives
include explicit memory barriers that are performed before and after
the operation.  It must be done such that all memory operations before
and after the atomic operation calls are strongly ordered with respect
to the atomic operation itself.

For example, it should behave as if a smp\_mb() call existed both
before and after the atomic operation.

If the atomic instructions used in an implementation provide explicit
memory barrier semantics which satisfy the above requirements, that is
fine as well.

Let's move on:

\begin{Verbatim}[commandchars=\\\{\}]
int atomic\PYGZus{}add\PYGZus{}return(int i, atomic\PYGZus{}t *v);
int atomic\PYGZus{}sub\PYGZus{}return(int i, atomic\PYGZus{}t *v);
\end{Verbatim}

These behave just like atomic\_\{inc,dec\}\_return() except that an
explicit counter adjustment is given instead of the implicit ``1''.
This means that like atomic\_\{inc,dec\}\_return(), the memory barrier
semantics are required.

Next:

\begin{Verbatim}[commandchars=\\\{\}]
int atomic\PYGZus{}inc\PYGZus{}and\PYGZus{}test(atomic\PYGZus{}t *v);
int atomic\PYGZus{}dec\PYGZus{}and\PYGZus{}test(atomic\PYGZus{}t *v);
\end{Verbatim}

These two routines increment and decrement by 1, respectively, the
given atomic counter.  They return a boolean indicating whether the
resulting counter value was zero or not.

Again, these primitives provide explicit memory barrier semantics around
the atomic operation:

\begin{Verbatim}[commandchars=\\\{\}]
int atomic\PYGZus{}sub\PYGZus{}and\PYGZus{}test(int i, atomic\PYGZus{}t *v);
\end{Verbatim}

This is identical to atomic\_dec\_and\_test() except that an explicit
decrement is given instead of the implicit ``1''.  This primitive must
provide explicit memory barrier semantics around the operation:

\begin{Verbatim}[commandchars=\\\{\}]
int atomic\PYGZus{}add\PYGZus{}negative(int i, atomic\PYGZus{}t *v);
\end{Verbatim}

The given increment is added to the given atomic counter value.  A boolean
is return which indicates whether the resulting counter value is negative.
This primitive must provide explicit memory barrier semantics around
the operation.

Then:

\begin{Verbatim}[commandchars=\\\{\}]
int atomic\PYGZus{}xchg(atomic\PYGZus{}t *v, int new);
\end{Verbatim}

This performs an atomic exchange operation on the atomic variable v, setting
the given new value.  It returns the old value that the atomic variable v had
just before the operation.

atomic\_xchg must provide explicit memory barriers around the operation.

\begin{Verbatim}[commandchars=\\\{\}]
int atomic\PYGZus{}cmpxchg(atomic\PYGZus{}t *v, int old, int new);
\end{Verbatim}

This performs an atomic compare exchange operation on the atomic value v,
with the given old and new values. Like all atomic\_xxx operations,
atomic\_cmpxchg will only satisfy its atomicity semantics as long as all
other accesses of *v are performed through atomic\_xxx operations.

atomic\_cmpxchg must provide explicit memory barriers around the operation,
although if the comparison fails then no memory ordering guarantees are
required.

The semantics for atomic\_cmpxchg are the same as those defined for `cas'
below.

Finally:

\begin{Verbatim}[commandchars=\\\{\}]
int atomic\PYGZus{}add\PYGZus{}unless(atomic\PYGZus{}t *v, int a, int u);
\end{Verbatim}

If the atomic value v is not equal to u, this function adds a to v, and
returns non zero. If v is equal to u then it returns zero. This is done as
an atomic operation.

atomic\_add\_unless must provide explicit memory barriers around the
operation unless it fails (returns 0).

atomic\_inc\_not\_zero, equivalent to atomic\_add\_unless(v, 1, 0)

If a caller requires memory barrier semantics around an atomic\_t
operation which does not return a value, a set of interfaces are
defined which accomplish this:

\begin{Verbatim}[commandchars=\\\{\}]
void smp\PYGZus{}mb\PYGZus{}\PYGZus{}before\PYGZus{}atomic(void);
void smp\PYGZus{}mb\PYGZus{}\PYGZus{}after\PYGZus{}atomic(void);
\end{Verbatim}

Preceding a non-value-returning read-modify-write atomic operation with
smp\_mb\_\_before\_atomic() and following it with smp\_mb\_\_after\_atomic()
provides the same full ordering that is provided by value-returning
read-modify-write atomic operations.

For example, smp\_mb\_\_before\_atomic() can be used like so:

\begin{Verbatim}[commandchars=\\\{\}]
obj\PYGZhy{}\PYGZgt{}dead = 1;
smp\PYGZus{}mb\PYGZus{}\PYGZus{}before\PYGZus{}atomic();
atomic\PYGZus{}dec(\PYGZam{}obj\PYGZhy{}\PYGZgt{}ref\PYGZus{}count);
\end{Verbatim}

It makes sure that all memory operations preceding the atomic\_dec()
call are strongly ordered with respect to the atomic counter
operation.  In the above example, it guarantees that the assignment of
``1'' to obj-\textgreater{}dead will be globally visible to other cpus before the
atomic counter decrement.

Without the explicit smp\_mb\_\_before\_atomic() call, the
implementation could legally allow the atomic counter update visible
to other cpus before the ``obj-\textgreater{}dead = 1;'' assignment.

A missing memory barrier in the cases where they are required by the
atomic\_t implementation above can have disastrous results.  Here is
an example, which follows a pattern occurring frequently in the Linux
kernel.  It is the use of atomic counters to implement reference
counting, and it works such that once the counter falls to zero it can
be guaranteed that no other entity can be accessing the object:

\begin{Verbatim}[commandchars=\\\{\}]
static void obj\PYGZus{}list\PYGZus{}add(struct obj *obj, struct list\PYGZus{}head *head)
\PYGZob{}
        obj\PYGZhy{}\PYGZgt{}active = 1;
        list\PYGZus{}add(\PYGZam{}obj\PYGZhy{}\PYGZgt{}list, head);
\PYGZcb{}

static void obj\PYGZus{}list\PYGZus{}del(struct obj *obj)
\PYGZob{}
        list\PYGZus{}del(\PYGZam{}obj\PYGZhy{}\PYGZgt{}list);
        obj\PYGZhy{}\PYGZgt{}active = 0;
\PYGZcb{}

static void obj\PYGZus{}destroy(struct obj *obj)
\PYGZob{}
        BUG\PYGZus{}ON(obj\PYGZhy{}\PYGZgt{}active);
        kfree(obj);
\PYGZcb{}

struct obj *obj\PYGZus{}list\PYGZus{}peek(struct list\PYGZus{}head *head)
\PYGZob{}
        if (!list\PYGZus{}empty(head)) \PYGZob{}
                struct obj *obj;

                obj = list\PYGZus{}entry(head\PYGZhy{}\PYGZgt{}next, struct obj, list);
                atomic\PYGZus{}inc(\PYGZam{}obj\PYGZhy{}\PYGZgt{}refcnt);
                return obj;
        \PYGZcb{}
        return NULL;
\PYGZcb{}

void obj\PYGZus{}poke(void)
\PYGZob{}
        struct obj *obj;

        spin\PYGZus{}lock(\PYGZam{}global\PYGZus{}list\PYGZus{}lock);
        obj = obj\PYGZus{}list\PYGZus{}peek(\PYGZam{}global\PYGZus{}list);
        spin\PYGZus{}unlock(\PYGZam{}global\PYGZus{}list\PYGZus{}lock);

        if (obj) \PYGZob{}
                obj\PYGZhy{}\PYGZgt{}ops\PYGZhy{}\PYGZgt{}poke(obj);
                if (atomic\PYGZus{}dec\PYGZus{}and\PYGZus{}test(\PYGZam{}obj\PYGZhy{}\PYGZgt{}refcnt))
                        obj\PYGZus{}destroy(obj);
        \PYGZcb{}
\PYGZcb{}

void obj\PYGZus{}timeout(struct obj *obj)
\PYGZob{}
        spin\PYGZus{}lock(\PYGZam{}global\PYGZus{}list\PYGZus{}lock);
        obj\PYGZus{}list\PYGZus{}del(obj);
        spin\PYGZus{}unlock(\PYGZam{}global\PYGZus{}list\PYGZus{}lock);

        if (atomic\PYGZus{}dec\PYGZus{}and\PYGZus{}test(\PYGZam{}obj\PYGZhy{}\PYGZgt{}refcnt))
                obj\PYGZus{}destroy(obj);
\PYGZcb{}
\end{Verbatim}

\begin{notice}{note}{Note:}
This is a simplification of the ARP queue management in the generic
neighbour discover code of the networking.  Olaf Kirch found a bug wrt.
memory barriers in kfree\_skb() that exposed the atomic\_t memory barrier
requirements quite clearly.
\end{notice}

Given the above scheme, it must be the case that the obj-\textgreater{}active
update done by the obj list deletion be visible to other processors
before the atomic counter decrement is performed.

Otherwise, the counter could fall to zero, yet obj-\textgreater{}active would still
be set, thus triggering the assertion in obj\_destroy().  The error
sequence looks like this:

\begin{Verbatim}[commandchars=\\\{\}]
cpu 0                           cpu 1
obj\PYGZus{}poke()                      obj\PYGZus{}timeout()
obj = obj\PYGZus{}list\PYGZus{}peek();
... gains ref to obj, refcnt=2
                                obj\PYGZus{}list\PYGZus{}del(obj);
                                obj\PYGZhy{}\PYGZgt{}active = 0 ...
                                ... visibility delayed ...
                                atomic\PYGZus{}dec\PYGZus{}and\PYGZus{}test()
                                ... refcnt drops to 1 ...
atomic\PYGZus{}dec\PYGZus{}and\PYGZus{}test()
... refcount drops to 0 ...
obj\PYGZus{}destroy()
BUG() triggers since obj\PYGZhy{}\PYGZgt{}active
still seen as one
                                obj\PYGZhy{}\PYGZgt{}active update visibility occurs
\end{Verbatim}

With the memory barrier semantics required of the atomic\_t operations
which return values, the above sequence of memory visibility can never
happen.  Specifically, in the above case the atomic\_dec\_and\_test()
counter decrement would not become globally visible until the
obj-\textgreater{}active update does.

As a historical note, 32-bit Sparc used to only allow usage of
24-bits of its atomic\_t type.  This was because it used 8 bits
as a spinlock for SMP safety.  Sparc32 lacked a ``compare and swap''
type instruction.  However, 32-bit Sparc has since been moved over
to a ``hash table of spinlocks'' scheme, that allows the full 32-bit
counter to be realized.  Essentially, an array of spinlocks are
indexed into based upon the address of the atomic\_t being operated
on, and that lock protects the atomic operation.  Parisc uses the
same scheme.

Another note is that the atomic\_t operations returning values are
extremely slow on an old 386.


\subsection{Atomic Bitmask}
\label{core-api/atomic_ops:atomic-bitmask}
We will now cover the atomic bitmask operations.  You will find that
their SMP and memory barrier semantics are similar in shape and scope
to the atomic\_t ops above.

Native atomic bit operations are defined to operate on objects aligned
to the size of an ``unsigned long'' C data type, and are least of that
size.  The endianness of the bits within each ``unsigned long'' are the
native endianness of the cpu.

\begin{Verbatim}[commandchars=\\\{\}]
void set\PYGZus{}bit(unsigned long nr, volatile unsigned long *addr);
void clear\PYGZus{}bit(unsigned long nr, volatile unsigned long *addr);
void change\PYGZus{}bit(unsigned long nr, volatile unsigned long *addr);
\end{Verbatim}

These routines set, clear, and change, respectively, the bit number
indicated by ``nr'' on the bit mask pointed to by ``ADDR''.

They must execute atomically, yet there are no implicit memory barrier
semantics required of these interfaces.

\begin{Verbatim}[commandchars=\\\{\}]
int test\PYGZus{}and\PYGZus{}set\PYGZus{}bit(unsigned long nr, volatile unsigned long *addr);
int test\PYGZus{}and\PYGZus{}clear\PYGZus{}bit(unsigned long nr, volatile unsigned long *addr);
int test\PYGZus{}and\PYGZus{}change\PYGZus{}bit(unsigned long nr, volatile unsigned long *addr);
\end{Verbatim}

Like the above, except that these routines return a boolean which
indicates whether the changed bit was set \_BEFORE\_ the atomic bit
operation.

WARNING! It is incredibly important that the value be a boolean,
ie. ``0'' or ``1''.  Do not try to be fancy and save a few instructions by
declaring the above to return ``long'' and just returning something like
``old\_val \& mask'' because that will not work.

For one thing, this return value gets truncated to int in many code
paths using these interfaces, so on 64-bit if the bit is set in the
upper 32-bits then testers will never see that.

One great example of where this problem crops up are the thread\_info
flag operations.  Routines such as test\_and\_set\_ti\_thread\_flag() chop
the return value into an int.  There are other places where things
like this occur as well.

These routines, like the atomic\_t counter operations returning values,
must provide explicit memory barrier semantics around their execution.
All memory operations before the atomic bit operation call must be
made visible globally before the atomic bit operation is made visible.
Likewise, the atomic bit operation must be visible globally before any
subsequent memory operation is made visible.  For example:

\begin{Verbatim}[commandchars=\\\{\}]
obj\PYGZhy{}\PYGZgt{}dead = 1;
if (test\PYGZus{}and\PYGZus{}set\PYGZus{}bit(0, \PYGZam{}obj\PYGZhy{}\PYGZgt{}flags))
        /* ... */;
obj\PYGZhy{}\PYGZgt{}killed = 1;
\end{Verbatim}

The implementation of test\_and\_set\_bit() must guarantee that
``obj-\textgreater{}dead = 1;'' is visible to cpus before the atomic memory operation
done by test\_and\_set\_bit() becomes visible.  Likewise, the atomic
memory operation done by test\_and\_set\_bit() must become visible before
``obj-\textgreater{}killed = 1;'' is visible.

Finally there is the basic operation:

\begin{Verbatim}[commandchars=\\\{\}]
int test\PYGZus{}bit(unsigned long nr, \PYGZus{}\PYGZus{}const\PYGZus{}\PYGZus{} volatile unsigned long *addr);
\end{Verbatim}

Which returns a boolean indicating if bit ``nr'' is set in the bitmask
pointed to by ``addr''.

If explicit memory barriers are required around \{set,clear\}\_bit() (which do
not return a value, and thus does not need to provide memory barrier
semantics), two interfaces are provided:

\begin{Verbatim}[commandchars=\\\{\}]
void smp\PYGZus{}mb\PYGZus{}\PYGZus{}before\PYGZus{}atomic(void);
void smp\PYGZus{}mb\PYGZus{}\PYGZus{}after\PYGZus{}atomic(void);
\end{Verbatim}

They are used as follows, and are akin to their atomic\_t operation
brothers:

\begin{Verbatim}[commandchars=\\\{\}]
/* All memory operations before this call will
 * be globally visible before the clear\PYGZus{}bit().
 */
smp\PYGZus{}mb\PYGZus{}\PYGZus{}before\PYGZus{}atomic();
clear\PYGZus{}bit( ... );

/* The clear\PYGZus{}bit() will be visible before all
 * subsequent memory operations.
 */
 smp\PYGZus{}mb\PYGZus{}\PYGZus{}after\PYGZus{}atomic();
\end{Verbatim}

There are two special bitops with lock barrier semantics (acquire/release,
same as spinlocks). These operate in the same way as their non-\_lock/unlock
postfixed variants, except that they are to provide acquire/release semantics,
respectively. This means they can be used for bit\_spin\_trylock and
bit\_spin\_unlock type operations without specifying any more barriers.

\begin{Verbatim}[commandchars=\\\{\}]
int test\PYGZus{}and\PYGZus{}set\PYGZus{}bit\PYGZus{}lock(unsigned long nr, unsigned long *addr);
void clear\PYGZus{}bit\PYGZus{}unlock(unsigned long nr, unsigned long *addr);
void \PYGZus{}\PYGZus{}clear\PYGZus{}bit\PYGZus{}unlock(unsigned long nr, unsigned long *addr);
\end{Verbatim}

The \_\_clear\_bit\_unlock version is non-atomic, however it still implements
unlock barrier semantics. This can be useful if the lock itself is protecting
the other bits in the word.

Finally, there are non-atomic versions of the bitmask operations
provided.  They are used in contexts where some other higher-level SMP
locking scheme is being used to protect the bitmask, and thus less
expensive non-atomic operations may be used in the implementation.
They have names similar to the above bitmask operation interfaces,
except that two underscores are prefixed to the interface name.

\begin{Verbatim}[commandchars=\\\{\}]
void \PYGZus{}\PYGZus{}set\PYGZus{}bit(unsigned long nr, volatile unsigned long *addr);
void \PYGZus{}\PYGZus{}clear\PYGZus{}bit(unsigned long nr, volatile unsigned long *addr);
void \PYGZus{}\PYGZus{}change\PYGZus{}bit(unsigned long nr, volatile unsigned long *addr);
int \PYGZus{}\PYGZus{}test\PYGZus{}and\PYGZus{}set\PYGZus{}bit(unsigned long nr, volatile unsigned long *addr);
int \PYGZus{}\PYGZus{}test\PYGZus{}and\PYGZus{}clear\PYGZus{}bit(unsigned long nr, volatile unsigned long *addr);
int \PYGZus{}\PYGZus{}test\PYGZus{}and\PYGZus{}change\PYGZus{}bit(unsigned long nr, volatile unsigned long *addr);
\end{Verbatim}

These non-atomic variants also do not require any special memory
barrier semantics.

The routines xchg() and cmpxchg() must provide the same exact
memory-barrier semantics as the atomic and bit operations returning
values.

\begin{notice}{note}{Note:}
If someone wants to use xchg(), cmpxchg() and their variants,
linux/atomic.h should be included rather than asm/cmpxchg.h, unless the
code is in arch/* and can take care of itself.
\end{notice}

Spinlocks and rwlocks have memory barrier expectations as well.
The rule to follow is simple:
\begin{enumerate}
\item {} 
When acquiring a lock, the implementation must make it globally
visible before any subsequent memory operation.

\item {} 
When releasing a lock, the implementation must make it such that
all previous memory operations are globally visible before the
lock release.

\end{enumerate}

Which finally brings us to \_atomic\_dec\_and\_lock().  There is an
architecture-neutral version implemented in lib/dec\_and\_lock.c,
but most platforms will wish to optimize this in assembler.

\begin{Verbatim}[commandchars=\\\{\}]
int \PYGZus{}atomic\PYGZus{}dec\PYGZus{}and\PYGZus{}lock(atomic\PYGZus{}t *atomic, spinlock\PYGZus{}t *lock);
\end{Verbatim}

Atomically decrement the given counter, and if will drop to zero
atomically acquire the given spinlock and perform the decrement
of the counter to zero.  If it does not drop to zero, do nothing
with the spinlock.

It is actually pretty simple to get the memory barrier correct.
Simply satisfy the spinlock grab requirements, which is make
sure the spinlock operation is globally visible before any
subsequent memory operation.

We can demonstrate this operation more clearly if we define
an abstract atomic operation:

\begin{Verbatim}[commandchars=\\\{\}]
long cas(long *mem, long old, long new);
\end{Verbatim}

``cas'' stands for ``compare and swap''.  It atomically:
\begin{enumerate}
\item {} 
Compares ``old'' with the value currently at ``mem''.

\item {} 
If they are equal, ``new'' is written to ``mem''.

\item {} 
Regardless, the current value at ``mem'' is returned.

\end{enumerate}

As an example usage, here is what an atomic counter update
might look like:

\begin{Verbatim}[commandchars=\\\{\}]
void example\PYGZus{}atomic\PYGZus{}inc(long *counter)
\PYGZob{}
        long old, new, ret;

        while (1) \PYGZob{}
                old = *counter;
                new = old + 1;

                ret = cas(counter, old, new);
                if (ret == old)
                        break;
        \PYGZcb{}
\PYGZcb{}
\end{Verbatim}

Let's use cas() in order to build a pseudo-C atomic\_dec\_and\_lock():

\begin{Verbatim}[commandchars=\\\{\}]
int \PYGZus{}atomic\PYGZus{}dec\PYGZus{}and\PYGZus{}lock(atomic\PYGZus{}t *atomic, spinlock\PYGZus{}t *lock)
\PYGZob{}
        long old, new, ret;
        int went\PYGZus{}to\PYGZus{}zero;

        went\PYGZus{}to\PYGZus{}zero = 0;
        while (1) \PYGZob{}
                old = atomic\PYGZus{}read(atomic);
                new = old \PYGZhy{} 1;
                if (new == 0) \PYGZob{}
                        went\PYGZus{}to\PYGZus{}zero = 1;
                        spin\PYGZus{}lock(lock);
                \PYGZcb{}
                ret = cas(atomic, old, new);
                if (ret == old)
                        break;
                if (went\PYGZus{}to\PYGZus{}zero) \PYGZob{}
                        spin\PYGZus{}unlock(lock);
                        went\PYGZus{}to\PYGZus{}zero = 0;
                \PYGZcb{}
        \PYGZcb{}

        return went\PYGZus{}to\PYGZus{}zero;
\PYGZcb{}
\end{Verbatim}

Now, as far as memory barriers go, as long as spin\_lock()
strictly orders all subsequent memory operations (including
the cas()) with respect to itself, things will be fine.

Said another way, \_atomic\_dec\_and\_lock() must guarantee that
a counter dropping to zero is never made visible before the
spinlock being acquired.

\begin{notice}{note}{Note:}
Note that this also means that for the case where the counter is not
dropping to zero, there are no memory ordering requirements.
\end{notice}


\section{refcount\_t API compared to atomic\_t}
\label{core-api/refcount-vs-atomic:refcount-t-api-compared-to-atomic-t}\label{core-api/refcount-vs-atomic::doc}\setbox0\vbox{
\begin{minipage}{0.95\linewidth}
\begin{itemize}
\item {} 
\phantomsection\label{core-api/refcount-vs-atomic:id1}{\hyperref[core\string-api/refcount\string-vs\string-atomic:introduction]{\emph{Introduction}}}

\item {} 
\phantomsection\label{core-api/refcount-vs-atomic:id2}{\hyperref[core\string-api/refcount\string-vs\string-atomic:relevant\string-types\string-of\string-memory\string-ordering]{\emph{Relevant types of memory ordering}}}

\item {} 
\phantomsection\label{core-api/refcount-vs-atomic:id3}{\hyperref[core\string-api/refcount\string-vs\string-atomic:comparison\string-of\string-functions]{\emph{Comparison of functions}}}
\begin{itemize}
\item {} 
\phantomsection\label{core-api/refcount-vs-atomic:id4}{\hyperref[core\string-api/refcount\string-vs\string-atomic:case\string-1\string-non\string-read\string-modify\string-write\string-rmw\string-ops]{\emph{case 1) - non-``Read/Modify/Write'' (RMW) ops}}}

\item {} 
\phantomsection\label{core-api/refcount-vs-atomic:id5}{\hyperref[core\string-api/refcount\string-vs\string-atomic:case\string-2\string-increment\string-based\string-ops\string-that\string-return\string-no\string-value]{\emph{case 2) - increment-based ops that return no value}}}

\item {} 
\phantomsection\label{core-api/refcount-vs-atomic:id6}{\hyperref[core\string-api/refcount\string-vs\string-atomic:case\string-3\string-decrement\string-based\string-rmw\string-ops\string-that\string-return\string-no\string-value]{\emph{case 3) - decrement-based RMW ops that return no value}}}

\item {} 
\phantomsection\label{core-api/refcount-vs-atomic:id7}{\hyperref[core\string-api/refcount\string-vs\string-atomic:case\string-4\string-increment\string-based\string-rmw\string-ops\string-that\string-return\string-a\string-value]{\emph{case 4) - increment-based RMW ops that return a value}}}

\item {} 
\phantomsection\label{core-api/refcount-vs-atomic:id8}{\hyperref[core\string-api/refcount\string-vs\string-atomic:case\string-5\string-decrement\string-based\string-rmw\string-ops\string-that\string-return\string-a\string-value]{\emph{case 5) - decrement-based RMW ops that return a value}}}

\item {} 
\phantomsection\label{core-api/refcount-vs-atomic:id9}{\hyperref[core\string-api/refcount\string-vs\string-atomic:case\string-6\string-lock\string-based\string-rmw]{\emph{case 6) - lock-based RMW}}}

\end{itemize}

\end{itemize}
\end{minipage}}
\begin{center}\setlength{\fboxsep}{5pt}\shadowbox{\box0}\end{center}


\subsection{Introduction}
\label{core-api/refcount-vs-atomic:introduction}
The goal of refcount\_t API is to provide a minimal API for implementing
an object's reference counters. While a generic architecture-independent
implementation from lib/refcount.c uses atomic operations underneath,
there are a number of differences between some of the \code{refcount\_*()} and
\code{atomic\_*()} functions with regards to the memory ordering guarantees.
This document outlines the differences and provides respective examples
in order to help maintainers validate their code against the change in
these memory ordering guarantees.

The terms used through this document try to follow the formal LKMM defined in
github.com/aparri/memory-model/blob/master/Documentation/explanation.txt

memory-barriers.txt and atomic\_t.txt provide more background to the
memory ordering in general and for atomic operations specifically.


\subsection{Relevant types of memory ordering}
\label{core-api/refcount-vs-atomic:relevant-types-of-memory-ordering}
\begin{notice}{note}{Note:}
The following section only covers some of the memory
ordering types that are relevant for the atomics and reference
counters and used through this document. For a much broader picture
please consult memory-barriers.txt document.
\end{notice}

In the absence of any memory ordering guarantees (i.e. fully unordered)
atomics \& refcounters only provide atomicity and
program order (po) relation (on the same CPU). It guarantees that
each \code{atomic\_*()} and \code{refcount\_*()} operation is atomic and instructions
are executed in program order on a single CPU.
This is implemented using \code{READ\_ONCE()}/\code{WRITE\_ONCE()} and
compare-and-swap primitives.

A strong (full) memory ordering guarantees that all prior loads and
stores (all po-earlier instructions) on the same CPU are completed
before any po-later instruction is executed on the same CPU.
It also guarantees that all po-earlier stores on the same CPU
and all propagated stores from other CPUs must propagate to all
other CPUs before any po-later instruction is executed on the original
CPU (A-cumulative property). This is implemented using \code{smp\_mb()}.

A RELEASE memory ordering guarantees that all prior loads and
stores (all po-earlier instructions) on the same CPU are completed
before the operation. It also guarantees that all po-earlier
stores on the same CPU and all propagated stores from other CPUs
must propagate to all other CPUs before the release operation
(A-cumulative property). This is implemented using
\code{smp\_store\_release()}.

A control dependency (on success) for refcounters guarantees that
if a reference for an object was successfully obtained (reference
counter increment or addition happened, function returned true),
then further stores are ordered against this operation.
Control dependency on stores are not implemented using any explicit
barriers, but rely on CPU not to speculate on stores. This is only
a single CPU relation and provides no guarantees for other CPUs.


\subsection{Comparison of functions}
\label{core-api/refcount-vs-atomic:comparison-of-functions}

\subsubsection{case 1) - non-``Read/Modify/Write'' (RMW) ops}
\label{core-api/refcount-vs-atomic:case-1-non-read-modify-write-rmw-ops}
Function changes:
\begin{itemize}
\item {} 
\code{atomic\_set()} --\textgreater{} \code{refcount\_set()}

\item {} 
\code{atomic\_read()} --\textgreater{} \code{refcount\_read()}

\end{itemize}

Memory ordering guarantee changes:
\begin{itemize}
\item {} 
none (both fully unordered)

\end{itemize}


\subsubsection{case 2) - increment-based ops that return no value}
\label{core-api/refcount-vs-atomic:case-2-increment-based-ops-that-return-no-value}
Function changes:
\begin{itemize}
\item {} 
\code{atomic\_inc()} --\textgreater{} \code{refcount\_inc()}

\item {} 
\code{atomic\_add()} --\textgreater{} \code{refcount\_add()}

\end{itemize}

Memory ordering guarantee changes:
\begin{itemize}
\item {} 
none (both fully unordered)

\end{itemize}


\subsubsection{case 3) - decrement-based RMW ops that return no value}
\label{core-api/refcount-vs-atomic:case-3-decrement-based-rmw-ops-that-return-no-value}
Function changes:
\begin{itemize}
\item {} 
\code{atomic\_dec()} --\textgreater{} \code{refcount\_dec()}

\end{itemize}

Memory ordering guarantee changes:
\begin{itemize}
\item {} 
fully unordered --\textgreater{} RELEASE ordering

\end{itemize}


\subsubsection{case 4) - increment-based RMW ops that return a value}
\label{core-api/refcount-vs-atomic:case-4-increment-based-rmw-ops-that-return-a-value}
Function changes:
\begin{itemize}
\item {} 
\code{atomic\_inc\_not\_zero()} --\textgreater{} \code{refcount\_inc\_not\_zero()}

\item {} 
no atomic counterpart --\textgreater{} \code{refcount\_add\_not\_zero()}

\end{itemize}

Memory ordering guarantees changes:
\begin{itemize}
\item {} 
fully ordered --\textgreater{} control dependency on success for stores

\end{itemize}

\begin{notice}{note}{Note:}
We really assume here that necessary ordering is provided as a
result of obtaining pointer to the object!
\end{notice}


\subsubsection{case 5) - decrement-based RMW ops that return a value}
\label{core-api/refcount-vs-atomic:case-5-decrement-based-rmw-ops-that-return-a-value}
Function changes:
\begin{itemize}
\item {} 
\code{atomic\_dec\_and\_test()} --\textgreater{} \code{refcount\_dec\_and\_test()}

\item {} 
\code{atomic\_sub\_and\_test()} --\textgreater{} \code{refcount\_sub\_and\_test()}

\item {} 
no atomic counterpart --\textgreater{} \code{refcount\_dec\_if\_one()}

\item {} 
\code{atomic\_add\_unless(\&var, -1, 1)} --\textgreater{} \code{refcount\_dec\_not\_one(\&var)}

\end{itemize}

Memory ordering guarantees changes:
\begin{itemize}
\item {} 
fully ordered --\textgreater{} RELEASE ordering + control dependency

\end{itemize}

\begin{notice}{note}{Note:}
\code{atomic\_add\_unless()} only provides full order on success.
\end{notice}


\subsubsection{case 6) - lock-based RMW}
\label{core-api/refcount-vs-atomic:case-6-lock-based-rmw}
Function changes:
\begin{itemize}
\item {} 
\code{atomic\_dec\_and\_lock()} --\textgreater{} \code{refcount\_dec\_and\_lock()}

\item {} 
\code{atomic\_dec\_and\_mutex\_lock()} --\textgreater{} \code{refcount\_dec\_and\_mutex\_lock()}

\end{itemize}

Memory ordering guarantees changes:
\begin{itemize}
\item {} 
fully ordered --\textgreater{} RELEASE ordering + control dependency + hold
\code{spin\_lock()} on success

\end{itemize}


\section{CPU hotplug in the Kernel}
\label{core-api/cpu_hotplug::doc}\label{core-api/cpu_hotplug:cpu-hotplug-in-the-kernel}\begin{quote}\begin{description}
\item[{Date}] \leavevmode
December, 2016

\item[{Author}] \leavevmode
Sebastian Andrzej Siewior \textless{}\href{mailto:bigeasy@linutronix.de}{bigeasy@linutronix.de}\textgreater{},
Rusty Russell \textless{}\href{mailto:rusty@rustcorp.com.au}{rusty@rustcorp.com.au}\textgreater{},
Srivatsa Vaddagiri \textless{}\href{mailto:vatsa@in.ibm.com}{vatsa@in.ibm.com}\textgreater{},
Ashok Raj \textless{}\href{mailto:ashok.raj@intel.com}{ashok.raj@intel.com}\textgreater{},
Joel Schopp \textless{}\href{mailto:jschopp@austin.ibm.com}{jschopp@austin.ibm.com}\textgreater{}

\end{description}\end{quote}


\subsection{Introduction}
\label{core-api/cpu_hotplug:introduction}
Modern advances in system architectures have introduced advanced error
reporting and correction capabilities in processors. There are couple OEMS that
support NUMA hardware which are hot pluggable as well, where physical node
insertion and removal require support for CPU hotplug.

Such advances require CPUs available to a kernel to be removed either for
provisioning reasons, or for RAS purposes to keep an offending CPU off
system execution path. Hence the need for CPU hotplug support in the
Linux kernel.

A more novel use of CPU-hotplug support is its use today in suspend resume
support for SMP. Dual-core and HT support makes even a laptop run SMP kernels
which didn't support these methods.


\subsection{Command Line Switches}
\label{core-api/cpu_hotplug:command-line-switches}\begin{description}
\item[{\code{maxcpus=n}}] \leavevmode
Restrict boot time CPUs to \emph{n}. Say if you have fourV CPUs, using
\code{maxcpus=2} will only boot two. You can choose to bring the
other CPUs later online.

\item[{\code{nr\_cpus=n}}] \leavevmode
Restrict the total amount CPUs the kernel will support. If the number
supplied here is lower than the number of physically available CPUs than
those CPUs can not be brought online later.

\item[{\code{additional\_cpus=n}}] \leavevmode
Use this to limit hotpluggable CPUs. This option sets
\code{cpu\_possible\_mask = cpu\_present\_mask + additional\_cpus}

This option is limited to the IA64 architecture.

\item[{\code{possible\_cpus=n}}] \leavevmode
This option sets \code{possible\_cpus} bits in \code{cpu\_possible\_mask}.

This option is limited to the X86 and S390 architecture.

\item[{\code{cede\_offline=\{"off","on"\}}}] \leavevmode
Use this option to disable/enable putting offlined processors to an extended
\code{H\_CEDE} state on supported pseries platforms. If nothing is specified,
\code{cede\_offline} is set to ``on''.

This option is limited to the PowerPC architecture.

\item[{\code{cpu0\_hotplug}}] \leavevmode
Allow to shutdown CPU0.

This option is limited to the X86 architecture.

\end{description}


\subsection{CPU maps}
\label{core-api/cpu_hotplug:cpu-maps}\begin{description}
\item[{\code{cpu\_possible\_mask}}] \leavevmode
Bitmap of possible CPUs that can ever be available in the
system. This is used to allocate some boot time memory for per\_cpu variables
that aren't designed to grow/shrink as CPUs are made available or removed.
Once set during boot time discovery phase, the map is static, i.e no bits
are added or removed anytime. Trimming it accurately for your system needs
upfront can save some boot time memory.

\item[{\code{cpu\_online\_mask}}] \leavevmode
Bitmap of all CPUs currently online. Its set in \code{\_\_cpu\_up()}
after a CPU is available for kernel scheduling and ready to receive
interrupts from devices. Its cleared when a CPU is brought down using
\code{\_\_cpu\_disable()}, before which all OS services including interrupts are
migrated to another target CPU.

\item[{\code{cpu\_present\_mask}}] \leavevmode
Bitmap of CPUs currently present in the system. Not all
of them may be online. When physical hotplug is processed by the relevant
subsystem (e.g ACPI) can change and new bit either be added or removed
from the map depending on the event is hot-add/hot-remove. There are currently
no locking rules as of now. Typical usage is to init topology during boot,
at which time hotplug is disabled.

\end{description}

You really don't need to manipulate any of the system CPU maps. They should
be read-only for most use. When setting up per-cpu resources almost always use
\code{cpu\_possible\_mask} or \code{for\_each\_possible\_cpu()} to iterate. To macro
\code{for\_each\_cpu()} can be used to iterate over a custom CPU mask.

Never use anything other than \code{cpumask\_t} to represent bitmap of CPUs.


\subsection{Using CPU hotplug}
\label{core-api/cpu_hotplug:using-cpu-hotplug}
The kernel option \emph{CONFIG\_HOTPLUG\_CPU} needs to be enabled. It is currently
available on multiple architectures including ARM, MIPS, PowerPC and X86. The
configuration is done via the sysfs interface:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} ls \PYGZhy{}lh /sys/devices/system/cpu
total 0
drwxr\PYGZhy{}xr\PYGZhy{}x  9 root root    0 Dec 21 16:33 cpu0
drwxr\PYGZhy{}xr\PYGZhy{}x  9 root root    0 Dec 21 16:33 cpu1
drwxr\PYGZhy{}xr\PYGZhy{}x  9 root root    0 Dec 21 16:33 cpu2
drwxr\PYGZhy{}xr\PYGZhy{}x  9 root root    0 Dec 21 16:33 cpu3
drwxr\PYGZhy{}xr\PYGZhy{}x  9 root root    0 Dec 21 16:33 cpu4
drwxr\PYGZhy{}xr\PYGZhy{}x  9 root root    0 Dec 21 16:33 cpu5
drwxr\PYGZhy{}xr\PYGZhy{}x  9 root root    0 Dec 21 16:33 cpu6
drwxr\PYGZhy{}xr\PYGZhy{}x  9 root root    0 Dec 21 16:33 cpu7
drwxr\PYGZhy{}xr\PYGZhy{}x  2 root root    0 Dec 21 16:33 hotplug
\PYGZhy{}r\PYGZhy{}\PYGZhy{}r\PYGZhy{}\PYGZhy{}r\PYGZhy{}\PYGZhy{}  1 root root 4.0K Dec 21 16:33 offline
\PYGZhy{}r\PYGZhy{}\PYGZhy{}r\PYGZhy{}\PYGZhy{}r\PYGZhy{}\PYGZhy{}  1 root root 4.0K Dec 21 16:33 online
\PYGZhy{}r\PYGZhy{}\PYGZhy{}r\PYGZhy{}\PYGZhy{}r\PYGZhy{}\PYGZhy{}  1 root root 4.0K Dec 21 16:33 possible
\PYGZhy{}r\PYGZhy{}\PYGZhy{}r\PYGZhy{}\PYGZhy{}r\PYGZhy{}\PYGZhy{}  1 root root 4.0K Dec 21 16:33 present
\end{Verbatim}

The files \emph{offline}, \emph{online}, \emph{possible}, \emph{present} represent the CPU masks.
Each CPU folder contains an \emph{online} file which controls the logical on (1) and
off (0) state. To logically shutdown CPU4:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} echo 0 \PYGZgt{} /sys/devices/system/cpu/cpu4/online
 smpboot: CPU 4 is now offline
\end{Verbatim}

Once the CPU is shutdown, it will be removed from \emph{/proc/interrupts},
\emph{/proc/cpuinfo} and should also not be shown visible by the \emph{top} command. To
bring CPU4 back online:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} echo 1 \PYGZgt{} /sys/devices/system/cpu/cpu4/online
smpboot: Booting Node 0 Processor 4 APIC 0x1
\end{Verbatim}

The CPU is usable again. This should work on all CPUs. CPU0 is often special
and excluded from CPU hotplug. On X86 the kernel option
\emph{CONFIG\_BOOTPARAM\_HOTPLUG\_CPU0} has to be enabled in order to be able to
shutdown CPU0. Alternatively the kernel command option \emph{cpu0\_hotplug} can be
used. Some known dependencies of CPU0:
\begin{itemize}
\item {} 
Resume from hibernate/suspend. Hibernate/suspend will fail if CPU0 is offline.

\item {} 
PIC interrupts. CPU0 can't be removed if a PIC interrupt is detected.

\end{itemize}

Please let Fenghua Yu \textless{}\href{mailto:fenghua.yu@intel.com}{fenghua.yu@intel.com}\textgreater{} know if you find any dependencies
on CPU0.


\subsection{The CPU hotplug coordination}
\label{core-api/cpu_hotplug:the-cpu-hotplug-coordination}

\subsubsection{The offline case}
\label{core-api/cpu_hotplug:the-offline-case}
Once a CPU has been logically shutdown the teardown callbacks of registered
hotplug states will be invoked, starting with \code{CPUHP\_ONLINE} and terminating
at state \code{CPUHP\_OFFLINE}. This includes:
\begin{itemize}
\item {} 
If tasks are frozen due to a suspend operation then \emph{cpuhp\_tasks\_frozen}
will be set to true.

\item {} 
All processes are migrated away from this outgoing CPU to new CPUs.
The new CPU is chosen from each process' current cpuset, which may be
a subset of all online CPUs.

\item {} 
All interrupts targeted to this CPU are migrated to a new CPU

\item {} 
timers are also migrated to a new CPU

\item {} 
Once all services are migrated, kernel calls an arch specific routine
\code{\_\_cpu\_disable()} to perform arch specific cleanup.

\end{itemize}


\subsubsection{Using the hotplug API}
\label{core-api/cpu_hotplug:using-the-hotplug-api}
It is possible to receive notifications once a CPU is offline or onlined. This
might be important to certain drivers which need to perform some kind of setup
or clean up functions based on the number of available CPUs:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZsh{}include \PYGZlt{}linux/cpuhotplug.h\PYGZgt{}

ret = cpuhp\PYGZus{}setup\PYGZus{}state(CPUHP\PYGZus{}AP\PYGZus{}ONLINE\PYGZus{}DYN, \PYGZdq{}X/Y:online\PYGZdq{},
                        Y\PYGZus{}online, Y\PYGZus{}prepare\PYGZus{}down);
\end{Verbatim}

\emph{X} is the subsystem and \emph{Y} the particular driver. The \emph{Y\_online} callback
will be invoked during registration on all online CPUs. If an error
occurs during the online callback the \emph{Y\_prepare\_down} callback will be
invoked on all CPUs on which the online callback was previously invoked.
After registration completed, the \emph{Y\_online} callback will be invoked
once a CPU is brought online and \emph{Y\_prepare\_down} will be invoked when a
CPU is shutdown. All resources which were previously allocated in
\emph{Y\_online} should be released in \emph{Y\_prepare\_down}.
The return value \emph{ret} is negative if an error occurred during the
registration process. Otherwise a positive value is returned which
contains the allocated hotplug for dynamically allocated states
(\emph{CPUHP\_AP\_ONLINE\_DYN}). It will return zero for predefined states.

The callback can be remove by invoking \code{cpuhp\_remove\_state()}. In case of a
dynamically allocated state (\emph{CPUHP\_AP\_ONLINE\_DYN}) use the returned state.
During the removal of a hotplug state the teardown callback will be invoked.


\paragraph{Multiple instances}
\label{core-api/cpu_hotplug:multiple-instances}
If a driver has multiple instances and each instance needs to perform the
callback independently then it is likely that a `'multi-state'' should be used.
First a multi-state state needs to be registered:

\begin{Verbatim}[commandchars=\\\{\}]
ret = cpuhp\PYGZus{}setup\PYGZus{}state\PYGZus{}multi(CPUHP\PYGZus{}AP\PYGZus{}ONLINE\PYGZus{}DYN, \PYGZdq{}X/Y:online,
                              Y\PYGZus{}online, Y\PYGZus{}prepare\PYGZus{}down);
Y\PYGZus{}hp\PYGZus{}online = ret;
\end{Verbatim}

The \code{cpuhp\_setup\_state\_multi()} behaves similar to \code{cpuhp\_setup\_state()}
except it prepares the callbacks for a multi state and does not invoke
the callbacks. This is a one time setup.
Once a new instance is allocated, you need to register this new instance:

\begin{Verbatim}[commandchars=\\\{\}]
ret = cpuhp\PYGZus{}state\PYGZus{}add\PYGZus{}instance(Y\PYGZus{}hp\PYGZus{}online, \PYGZam{}d\PYGZhy{}\PYGZgt{}node);
\end{Verbatim}

This function will add this instance to your previously allocated
\emph{Y\_hp\_online} state and invoke the previously registered callback
(\emph{Y\_online}) on all online CPUs. The \emph{node} element is a \code{struct
hlist\_node} member of your per-instance data structure.
\begin{description}
\item[{On removal of the instance: ::}] \leavevmode
cpuhp\_state\_remove\_instance(Y\_hp\_online, \&d-\textgreater{}node)

\end{description}

should be invoked which will invoke the teardown callback on all online
CPUs.


\paragraph{Manual setup}
\label{core-api/cpu_hotplug:manual-setup}
Usually it is handy to invoke setup and teardown callbacks on registration or
removal of a state because usually the operation needs to performed once a CPU
goes online (offline) and during initial setup (shutdown) of the driver. However
each registration and removal function is also available with a \code{\_nocalls}
suffix which does not invoke the provided callbacks if the invocation of the
callbacks is not desired. During the manual setup (or teardown) the functions
\code{get\_online\_cpus()} and \code{put\_online\_cpus()} should be used to inhibit CPU
hotplug operations.


\subsubsection{The ordering of the events}
\label{core-api/cpu_hotplug:the-ordering-of-the-events}
The hotplug states are defined in \code{include/linux/cpuhotplug.h}:
\begin{itemize}
\item {} 
The states \emph{CPUHP\_OFFLINE} … \emph{CPUHP\_AP\_OFFLINE} are invoked before the
CPU is up.

\item {} 
The states \emph{CPUHP\_AP\_OFFLINE} … \emph{CPUHP\_AP\_ONLINE} are invoked
just the after the CPU has been brought up. The interrupts are off and
the scheduler is not yet active on this CPU. Starting with \emph{CPUHP\_AP\_OFFLINE}
the callbacks are invoked on the target CPU.

\item {} 
The states between \emph{CPUHP\_AP\_ONLINE\_DYN} and \emph{CPUHP\_AP\_ONLINE\_DYN\_END} are
reserved for the dynamic allocation.

\item {} 
The states are invoked in the reverse order on CPU shutdown starting with
\emph{CPUHP\_ONLINE} and stopping at \emph{CPUHP\_OFFLINE}. Here the callbacks are
invoked on the CPU that will be shutdown until \emph{CPUHP\_AP\_OFFLINE}.

\end{itemize}

A dynamically allocated state via \emph{CPUHP\_AP\_ONLINE\_DYN} is often enough.
However if an earlier invocation during the bring up or shutdown is required
then an explicit state should be acquired. An explicit state might also be
required if the hotplug event requires specific ordering in respect to
another hotplug event.


\subsection{Testing of hotplug states}
\label{core-api/cpu_hotplug:testing-of-hotplug-states}
One way to verify whether a custom state is working as expected or not is to
shutdown a CPU and then put it online again. It is also possible to put the CPU
to certain state (for instance \emph{CPUHP\_AP\_ONLINE}) and then go back to
\emph{CPUHP\_ONLINE}. This would simulate an error one state after \emph{CPUHP\_AP\_ONLINE}
which would lead to rollback to the online state.

All registered states are enumerated in \code{/sys/devices/system/cpu/hotplug/states}:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} tail /sys/devices/system/cpu/hotplug/states
138: mm/vmscan:online
139: mm/vmstat:online
140: lib/percpu\PYGZus{}cnt:online
141: acpi/cpu\PYGZhy{}drv:online
142: base/cacheinfo:online
143: virtio/net:online
144: x86/mce:online
145: printk:online
168: sched:active
169: online
\end{Verbatim}

To rollback CPU4 to \code{lib/percpu\_cnt:online} and back online just issue:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} cat /sys/devices/system/cpu/cpu4/hotplug/state
169
\PYGZdl{} echo 140 \PYGZgt{} /sys/devices/system/cpu/cpu4/hotplug/target
\PYGZdl{} cat /sys/devices/system/cpu/cpu4/hotplug/state
140
\end{Verbatim}

It is important to note that the teardown callbac of state 140 have been
invoked. And now get back online:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} echo 169 \PYGZgt{} /sys/devices/system/cpu/cpu4/hotplug/target
\PYGZdl{} cat /sys/devices/system/cpu/cpu4/hotplug/state
169
\end{Verbatim}

With trace events enabled, the individual steps are visible, too:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZsh{}  TASK\PYGZhy{}PID   CPU\PYGZsh{}    TIMESTAMP  FUNCTION
\PYGZsh{}     \textbar{} \textbar{}       \textbar{}        \textbar{}         \textbar{}
    bash\PYGZhy{}394  [001]  22.976: cpuhp\PYGZus{}enter: cpu: 0004 target: 140 step: 169 (cpuhp\PYGZus{}kick\PYGZus{}ap\PYGZus{}work)
 cpuhp/4\PYGZhy{}31   [004]  22.977: cpuhp\PYGZus{}enter: cpu: 0004 target: 140 step: 168 (sched\PYGZus{}cpu\PYGZus{}deactivate)
 cpuhp/4\PYGZhy{}31   [004]  22.990: cpuhp\PYGZus{}exit:  cpu: 0004  state: 168 step: 168 ret: 0
 cpuhp/4\PYGZhy{}31   [004]  22.991: cpuhp\PYGZus{}enter: cpu: 0004 target: 140 step: 144 (mce\PYGZus{}cpu\PYGZus{}pre\PYGZus{}down)
 cpuhp/4\PYGZhy{}31   [004]  22.992: cpuhp\PYGZus{}exit:  cpu: 0004  state: 144 step: 144 ret: 0
 cpuhp/4\PYGZhy{}31   [004]  22.993: cpuhp\PYGZus{}multi\PYGZus{}enter: cpu: 0004 target: 140 step: 143 (virtnet\PYGZus{}cpu\PYGZus{}down\PYGZus{}prep)
 cpuhp/4\PYGZhy{}31   [004]  22.994: cpuhp\PYGZus{}exit:  cpu: 0004  state: 143 step: 143 ret: 0
 cpuhp/4\PYGZhy{}31   [004]  22.995: cpuhp\PYGZus{}enter: cpu: 0004 target: 140 step: 142 (cacheinfo\PYGZus{}cpu\PYGZus{}pre\PYGZus{}down)
 cpuhp/4\PYGZhy{}31   [004]  22.996: cpuhp\PYGZus{}exit:  cpu: 0004  state: 142 step: 142 ret: 0
    bash\PYGZhy{}394  [001]  22.997: cpuhp\PYGZus{}exit:  cpu: 0004  state: 140 step: 169 ret: 0
    bash\PYGZhy{}394  [005]  95.540: cpuhp\PYGZus{}enter: cpu: 0004 target: 169 step: 140 (cpuhp\PYGZus{}kick\PYGZus{}ap\PYGZus{}work)
 cpuhp/4\PYGZhy{}31   [004]  95.541: cpuhp\PYGZus{}enter: cpu: 0004 target: 169 step: 141 (acpi\PYGZus{}soft\PYGZus{}cpu\PYGZus{}online)
 cpuhp/4\PYGZhy{}31   [004]  95.542: cpuhp\PYGZus{}exit:  cpu: 0004  state: 141 step: 141 ret: 0
 cpuhp/4\PYGZhy{}31   [004]  95.543: cpuhp\PYGZus{}enter: cpu: 0004 target: 169 step: 142 (cacheinfo\PYGZus{}cpu\PYGZus{}online)
 cpuhp/4\PYGZhy{}31   [004]  95.544: cpuhp\PYGZus{}exit:  cpu: 0004  state: 142 step: 142 ret: 0
 cpuhp/4\PYGZhy{}31   [004]  95.545: cpuhp\PYGZus{}multi\PYGZus{}enter: cpu: 0004 target: 169 step: 143 (virtnet\PYGZus{}cpu\PYGZus{}online)
 cpuhp/4\PYGZhy{}31   [004]  95.546: cpuhp\PYGZus{}exit:  cpu: 0004  state: 143 step: 143 ret: 0
 cpuhp/4\PYGZhy{}31   [004]  95.547: cpuhp\PYGZus{}enter: cpu: 0004 target: 169 step: 144 (mce\PYGZus{}cpu\PYGZus{}online)
 cpuhp/4\PYGZhy{}31   [004]  95.548: cpuhp\PYGZus{}exit:  cpu: 0004  state: 144 step: 144 ret: 0
 cpuhp/4\PYGZhy{}31   [004]  95.549: cpuhp\PYGZus{}enter: cpu: 0004 target: 169 step: 145 (console\PYGZus{}cpu\PYGZus{}notify)
 cpuhp/4\PYGZhy{}31   [004]  95.550: cpuhp\PYGZus{}exit:  cpu: 0004  state: 145 step: 145 ret: 0
 cpuhp/4\PYGZhy{}31   [004]  95.551: cpuhp\PYGZus{}enter: cpu: 0004 target: 169 step: 168 (sched\PYGZus{}cpu\PYGZus{}activate)
 cpuhp/4\PYGZhy{}31   [004]  95.552: cpuhp\PYGZus{}exit:  cpu: 0004  state: 168 step: 168 ret: 0
    bash\PYGZhy{}394  [005]  95.553: cpuhp\PYGZus{}exit:  cpu: 0004  state: 169 step: 140 ret: 0
\end{Verbatim}

As it an be seen, CPU4 went down until timestamp 22.996 and then back up until
95.552. All invoked callbacks including their return codes are visible in the
trace.


\subsection{Architecture's requirements}
\label{core-api/cpu_hotplug:architecture-s-requirements}
The following functions and configurations are required:
\begin{description}
\item[{\code{CONFIG\_HOTPLUG\_CPU}}] \leavevmode
This entry needs to be enabled in Kconfig

\item[{\code{\_\_cpu\_up()}}] \leavevmode
Arch interface to bring up a CPU

\item[{\code{\_\_cpu\_disable()}}] \leavevmode
Arch interface to shutdown a CPU, no more interrupts can be handled by the
kernel after the routine returns. This includes the shutdown of the timer.

\item[{\code{\_\_cpu\_die()}}] \leavevmode
This actually supposed to ensure death of the CPU. Actually look at some
example code in other arch that implement CPU hotplug. The processor is taken
down from the \code{idle()} loop for that specific architecture. \code{\_\_cpu\_die()}
typically waits for some per\_cpu state to be set, to ensure the processor dead
routine is called to be sure positively.

\end{description}


\subsection{User Space Notification}
\label{core-api/cpu_hotplug:user-space-notification}
After CPU successfully onlined or offline udev events are sent. A udev rule like:

\begin{Verbatim}[commandchars=\\\{\}]
SUBSYSTEM==\PYGZdq{}cpu\PYGZdq{}, DRIVERS==\PYGZdq{}processor\PYGZdq{}, DEVPATH==\PYGZdq{}/devices/system/cpu/*\PYGZdq{}, RUN+=\PYGZdq{}the\PYGZus{}hotplug\PYGZus{}receiver.sh\PYGZdq{}
\end{Verbatim}

will receive all events. A script like:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZsh{}!/bin/sh

if [ \PYGZdq{}\PYGZdl{}\PYGZob{}ACTION\PYGZcb{}\PYGZdq{} = \PYGZdq{}offline\PYGZdq{} ]
then
    echo \PYGZdq{}CPU \PYGZdl{}\PYGZob{}DEVPATH\PYGZsh{}\PYGZsh{}*/\PYGZcb{} offline\PYGZdq{}

elif [ \PYGZdq{}\PYGZdl{}\PYGZob{}ACTION\PYGZcb{}\PYGZdq{} = \PYGZdq{}online\PYGZdq{} ]
then
    echo \PYGZdq{}CPU \PYGZdl{}\PYGZob{}DEVPATH\PYGZsh{}\PYGZsh{}*/\PYGZcb{} online\PYGZdq{}

fi
\end{Verbatim}

can process the event further.


\subsection{Kernel Inline Documentations Reference}
\label{core-api/cpu_hotplug:kernel-inline-documentations-reference}\index{cpuhp\_setup\_state (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/cpu_hotplug:c.cpuhp_setup_state}\pysiglinewithargsret{int \bfcode{cpuhp\_setup\_state}}{enum cpuhp\_state\emph{ state}, const char *\emph{ name}, int (*startup) (unsigned int\emph{ cpu}, int (*teardown) (unsigned int\emph{ cpu}}{}
Setup hotplug state callbacks with calling the callbacks

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{enum cpuhp\_state state}}] \leavevmode
The state for which the calls are installed

\item[{\code{const char * name}}] \leavevmode
Name of the callback (will be used in debug output)

\item[{\code{int (*)(unsigned int cpu) startup}}] \leavevmode
startup callback function

\item[{\code{int (*)(unsigned int cpu) teardown}}] \leavevmode
teardown callback function

\end{description}

\textbf{Description}

Installs the callback functions and invokes the startup callback on
the present cpus which have already reached the \textbf{state}.
\index{cpuhp\_setup\_state\_nocalls (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/cpu_hotplug:c.cpuhp_setup_state_nocalls}\pysiglinewithargsret{int \bfcode{cpuhp\_setup\_state\_nocalls}}{enum cpuhp\_state\emph{ state}, const char *\emph{ name}, int (*startup) (unsigned int\emph{ cpu}, int (*teardown) (unsigned int\emph{ cpu}}{}
Setup hotplug state callbacks without calling the callbacks

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{enum cpuhp\_state state}}] \leavevmode
The state for which the calls are installed

\item[{\code{const char * name}}] \leavevmode
Name of the callback.

\item[{\code{int (*)(unsigned int cpu) startup}}] \leavevmode
startup callback function

\item[{\code{int (*)(unsigned int cpu) teardown}}] \leavevmode
teardown callback function

\end{description}

\textbf{Description}

Same as \textbf{cpuhp\_setup\_state} except that no calls are executed are invoked
during installation of this callback. NOP if SMP=n or HOTPLUG\_CPU=n.
\index{cpuhp\_setup\_state\_multi (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/cpu_hotplug:c.cpuhp_setup_state_multi}\pysiglinewithargsret{int \bfcode{cpuhp\_setup\_state\_multi}}{enum cpuhp\_state\emph{ state}, const char *\emph{ name}, int (*startup) (unsigned int\emph{ cpu}, struct hlist\_node\emph{ *node}, int (*teardown) (unsigned int\emph{ cpu}, struct hlist\_node\emph{ *node}}{}
Add callbacks for multi state

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{enum cpuhp\_state state}}] \leavevmode
The state for which the calls are installed

\item[{\code{const char * name}}] \leavevmode
Name of the callback.

\item[{\code{int (*)(unsigned int cpu, struct hlist\_node *node) startup}}] \leavevmode
startup callback function

\item[{\code{int (*)(unsigned int cpu, struct hlist\_node *node) teardown}}] \leavevmode
teardown callback function

\end{description}

\textbf{Description}

Sets the internal multi\_instance flag and prepares a state to work as a multi
instance callback. No callbacks are invoked at this point. The callbacks are
invoked once an instance for this state are registered via
\textbf{cpuhp\_state\_add\_instance} or \textbf{cpuhp\_state\_add\_instance\_nocalls}.
\index{cpuhp\_state\_add\_instance (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/cpu_hotplug:c.cpuhp_state_add_instance}\pysiglinewithargsret{int \bfcode{cpuhp\_state\_add\_instance}}{enum cpuhp\_state\emph{ state}, struct hlist\_node *\emph{ node}}{}
Add an instance for a state and invoke startup callback.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{enum cpuhp\_state state}}] \leavevmode
The state for which the instance is installed

\item[{\code{struct hlist\_node * node}}] \leavevmode
The node for this individual state.

\end{description}

\textbf{Description}

Installs the instance for the \textbf{state} and invokes the startup callback on
the present cpus which have already reached the \textbf{state}. The \textbf{state} must have
been earlier marked as multi-instance by \textbf{cpuhp\_setup\_state\_multi}.
\index{cpuhp\_state\_add\_instance\_nocalls (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/cpu_hotplug:c.cpuhp_state_add_instance_nocalls}\pysiglinewithargsret{int \bfcode{cpuhp\_state\_add\_instance\_nocalls}}{enum cpuhp\_state\emph{ state}, struct hlist\_node *\emph{ node}}{}
Add an instance for a state without invoking the startup callback.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{enum cpuhp\_state state}}] \leavevmode
The state for which the instance is installed

\item[{\code{struct hlist\_node * node}}] \leavevmode
The node for this individual state.

\end{description}

\textbf{Description}

Installs the instance for the \textbf{state} The \textbf{state} must have been earlier
marked as multi-instance by \textbf{cpuhp\_setup\_state\_multi}.
\index{cpuhp\_remove\_state (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/cpu_hotplug:c.cpuhp_remove_state}\pysiglinewithargsret{void \bfcode{cpuhp\_remove\_state}}{enum cpuhp\_state\emph{ state}}{}
Remove hotplug state callbacks and invoke the teardown

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{enum cpuhp\_state state}}] \leavevmode
The state for which the calls are removed

\end{description}

\textbf{Description}

Removes the callback functions and invokes the teardown callback on
the present cpus which have already reached the \textbf{state}.
\index{cpuhp\_remove\_state\_nocalls (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/cpu_hotplug:c.cpuhp_remove_state_nocalls}\pysiglinewithargsret{void \bfcode{cpuhp\_remove\_state\_nocalls}}{enum cpuhp\_state\emph{ state}}{}
Remove hotplug state callbacks without invoking teardown

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{enum cpuhp\_state state}}] \leavevmode
The state for which the calls are removed

\end{description}
\index{cpuhp\_remove\_multi\_state (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/cpu_hotplug:c.cpuhp_remove_multi_state}\pysiglinewithargsret{void \bfcode{cpuhp\_remove\_multi\_state}}{enum cpuhp\_state\emph{ state}}{}
Remove hotplug multi state callback

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{enum cpuhp\_state state}}] \leavevmode
The state for which the calls are removed

\end{description}

\textbf{Description}

Removes the callback functions from a multi state. This is the reverse of
{\hyperref[core\string-api/cpu_hotplug:c.cpuhp_setup_state_multi]{\emph{\code{cpuhp\_setup\_state\_multi()}}}}. All instances should have been removed before
invoking this function.
\index{cpuhp\_state\_remove\_instance (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/cpu_hotplug:c.cpuhp_state_remove_instance}\pysiglinewithargsret{int \bfcode{cpuhp\_state\_remove\_instance}}{enum cpuhp\_state\emph{ state}, struct hlist\_node *\emph{ node}}{}
Remove hotplug instance from state and invoke the teardown callback

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{enum cpuhp\_state state}}] \leavevmode
The state from which the instance is removed

\item[{\code{struct hlist\_node * node}}] \leavevmode
The node for this individual state.

\end{description}

\textbf{Description}

Removes the instance and invokes the teardown callback on the present cpus
which have already reached the \textbf{state}.
\index{cpuhp\_state\_remove\_instance\_nocalls (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/cpu_hotplug:c.cpuhp_state_remove_instance_nocalls}\pysiglinewithargsret{int \bfcode{cpuhp\_state\_remove\_instance\_nocalls}}{enum cpuhp\_state\emph{ state}, struct hlist\_node *\emph{ node}}{}
Remove hotplug instance from state without invoking the reatdown callback

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{enum cpuhp\_state state}}] \leavevmode
The state from which the instance is removed

\item[{\code{struct hlist\_node * node}}] \leavevmode
The node for this individual state.

\end{description}

\textbf{Description}

Removes the instance without invoking the teardown callback.


\section{ID Allocation}
\label{core-api/idr::doc}\label{core-api/idr:id-allocation}\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Matthew Wilcox

\end{description}\end{quote}


\subsection{Overview}
\label{core-api/idr:overview}
A common problem to solve is allocating identifiers (IDs); generally
small numbers which identify a thing.  Examples include file descriptors,
process IDs, packet identifiers in networking protocols, SCSI tags
and device instance numbers.  The IDR and the IDA provide a reasonable
solution to the problem to avoid everybody inventing their own.  The IDR
provides the ability to map an ID to a pointer, while the IDA provides
only ID allocation, and as a result is much more memory-efficient.


\subsection{IDR usage}
\label{core-api/idr:idr-usage}
Start by initialising an IDR, either with {\hyperref[core\string-api/idr:c.DEFINE_IDR]{\emph{\code{DEFINE\_IDR()}}}}
for statically allocated IDRs or {\hyperref[core\string-api/idr:c.idr_init]{\emph{\code{idr\_init()}}}} for dynamically
allocated IDRs.

You can call {\hyperref[core\string-api/idr:c.idr_alloc]{\emph{\code{idr\_alloc()}}}} to allocate an unused ID.  Look up
the pointer you associated with the ID by calling {\hyperref[core\string-api/idr:c.idr_find]{\emph{\code{idr\_find()}}}}
and free the ID by calling {\hyperref[core\string-api/idr:c.idr_remove]{\emph{\code{idr\_remove()}}}}.

If you need to change the pointer associated with an ID, you can call
{\hyperref[core\string-api/idr:c.idr_replace]{\emph{\code{idr\_replace()}}}}.  One common reason to do this is to reserve an
ID by passing a \code{NULL} pointer to the allocation function; initialise the
object with the reserved ID and finally insert the initialised object
into the IDR.

Some users need to allocate IDs larger than \code{INT\_MAX}.  So far all of
these users have been content with a \code{UINT\_MAX} limit, and they use
{\hyperref[core\string-api/idr:c.idr_alloc_u32]{\emph{\code{idr\_alloc\_u32()}}}}.  If you need IDs that will not fit in a u32,
we will work with you to address your needs.

If you need to allocate IDs sequentially, you can use
{\hyperref[core\string-api/idr:c.idr_alloc_cyclic]{\emph{\code{idr\_alloc\_cyclic()}}}}.  The IDR becomes less efficient when dealing
with larger IDs, so using this function comes at a slight cost.

To perform an action on all pointers used by the IDR, you can
either use the callback-based {\hyperref[core\string-api/idr:c.idr_for_each]{\emph{\code{idr\_for\_each()}}}} or the
iterator-style {\hyperref[core\string-api/idr:c.idr_for_each_entry]{\emph{\code{idr\_for\_each\_entry()}}}}.  You may need to use
{\hyperref[core\string-api/idr:c.idr_for_each_entry_continue]{\emph{\code{idr\_for\_each\_entry\_continue()}}}} to continue an iteration.  You can
also use {\hyperref[core\string-api/idr:c.idr_get_next]{\emph{\code{idr\_get\_next()}}}} if the iterator doesn't fit your needs.

When you have finished using an IDR, you can call \code{idr\_destroy()}
to release the memory used by the IDR.  This will not free the objects
pointed to from the IDR; if you want to do that, use one of the iterators
to do it.

You can use {\hyperref[core\string-api/idr:c.idr_is_empty]{\emph{\code{idr\_is\_empty()}}}} to find out whether there are any
IDs currently allocated.

If you need to take a lock while allocating a new ID from the IDR,
you may need to pass a restrictive set of GFP flags, which can lead
to the IDR being unable to allocate memory.  To work around this,
you can call \code{idr\_preload()} before taking the lock, and then
{\hyperref[core\string-api/idr:c.idr_preload_end]{\emph{\code{idr\_preload\_end()}}}} after the allocation.

idr synchronization (stolen from radix-tree.h)

{\hyperref[core\string-api/idr:c.idr_find]{\emph{\code{idr\_find()}}}} is able to be called locklessly, using RCU. The caller must
ensure calls to this function are made within {\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock]{\emph{\code{rcu\_read\_lock()}}}} regions.
Other readers (lock-free or otherwise) and modifications may be running
concurrently.

It is still required that the caller manage the synchronization and
lifetimes of the items. So if RCU lock-free lookups are used, typically
this would mean that the items have their own locks, or are amenable to
lock-free access; and that the items are freed by RCU (or only freed after
having been deleted from the idr tree \emph{and} a \code{synchronize\_rcu()} grace
period).


\subsection{IDA usage}
\label{core-api/idr:ida-usage}
The IDA is an ID allocator which does not provide the ability to
associate an ID with a pointer.  As such, it only needs to store one
bit per ID, and so is more space efficient than an IDR.  To use an IDA,
define it using \code{DEFINE\_IDA()} (or embed a \code{struct ida} in a data structure,
then initialise it using \code{ida\_init()}).  To allocate a new ID, call
{\hyperref[core\string-api/idr:c.ida_simple_get]{\emph{\code{ida\_simple\_get()}}}}.  To free an ID, call {\hyperref[core\string-api/idr:c.ida_simple_remove]{\emph{\code{ida\_simple\_remove()}}}}.

If you have more complex locking requirements, use a loop around
\code{ida\_pre\_get()} and {\hyperref[core\string-api/idr:c.ida_get_new]{\emph{\code{ida\_get\_new()}}}} to allocate a new ID.  Then use
{\hyperref[core\string-api/idr:c.ida_remove]{\emph{\code{ida\_remove()}}}} to free an ID.  You must make sure that {\hyperref[core\string-api/idr:c.ida_get_new]{\emph{\code{ida\_get\_new()}}}} and
{\hyperref[core\string-api/idr:c.ida_remove]{\emph{\code{ida\_remove()}}}} cannot be called at the same time as each other for the
same IDA.

You can also use {\hyperref[core\string-api/idr:c.ida_get_new_above]{\emph{\code{ida\_get\_new\_above()}}}} if you need an ID to be allocated
above a particular number.  {\hyperref[core\string-api/idr:c.ida_destroy]{\emph{\code{ida\_destroy()}}}} can be used to dispose of an
IDA without needing to free the individual IDs in it.  You can use
\code{ida\_is\_empty()} to find out whether the IDA has any IDs currently allocated.

IDs are currently limited to the range {[}0-INT\_MAX{]}.  If this is an awkward
limitation, it should be quite straightforward to raise the maximum.


\subsection{Functions and structures}
\label{core-api/idr:functions-and-structures}\index{IDR\_INIT (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/idr:c.IDR_INIT}\pysiglinewithargsret{\bfcode{IDR\_INIT}}{}{}
Initialise an IDR.

\end{fulllineitems}


\textbf{Parameters}

\textbf{Description}

A freshly-initialised IDR contains no IDs.
\index{DEFINE\_IDR (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/idr:c.DEFINE_IDR}\pysiglinewithargsret{\bfcode{DEFINE\_IDR}}{\emph{name}}{}
Define a statically-allocated IDR

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{name}}] \leavevmode
Name of IDR

\end{description}

\textbf{Description}

An IDR defined using this macro is ready for use with no additional
initialisation required.  It contains no IDs.
\index{idr\_get\_cursor (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/idr:c.idr_get_cursor}\pysiglinewithargsret{unsigned int \bfcode{idr\_get\_cursor}}{const struct idr *\emph{ idr}}{}
Return the current position of the cyclic allocator

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const struct idr * idr}}] \leavevmode
idr handle

\end{description}

\textbf{Description}

The value returned is the value that will be next returned from
{\hyperref[core\string-api/idr:c.idr_alloc_cyclic]{\emph{\code{idr\_alloc\_cyclic()}}}} if it is free (otherwise the search will start from
this position).
\index{idr\_set\_cursor (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/idr:c.idr_set_cursor}\pysiglinewithargsret{void \bfcode{idr\_set\_cursor}}{struct idr *\emph{ idr}, unsigned int\emph{ val}}{}
Set the current position of the cyclic allocator

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct idr * idr}}] \leavevmode
idr handle

\item[{\code{unsigned int val}}] \leavevmode
new position

\end{description}

\textbf{Description}

The next call to {\hyperref[core\string-api/idr:c.idr_alloc_cyclic]{\emph{\code{idr\_alloc\_cyclic()}}}} will return \textbf{val} if it is free
(otherwise the search will start from this position).

\textbf{idr sync}

idr synchronization (stolen from radix-tree.h)

{\hyperref[core\string-api/idr:c.idr_find]{\emph{\code{idr\_find()}}}} is able to be called locklessly, using RCU. The caller must
ensure calls to this function are made within {\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock]{\emph{\code{rcu\_read\_lock()}}}} regions.
Other readers (lock-free or otherwise) and modifications may be running
concurrently.

It is still required that the caller manage the synchronization and
lifetimes of the items. So if RCU lock-free lookups are used, typically
this would mean that the items have their own locks, or are amenable to
lock-free access; and that the items are freed by RCU (or only freed after
having been deleted from the idr tree \emph{and} a \code{synchronize\_rcu()} grace
period).
\index{idr\_init\_base (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/idr:c.idr_init_base}\pysiglinewithargsret{void \bfcode{idr\_init\_base}}{struct idr *\emph{ idr}, int\emph{ base}}{}
Initialise an IDR.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct idr * idr}}] \leavevmode
IDR handle.

\item[{\code{int base}}] \leavevmode
The base value for the IDR.

\end{description}

\textbf{Description}

This variation of {\hyperref[core\string-api/idr:c.idr_init]{\emph{\code{idr\_init()}}}} creates an IDR which will allocate IDs
starting at \code{base}.
\index{idr\_init (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/idr:c.idr_init}\pysiglinewithargsret{void \bfcode{idr\_init}}{struct idr *\emph{ idr}}{}
Initialise an IDR.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct idr * idr}}] \leavevmode
IDR handle.

\end{description}

\textbf{Description}

Initialise a dynamically allocated IDR.  To initialise a
statically allocated IDR, use {\hyperref[core\string-api/idr:c.DEFINE_IDR]{\emph{\code{DEFINE\_IDR()}}}}.
\index{idr\_is\_empty (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/idr:c.idr_is_empty}\pysiglinewithargsret{bool \bfcode{idr\_is\_empty}}{const struct idr *\emph{ idr}}{}
Are there any IDs allocated?

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const struct idr * idr}}] \leavevmode
IDR handle.

\end{description}

\textbf{Return}

\code{true} if any IDs have been allocated from this IDR.
\index{idr\_preload\_end (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/idr:c.idr_preload_end}\pysiglinewithargsret{void \bfcode{idr\_preload\_end}}{void}{}
end preload section started with \code{idr\_preload()}

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

Each \code{idr\_preload()} should be matched with an invocation of this
function.  See \code{idr\_preload()} for details.
\index{idr\_for\_each\_entry (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/idr:c.idr_for_each_entry}\pysiglinewithargsret{\bfcode{idr\_for\_each\_entry}}{\emph{idr}, \emph{entry}, \emph{id}}{}
Iterate over an IDR's elements of a given type.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{idr}}] \leavevmode
IDR handle.

\item[{\code{entry}}] \leavevmode
The type * to use as cursor

\item[{\code{id}}] \leavevmode
Entry ID.

\end{description}

\textbf{Description}

\textbf{entry} and \textbf{id} do not need to be initialized before the loop, and
after normal termination \textbf{entry} is left with the value NULL.  This
is convenient for a ``not found'' value.
\index{idr\_for\_each\_entry\_ul (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/idr:c.idr_for_each_entry_ul}\pysiglinewithargsret{\bfcode{idr\_for\_each\_entry\_ul}}{\emph{idr}, \emph{entry}, \emph{id}}{}
Iterate over an IDR's elements of a given type.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{idr}}] \leavevmode
IDR handle.

\item[{\code{entry}}] \leavevmode
The type * to use as cursor.

\item[{\code{id}}] \leavevmode
Entry ID.

\end{description}

\textbf{Description}

\textbf{entry} and \textbf{id} do not need to be initialized before the loop, and
after normal termination \textbf{entry} is left with the value NULL.  This
is convenient for a ``not found'' value.
\index{idr\_for\_each\_entry\_continue (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/idr:c.idr_for_each_entry_continue}\pysiglinewithargsret{\bfcode{idr\_for\_each\_entry\_continue}}{\emph{idr}, \emph{entry}, \emph{id}}{}
Continue iteration over an IDR's elements of a given type

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{idr}}] \leavevmode
IDR handle.

\item[{\code{entry}}] \leavevmode
The type * to use as a cursor.

\item[{\code{id}}] \leavevmode
Entry ID.

\end{description}

\textbf{Description}

Continue to iterate over entries, continuing after the current position.
\index{ida\_get\_new (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/idr:c.ida_get_new}\pysiglinewithargsret{int \bfcode{ida\_get\_new}}{struct ida *\emph{ ida}, int *\emph{ p\_id}}{}
allocate new ID

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct ida * ida}}] \leavevmode
idr handle

\item[{\code{int * p\_id}}] \leavevmode
pointer to the allocated handle

\end{description}

\textbf{Description}

Simple wrapper around {\hyperref[core\string-api/idr:c.ida_get_new_above]{\emph{\code{ida\_get\_new\_above()}}}} w/ \textbf{starting\_id} of zero.
\index{idr\_alloc\_u32 (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/idr:c.idr_alloc_u32}\pysiglinewithargsret{int \bfcode{idr\_alloc\_u32}}{struct idr *\emph{ idr}, void *\emph{ ptr}, u32 *\emph{ nextid}, unsigned long\emph{ max}, gfp\_t\emph{ gfp}}{}
Allocate an ID.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct idr * idr}}] \leavevmode
IDR handle.

\item[{\code{void * ptr}}] \leavevmode
Pointer to be associated with the new ID.

\item[{\code{u32 * nextid}}] \leavevmode
Pointer to an ID.

\item[{\code{unsigned long max}}] \leavevmode
The maximum ID to allocate (inclusive).

\item[{\code{gfp\_t gfp}}] \leavevmode
Memory allocation flags.

\end{description}

\textbf{Description}

Allocates an unused ID in the range specified by \textbf{nextid} and \textbf{max}.
Note that \textbf{max} is inclusive whereas the \textbf{end} parameter to {\hyperref[core\string-api/idr:c.idr_alloc]{\emph{\code{idr\_alloc()}}}}
is exclusive.  The new ID is assigned to \textbf{nextid} before the pointer
is inserted into the IDR, so if \textbf{nextid} points into the object pointed
to by \textbf{ptr}, a concurrent lookup will not find an uninitialised ID.

The caller should provide their own locking to ensure that two
concurrent modifications to the IDR are not possible.  Read-only
accesses to the IDR may be done under the RCU read lock or may
exclude simultaneous writers.

\textbf{Return}

0 if an ID was allocated, -ENOMEM if memory allocation failed,
or -ENOSPC if no free IDs could be found.  If an error occurred,
\textbf{nextid} is unchanged.
\index{idr\_alloc (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/idr:c.idr_alloc}\pysiglinewithargsret{int \bfcode{idr\_alloc}}{struct idr *\emph{ idr}, void *\emph{ ptr}, int\emph{ start}, int\emph{ end}, gfp\_t\emph{ gfp}}{}
Allocate an ID.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct idr * idr}}] \leavevmode
IDR handle.

\item[{\code{void * ptr}}] \leavevmode
Pointer to be associated with the new ID.

\item[{\code{int start}}] \leavevmode
The minimum ID (inclusive).

\item[{\code{int end}}] \leavevmode
The maximum ID (exclusive).

\item[{\code{gfp\_t gfp}}] \leavevmode
Memory allocation flags.

\end{description}

\textbf{Description}

Allocates an unused ID in the range specified by \textbf{start} and \textbf{end}.  If
\textbf{end} is \textless{}= 0, it is treated as one larger than \code{INT\_MAX}.  This allows
callers to use \textbf{start} + N as \textbf{end} as long as N is within integer range.

The caller should provide their own locking to ensure that two
concurrent modifications to the IDR are not possible.  Read-only
accesses to the IDR may be done under the RCU read lock or may
exclude simultaneous writers.

\textbf{Return}

The newly allocated ID, -ENOMEM if memory allocation failed,
or -ENOSPC if no free IDs could be found.
\index{idr\_alloc\_cyclic (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/idr:c.idr_alloc_cyclic}\pysiglinewithargsret{int \bfcode{idr\_alloc\_cyclic}}{struct idr *\emph{ idr}, void *\emph{ ptr}, int\emph{ start}, int\emph{ end}, gfp\_t\emph{ gfp}}{}
Allocate an ID cyclically.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct idr * idr}}] \leavevmode
IDR handle.

\item[{\code{void * ptr}}] \leavevmode
Pointer to be associated with the new ID.

\item[{\code{int start}}] \leavevmode
The minimum ID (inclusive).

\item[{\code{int end}}] \leavevmode
The maximum ID (exclusive).

\item[{\code{gfp\_t gfp}}] \leavevmode
Memory allocation flags.

\end{description}

\textbf{Description}

Allocates an unused ID in the range specified by \textbf{nextid} and \textbf{end}.  If
\textbf{end} is \textless{}= 0, it is treated as one larger than \code{INT\_MAX}.  This allows
callers to use \textbf{start} + N as \textbf{end} as long as N is within integer range.
The search for an unused ID will start at the last ID allocated and will
wrap around to \textbf{start} if no free IDs are found before reaching \textbf{end}.

The caller should provide their own locking to ensure that two
concurrent modifications to the IDR are not possible.  Read-only
accesses to the IDR may be done under the RCU read lock or may
exclude simultaneous writers.

\textbf{Return}

The newly allocated ID, -ENOMEM if memory allocation failed,
or -ENOSPC if no free IDs could be found.
\index{idr\_remove (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/idr:c.idr_remove}\pysiglinewithargsret{void * \bfcode{idr\_remove}}{struct idr *\emph{ idr}, unsigned long\emph{ id}}{}
Remove an ID from the IDR.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct idr * idr}}] \leavevmode
IDR handle.

\item[{\code{unsigned long id}}] \leavevmode
Pointer ID.

\end{description}

\textbf{Description}

Removes this ID from the IDR.  If the ID was not previously in the IDR,
this function returns \code{NULL}.

Since this function modifies the IDR, the caller should provide their
own locking to ensure that concurrent modification of the same IDR is
not possible.

\textbf{Return}

The pointer formerly associated with this ID.
\index{idr\_find (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/idr:c.idr_find}\pysiglinewithargsret{void * \bfcode{idr\_find}}{const struct idr *\emph{ idr}, unsigned long\emph{ id}}{}
Return pointer for given ID.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const struct idr * idr}}] \leavevmode
IDR handle.

\item[{\code{unsigned long id}}] \leavevmode
Pointer ID.

\end{description}

\textbf{Description}

Looks up the pointer associated with this ID.  A \code{NULL} pointer may
indicate that \textbf{id} is not allocated or that the \code{NULL} pointer was
associated with this ID.

This function can be called under {\hyperref[core\string-api/kernel\string-api:c.rcu_read_lock]{\emph{\code{rcu\_read\_lock()}}}}, given that the leaf
pointers lifetimes are correctly managed.

\textbf{Return}

The pointer associated with this ID.
\index{idr\_for\_each (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/idr:c.idr_for_each}\pysiglinewithargsret{int \bfcode{idr\_for\_each}}{const struct idr *\emph{ idr}, int (*fn) (int\emph{ id}, void\emph{ *p}, void\emph{ *data}, void *\emph{ data}}{}
Iterate through all stored pointers.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const struct idr * idr}}] \leavevmode
IDR handle.

\item[{\code{int (*)(int id, void *p, void *data) fn}}] \leavevmode
Function to be called for each pointer.

\item[{\code{void * data}}] \leavevmode
Data passed to callback function.

\end{description}

\textbf{Description}

The callback function will be called for each entry in \textbf{idr}, passing
the ID, the entry and \textbf{data}.

If \textbf{fn} returns anything other than \code{0}, the iteration stops and that
value is returned from this function.

{\hyperref[core\string-api/idr:c.idr_for_each]{\emph{\code{idr\_for\_each()}}}} can be called concurrently with {\hyperref[core\string-api/idr:c.idr_alloc]{\emph{\code{idr\_alloc()}}}} and
{\hyperref[core\string-api/idr:c.idr_remove]{\emph{\code{idr\_remove()}}}} if protected by RCU.  Newly added entries may not be
seen and deleted entries may be seen, but adding and removing entries
will not cause other entries to be skipped, nor spurious ones to be seen.
\index{idr\_get\_next (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/idr:c.idr_get_next}\pysiglinewithargsret{void * \bfcode{idr\_get\_next}}{struct idr *\emph{ idr}, int *\emph{ nextid}}{}
Find next populated entry.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct idr * idr}}] \leavevmode
IDR handle.

\item[{\code{int * nextid}}] \leavevmode
Pointer to an ID.

\end{description}

\textbf{Description}

Returns the next populated entry in the tree with an ID greater than
or equal to the value pointed to by \textbf{nextid}.  On exit, \textbf{nextid} is updated
to the ID of the found value.  To use in a loop, the value pointed to by
nextid must be incremented by the user.
\index{idr\_get\_next\_ul (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/idr:c.idr_get_next_ul}\pysiglinewithargsret{void * \bfcode{idr\_get\_next\_ul}}{struct idr *\emph{ idr}, unsigned long *\emph{ nextid}}{}
Find next populated entry.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct idr * idr}}] \leavevmode
IDR handle.

\item[{\code{unsigned long * nextid}}] \leavevmode
Pointer to an ID.

\end{description}

\textbf{Description}

Returns the next populated entry in the tree with an ID greater than
or equal to the value pointed to by \textbf{nextid}.  On exit, \textbf{nextid} is updated
to the ID of the found value.  To use in a loop, the value pointed to by
nextid must be incremented by the user.
\index{idr\_replace (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/idr:c.idr_replace}\pysiglinewithargsret{void * \bfcode{idr\_replace}}{struct idr *\emph{ idr}, void *\emph{ ptr}, unsigned long\emph{ id}}{}
replace pointer for given ID.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct idr * idr}}] \leavevmode
IDR handle.

\item[{\code{void * ptr}}] \leavevmode
New pointer to associate with the ID.

\item[{\code{unsigned long id}}] \leavevmode
ID to change.

\end{description}

\textbf{Description}

Replace the pointer registered with an ID and return the old value.
This function can be called under the RCU read lock concurrently with
{\hyperref[core\string-api/idr:c.idr_alloc]{\emph{\code{idr\_alloc()}}}} and {\hyperref[core\string-api/idr:c.idr_remove]{\emph{\code{idr\_remove()}}}} (as long as the ID being removed is not
the one being replaced!).

\textbf{Return}

the old value on success.  \code{-ENOENT} indicates that \textbf{id} was not
found.  \code{-EINVAL} indicates that \textbf{ptr} was not valid.

\textbf{IDA description}

The IDA is an ID allocator which does not provide the ability to
associate an ID with a pointer.  As such, it only needs to store one
bit per ID, and so is more space efficient than an IDR.  To use an IDA,
define it using \code{DEFINE\_IDA()} (or embed a \code{struct ida} in a data structure,
then initialise it using \code{ida\_init()}).  To allocate a new ID, call
{\hyperref[core\string-api/idr:c.ida_simple_get]{\emph{\code{ida\_simple\_get()}}}}.  To free an ID, call {\hyperref[core\string-api/idr:c.ida_simple_remove]{\emph{\code{ida\_simple\_remove()}}}}.

If you have more complex locking requirements, use a loop around
\code{ida\_pre\_get()} and {\hyperref[core\string-api/idr:c.ida_get_new]{\emph{\code{ida\_get\_new()}}}} to allocate a new ID.  Then use
{\hyperref[core\string-api/idr:c.ida_remove]{\emph{\code{ida\_remove()}}}} to free an ID.  You must make sure that {\hyperref[core\string-api/idr:c.ida_get_new]{\emph{\code{ida\_get\_new()}}}} and
{\hyperref[core\string-api/idr:c.ida_remove]{\emph{\code{ida\_remove()}}}} cannot be called at the same time as each other for the
same IDA.

You can also use {\hyperref[core\string-api/idr:c.ida_get_new_above]{\emph{\code{ida\_get\_new\_above()}}}} if you need an ID to be allocated
above a particular number.  {\hyperref[core\string-api/idr:c.ida_destroy]{\emph{\code{ida\_destroy()}}}} can be used to dispose of an
IDA without needing to free the individual IDs in it.  You can use
\code{ida\_is\_empty()} to find out whether the IDA has any IDs currently allocated.

IDs are currently limited to the range {[}0-INT\_MAX{]}.  If this is an awkward
limitation, it should be quite straightforward to raise the maximum.
\index{ida\_get\_new\_above (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/idr:c.ida_get_new_above}\pysiglinewithargsret{int \bfcode{ida\_get\_new\_above}}{struct ida *\emph{ ida}, int\emph{ start}, int *\emph{ id}}{}
allocate new ID above or equal to a start id

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct ida * ida}}] \leavevmode
ida handle

\item[{\code{int start}}] \leavevmode
id to start search at

\item[{\code{int * id}}] \leavevmode
pointer to the allocated handle

\end{description}

\textbf{Description}

Allocate new ID above or equal to \textbf{start}.  It should be called
with any required locks to ensure that concurrent calls to
{\hyperref[core\string-api/idr:c.ida_get_new_above]{\emph{\code{ida\_get\_new\_above()}}}} / {\hyperref[core\string-api/idr:c.ida_get_new]{\emph{\code{ida\_get\_new()}}}} / {\hyperref[core\string-api/idr:c.ida_remove]{\emph{\code{ida\_remove()}}}} are not allowed.
Consider using {\hyperref[core\string-api/idr:c.ida_simple_get]{\emph{\code{ida\_simple\_get()}}}} if you do not have complex locking
requirements.

If memory is required, it will return \code{-EAGAIN}, you should unlock
and go back to the \code{ida\_pre\_get()} call.  If the ida is full, it will
return \code{-ENOSPC}.  On success, it will return 0.

\textbf{id} returns a value in the range \textbf{start} ... \code{0x7fffffff}.
\index{ida\_remove (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/idr:c.ida_remove}\pysiglinewithargsret{void \bfcode{ida\_remove}}{struct ida *\emph{ ida}, int\emph{ id}}{}
Free the given ID

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct ida * ida}}] \leavevmode
ida handle

\item[{\code{int id}}] \leavevmode
ID to free

\end{description}

\textbf{Description}

This function should not be called at the same time as {\hyperref[core\string-api/idr:c.ida_get_new_above]{\emph{\code{ida\_get\_new\_above()}}}}.
\index{ida\_destroy (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/idr:c.ida_destroy}\pysiglinewithargsret{void \bfcode{ida\_destroy}}{struct ida *\emph{ ida}}{}
Free the contents of an ida

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct ida * ida}}] \leavevmode
ida handle

\end{description}

\textbf{Description}

Calling this function releases all resources associated with an IDA.  When
this call returns, the IDA is empty and can be reused or freed.  The caller
should not allow {\hyperref[core\string-api/idr:c.ida_remove]{\emph{\code{ida\_remove()}}}} or {\hyperref[core\string-api/idr:c.ida_get_new_above]{\emph{\code{ida\_get\_new\_above()}}}} to be called at the
same time.
\index{ida\_simple\_get (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/idr:c.ida_simple_get}\pysiglinewithargsret{int \bfcode{ida\_simple\_get}}{struct ida *\emph{ ida}, unsigned int\emph{ start}, unsigned int\emph{ end}, gfp\_t\emph{ gfp\_mask}}{}
get a new id.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct ida * ida}}] \leavevmode
the (initialized) ida.

\item[{\code{unsigned int start}}] \leavevmode
the minimum id (inclusive, \textless{} 0x8000000)

\item[{\code{unsigned int end}}] \leavevmode
the maximum id (exclusive, \textless{} 0x8000000 or 0)

\item[{\code{gfp\_t gfp\_mask}}] \leavevmode
memory allocation flags

\end{description}

\textbf{Description}

Allocates an id in the range start \textless{}= id \textless{} end, or returns -ENOSPC.
On memory allocation failure, returns -ENOMEM.

Compared to {\hyperref[core\string-api/idr:c.ida_get_new_above]{\emph{\code{ida\_get\_new\_above()}}}} this function does its own locking, and
should be used unless there are special requirements.

Use {\hyperref[core\string-api/idr:c.ida_simple_remove]{\emph{\code{ida\_simple\_remove()}}}} to get rid of an id.
\index{ida\_simple\_remove (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/idr:c.ida_simple_remove}\pysiglinewithargsret{void \bfcode{ida\_simple\_remove}}{struct ida *\emph{ ida}, unsigned int\emph{ id}}{}
remove an allocated id.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct ida * ida}}] \leavevmode
the (initialized) ida.

\item[{\code{unsigned int id}}] \leavevmode
the id returned by ida\_simple\_get.

\end{description}

\textbf{Description}

Use to release an id allocated with {\hyperref[core\string-api/idr:c.ida_simple_get]{\emph{\code{ida\_simple\_get()}}}}.

Compared to {\hyperref[core\string-api/idr:c.ida_remove]{\emph{\code{ida\_remove()}}}} this function does its own locking, and should be
used unless there are special requirements.


\section{Semantics and Behavior of Local Atomic Operations}
\label{core-api/local_ops::doc}\label{core-api/local_ops:semantics-and-behavior-of-local-atomic-operations}\label{core-api/local_ops:local-ops}\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Mathieu Desnoyers

\end{description}\end{quote}

This document explains the purpose of the local atomic operations, how
to implement them for any given architecture and shows how they can be used
properly. It also stresses on the precautions that must be taken when reading
those local variables across CPUs when the order of memory writes matters.

\begin{notice}{note}{Note:}
Note that \code{local\_t} based operations are not recommended for general
kernel use. Please use the \code{this\_cpu} operations instead unless there is
really a special purpose. Most uses of \code{local\_t} in the kernel have been
replaced by \code{this\_cpu} operations. \code{this\_cpu} operations combine the
relocation with the \code{local\_t} like semantics in a single instruction and
yield more compact and faster executing code.
\end{notice}


\subsection{Purpose of local atomic operations}
\label{core-api/local_ops:purpose-of-local-atomic-operations}
Local atomic operations are meant to provide fast and highly reentrant per CPU
counters. They minimize the performance cost of standard atomic operations by
removing the LOCK prefix and memory barriers normally required to synchronize
across CPUs.

Having fast per CPU atomic counters is interesting in many cases: it does not
require disabling interrupts to protect from interrupt handlers and it permits
coherent counters in NMI handlers. It is especially useful for tracing purposes
and for various performance monitoring counters.

Local atomic operations only guarantee variable modification atomicity wrt the
CPU which owns the data. Therefore, care must taken to make sure that only one
CPU writes to the \code{local\_t} data. This is done by using per cpu data and
making sure that we modify it from within a preemption safe context. It is
however permitted to read \code{local\_t} data from any CPU: it will then appear to
be written out of order wrt other memory writes by the owner CPU.


\subsection{Implementation for a given architecture}
\label{core-api/local_ops:implementation-for-a-given-architecture}
It can be done by slightly modifying the standard atomic operations: only
their UP variant must be kept. It typically means removing LOCK prefix (on
i386 and x86\_64) and any SMP synchronization barrier. If the architecture does
not have a different behavior between SMP and UP, including
\code{asm-generic/local.h} in your architecture's \code{local.h} is sufficient.

The \code{local\_t} type is defined as an opaque \code{signed long} by embedding an
\code{atomic\_long\_t} inside a structure. This is made so a cast from this type to
a \code{long} fails. The definition looks like:

\begin{Verbatim}[commandchars=\\\{\}]
typedef struct \PYGZob{} atomic\PYGZus{}long\PYGZus{}t a; \PYGZcb{} local\PYGZus{}t;
\end{Verbatim}


\subsection{Rules to follow when using local atomic operations}
\label{core-api/local_ops:rules-to-follow-when-using-local-atomic-operations}\begin{itemize}
\item {} 
Variables touched by local ops must be per cpu variables.

\item {} 
\emph{Only} the CPU owner of these variables must write to them.

\item {} 
This CPU can use local ops from any context (process, irq, softirq, nmi, ...)
to update its \code{local\_t} variables.

\item {} 
Preemption (or interrupts) must be disabled when using local ops in
process context to make sure the process won't be migrated to a
different CPU between getting the per-cpu variable and doing the
actual local op.

\item {} 
When using local ops in interrupt context, no special care must be
taken on a mainline kernel, since they will run on the local CPU with
preemption already disabled. I suggest, however, to explicitly
disable preemption anyway to make sure it will still work correctly on
-rt kernels.

\item {} 
Reading the local cpu variable will provide the current copy of the
variable.

\item {} 
Reads of these variables can be done from any CPU, because updates to
``\code{long}'', aligned, variables are always atomic. Since no memory
synchronization is done by the writer CPU, an outdated copy of the
variable can be read when reading some \emph{other} cpu's variables.

\end{itemize}


\subsection{How to use local atomic operations}
\label{core-api/local_ops:how-to-use-local-atomic-operations}
\begin{Verbatim}[commandchars=\\\{\}]
\PYGZsh{}include \PYGZlt{}linux/percpu.h\PYGZgt{}
\PYGZsh{}include \PYGZlt{}asm/local.h\PYGZgt{}

static DEFINE\PYGZus{}PER\PYGZus{}CPU(local\PYGZus{}t, counters) = LOCAL\PYGZus{}INIT(0);
\end{Verbatim}


\subsection{Counting}
\label{core-api/local_ops:counting}
Counting is done on all the bits of a signed long.

In preemptible context, use \code{get\_cpu\_var()} and \code{put\_cpu\_var()} around
local atomic operations: it makes sure that preemption is disabled around write
access to the per cpu variable. For instance:

\begin{Verbatim}[commandchars=\\\{\}]
local\PYGZus{}inc(\PYGZam{}get\PYGZus{}cpu\PYGZus{}var(counters));
put\PYGZus{}cpu\PYGZus{}var(counters);
\end{Verbatim}

If you are already in a preemption-safe context, you can use
\code{this\_cpu\_ptr()} instead:

\begin{Verbatim}[commandchars=\\\{\}]
local\PYGZus{}inc(this\PYGZus{}cpu\PYGZus{}ptr(\PYGZam{}counters));
\end{Verbatim}


\subsection{Reading the counters}
\label{core-api/local_ops:reading-the-counters}
Those local counters can be read from foreign CPUs to sum the count. Note that
the data seen by local\_read across CPUs must be considered to be out of order
relatively to other memory writes happening on the CPU that owns the data:

\begin{Verbatim}[commandchars=\\\{\}]
long sum = 0;
for\PYGZus{}each\PYGZus{}online\PYGZus{}cpu(cpu)
        sum += local\PYGZus{}read(\PYGZam{}per\PYGZus{}cpu(counters, cpu));
\end{Verbatim}

If you want to use a remote local\_read to synchronize access to a resource
between CPUs, explicit \code{smp\_wmb()} and \code{smp\_rmb()} memory barriers must be used
respectively on the writer and the reader CPUs. It would be the case if you use
the \code{local\_t} variable as a counter of bytes written in a buffer: there should
be a \code{smp\_wmb()} between the buffer write and the counter increment and also a
\code{smp\_rmb()} between the counter read and the buffer read.

Here is a sample module which implements a basic per cpu counter using
\code{local.h}:

\begin{Verbatim}[commandchars=\\\{\}]
/* test\PYGZhy{}local.c
 *
 * Sample module for local.h usage.
 */


\PYGZsh{}include \PYGZlt{}asm/local.h\PYGZgt{}
\PYGZsh{}include \PYGZlt{}linux/module.h\PYGZgt{}
\PYGZsh{}include \PYGZlt{}linux/timer.h\PYGZgt{}

static DEFINE\PYGZus{}PER\PYGZus{}CPU(local\PYGZus{}t, counters) = LOCAL\PYGZus{}INIT(0);

static struct timer\PYGZus{}list test\PYGZus{}timer;

/* IPI called on each CPU. */
static void test\PYGZus{}each(void *info)
\PYGZob{}
        /* Increment the counter from a non preemptible context */
        printk(\PYGZdq{}Increment on cpu \PYGZpc{}d\PYGZbs{}n\PYGZdq{}, smp\PYGZus{}processor\PYGZus{}id());
        local\PYGZus{}inc(this\PYGZus{}cpu\PYGZus{}ptr(\PYGZam{}counters));

        /* This is what incrementing the variable would look like within a
         * preemptible context (it disables preemption) :
         *
         * local\PYGZus{}inc(\PYGZam{}get\PYGZus{}cpu\PYGZus{}var(counters));
         * put\PYGZus{}cpu\PYGZus{}var(counters);
         */
\PYGZcb{}

static void do\PYGZus{}test\PYGZus{}timer(unsigned long data)
\PYGZob{}
        int cpu;

        /* Increment the counters */
        on\PYGZus{}each\PYGZus{}cpu(test\PYGZus{}each, NULL, 1);
        /* Read all the counters */
        printk(\PYGZdq{}Counters read from CPU \PYGZpc{}d\PYGZbs{}n\PYGZdq{}, smp\PYGZus{}processor\PYGZus{}id());
        for\PYGZus{}each\PYGZus{}online\PYGZus{}cpu(cpu) \PYGZob{}
                printk(\PYGZdq{}Read : CPU \PYGZpc{}d, count \PYGZpc{}ld\PYGZbs{}n\PYGZdq{}, cpu,
                        local\PYGZus{}read(\PYGZam{}per\PYGZus{}cpu(counters, cpu)));
        \PYGZcb{}
        mod\PYGZus{}timer(\PYGZam{}test\PYGZus{}timer, jiffies + 1000);
\PYGZcb{}

static int \PYGZus{}\PYGZus{}init test\PYGZus{}init(void)
\PYGZob{}
        /* initialize the timer that will increment the counter */
        timer\PYGZus{}setup(\PYGZam{}test\PYGZus{}timer, do\PYGZus{}test\PYGZus{}timer, 0);
        mod\PYGZus{}timer(\PYGZam{}test\PYGZus{}timer, jiffies + 1);

        return 0;
\PYGZcb{}

static void \PYGZus{}\PYGZus{}exit test\PYGZus{}exit(void)
\PYGZob{}
        del\PYGZus{}timer\PYGZus{}sync(\PYGZam{}test\PYGZus{}timer);
\PYGZcb{}

module\PYGZus{}init(test\PYGZus{}init);
module\PYGZus{}exit(test\PYGZus{}exit);

MODULE\PYGZus{}LICENSE(\PYGZdq{}GPL\PYGZdq{});
MODULE\PYGZus{}AUTHOR(\PYGZdq{}Mathieu Desnoyers\PYGZdq{});
MODULE\PYGZus{}DESCRIPTION(\PYGZdq{}Local Atomic Ops\PYGZdq{});
\end{Verbatim}


\section{Concurrency Managed Workqueue (cmwq)}
\label{core-api/workqueue:concurrency-managed-workqueue-cmwq}\label{core-api/workqueue::doc}\begin{quote}\begin{description}
\item[{Date}] \leavevmode
September, 2010

\item[{Author}] \leavevmode
Tejun Heo \textless{}\href{mailto:tj@kernel.org}{tj@kernel.org}\textgreater{}

\item[{Author}] \leavevmode
Florian Mickler \textless{}\href{mailto:florian@mickler.org}{florian@mickler.org}\textgreater{}

\end{description}\end{quote}


\subsection{Introduction}
\label{core-api/workqueue:introduction}
There are many cases where an asynchronous process execution context
is needed and the workqueue (wq) API is the most commonly used
mechanism for such cases.

When such an asynchronous execution context is needed, a work item
describing which function to execute is put on a queue.  An
independent thread serves as the asynchronous execution context.  The
queue is called workqueue and the thread is called worker.

While there are work items on the workqueue the worker executes the
functions associated with the work items one after the other.  When
there is no work item left on the workqueue the worker becomes idle.
When a new work item gets queued, the worker begins executing again.


\subsection{Why cmwq?}
\label{core-api/workqueue:why-cmwq}
In the original wq implementation, a multi threaded (MT) wq had one
worker thread per CPU and a single threaded (ST) wq had one worker
thread system-wide.  A single MT wq needed to keep around the same
number of workers as the number of CPUs.  The kernel grew a lot of MT
wq users over the years and with the number of CPU cores continuously
rising, some systems saturated the default 32k PID space just booting
up.

Although MT wq wasted a lot of resource, the level of concurrency
provided was unsatisfactory.  The limitation was common to both ST and
MT wq albeit less severe on MT.  Each wq maintained its own separate
worker pool.  An MT wq could provide only one execution context per CPU
while an ST wq one for the whole system.  Work items had to compete for
those very limited execution contexts leading to various problems
including proneness to deadlocks around the single execution context.

The tension between the provided level of concurrency and resource
usage also forced its users to make unnecessary tradeoffs like libata
choosing to use ST wq for polling PIOs and accepting an unnecessary
limitation that no two polling PIOs can progress at the same time.  As
MT wq don't provide much better concurrency, users which require
higher level of concurrency, like async or fscache, had to implement
their own thread pool.

Concurrency Managed Workqueue (cmwq) is a reimplementation of wq with
focus on the following goals.
\begin{itemize}
\item {} 
Maintain compatibility with the original workqueue API.

\item {} 
Use per-CPU unified worker pools shared by all wq to provide
flexible level of concurrency on demand without wasting a lot of
resource.

\item {} 
Automatically regulate worker pool and level of concurrency so that
the API users don't need to worry about such details.

\end{itemize}


\subsection{The Design}
\label{core-api/workqueue:the-design}
In order to ease the asynchronous execution of functions a new
abstraction, the work item, is introduced.

A work item is a simple struct that holds a pointer to the function
that is to be executed asynchronously.  Whenever a driver or subsystem
wants a function to be executed asynchronously it has to set up a work
item pointing to that function and queue that work item on a
workqueue.

Special purpose threads, called worker threads, execute the functions
off of the queue, one after the other.  If no work is queued, the
worker threads become idle.  These worker threads are managed in so
called worker-pools.

The cmwq design differentiates between the user-facing workqueues that
subsystems and drivers queue work items on and the backend mechanism
which manages worker-pools and processes the queued work items.

There are two worker-pools, one for normal work items and the other
for high priority ones, for each possible CPU and some extra
worker-pools to serve work items queued on unbound workqueues - the
number of these backing pools is dynamic.

Subsystems and drivers can create and queue work items through special
workqueue API functions as they see fit. They can influence some
aspects of the way the work items are executed by setting flags on the
workqueue they are putting the work item on. These flags include
things like CPU locality, concurrency limits, priority and more.  To
get a detailed overview refer to the API description of
\code{alloc\_workqueue()} below.

When a work item is queued to a workqueue, the target worker-pool is
determined according to the queue parameters and workqueue attributes
and appended on the shared worklist of the worker-pool.  For example,
unless specifically overridden, a work item of a bound workqueue will
be queued on the worklist of either normal or highpri worker-pool that
is associated to the CPU the issuer is running on.

For any worker pool implementation, managing the concurrency level
(how many execution contexts are active) is an important issue.  cmwq
tries to keep the concurrency at a minimal but sufficient level.
Minimal to save resources and sufficient in that the system is used at
its full capacity.

Each worker-pool bound to an actual CPU implements concurrency
management by hooking into the scheduler.  The worker-pool is notified
whenever an active worker wakes up or sleeps and keeps track of the
number of the currently runnable workers.  Generally, work items are
not expected to hog a CPU and consume many cycles.  That means
maintaining just enough concurrency to prevent work processing from
stalling should be optimal.  As long as there are one or more runnable
workers on the CPU, the worker-pool doesn't start execution of a new
work, but, when the last running worker goes to sleep, it immediately
schedules a new worker so that the CPU doesn't sit idle while there
are pending work items.  This allows using a minimal number of workers
without losing execution bandwidth.

Keeping idle workers around doesn't cost other than the memory space
for kthreads, so cmwq holds onto idle ones for a while before killing
them.

For unbound workqueues, the number of backing pools is dynamic.
Unbound workqueue can be assigned custom attributes using
\code{apply\_workqueue\_attrs()} and workqueue will automatically create
backing worker pools matching the attributes.  The responsibility of
regulating concurrency level is on the users.  There is also a flag to
mark a bound wq to ignore the concurrency management.  Please refer to
the API section for details.

Forward progress guarantee relies on that workers can be created when
more execution contexts are necessary, which in turn is guaranteed
through the use of rescue workers.  All work items which might be used
on code paths that handle memory reclaim are required to be queued on
wq's that have a rescue-worker reserved for execution under memory
pressure.  Else it is possible that the worker-pool deadlocks waiting
for execution contexts to free up.


\subsection{Application Programming Interface (API)}
\label{core-api/workqueue:application-programming-interface-api}
\code{alloc\_workqueue()} allocates a wq.  The original
\code{create\_*workqueue()} functions are deprecated and scheduled for
removal.  \code{alloc\_workqueue()} takes three arguments - \code{@name},
\code{@flags} and \code{@max\_active}.  \code{@name} is the name of the wq and
also used as the name of the rescuer thread if there is one.

A wq no longer manages execution resources but serves as a domain for
forward progress guarantee, flush and work item attributes. \code{@flags}
and \code{@max\_active} control how work items are assigned execution
resources, scheduled and executed.


\subsubsection{\texttt{flags}}
\label{core-api/workqueue:flags}\begin{description}
\item[{\code{WQ\_UNBOUND}}] \leavevmode
Work items queued to an unbound wq are served by the special
worker-pools which host workers which are not bound to any
specific CPU.  This makes the wq behave as a simple execution
context provider without concurrency management.  The unbound
worker-pools try to start execution of work items as soon as
possible.  Unbound wq sacrifices locality but is useful for
the following cases.
\begin{itemize}
\item {} 
Wide fluctuation in the concurrency level requirement is
expected and using bound wq may end up creating large number
of mostly unused workers across different CPUs as the issuer
hops through different CPUs.

\item {} 
Long running CPU intensive workloads which can be better
managed by the system scheduler.

\end{itemize}

\item[{\code{WQ\_FREEZABLE}}] \leavevmode
A freezable wq participates in the freeze phase of the system
suspend operations.  Work items on the wq are drained and no
new work item starts execution until thawed.

\item[{\code{WQ\_MEM\_RECLAIM}}] \leavevmode
All wq which might be used in the memory reclaim paths \textbf{MUST}
have this flag set.  The wq is guaranteed to have at least one
execution context regardless of memory pressure.

\item[{\code{WQ\_HIGHPRI}}] \leavevmode
Work items of a highpri wq are queued to the highpri
worker-pool of the target cpu.  Highpri worker-pools are
served by worker threads with elevated nice level.

Note that normal and highpri worker-pools don't interact with
each other.  Each maintains its separate pool of workers and
implements concurrency management among its workers.

\item[{\code{WQ\_CPU\_INTENSIVE}}] \leavevmode
Work items of a CPU intensive wq do not contribute to the
concurrency level.  In other words, runnable CPU intensive
work items will not prevent other work items in the same
worker-pool from starting execution.  This is useful for bound
work items which are expected to hog CPU cycles so that their
execution is regulated by the system scheduler.

Although CPU intensive work items don't contribute to the
concurrency level, start of their executions is still
regulated by the concurrency management and runnable
non-CPU-intensive work items can delay execution of CPU
intensive work items.

This flag is meaningless for unbound wq.

\end{description}

Note that the flag \code{WQ\_NON\_REENTRANT} no longer exists as all
workqueues are now non-reentrant - any work item is guaranteed to be
executed by at most one worker system-wide at any given time.


\subsubsection{\texttt{max\_active}}
\label{core-api/workqueue:max-active}
\code{@max\_active} determines the maximum number of execution contexts
per CPU which can be assigned to the work items of a wq.  For example,
with \code{@max\_active} of 16, at most 16 work items of the wq can be
executing at the same time per CPU.

Currently, for a bound wq, the maximum limit for \code{@max\_active} is
512 and the default value used when 0 is specified is 256.  For an
unbound wq, the limit is higher of 512 and 4 *
\code{num\_possible\_cpus()}.  These values are chosen sufficiently high
such that they are not the limiting factor while providing protection
in runaway cases.

The number of active work items of a wq is usually regulated by the
users of the wq, more specifically, by how many work items the users
may queue at the same time.  Unless there is a specific need for
throttling the number of active work items, specifying `0' is
recommended.

Some users depend on the strict execution ordering of ST wq.  The
combination of \code{@max\_active} of 1 and \code{WQ\_UNBOUND} used to
achieve this behavior.  Work items on such wq were always queued to the
unbound worker-pools and only one work item could be active at any given
time thus achieving the same ordering property as ST wq.

In the current implementation the above configuration only guarantees
ST behavior within a given NUMA node. Instead \code{alloc\_ordered\_queue()} should
be used to achieve system-wide ST behavior.


\subsection{Example Execution Scenarios}
\label{core-api/workqueue:example-execution-scenarios}
The following example execution scenarios try to illustrate how cmwq
behave under different configurations.
\begin{quote}

Work items w0, w1, w2 are queued to a bound wq q0 on the same CPU.
w0 burns CPU for 5ms then sleeps for 10ms then burns CPU for 5ms
again before finishing.  w1 and w2 burn CPU for 5ms then sleep for
10ms.
\end{quote}

Ignoring all other tasks, works and processing overhead, and assuming
simple FIFO scheduling, the following is one highly simplified version
of possible sequences of events with the original wq.

\begin{Verbatim}[commandchars=\\\{\}]
TIME IN MSECS  EVENT
0              w0 starts and burns CPU
5              w0 sleeps
15             w0 wakes up and burns CPU
20             w0 finishes
20             w1 starts and burns CPU
25             w1 sleeps
35             w1 wakes up and finishes
35             w2 starts and burns CPU
40             w2 sleeps
50             w2 wakes up and finishes
\end{Verbatim}

And with cmwq with \code{@max\_active} \textgreater{}= 3,

\begin{Verbatim}[commandchars=\\\{\}]
TIME IN MSECS  EVENT
0              w0 starts and burns CPU
5              w0 sleeps
5              w1 starts and burns CPU
10             w1 sleeps
10             w2 starts and burns CPU
15             w2 sleeps
15             w0 wakes up and burns CPU
20             w0 finishes
20             w1 wakes up and finishes
25             w2 wakes up and finishes
\end{Verbatim}

If \code{@max\_active} == 2,

\begin{Verbatim}[commandchars=\\\{\}]
TIME IN MSECS  EVENT
0              w0 starts and burns CPU
5              w0 sleeps
5              w1 starts and burns CPU
10             w1 sleeps
15             w0 wakes up and burns CPU
20             w0 finishes
20             w1 wakes up and finishes
20             w2 starts and burns CPU
25             w2 sleeps
35             w2 wakes up and finishes
\end{Verbatim}

Now, let's assume w1 and w2 are queued to a different wq q1 which has
\code{WQ\_CPU\_INTENSIVE} set,

\begin{Verbatim}[commandchars=\\\{\}]
TIME IN MSECS  EVENT
0              w0 starts and burns CPU
5              w0 sleeps
5              w1 and w2 start and burn CPU
10             w1 sleeps
15             w2 sleeps
15             w0 wakes up and burns CPU
20             w0 finishes
20             w1 wakes up and finishes
25             w2 wakes up and finishes
\end{Verbatim}


\subsection{Guidelines}
\label{core-api/workqueue:guidelines}\begin{itemize}
\item {} 
Do not forget to use \code{WQ\_MEM\_RECLAIM} if a wq may process work
items which are used during memory reclaim.  Each wq with
\code{WQ\_MEM\_RECLAIM} set has an execution context reserved for it.  If
there is dependency among multiple work items used during memory
reclaim, they should be queued to separate wq each with
\code{WQ\_MEM\_RECLAIM}.

\item {} 
Unless strict ordering is required, there is no need to use ST wq.

\item {} 
Unless there is a specific need, using 0 for @max\_active is
recommended.  In most use cases, concurrency level usually stays
well under the default limit.

\item {} 
A wq serves as a domain for forward progress guarantee
(\code{WQ\_MEM\_RECLAIM}, flush and work item attributes.  Work items
which are not involved in memory reclaim and don't need to be
flushed as a part of a group of work items, and don't require any
special attribute, can use one of the system wq.  There is no
difference in execution characteristics between using a dedicated wq
and a system wq.

\item {} 
Unless work items are expected to consume a huge amount of CPU
cycles, using a bound wq is usually beneficial due to the increased
level of locality in wq operations and work item execution.

\end{itemize}


\subsection{Debugging}
\label{core-api/workqueue:debugging}
Because the work functions are executed by generic worker threads
there are a few tricks needed to shed some light on misbehaving
workqueue users.

Worker threads show up in the process list as:

\begin{Verbatim}[commandchars=\\\{\}]
root      5671  0.0  0.0      0     0 ?        S    12:07   0:00 [kworker/0:1]
root      5672  0.0  0.0      0     0 ?        S    12:07   0:00 [kworker/1:2]
root      5673  0.0  0.0      0     0 ?        S    12:12   0:00 [kworker/0:0]
root      5674  0.0  0.0      0     0 ?        S    12:13   0:00 [kworker/1:0]
\end{Verbatim}

If kworkers are going crazy (using too much cpu), there are two types
of possible problems:
\begin{enumerate}
\item {} 
Something being scheduled in rapid succession

\item {} 
A single work item that consumes lots of cpu cycles

\end{enumerate}

The first one can be tracked using tracing:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} echo workqueue:workqueue\PYGZus{}queue\PYGZus{}work \PYGZgt{} /sys/kernel/debug/tracing/set\PYGZus{}event
\PYGZdl{} cat /sys/kernel/debug/tracing/trace\PYGZus{}pipe \PYGZgt{} out.txt
(wait a few secs)
\PYGZca{}C
\end{Verbatim}

If something is busy looping on work queueing, it would be dominating
the output and the offender can be determined with the work item
function.

For the second type of problems it should be possible to just check
the stack trace of the offending worker thread.

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} cat /proc/THE\PYGZus{}OFFENDING\PYGZus{}KWORKER/stack
\end{Verbatim}

The work item's function should be trivially visible in the stack
trace.


\subsection{Kernel Inline Documentations Reference}
\label{core-api/workqueue:kernel-inline-documentations-reference}\index{workqueue\_attrs (C type)}

\begin{fulllineitems}
\phantomsection\label{core-api/workqueue:c.workqueue_attrs}\pysigline{struct \bfcode{workqueue\_attrs}}
A struct for workqueue attributes.

\end{fulllineitems}


\textbf{Definition}

\begin{Verbatim}[commandchars=\\\{\}]
struct workqueue\PYGZus{}attrs \PYGZob{}
  int nice;
  cpumask\PYGZus{}var\PYGZus{}t cpumask;
  bool no\PYGZus{}numa;
\PYGZcb{};
\end{Verbatim}

\textbf{Members}
\begin{description}
\item[{\code{nice}}] \leavevmode
nice level

\item[{\code{cpumask}}] \leavevmode
allowed CPUs

\item[{\code{no\_numa}}] \leavevmode
disable NUMA affinity

Unlike other fields, \code{no\_numa} isn't a property of a worker\_pool. It
only modifies how \code{apply\_workqueue\_attrs()} select pools and thus
doesn't participate in pool hash calculations or equality comparisons.

\end{description}

\textbf{Description}

This can be used to change attributes of an unbound workqueue.
\index{work\_pending (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/workqueue:c.work_pending}\pysiglinewithargsret{\bfcode{work\_pending}}{\emph{work}}{}
Find out whether a work item is currently pending

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{work}}] \leavevmode
The work item in question

\end{description}
\index{delayed\_work\_pending (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/workqueue:c.delayed_work_pending}\pysiglinewithargsret{\bfcode{delayed\_work\_pending}}{\emph{w}}{}
Find out whether a delayable work item is currently pending

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{w}}] \leavevmode
The work item in question

\end{description}
\index{alloc\_workqueue (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/workqueue:c.alloc_workqueue}\pysiglinewithargsret{\bfcode{alloc\_workqueue}}{\emph{fmt}, \emph{flags}, \emph{max\_active}, \emph{args...}}{}
allocate a workqueue

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{fmt}}] \leavevmode
printf format for the name of the workqueue

\item[{\code{flags}}] \leavevmode
WQ\_* flags

\item[{\code{max\_active}}] \leavevmode
max in-flight work items, 0 for default

\item[{\code{args...}}] \leavevmode
args for \textbf{fmt}

\end{description}

\textbf{Description}

Allocate a workqueue with the specified parameters.  For detailed
information on WQ\_* flags, please refer to
Documentation/core-api/workqueue.rst.

The \_\_lock\_name macro dance is to guarantee that single lock\_class\_key
doesn't end up with different namesm, which isn't allowed by lockdep.

\textbf{Return}

Pointer to the allocated workqueue on success, \code{NULL} on failure.
\index{alloc\_ordered\_workqueue (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/workqueue:c.alloc_ordered_workqueue}\pysiglinewithargsret{\bfcode{alloc\_ordered\_workqueue}}{\emph{fmt}, \emph{flags}, \emph{args...}}{}
allocate an ordered workqueue

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{fmt}}] \leavevmode
printf format for the name of the workqueue

\item[{\code{flags}}] \leavevmode
WQ\_* flags (only WQ\_FREEZABLE and WQ\_MEM\_RECLAIM are meaningful)

\item[{\code{args...}}] \leavevmode
args for \textbf{fmt}

\end{description}

\textbf{Description}

Allocate an ordered workqueue.  An ordered workqueue executes at
most one work item at any given time in the queued order.  They are
implemented as unbound workqueues with \textbf{max\_active} of one.

\textbf{Return}

Pointer to the allocated workqueue on success, \code{NULL} on failure.
\index{queue\_work (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/workqueue:c.queue_work}\pysiglinewithargsret{bool \bfcode{queue\_work}}{struct workqueue\_struct *\emph{ wq}, struct work\_struct *\emph{ work}}{}
queue work on a workqueue

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct workqueue\_struct * wq}}] \leavevmode
workqueue to use

\item[{\code{struct work\_struct * work}}] \leavevmode
work to queue

\end{description}

\textbf{Description}

Returns \code{false} if \textbf{work} was already on a queue, \code{true} otherwise.

We queue the work to the CPU on which it was submitted, but if the CPU dies
it can be processed by another CPU.
\index{queue\_delayed\_work (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/workqueue:c.queue_delayed_work}\pysiglinewithargsret{bool \bfcode{queue\_delayed\_work}}{struct workqueue\_struct *\emph{ wq}, struct delayed\_work *\emph{ dwork}, unsigned long\emph{ delay}}{}
queue work on a workqueue after delay

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct workqueue\_struct * wq}}] \leavevmode
workqueue to use

\item[{\code{struct delayed\_work * dwork}}] \leavevmode
delayable work to queue

\item[{\code{unsigned long delay}}] \leavevmode
number of jiffies to wait before queueing

\end{description}

\textbf{Description}

Equivalent to \code{queue\_delayed\_work\_on()} but tries to use the local CPU.
\index{mod\_delayed\_work (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/workqueue:c.mod_delayed_work}\pysiglinewithargsret{bool \bfcode{mod\_delayed\_work}}{struct workqueue\_struct *\emph{ wq}, struct delayed\_work *\emph{ dwork}, unsigned long\emph{ delay}}{}
modify delay of or queue a delayed work

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct workqueue\_struct * wq}}] \leavevmode
workqueue to use

\item[{\code{struct delayed\_work * dwork}}] \leavevmode
work to queue

\item[{\code{unsigned long delay}}] \leavevmode
number of jiffies to wait before queueing

\end{description}

\textbf{Description}

\code{mod\_delayed\_work\_on()} on local CPU.
\index{schedule\_work\_on (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/workqueue:c.schedule_work_on}\pysiglinewithargsret{bool \bfcode{schedule\_work\_on}}{int\emph{ cpu}, struct work\_struct *\emph{ work}}{}
put work task on a specific cpu

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{int cpu}}] \leavevmode
cpu to put the work task on

\item[{\code{struct work\_struct * work}}] \leavevmode
job to be done

\end{description}

\textbf{Description}

This puts a job on a specific cpu
\index{schedule\_work (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/workqueue:c.schedule_work}\pysiglinewithargsret{bool \bfcode{schedule\_work}}{struct work\_struct *\emph{ work}}{}
put work task in global workqueue

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct work\_struct * work}}] \leavevmode
job to be done

\end{description}

\textbf{Description}

Returns \code{false} if \textbf{work} was already on the kernel-global workqueue and
\code{true} otherwise.

This puts a job in the kernel-global workqueue if it was not already
queued and leaves it in the same position on the kernel-global
workqueue otherwise.
\index{flush\_scheduled\_work (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/workqueue:c.flush_scheduled_work}\pysiglinewithargsret{void \bfcode{flush\_scheduled\_work}}{void}{}
ensure that any scheduled work has run to completion.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}

Forces execution of the kernel-global workqueue and blocks until its
completion.

Think twice before calling this function!  It's very easy to get into
trouble if you don't take great care.  Either of the following situations
will lead to deadlock:
\begin{quote}

One of the work items currently on the workqueue needs to acquire
a lock held by your code or its caller.

Your code is running in the context of a work routine.
\end{quote}

They will be detected by lockdep when they occur, but the first might not
occur very often.  It depends on what work items are on the workqueue and
what locks they need, which you have no control over.

In most situations flushing the entire workqueue is overkill; you merely
need to know that a particular work item isn't queued and isn't running.
In such cases you should use \code{cancel\_delayed\_work\_sync()} or
\code{cancel\_work\_sync()} instead.
\index{schedule\_delayed\_work\_on (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/workqueue:c.schedule_delayed_work_on}\pysiglinewithargsret{bool \bfcode{schedule\_delayed\_work\_on}}{int\emph{ cpu}, struct delayed\_work *\emph{ dwork}, unsigned long\emph{ delay}}{}
queue work in global workqueue on CPU after delay

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{int cpu}}] \leavevmode
cpu to use

\item[{\code{struct delayed\_work * dwork}}] \leavevmode
job to be done

\item[{\code{unsigned long delay}}] \leavevmode
number of jiffies to wait

\end{description}

\textbf{Description}

After waiting for a given time this puts a job in the kernel-global
workqueue on the specified CPU.
\index{schedule\_delayed\_work (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/workqueue:c.schedule_delayed_work}\pysiglinewithargsret{bool \bfcode{schedule\_delayed\_work}}{struct delayed\_work *\emph{ dwork}, unsigned long\emph{ delay}}{}
put work task in global workqueue after delay

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct delayed\_work * dwork}}] \leavevmode
job to be done

\item[{\code{unsigned long delay}}] \leavevmode
number of jiffies to wait or 0 for immediate execution

\end{description}

\textbf{Description}

After waiting for a given time this puts a job in the kernel-global
workqueue.


\section{Linux generic IRQ handling}
\label{core-api/genericirq:linux-generic-irq-handling}\label{core-api/genericirq::doc}\begin{quote}\begin{description}
\item[{Copyright}] \leavevmode
© 2005-2010: Thomas Gleixner

\item[{Copyright}] \leavevmode
© 2005-2006:  Ingo Molnar

\end{description}\end{quote}


\subsection{Introduction}
\label{core-api/genericirq:introduction}
The generic interrupt handling layer is designed to provide a complete
abstraction of interrupt handling for device drivers. It is able to
handle all the different types of interrupt controller hardware. Device
drivers use generic API functions to request, enable, disable and free
interrupts. The drivers do not have to know anything about interrupt
hardware details, so they can be used on different platforms without
code changes.

This documentation is provided to developers who want to implement an
interrupt subsystem based for their architecture, with the help of the
generic IRQ handling layer.


\subsection{Rationale}
\label{core-api/genericirq:rationale}
The original implementation of interrupt handling in Linux uses the
\code{\_\_do\_IRQ()} super-handler, which is able to deal with every type of
interrupt logic.

Originally, Russell King identified different types of handlers to build
a quite universal set for the ARM interrupt handler implementation in
Linux 2.5/2.6. He distinguished between:
\begin{itemize}
\item {} 
Level type

\item {} 
Edge type

\item {} 
Simple type

\end{itemize}

During the implementation we identified another type:
\begin{itemize}
\item {} 
Fast EOI type

\end{itemize}

In the SMP world of the \code{\_\_do\_IRQ()} super-handler another type was
identified:
\begin{itemize}
\item {} 
Per CPU type

\end{itemize}

This split implementation of high-level IRQ handlers allows us to
optimize the flow of the interrupt handling for each specific interrupt
type. This reduces complexity in that particular code path and allows
the optimized handling of a given type.

The original general IRQ implementation used hw\_interrupt\_type
structures and their \code{-\textgreater{}ack}, \code{-\textgreater{}end} {[}etc.{]} callbacks to differentiate
the flow control in the super-handler. This leads to a mix of flow logic
and low-level hardware logic, and it also leads to unnecessary code
duplication: for example in i386, there is an \code{ioapic\_level\_irq} and an
\code{ioapic\_edge\_irq} IRQ-type which share many of the low-level details but
have different flow handling.

A more natural abstraction is the clean separation of the `irq flow' and
the `chip details'.

Analysing a couple of architecture's IRQ subsystem implementations
reveals that most of them can use a generic set of `irq flow' methods
and only need to add the chip-level specific code. The separation is
also valuable for (sub)architectures which need specific quirks in the
IRQ flow itself but not in the chip details - and thus provides a more
transparent IRQ subsystem design.

Each interrupt descriptor is assigned its own high-level flow handler,
which is normally one of the generic implementations. (This high-level
flow handler implementation also makes it simple to provide
demultiplexing handlers which can be found in embedded platforms on
various architectures.)

The separation makes the generic interrupt handling layer more flexible
and extensible. For example, an (sub)architecture can use a generic
IRQ-flow implementation for `level type' interrupts and add a
(sub)architecture specific `edge type' implementation.

To make the transition to the new model easier and prevent the breakage
of existing implementations, the \code{\_\_do\_IRQ()} super-handler is still
available. This leads to a kind of duality for the time being. Over time
the new model should be used in more and more architectures, as it
enables smaller and cleaner IRQ subsystems. It's deprecated for three
years now and about to be removed.


\subsection{Known Bugs And Assumptions}
\label{core-api/genericirq:known-bugs-and-assumptions}
None (knock on wood).


\subsection{Abstraction layers}
\label{core-api/genericirq:abstraction-layers}
There are three main levels of abstraction in the interrupt code:
\begin{enumerate}
\item {} 
High-level driver API

\item {} 
High-level IRQ flow handlers

\item {} 
Chip-level hardware encapsulation

\end{enumerate}


\subsubsection{Interrupt control flow}
\label{core-api/genericirq:interrupt-control-flow}
Each interrupt is described by an interrupt descriptor structure
irq\_desc. The interrupt is referenced by an `unsigned int' numeric
value which selects the corresponding interrupt description structure in
the descriptor structures array. The descriptor structure contains
status information and pointers to the interrupt flow method and the
interrupt chip structure which are assigned to this interrupt.

Whenever an interrupt triggers, the low-level architecture code calls
into the generic interrupt code by calling \code{desc-\textgreater{}handle\_irq()}. This
high-level IRQ handling function only uses desc-\textgreater{}irq\_data.chip
primitives referenced by the assigned chip descriptor structure.


\subsubsection{High-level Driver API}
\label{core-api/genericirq:high-level-driver-api}
The high-level Driver API consists of following functions:
\begin{itemize}
\item {} 
\code{request\_irq()}

\item {} 
{\hyperref[core\string-api/kernel\string-api:c.free_irq]{\emph{\code{free\_irq()}}}}

\item {} 
{\hyperref[core\string-api/kernel\string-api:c.disable_irq]{\emph{\code{disable\_irq()}}}}

\item {} 
{\hyperref[core\string-api/kernel\string-api:c.enable_irq]{\emph{\code{enable\_irq()}}}}

\item {} 
{\hyperref[core\string-api/kernel\string-api:c.disable_irq_nosync]{\emph{\code{disable\_irq\_nosync()}}}} (SMP only)

\item {} 
{\hyperref[core\string-api/kernel\string-api:c.synchronize_irq]{\emph{\code{synchronize\_irq()}}}} (SMP only)

\item {} 
{\hyperref[core\string-api/genericirq:c.irq_set_irq_type]{\emph{\code{irq\_set\_irq\_type()}}}}

\item {} 
{\hyperref[core\string-api/kernel\string-api:c.irq_set_irq_wake]{\emph{\code{irq\_set\_irq\_wake()}}}}

\item {} 
{\hyperref[core\string-api/genericirq:c.irq_set_handler_data]{\emph{\code{irq\_set\_handler\_data()}}}}

\item {} 
{\hyperref[core\string-api/genericirq:c.irq_set_chip]{\emph{\code{irq\_set\_chip()}}}}

\item {} 
{\hyperref[core\string-api/genericirq:c.irq_set_chip_data]{\emph{\code{irq\_set\_chip\_data()}}}}

\end{itemize}

See the autogenerated function documentation for details.


\subsubsection{High-level IRQ flow handlers}
\label{core-api/genericirq:high-level-irq-flow-handlers}
The generic layer provides a set of pre-defined irq-flow methods:
\begin{itemize}
\item {} 
{\hyperref[core\string-api/genericirq:c.handle_level_irq]{\emph{\code{handle\_level\_irq()}}}}

\item {} 
{\hyperref[core\string-api/genericirq:c.handle_edge_irq]{\emph{\code{handle\_edge\_irq()}}}}

\item {} 
{\hyperref[core\string-api/genericirq:c.handle_fasteoi_irq]{\emph{\code{handle\_fasteoi\_irq()}}}}

\item {} 
{\hyperref[core\string-api/genericirq:c.handle_simple_irq]{\emph{\code{handle\_simple\_irq()}}}}

\item {} 
{\hyperref[core\string-api/genericirq:c.handle_percpu_irq]{\emph{\code{handle\_percpu\_irq()}}}}

\item {} 
{\hyperref[core\string-api/genericirq:c.handle_edge_eoi_irq]{\emph{\code{handle\_edge\_eoi\_irq()}}}}

\item {} 
{\hyperref[core\string-api/genericirq:c.handle_bad_irq]{\emph{\code{handle\_bad\_irq()}}}}

\end{itemize}

The interrupt flow handlers (either pre-defined or architecture
specific) are assigned to specific interrupts by the architecture either
during bootup or during device initialization.


\paragraph{Default flow implementations}
\label{core-api/genericirq:default-flow-implementations}

\subparagraph{Helper functions}
\label{core-api/genericirq:helper-functions}
The helper functions call the chip primitives and are used by the
default flow implementations. The following helper functions are
implemented (simplified excerpt):

\begin{Verbatim}[commandchars=\\\{\}]
default\PYGZus{}enable(struct irq\PYGZus{}data *data)
\PYGZob{}
    desc\PYGZhy{}\PYGZgt{}irq\PYGZus{}data.chip\PYGZhy{}\PYGZgt{}irq\PYGZus{}unmask(data);
\PYGZcb{}

default\PYGZus{}disable(struct irq\PYGZus{}data *data)
\PYGZob{}
    if (!delay\PYGZus{}disable(data))
        desc\PYGZhy{}\PYGZgt{}irq\PYGZus{}data.chip\PYGZhy{}\PYGZgt{}irq\PYGZus{}mask(data);
\PYGZcb{}

default\PYGZus{}ack(struct irq\PYGZus{}data *data)
\PYGZob{}
    chip\PYGZhy{}\PYGZgt{}irq\PYGZus{}ack(data);
\PYGZcb{}

default\PYGZus{}mask\PYGZus{}ack(struct irq\PYGZus{}data *data)
\PYGZob{}
    if (chip\PYGZhy{}\PYGZgt{}irq\PYGZus{}mask\PYGZus{}ack) \PYGZob{}
        chip\PYGZhy{}\PYGZgt{}irq\PYGZus{}mask\PYGZus{}ack(data);
    \PYGZcb{} else \PYGZob{}
        chip\PYGZhy{}\PYGZgt{}irq\PYGZus{}mask(data);
        chip\PYGZhy{}\PYGZgt{}irq\PYGZus{}ack(data);
    \PYGZcb{}
\PYGZcb{}

noop(struct irq\PYGZus{}data *data))
\PYGZob{}
\PYGZcb{}
\end{Verbatim}


\paragraph{Default flow handler implementations}
\label{core-api/genericirq:default-flow-handler-implementations}

\subparagraph{Default Level IRQ flow handler}
\label{core-api/genericirq:default-level-irq-flow-handler}
handle\_level\_irq provides a generic implementation for level-triggered
interrupts.

The following control flow is implemented (simplified excerpt):

\begin{Verbatim}[commandchars=\\\{\}]
desc\PYGZhy{}\PYGZgt{}irq\PYGZus{}data.chip\PYGZhy{}\PYGZgt{}irq\PYGZus{}mask\PYGZus{}ack();
handle\PYGZus{}irq\PYGZus{}event(desc\PYGZhy{}\PYGZgt{}action);
desc\PYGZhy{}\PYGZgt{}irq\PYGZus{}data.chip\PYGZhy{}\PYGZgt{}irq\PYGZus{}unmask();
\end{Verbatim}


\subparagraph{Default Fast EOI IRQ flow handler}
\label{core-api/genericirq:default-fast-eoi-irq-flow-handler}
handle\_fasteoi\_irq provides a generic implementation for interrupts,
which only need an EOI at the end of the handler.

The following control flow is implemented (simplified excerpt):

\begin{Verbatim}[commandchars=\\\{\}]
handle\PYGZus{}irq\PYGZus{}event(desc\PYGZhy{}\PYGZgt{}action);
desc\PYGZhy{}\PYGZgt{}irq\PYGZus{}data.chip\PYGZhy{}\PYGZgt{}irq\PYGZus{}eoi();
\end{Verbatim}


\subparagraph{Default Edge IRQ flow handler}
\label{core-api/genericirq:default-edge-irq-flow-handler}
handle\_edge\_irq provides a generic implementation for edge-triggered
interrupts.

The following control flow is implemented (simplified excerpt):

\begin{Verbatim}[commandchars=\\\{\}]
if (desc\PYGZhy{}\PYGZgt{}status \PYGZam{} running) \PYGZob{}
    desc\PYGZhy{}\PYGZgt{}irq\PYGZus{}data.chip\PYGZhy{}\PYGZgt{}irq\PYGZus{}mask\PYGZus{}ack();
    desc\PYGZhy{}\PYGZgt{}status \textbar{}= pending \textbar{} masked;
    return;
\PYGZcb{}
desc\PYGZhy{}\PYGZgt{}irq\PYGZus{}data.chip\PYGZhy{}\PYGZgt{}irq\PYGZus{}ack();
desc\PYGZhy{}\PYGZgt{}status \textbar{}= running;
do \PYGZob{}
    if (desc\PYGZhy{}\PYGZgt{}status \PYGZam{} masked)
        desc\PYGZhy{}\PYGZgt{}irq\PYGZus{}data.chip\PYGZhy{}\PYGZgt{}irq\PYGZus{}unmask();
    desc\PYGZhy{}\PYGZgt{}status \PYGZam{}= \PYGZti{}pending;
    handle\PYGZus{}irq\PYGZus{}event(desc\PYGZhy{}\PYGZgt{}action);
\PYGZcb{} while (status \PYGZam{} pending);
desc\PYGZhy{}\PYGZgt{}status \PYGZam{}= \PYGZti{}running;
\end{Verbatim}


\subparagraph{Default simple IRQ flow handler}
\label{core-api/genericirq:default-simple-irq-flow-handler}
handle\_simple\_irq provides a generic implementation for simple
interrupts.

\begin{notice}{note}{Note:}
The simple flow handler does not call any handler/chip primitives.
\end{notice}

The following control flow is implemented (simplified excerpt):

\begin{Verbatim}[commandchars=\\\{\}]
handle\PYGZus{}irq\PYGZus{}event(desc\PYGZhy{}\PYGZgt{}action);
\end{Verbatim}


\subparagraph{Default per CPU flow handler}
\label{core-api/genericirq:default-per-cpu-flow-handler}
handle\_percpu\_irq provides a generic implementation for per CPU
interrupts.

Per CPU interrupts are only available on SMP and the handler provides a
simplified version without locking.

The following control flow is implemented (simplified excerpt):

\begin{Verbatim}[commandchars=\\\{\}]
if (desc\PYGZhy{}\PYGZgt{}irq\PYGZus{}data.chip\PYGZhy{}\PYGZgt{}irq\PYGZus{}ack)
    desc\PYGZhy{}\PYGZgt{}irq\PYGZus{}data.chip\PYGZhy{}\PYGZgt{}irq\PYGZus{}ack();
handle\PYGZus{}irq\PYGZus{}event(desc\PYGZhy{}\PYGZgt{}action);
if (desc\PYGZhy{}\PYGZgt{}irq\PYGZus{}data.chip\PYGZhy{}\PYGZgt{}irq\PYGZus{}eoi)
    desc\PYGZhy{}\PYGZgt{}irq\PYGZus{}data.chip\PYGZhy{}\PYGZgt{}irq\PYGZus{}eoi();
\end{Verbatim}


\subparagraph{EOI Edge IRQ flow handler}
\label{core-api/genericirq:eoi-edge-irq-flow-handler}
handle\_edge\_eoi\_irq provides an abnomination of the edge handler
which is solely used to tame a badly wreckaged irq controller on
powerpc/cell.


\subparagraph{Bad IRQ flow handler}
\label{core-api/genericirq:bad-irq-flow-handler}
handle\_bad\_irq is used for spurious interrupts which have no real
handler assigned..


\paragraph{Quirks and optimizations}
\label{core-api/genericirq:quirks-and-optimizations}
The generic functions are intended for `clean' architectures and chips,
which have no platform-specific IRQ handling quirks. If an architecture
needs to implement quirks on the `flow' level then it can do so by
overriding the high-level irq-flow handler.


\paragraph{Delayed interrupt disable}
\label{core-api/genericirq:delayed-interrupt-disable}
This per interrupt selectable feature, which was introduced by Russell
King in the ARM interrupt implementation, does not mask an interrupt at
the hardware level when {\hyperref[core\string-api/kernel\string-api:c.disable_irq]{\emph{\code{disable\_irq()}}}} is called. The interrupt is kept
enabled and is masked in the flow handler when an interrupt event
happens. This prevents losing edge interrupts on hardware which does not
store an edge interrupt event while the interrupt is disabled at the
hardware level. When an interrupt arrives while the IRQ\_DISABLED flag
is set, then the interrupt is masked at the hardware level and the
IRQ\_PENDING bit is set. When the interrupt is re-enabled by
{\hyperref[core\string-api/kernel\string-api:c.enable_irq]{\emph{\code{enable\_irq()}}}} the pending bit is checked and if it is set, the interrupt
is resent either via hardware or by a software resend mechanism. (It's
necessary to enable CONFIG\_HARDIRQS\_SW\_RESEND when you want to use
the delayed interrupt disable feature and your hardware is not capable
of retriggering an interrupt.) The delayed interrupt disable is not
configurable.


\subsubsection{Chip-level hardware encapsulation}
\label{core-api/genericirq:chip-level-hardware-encapsulation}
The chip-level hardware descriptor structure {\hyperref[core\string-api/genericirq:c.irq_chip]{\emph{\code{irq\_chip}}}} contains all
the direct chip relevant functions, which can be utilized by the irq flow
implementations.
\begin{itemize}
\item {} 
\code{irq\_ack}

\item {} 
\code{irq\_mask\_ack} - Optional, recommended for performance

\item {} 
\code{irq\_mask}

\item {} 
\code{irq\_unmask}

\item {} 
\code{irq\_eoi} - Optional, required for EOI flow handlers

\item {} 
\code{irq\_retrigger} - Optional

\item {} 
\code{irq\_set\_type} - Optional

\item {} 
\code{irq\_set\_wake} - Optional

\end{itemize}

These primitives are strictly intended to mean what they say: ack means
ACK, masking means masking of an IRQ line, etc. It is up to the flow
handler(s) to use these basic units of low-level functionality.


\subsection{\_\_do\_IRQ entry point}
\label{core-api/genericirq:do-irq-entry-point}
The original implementation \code{\_\_do\_IRQ()} was an alternative entry point
for all types of interrupts. It no longer exists.

This handler turned out to be not suitable for all interrupt hardware
and was therefore reimplemented with split functionality for
edge/level/simple/percpu interrupts. This is not only a functional
optimization. It also shortens code paths for interrupts.


\subsection{Locking on SMP}
\label{core-api/genericirq:locking-on-smp}
The locking of chip registers is up to the architecture that defines the
chip primitives. The per-irq structure is protected via desc-\textgreater{}lock, by
the generic layer.


\subsection{Generic interrupt chip}
\label{core-api/genericirq:generic-interrupt-chip}
To avoid copies of identical implementations of IRQ chips the core
provides a configurable generic interrupt chip implementation.
Developers should check carefully whether the generic chip fits their
needs before implementing the same functionality slightly differently
themselves.
\index{irq\_gc\_mask\_set\_bit (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_gc_mask_set_bit}\pysiglinewithargsret{void \bfcode{irq\_gc\_mask\_set\_bit}}{struct {\hyperref[core\string-api/genericirq:c.irq_data]{\emph{irq\_data}}} *\emph{ d}}{}
Mask chip via setting bit in mask register

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_data * d}}] \leavevmode
irq\_data

\end{description}

\textbf{Description}

Chip has a single mask register. Values of this register are cached
and protected by gc-\textgreater{}lock
\index{irq\_gc\_mask\_clr\_bit (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_gc_mask_clr_bit}\pysiglinewithargsret{void \bfcode{irq\_gc\_mask\_clr\_bit}}{struct {\hyperref[core\string-api/genericirq:c.irq_data]{\emph{irq\_data}}} *\emph{ d}}{}
Mask chip via clearing bit in mask register

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_data * d}}] \leavevmode
irq\_data

\end{description}

\textbf{Description}

Chip has a single mask register. Values of this register are cached
and protected by gc-\textgreater{}lock
\index{irq\_gc\_ack\_set\_bit (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_gc_ack_set_bit}\pysiglinewithargsret{void \bfcode{irq\_gc\_ack\_set\_bit}}{struct {\hyperref[core\string-api/genericirq:c.irq_data]{\emph{irq\_data}}} *\emph{ d}}{}
Ack pending interrupt via setting bit

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_data * d}}] \leavevmode
irq\_data

\end{description}
\index{irq\_alloc\_generic\_chip (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_alloc_generic_chip}\pysiglinewithargsret{struct {\hyperref[core\string-api/genericirq:c.irq_chip_generic]{\emph{irq\_chip\_generic}}} * \bfcode{irq\_alloc\_generic\_chip}}{const char *\emph{ name}, int\emph{ num\_ct}, unsigned int\emph{ irq\_base}, void \_\_iomem *\emph{ reg\_base}, irq\_flow\_handler\_t\emph{ handler}}{}
Allocate a generic chip and initialize it

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{const char * name}}] \leavevmode
Name of the irq chip

\item[{\code{int num\_ct}}] \leavevmode
Number of irq\_chip\_type instances associated with this

\item[{\code{unsigned int irq\_base}}] \leavevmode
Interrupt base nr for this chip

\item[{\code{void \_\_iomem * reg\_base}}] \leavevmode
Register base address (virtual)

\item[{\code{irq\_flow\_handler\_t handler}}] \leavevmode
Default flow handler associated with this chip

\end{description}

\textbf{Description}

Returns an initialized irq\_chip\_generic structure. The chip defaults
to the primary (index 0) irq\_chip\_type and \textbf{handler}
\index{\_\_irq\_alloc\_domain\_generic\_chips (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.__irq_alloc_domain_generic_chips}\pysiglinewithargsret{int \bfcode{\_\_irq\_alloc\_domain\_generic\_chips}}{struct irq\_domain *\emph{ d}, int\emph{ irqs\_per\_chip}, int\emph{ num\_ct}, const char *\emph{ name}, irq\_flow\_handler\_t\emph{ handler}, unsigned int\emph{ clr}, unsigned int\emph{ set}, enum {\hyperref[core\string-api/genericirq:c.irq_gc_flags]{\emph{irq\_gc\_flags}}}\emph{ gcflags}}{}
Allocate generic chips for an irq domain

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_domain * d}}] \leavevmode
irq domain for which to allocate chips

\item[{\code{int irqs\_per\_chip}}] \leavevmode
Number of interrupts each chip handles (max 32)

\item[{\code{int num\_ct}}] \leavevmode
Number of irq\_chip\_type instances associated with this

\item[{\code{const char * name}}] \leavevmode
Name of the irq chip

\item[{\code{irq\_flow\_handler\_t handler}}] \leavevmode
Default flow handler associated with these chips

\item[{\code{unsigned int clr}}] \leavevmode
IRQ\_* bits to clear in the mapping function

\item[{\code{unsigned int set}}] \leavevmode
IRQ\_* bits to set in the mapping function

\item[{\code{enum irq\_gc\_flags gcflags}}] \leavevmode
Generic chip specific setup flags

\end{description}
\index{irq\_get\_domain\_generic\_chip (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_get_domain_generic_chip}\pysiglinewithargsret{struct {\hyperref[core\string-api/genericirq:c.irq_chip_generic]{\emph{irq\_chip\_generic}}} * \bfcode{irq\_get\_domain\_generic\_chip}}{struct irq\_domain *\emph{ d}, unsigned int\emph{ hw\_irq}}{}
Get a pointer to the generic chip of a hw\_irq

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_domain * d}}] \leavevmode
irq domain pointer

\item[{\code{unsigned int hw\_irq}}] \leavevmode
Hardware interrupt number

\end{description}
\index{irq\_setup\_generic\_chip (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_setup_generic_chip}\pysiglinewithargsret{void \bfcode{irq\_setup\_generic\_chip}}{struct {\hyperref[core\string-api/genericirq:c.irq_chip_generic]{\emph{irq\_chip\_generic}}} *\emph{ gc}, u32\emph{ msk}, enum {\hyperref[core\string-api/genericirq:c.irq_gc_flags]{\emph{irq\_gc\_flags}}}\emph{ flags}, unsigned int\emph{ clr}, unsigned int\emph{ set}}{}
Setup a range of interrupts with a generic chip

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_chip\_generic * gc}}] \leavevmode
Generic irq chip holding all data

\item[{\code{u32 msk}}] \leavevmode
Bitmask holding the irqs to initialize relative to gc-\textgreater{}irq\_base

\item[{\code{enum irq\_gc\_flags flags}}] \leavevmode
Flags for initialization

\item[{\code{unsigned int clr}}] \leavevmode
IRQ\_* bits to clear

\item[{\code{unsigned int set}}] \leavevmode
IRQ\_* bits to set

\end{description}

\textbf{Description}

Set up max. 32 interrupts starting from gc-\textgreater{}irq\_base. Note, this
initializes all interrupts to the primary irq\_chip\_type and its
associated handler.
\index{irq\_setup\_alt\_chip (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_setup_alt_chip}\pysiglinewithargsret{int \bfcode{irq\_setup\_alt\_chip}}{struct {\hyperref[core\string-api/genericirq:c.irq_data]{\emph{irq\_data}}} *\emph{ d}, unsigned int\emph{ type}}{}
Switch to alternative chip

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_data * d}}] \leavevmode
irq\_data for this interrupt

\item[{\code{unsigned int type}}] \leavevmode
Flow type to be initialized

\end{description}

\textbf{Description}

Only to be called from chip-\textgreater{}:c:func:\emph{irq\_set\_type()} callbacks.
\index{irq\_remove\_generic\_chip (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_remove_generic_chip}\pysiglinewithargsret{void \bfcode{irq\_remove\_generic\_chip}}{struct {\hyperref[core\string-api/genericirq:c.irq_chip_generic]{\emph{irq\_chip\_generic}}} *\emph{ gc}, u32\emph{ msk}, unsigned int\emph{ clr}, unsigned int\emph{ set}}{}
Remove a chip

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_chip\_generic * gc}}] \leavevmode
Generic irq chip holding all data

\item[{\code{u32 msk}}] \leavevmode
Bitmask holding the irqs to initialize relative to gc-\textgreater{}irq\_base

\item[{\code{unsigned int clr}}] \leavevmode
IRQ\_* bits to clear

\item[{\code{unsigned int set}}] \leavevmode
IRQ\_* bits to set

\end{description}

\textbf{Description}

Remove up to 32 interrupts starting from gc-\textgreater{}irq\_base.


\subsection{Structures}
\label{core-api/genericirq:structures}
This chapter contains the autogenerated documentation of the structures
which are used in the generic IRQ layer.
\index{irq\_common\_data (C type)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_common_data}\pysigline{struct \bfcode{irq\_common\_data}}
per irq data shared by all irqchips

\end{fulllineitems}


\textbf{Definition}

\begin{Verbatim}[commandchars=\\\{\}]
struct irq\PYGZus{}common\PYGZus{}data \PYGZob{}
  unsigned int            \PYGZus{}\PYGZus{}private state\PYGZus{}use\PYGZus{}accessors;
\PYGZsh{}ifdef CONFIG\PYGZus{}NUMA;
  unsigned int            node;
\PYGZsh{}endif;
  void *handler\PYGZus{}data;
  struct msi\PYGZus{}desc         *msi\PYGZus{}desc;
  cpumask\PYGZus{}var\PYGZus{}t affinity;
\PYGZsh{}ifdef CONFIG\PYGZus{}GENERIC\PYGZus{}IRQ\PYGZus{}EFFECTIVE\PYGZus{}AFF\PYGZus{}MASK;
  cpumask\PYGZus{}var\PYGZus{}t effective\PYGZus{}affinity;
\PYGZsh{}endif;
\PYGZsh{}ifdef CONFIG\PYGZus{}GENERIC\PYGZus{}IRQ\PYGZus{}IPI;
  unsigned int            ipi\PYGZus{}offset;
\PYGZsh{}endif;
\PYGZcb{};
\end{Verbatim}

\textbf{Members}
\begin{description}
\item[{\code{state\_use\_accessors}}] \leavevmode
status information for irq chip functions.
Use accessor functions to deal with it

\item[{\code{node}}] \leavevmode
node index useful for balancing

\item[{\code{handler\_data}}] \leavevmode
per-IRQ data for the irq\_chip methods

\item[{\code{msi\_desc}}] \leavevmode
MSI descriptor

\item[{\code{affinity}}] \leavevmode
IRQ affinity on SMP. If this is an IPI
related irq, then this is the mask of the
CPUs to which an IPI can be sent.

\item[{\code{effective\_affinity}}] \leavevmode
The effective IRQ affinity on SMP as some irq
chips do not allow multi CPU destinations.
A subset of \textbf{affinity}.

\item[{\code{ipi\_offset}}] \leavevmode
Offset of first IPI target cpu in \textbf{affinity}. Optional.

\end{description}
\index{irq\_data (C type)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_data}\pysigline{struct \bfcode{irq\_data}}
per irq chip data passed down to chip functions

\end{fulllineitems}


\textbf{Definition}

\begin{Verbatim}[commandchars=\\\{\}]
struct irq\PYGZus{}data \PYGZob{}
  u32 mask;
  unsigned int            irq;
  unsigned long           hwirq;
  struct irq\PYGZus{}common\PYGZus{}data  *common;
  struct irq\PYGZus{}chip         *chip;
  struct irq\PYGZus{}domain       *domain;
\PYGZsh{}ifdef CONFIG\PYGZus{}IRQ\PYGZus{}DOMAIN\PYGZus{}HIERARCHY;
  struct irq\PYGZus{}data         *parent\PYGZus{}data;
\PYGZsh{}endif;
  void *chip\PYGZus{}data;
\PYGZcb{};
\end{Verbatim}

\textbf{Members}
\begin{description}
\item[{\code{mask}}] \leavevmode
precomputed bitmask for accessing the chip registers

\item[{\code{irq}}] \leavevmode
interrupt number

\item[{\code{hwirq}}] \leavevmode
hardware interrupt number, local to the interrupt domain

\item[{\code{common}}] \leavevmode
point to data shared by all irqchips

\item[{\code{chip}}] \leavevmode
low level interrupt hardware access

\item[{\code{domain}}] \leavevmode
Interrupt translation domain; responsible for mapping
between hwirq number and linux irq number.

\item[{\code{parent\_data}}] \leavevmode
pointer to parent struct irq\_data to support hierarchy
irq\_domain

\item[{\code{chip\_data}}] \leavevmode
platform-specific per-chip private data for the chip
methods, to allow shared chip implementations

\end{description}
\index{irq\_chip (C type)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_chip}\pysigline{struct \bfcode{irq\_chip}}
hardware interrupt chip descriptor

\end{fulllineitems}


\textbf{Definition}

\begin{Verbatim}[commandchars=\\\{\}]
struct irq\PYGZus{}chip \PYGZob{}
  struct device   *parent\PYGZus{}device;
  const char      *name;
  unsigned int    (*irq\PYGZus{}startup)(struct irq\PYGZus{}data *data);
  void (*irq\PYGZus{}shutdown)(struct irq\PYGZus{}data *data);
  void (*irq\PYGZus{}enable)(struct irq\PYGZus{}data *data);
  void (*irq\PYGZus{}disable)(struct irq\PYGZus{}data *data);
  void (*irq\PYGZus{}ack)(struct irq\PYGZus{}data *data);
  void (*irq\PYGZus{}mask)(struct irq\PYGZus{}data *data);
  void (*irq\PYGZus{}mask\PYGZus{}ack)(struct irq\PYGZus{}data *data);
  void (*irq\PYGZus{}unmask)(struct irq\PYGZus{}data *data);
  void (*irq\PYGZus{}eoi)(struct irq\PYGZus{}data *data);
  int (*irq\PYGZus{}set\PYGZus{}affinity)(struct irq\PYGZus{}data *data, const struct cpumask *dest, bool force);
  int (*irq\PYGZus{}retrigger)(struct irq\PYGZus{}data *data);
  int (*irq\PYGZus{}set\PYGZus{}type)(struct irq\PYGZus{}data *data, unsigned int flow\PYGZus{}type);
  int (*irq\PYGZus{}set\PYGZus{}wake)(struct irq\PYGZus{}data *data, unsigned int on);
  void (*irq\PYGZus{}bus\PYGZus{}lock)(struct irq\PYGZus{}data *data);
  void (*irq\PYGZus{}bus\PYGZus{}sync\PYGZus{}unlock)(struct irq\PYGZus{}data *data);
  void (*irq\PYGZus{}cpu\PYGZus{}online)(struct irq\PYGZus{}data *data);
  void (*irq\PYGZus{}cpu\PYGZus{}offline)(struct irq\PYGZus{}data *data);
  void (*irq\PYGZus{}suspend)(struct irq\PYGZus{}data *data);
  void (*irq\PYGZus{}resume)(struct irq\PYGZus{}data *data);
  void (*irq\PYGZus{}pm\PYGZus{}shutdown)(struct irq\PYGZus{}data *data);
  void (*irq\PYGZus{}calc\PYGZus{}mask)(struct irq\PYGZus{}data *data);
  void (*irq\PYGZus{}print\PYGZus{}chip)(struct irq\PYGZus{}data *data, struct seq\PYGZus{}file *p);
  int (*irq\PYGZus{}request\PYGZus{}resources)(struct irq\PYGZus{}data *data);
  void (*irq\PYGZus{}release\PYGZus{}resources)(struct irq\PYGZus{}data *data);
  void (*irq\PYGZus{}compose\PYGZus{}msi\PYGZus{}msg)(struct irq\PYGZus{}data *data, struct msi\PYGZus{}msg *msg);
  void (*irq\PYGZus{}write\PYGZus{}msi\PYGZus{}msg)(struct irq\PYGZus{}data *data, struct msi\PYGZus{}msg *msg);
  int (*irq\PYGZus{}get\PYGZus{}irqchip\PYGZus{}state)(struct irq\PYGZus{}data *data, enum irqchip\PYGZus{}irq\PYGZus{}state which, bool *state);
  int (*irq\PYGZus{}set\PYGZus{}irqchip\PYGZus{}state)(struct irq\PYGZus{}data *data, enum irqchip\PYGZus{}irq\PYGZus{}state which, bool state);
  int (*irq\PYGZus{}set\PYGZus{}vcpu\PYGZus{}affinity)(struct irq\PYGZus{}data *data, void *vcpu\PYGZus{}info);
  void (*ipi\PYGZus{}send\PYGZus{}single)(struct irq\PYGZus{}data *data, unsigned int cpu);
  void (*ipi\PYGZus{}send\PYGZus{}mask)(struct irq\PYGZus{}data *data, const struct cpumask *dest);
  unsigned long   flags;
\PYGZcb{};
\end{Verbatim}

\textbf{Members}
\begin{description}
\item[{\code{parent\_device}}] \leavevmode
pointer to parent device for irqchip

\item[{\code{name}}] \leavevmode
name for /proc/interrupts

\item[{\code{irq\_startup}}] \leavevmode
start up the interrupt (defaults to -\textgreater{}enable if NULL)

\item[{\code{irq\_shutdown}}] \leavevmode
shut down the interrupt (defaults to -\textgreater{}disable if NULL)

\item[{\code{irq\_enable}}] \leavevmode
enable the interrupt (defaults to chip-\textgreater{}unmask if NULL)

\item[{\code{irq\_disable}}] \leavevmode
disable the interrupt

\item[{\code{irq\_ack}}] \leavevmode
start of a new interrupt

\item[{\code{irq\_mask}}] \leavevmode
mask an interrupt source

\item[{\code{irq\_mask\_ack}}] \leavevmode
ack and mask an interrupt source

\item[{\code{irq\_unmask}}] \leavevmode
unmask an interrupt source

\item[{\code{irq\_eoi}}] \leavevmode
end of interrupt

\item[{\code{irq\_set\_affinity}}] \leavevmode
Set the CPU affinity on SMP machines. If the force
argument is true, it tells the driver to
unconditionally apply the affinity setting. Sanity
checks against the supplied affinity mask are not
required. This is used for CPU hotplug where the
target CPU is not yet set in the cpu\_online\_mask.

\item[{\code{irq\_retrigger}}] \leavevmode
resend an IRQ to the CPU

\item[{\code{irq\_set\_type}}] \leavevmode
set the flow type (IRQ\_TYPE\_LEVEL/etc.) of an IRQ

\item[{\code{irq\_set\_wake}}] \leavevmode
enable/disable power-management wake-on of an IRQ

\item[{\code{irq\_bus\_lock}}] \leavevmode
function to lock access to slow bus (i2c) chips

\item[{\code{irq\_bus\_sync\_unlock}}] \leavevmode
function to sync and unlock slow bus (i2c) chips

\item[{\code{irq\_cpu\_online}}] \leavevmode
configure an interrupt source for a secondary CPU

\item[{\code{irq\_cpu\_offline}}] \leavevmode
un-configure an interrupt source for a secondary CPU

\item[{\code{irq\_suspend}}] \leavevmode
function called from core code on suspend once per
chip, when one or more interrupts are installed

\item[{\code{irq\_resume}}] \leavevmode
function called from core code on resume once per chip,
when one ore more interrupts are installed

\item[{\code{irq\_pm\_shutdown}}] \leavevmode
function called from core code on shutdown once per chip

\item[{\code{irq\_calc\_mask}}] \leavevmode
Optional function to set irq\_data.mask for special cases

\item[{\code{irq\_print\_chip}}] \leavevmode
optional to print special chip info in show\_interrupts

\item[{\code{irq\_request\_resources}}] \leavevmode
optional to request resources before calling
any other callback related to this irq

\item[{\code{irq\_release\_resources}}] \leavevmode
optional to release resources acquired with
irq\_request\_resources

\item[{\code{irq\_compose\_msi\_msg}}] \leavevmode
optional to compose message content for MSI

\item[{\code{irq\_write\_msi\_msg}}] \leavevmode
optional to write message content for MSI

\item[{\code{irq\_get\_irqchip\_state}}] \leavevmode
return the internal state of an interrupt

\item[{\code{irq\_set\_irqchip\_state}}] \leavevmode
set the internal state of a interrupt

\item[{\code{irq\_set\_vcpu\_affinity}}] \leavevmode
optional to target a vCPU in a virtual machine

\item[{\code{ipi\_send\_single}}] \leavevmode
send a single IPI to destination cpus

\item[{\code{ipi\_send\_mask}}] \leavevmode
send an IPI to destination cpus in cpumask

\item[{\code{flags}}] \leavevmode
chip specific flags

\end{description}
\index{irq\_chip\_regs (C type)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_chip_regs}\pysigline{struct \bfcode{irq\_chip\_regs}}
register offsets for struct irq\_gci

\end{fulllineitems}


\textbf{Definition}

\begin{Verbatim}[commandchars=\\\{\}]
struct irq\PYGZus{}chip\PYGZus{}regs \PYGZob{}
  unsigned long           enable;
  unsigned long           disable;
  unsigned long           mask;
  unsigned long           ack;
  unsigned long           eoi;
  unsigned long           type;
  unsigned long           polarity;
\PYGZcb{};
\end{Verbatim}

\textbf{Members}
\begin{description}
\item[{\code{enable}}] \leavevmode
Enable register offset to reg\_base

\item[{\code{disable}}] \leavevmode
Disable register offset to reg\_base

\item[{\code{mask}}] \leavevmode
Mask register offset to reg\_base

\item[{\code{ack}}] \leavevmode
Ack register offset to reg\_base

\item[{\code{eoi}}] \leavevmode
Eoi register offset to reg\_base

\item[{\code{type}}] \leavevmode
Type configuration register offset to reg\_base

\item[{\code{polarity}}] \leavevmode
Polarity configuration register offset to reg\_base

\end{description}
\index{irq\_chip\_type (C type)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_chip_type}\pysigline{struct \bfcode{irq\_chip\_type}}
Generic interrupt chip instance for a flow type

\end{fulllineitems}


\textbf{Definition}

\begin{Verbatim}[commandchars=\\\{\}]
struct irq\PYGZus{}chip\PYGZus{}type \PYGZob{}
  struct irq\PYGZus{}chip         chip;
  struct irq\PYGZus{}chip\PYGZus{}regs    regs;
  irq\PYGZus{}flow\PYGZus{}handler\PYGZus{}t handler;
  u32 type;
  u32 mask\PYGZus{}cache\PYGZus{}priv;
  u32 *mask\PYGZus{}cache;
\PYGZcb{};
\end{Verbatim}

\textbf{Members}
\begin{description}
\item[{\code{chip}}] \leavevmode
The real interrupt chip which provides the callbacks

\item[{\code{regs}}] \leavevmode
Register offsets for this chip

\item[{\code{handler}}] \leavevmode
Flow handler associated with this chip

\item[{\code{type}}] \leavevmode
Chip can handle these flow types

\item[{\code{mask\_cache\_priv}}] \leavevmode
Cached mask register private to the chip type

\item[{\code{mask\_cache}}] \leavevmode
Pointer to cached mask register

\end{description}

\textbf{Description}

A irq\_generic\_chip can have several instances of irq\_chip\_type when
it requires different functions and register offsets for different
flow types.
\index{irq\_chip\_generic (C type)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_chip_generic}\pysigline{struct \bfcode{irq\_chip\_generic}}
Generic irq chip data structure

\end{fulllineitems}


\textbf{Definition}

\begin{Verbatim}[commandchars=\\\{\}]
struct irq\PYGZus{}chip\PYGZus{}generic \PYGZob{}
  raw\PYGZus{}spinlock\PYGZus{}t lock;
  void \PYGZus{}\PYGZus{}iomem            *reg\PYGZus{}base;
  u32 (*reg\PYGZus{}readl)(void \PYGZus{}\PYGZus{}iomem *addr);
  void (*reg\PYGZus{}writel)(u32 val, void \PYGZus{}\PYGZus{}iomem *addr);
  void (*suspend)(struct irq\PYGZus{}chip\PYGZus{}generic *gc);
  void (*resume)(struct irq\PYGZus{}chip\PYGZus{}generic *gc);
  unsigned int            irq\PYGZus{}base;
  unsigned int            irq\PYGZus{}cnt;
  u32 mask\PYGZus{}cache;
  u32 type\PYGZus{}cache;
  u32 polarity\PYGZus{}cache;
  u32 wake\PYGZus{}enabled;
  u32 wake\PYGZus{}active;
  unsigned int            num\PYGZus{}ct;
  void *private;
  unsigned long           installed;
  unsigned long           unused;
  struct irq\PYGZus{}domain       *domain;
  struct list\PYGZus{}head        list;
  struct irq\PYGZus{}chip\PYGZus{}type    chip\PYGZus{}types[0];
\PYGZcb{};
\end{Verbatim}

\textbf{Members}
\begin{description}
\item[{\code{lock}}] \leavevmode
Lock to protect register and cache data access

\item[{\code{reg\_base}}] \leavevmode
Register base address (virtual)

\item[{\code{reg\_readl}}] \leavevmode
Alternate I/O accessor (defaults to readl if NULL)

\item[{\code{reg\_writel}}] \leavevmode
Alternate I/O accessor (defaults to writel if NULL)

\item[{\code{suspend}}] \leavevmode
Function called from core code on suspend once per
chip; can be useful instead of irq\_chip::suspend to
handle chip details even when no interrupts are in use

\item[{\code{resume}}] \leavevmode
Function called from core code on resume once per chip;
can be useful instead of irq\_chip::suspend to handle
chip details even when no interrupts are in use

\item[{\code{irq\_base}}] \leavevmode
Interrupt base nr for this chip

\item[{\code{irq\_cnt}}] \leavevmode
Number of interrupts handled by this chip

\item[{\code{mask\_cache}}] \leavevmode
Cached mask register shared between all chip types

\item[{\code{type\_cache}}] \leavevmode
Cached type register

\item[{\code{polarity\_cache}}] \leavevmode
Cached polarity register

\item[{\code{wake\_enabled}}] \leavevmode
Interrupt can wakeup from suspend

\item[{\code{wake\_active}}] \leavevmode
Interrupt is marked as an wakeup from suspend source

\item[{\code{num\_ct}}] \leavevmode
Number of available irq\_chip\_type instances (usually 1)

\item[{\code{private}}] \leavevmode
Private data for non generic chip callbacks

\item[{\code{installed}}] \leavevmode
bitfield to denote installed interrupts

\item[{\code{unused}}] \leavevmode
bitfield to denote unused interrupts

\item[{\code{domain}}] \leavevmode
irq domain pointer

\item[{\code{list}}] \leavevmode
List head for keeping track of instances

\item[{\code{chip\_types}}] \leavevmode
Array of interrupt irq\_chip\_types

\end{description}

\textbf{Description}

Note, that irq\_chip\_generic can have multiple irq\_chip\_type
implementations which can be associated to a particular irq line of
an irq\_chip\_generic instance. That allows to share and protect
state in an irq\_chip\_generic instance when we need to implement
different flow mechanisms (level/edge) for it.
\index{irq\_gc\_flags (C type)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_gc_flags}\pysigline{enum \bfcode{irq\_gc\_flags}}
Initialization flags for generic irq chips

\end{fulllineitems}


\textbf{Constants}
\begin{description}
\item[{\code{IRQ\_GC\_INIT\_MASK\_CACHE}}] \leavevmode
Initialize the mask\_cache by reading mask reg

\item[{\code{IRQ\_GC\_INIT\_NESTED\_LOCK}}] \leavevmode
Set the lock class of the irqs to nested for
irq chips which need to call \code{irq\_set\_wake()} on
the parent irq. Usually GPIO implementations

\item[{\code{IRQ\_GC\_MASK\_CACHE\_PER\_TYPE}}] \leavevmode
Mask cache is chip type private

\item[{\code{IRQ\_GC\_NO\_MASK}}] \leavevmode
Do not calculate irq\_data-\textgreater{}mask

\item[{\code{IRQ\_GC\_BE\_IO}}] \leavevmode
Use big-endian register accesses (default: LE)

\end{description}
\index{irqaction (C type)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irqaction}\pysigline{struct \bfcode{irqaction}}
per interrupt action descriptor

\end{fulllineitems}


\textbf{Definition}

\begin{Verbatim}[commandchars=\\\{\}]
struct irqaction \PYGZob{}
  irq\PYGZus{}handler\PYGZus{}t handler;
  void *dev\PYGZus{}id;
  void \PYGZus{}\PYGZus{}percpu           *percpu\PYGZus{}dev\PYGZus{}id;
  struct irqaction        *next;
  irq\PYGZus{}handler\PYGZus{}t thread\PYGZus{}fn;
  struct task\PYGZus{}struct      *thread;
  struct irqaction        *secondary;
  unsigned int            irq;
  unsigned int            flags;
  unsigned long           thread\PYGZus{}flags;
  unsigned long           thread\PYGZus{}mask;
  const char              *name;
  struct proc\PYGZus{}dir\PYGZus{}entry   *dir;
\PYGZcb{};
\end{Verbatim}

\textbf{Members}
\begin{description}
\item[{\code{handler}}] \leavevmode
interrupt handler function

\item[{\code{dev\_id}}] \leavevmode
cookie to identify the device

\item[{\code{percpu\_dev\_id}}] \leavevmode
cookie to identify the device

\item[{\code{next}}] \leavevmode
pointer to the next irqaction for shared interrupts

\item[{\code{thread\_fn}}] \leavevmode
interrupt handler function for threaded interrupts

\item[{\code{thread}}] \leavevmode
thread pointer for threaded interrupts

\item[{\code{secondary}}] \leavevmode
pointer to secondary irqaction (force threading)

\item[{\code{irq}}] \leavevmode
interrupt number

\item[{\code{flags}}] \leavevmode
flags (see IRQF\_* above)

\item[{\code{thread\_flags}}] \leavevmode
flags related to \textbf{thread}

\item[{\code{thread\_mask}}] \leavevmode
bitmask for keeping track of \textbf{thread} activity

\item[{\code{name}}] \leavevmode
name of the device

\item[{\code{dir}}] \leavevmode
pointer to the proc/irq/NN/name entry

\end{description}
\index{irq\_affinity\_notify (C type)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_affinity_notify}\pysigline{struct \bfcode{irq\_affinity\_notify}}
context for notification of IRQ affinity changes

\end{fulllineitems}


\textbf{Definition}

\begin{Verbatim}[commandchars=\\\{\}]
struct irq\PYGZus{}affinity\PYGZus{}notify \PYGZob{}
  unsigned int irq;
  struct kref kref;
  struct work\PYGZus{}struct work;
  void (*notify)(struct irq\PYGZus{}affinity\PYGZus{}notify *, const cpumask\PYGZus{}t *mask);
  void (*release)(struct kref *ref);
\PYGZcb{};
\end{Verbatim}

\textbf{Members}
\begin{description}
\item[{\code{irq}}] \leavevmode
Interrupt to which notification applies

\item[{\code{kref}}] \leavevmode
Reference count, for internal use

\item[{\code{work}}] \leavevmode
Work item, for internal use

\item[{\code{notify}}] \leavevmode
Function to be called on change.  This will be
called in process context.

\item[{\code{release}}] \leavevmode
Function to be called on release.  This will be
called in process context.  Once registered, the
structure must only be freed when this function is
called or later.

\end{description}
\index{irq\_affinity (C type)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_affinity}\pysigline{struct \bfcode{irq\_affinity}}
Description for automatic irq affinity assignements

\end{fulllineitems}


\textbf{Definition}

\begin{Verbatim}[commandchars=\\\{\}]
struct irq\PYGZus{}affinity \PYGZob{}
  int pre\PYGZus{}vectors;
  int post\PYGZus{}vectors;
\PYGZcb{};
\end{Verbatim}

\textbf{Members}
\begin{description}
\item[{\code{pre\_vectors}}] \leavevmode
Don't apply affinity to \textbf{pre\_vectors} at beginning of
the MSI(-X) vector space

\item[{\code{post\_vectors}}] \leavevmode
Don't apply affinity to \textbf{post\_vectors} at end of
the MSI(-X) vector space

\end{description}
\index{irq\_set\_affinity (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_set_affinity}\pysiglinewithargsret{int \bfcode{irq\_set\_affinity}}{unsigned int\emph{ irq}, const struct cpumask *\emph{ cpumask}}{}
Set the irq affinity of a given irq

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt to set affinity

\item[{\code{const struct cpumask * cpumask}}] \leavevmode
cpumask

\end{description}

\textbf{Description}

Fails if cpumask does not contain an online CPU
\index{irq\_force\_affinity (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_force_affinity}\pysiglinewithargsret{int \bfcode{irq\_force\_affinity}}{unsigned int\emph{ irq}, const struct cpumask *\emph{ cpumask}}{}
Force the irq affinity of a given irq

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt to set affinity

\item[{\code{const struct cpumask * cpumask}}] \leavevmode
cpumask

\end{description}

\textbf{Description}

Same as irq\_set\_affinity, but without checking the mask against
online cpus.

Solely for low level cpu hotplug code, where we need to make per
cpu interrupts affine before the cpu becomes online.


\subsection{Public Functions Provided}
\label{core-api/genericirq:public-functions-provided}
This chapter contains the autogenerated documentation of the kernel API
functions which are exported.
\index{synchronize\_hardirq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.synchronize_hardirq}\pysiglinewithargsret{bool \bfcode{synchronize\_hardirq}}{unsigned int\emph{ irq}}{}
wait for pending hard IRQ handlers (on other CPUs)

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
interrupt number to wait for

\end{description}

\textbf{Description}
\begin{quote}

This function waits for any pending hard IRQ handlers for this
interrupt to complete before returning. If you use this
function while holding a resource the IRQ handler may need you
will deadlock. It does not take associated threaded handlers
into account.

Do not use this for shutdown scenarios where you must be sure
that all parts (hardirq and threaded handler) have completed.
\end{quote}

\textbf{Return}

false if a threaded handler is active.
\begin{quote}

This function may be called - with care - from IRQ context.
\end{quote}
\index{synchronize\_irq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.synchronize_irq}\pysiglinewithargsret{void \bfcode{synchronize\_irq}}{unsigned int\emph{ irq}}{}
wait for pending IRQ handlers (on other CPUs)

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
interrupt number to wait for

\end{description}

\textbf{Description}
\begin{quote}

This function waits for any pending IRQ handlers for this interrupt
to complete before returning. If you use this function while
holding a resource the IRQ handler may need you will deadlock.

This function may be called - with care - from IRQ context.
\end{quote}
\index{irq\_can\_set\_affinity (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_can_set_affinity}\pysiglinewithargsret{int \bfcode{irq\_can\_set\_affinity}}{unsigned int\emph{ irq}}{}
Check if the affinity of a given irq can be set

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt to check

\end{description}
\index{irq\_can\_set\_affinity\_usr (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_can_set_affinity_usr}\pysiglinewithargsret{bool \bfcode{irq\_can\_set\_affinity\_usr}}{unsigned int\emph{ irq}}{}
Check if affinity of a irq can be set from user space

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt to check

\end{description}

\textbf{Description}

Like {\hyperref[core\string-api/genericirq:c.irq_can_set_affinity]{\emph{\code{irq\_can\_set\_affinity()}}}} above, but additionally checks for the
AFFINITY\_MANAGED flag.
\index{irq\_set\_thread\_affinity (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_set_thread_affinity}\pysiglinewithargsret{void \bfcode{irq\_set\_thread\_affinity}}{struct irq\_desc *\emph{ desc}}{}
Notify irq threads to adjust affinity

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_desc * desc}}] \leavevmode
irq descriptor which has affitnity changed

\end{description}

\textbf{Description}
\begin{quote}

We just set IRQTF\_AFFINITY and delegate the affinity setting
to the interrupt thread itself. We can not call
\code{set\_cpus\_allowed\_ptr()} here as we hold desc-\textgreater{}lock and this
code can be called from hard interrupt context.
\end{quote}
\index{irq\_set\_affinity\_notifier (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_set_affinity_notifier}\pysiglinewithargsret{int \bfcode{irq\_set\_affinity\_notifier}}{unsigned int\emph{ irq}, struct {\hyperref[core\string-api/genericirq:c.irq_affinity_notify]{\emph{irq\_affinity\_notify}}} *\emph{ notify}}{}
control notification of IRQ affinity changes

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt for which to enable/disable notification

\item[{\code{struct irq\_affinity\_notify * notify}}] \leavevmode
Context for notification, or \code{NULL} to disable
notification.  Function pointers must be initialised;
the other fields will be initialised by this function.

\end{description}

\textbf{Description}
\begin{quote}

Must be called in process context.  Notification may only be enabled
after the IRQ is allocated and must be disabled before the IRQ is
freed using {\hyperref[core\string-api/kernel\string-api:c.free_irq]{\emph{\code{free\_irq()}}}}.
\end{quote}
\index{irq\_set\_vcpu\_affinity (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_set_vcpu_affinity}\pysiglinewithargsret{int \bfcode{irq\_set\_vcpu\_affinity}}{unsigned int\emph{ irq}, void *\emph{ vcpu\_info}}{}
Set vcpu affinity for the interrupt

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
interrupt number to set affinity

\item[{\code{void * vcpu\_info}}] \leavevmode
vCPU specific data or pointer to a percpu array of vCPU
specific data for percpu\_devid interrupts

\end{description}

\textbf{Description}
\begin{quote}

This function uses the vCPU specific data to set the vCPU
affinity for an irq. The vCPU specific data is passed from
outside, such as KVM. One example code path is as below:
KVM -\textgreater{} IOMMU -\textgreater{} {\hyperref[core\string-api/kernel\string-api:c.irq_set_vcpu_affinity]{\emph{\code{irq\_set\_vcpu\_affinity()}}}}.
\end{quote}
\index{disable\_irq\_nosync (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.disable_irq_nosync}\pysiglinewithargsret{void \bfcode{disable\_irq\_nosync}}{unsigned int\emph{ irq}}{}
disable an irq without waiting

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt to disable

\end{description}

\textbf{Description}
\begin{quote}

Disable the selected interrupt line.  Disables and Enables are
nested.
Unlike {\hyperref[core\string-api/kernel\string-api:c.disable_irq]{\emph{\code{disable\_irq()}}}}, this function does not ensure existing
instances of the IRQ handler have completed before returning.

This function may be called from IRQ context.
\end{quote}
\index{disable\_irq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.disable_irq}\pysiglinewithargsret{void \bfcode{disable\_irq}}{unsigned int\emph{ irq}}{}
disable an irq and wait for completion

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt to disable

\end{description}

\textbf{Description}
\begin{quote}

Disable the selected interrupt line.  Enables and Disables are
nested.
This function waits for any pending IRQ handlers for this interrupt
to complete before returning. If you use this function while
holding a resource the IRQ handler may need you will deadlock.

This function may be called - with care - from IRQ context.
\end{quote}
\index{disable\_hardirq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.disable_hardirq}\pysiglinewithargsret{bool \bfcode{disable\_hardirq}}{unsigned int\emph{ irq}}{}
disables an irq and waits for hardirq completion

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt to disable

\end{description}

\textbf{Description}
\begin{quote}

Disable the selected interrupt line.  Enables and Disables are
nested.
This function waits for any pending hard IRQ handlers for this
interrupt to complete before returning. If you use this function while
holding a resource the hard IRQ handler may need you will deadlock.

When used to optimistically disable an interrupt from atomic context
the return value must be checked.
\end{quote}

\textbf{Return}

false if a threaded handler is active.
\begin{quote}

This function may be called - with care - from IRQ context.
\end{quote}
\index{enable\_irq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.enable_irq}\pysiglinewithargsret{void \bfcode{enable\_irq}}{unsigned int\emph{ irq}}{}
enable handling of an irq

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt to enable

\end{description}

\textbf{Description}
\begin{quote}

Undoes the effect of one call to {\hyperref[core\string-api/kernel\string-api:c.disable_irq]{\emph{\code{disable\_irq()}}}}.  If this
matches the last disable, processing of interrupts on this
IRQ line is re-enabled.

This function may be called from IRQ context only when
desc-\textgreater{}irq\_data.chip-\textgreater{}bus\_lock and desc-\textgreater{}chip-\textgreater{}bus\_sync\_unlock are NULL !
\end{quote}
\index{irq\_set\_irq\_wake (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_set_irq_wake}\pysiglinewithargsret{int \bfcode{irq\_set\_irq\_wake}}{unsigned int\emph{ irq}, unsigned int\emph{ on}}{}
control irq power management wakeup

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
interrupt to control

\item[{\code{unsigned int on}}] \leavevmode
enable/disable power management wakeup

\end{description}

\textbf{Description}
\begin{quote}

Enable/disable power management wakeup mode, which is
disabled by default.  Enables and disables must match,
just as they match for non-wakeup mode support.

Wakeup mode lets this IRQ wake the system from sleep
states like ``suspend to RAM''.
\end{quote}
\index{irq\_wake\_thread (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_wake_thread}\pysiglinewithargsret{void \bfcode{irq\_wake\_thread}}{unsigned int\emph{ irq}, void *\emph{ dev\_id}}{}
wake the irq thread for the action identified by dev\_id

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt line

\item[{\code{void * dev\_id}}] \leavevmode
Device identity for which the thread should be woken

\end{description}
\index{setup\_irq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.setup_irq}\pysiglinewithargsret{int \bfcode{setup\_irq}}{unsigned int\emph{ irq}, struct {\hyperref[core\string-api/genericirq:c.irqaction]{\emph{irqaction}}} *\emph{ act}}{}
setup an interrupt

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt line to setup

\item[{\code{struct irqaction * act}}] \leavevmode
irqaction for the interrupt

\end{description}

\textbf{Description}

Used to statically setup interrupts in the early boot process.
\index{remove\_irq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.remove_irq}\pysiglinewithargsret{void \bfcode{remove\_irq}}{unsigned int\emph{ irq}, struct {\hyperref[core\string-api/genericirq:c.irqaction]{\emph{irqaction}}} *\emph{ act}}{}
free an interrupt

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt line to free

\item[{\code{struct irqaction * act}}] \leavevmode
irqaction for the interrupt

\end{description}

\textbf{Description}

Used to remove interrupts statically setup by the early boot process.
\index{free\_irq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.free_irq}\pysiglinewithargsret{const void * \bfcode{free\_irq}}{unsigned int\emph{ irq}, void *\emph{ dev\_id}}{}
free an interrupt allocated with request\_irq

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt line to free

\item[{\code{void * dev\_id}}] \leavevmode
Device identity to free

\end{description}

\textbf{Description}
\begin{quote}

Remove an interrupt handler. The handler is removed and if the
interrupt line is no longer in use by any driver it is disabled.
On a shared IRQ the caller must ensure the interrupt is disabled
on the card it drives before calling this function. The function
does not return until any executing interrupts for this IRQ
have completed.

This function must not be called from interrupt context.

Returns the devname argument passed to request\_irq.
\end{quote}
\index{request\_threaded\_irq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.request_threaded_irq}\pysiglinewithargsret{int \bfcode{request\_threaded\_irq}}{unsigned int\emph{ irq}, irq\_handler\_t\emph{ handler}, irq\_handler\_t\emph{ thread\_fn}, unsigned long\emph{ irqflags}, const char *\emph{ devname}, void *\emph{ dev\_id}}{}
allocate an interrupt line

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt line to allocate

\item[{\code{irq\_handler\_t handler}}] \leavevmode
Function to be called when the IRQ occurs.
Primary handler for threaded interrupts
If NULL and thread\_fn != NULL the default
primary handler is installed

\item[{\code{irq\_handler\_t thread\_fn}}] \leavevmode
Function called from the irq handler thread
If NULL, no irq thread is created

\item[{\code{unsigned long irqflags}}] \leavevmode
Interrupt type flags

\item[{\code{const char * devname}}] \leavevmode
An ascii name for the claiming device

\item[{\code{void * dev\_id}}] \leavevmode
A cookie passed back to the handler function

\end{description}

\textbf{Description}
\begin{quote}

This call allocates interrupt resources and enables the
interrupt line and IRQ handling. From the point this
call is made your handler function may be invoked. Since
your handler function must clear any interrupt the board
raises, you must take care both to initialise your hardware
and to set up the interrupt handler in the right order.

If you want to set up a threaded irq handler for your device
then you need to supply \textbf{handler} and \textbf{thread\_fn}. \textbf{handler} is
still called in hard interrupt context and has to check
whether the interrupt originates from the device. If yes it
needs to disable the interrupt on the device and return
IRQ\_WAKE\_THREAD which will wake up the handler thread and run
\textbf{thread\_fn}. This split handler design is necessary to support
shared interrupts.

Dev\_id must be globally unique. Normally the address of the
device data structure is used as the cookie. Since the handler
receives this value it makes sense to use it.

If your interrupt is shared you must pass a non NULL dev\_id
as this is required when freeing the interrupt.

Flags:

IRQF\_SHARED             Interrupt is shared
IRQF\_TRIGGER\_*          Specify active edge(s) or level
\end{quote}
\index{request\_any\_context\_irq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.request_any_context_irq}\pysiglinewithargsret{int \bfcode{request\_any\_context\_irq}}{unsigned int\emph{ irq}, irq\_handler\_t\emph{ handler}, unsigned long\emph{ flags}, const char *\emph{ name}, void *\emph{ dev\_id}}{}
allocate an interrupt line

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt line to allocate

\item[{\code{irq\_handler\_t handler}}] \leavevmode
Function to be called when the IRQ occurs.
Threaded handler for threaded interrupts.

\item[{\code{unsigned long flags}}] \leavevmode
Interrupt type flags

\item[{\code{const char * name}}] \leavevmode
An ascii name for the claiming device

\item[{\code{void * dev\_id}}] \leavevmode
A cookie passed back to the handler function

\end{description}

\textbf{Description}
\begin{quote}

This call allocates interrupt resources and enables the
interrupt line and IRQ handling. It selects either a
hardirq or threaded handling method depending on the
context.

On failure, it returns a negative value. On success,
it returns either IRQC\_IS\_HARDIRQ or IRQC\_IS\_NESTED.
\end{quote}
\index{irq\_percpu\_is\_enabled (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_percpu_is_enabled}\pysiglinewithargsret{bool \bfcode{irq\_percpu\_is\_enabled}}{unsigned int\emph{ irq}}{}
Check whether the per cpu irq is enabled

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Linux irq number to check for

\end{description}

\textbf{Description}

Must be called from a non migratable context. Returns the enable
state of a per cpu interrupt on the current cpu.
\index{remove\_percpu\_irq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.remove_percpu_irq}\pysiglinewithargsret{void \bfcode{remove\_percpu\_irq}}{unsigned int\emph{ irq}, struct {\hyperref[core\string-api/genericirq:c.irqaction]{\emph{irqaction}}} *\emph{ act}}{}
free a per-cpu interrupt

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt line to free

\item[{\code{struct irqaction * act}}] \leavevmode
irqaction for the interrupt

\end{description}

\textbf{Description}

Used to remove interrupts statically setup by the early boot process.
\index{free\_percpu\_irq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.free_percpu_irq}\pysiglinewithargsret{void \bfcode{free\_percpu\_irq}}{unsigned int\emph{ irq}, void \_\_percpu *\emph{ dev\_id}}{}
free an interrupt allocated with request\_percpu\_irq

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt line to free

\item[{\code{void \_\_percpu * dev\_id}}] \leavevmode
Device identity to free

\end{description}

\textbf{Description}
\begin{quote}

Remove a percpu interrupt handler. The handler is removed, but
the interrupt line is not disabled. This must be done on each
CPU before calling this function. The function does not return
until any executing interrupts for this IRQ have completed.

This function must not be called from interrupt context.
\end{quote}
\index{setup\_percpu\_irq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.setup_percpu_irq}\pysiglinewithargsret{int \bfcode{setup\_percpu\_irq}}{unsigned int\emph{ irq}, struct {\hyperref[core\string-api/genericirq:c.irqaction]{\emph{irqaction}}} *\emph{ act}}{}
setup a per-cpu interrupt

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt line to setup

\item[{\code{struct irqaction * act}}] \leavevmode
irqaction for the interrupt

\end{description}

\textbf{Description}

Used to statically setup per-cpu interrupts in the early boot process.
\index{\_\_request\_percpu\_irq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.__request_percpu_irq}\pysiglinewithargsret{int \bfcode{\_\_request\_percpu\_irq}}{unsigned int\emph{ irq}, irq\_handler\_t\emph{ handler}, unsigned long\emph{ flags}, const char *\emph{ devname}, void \_\_percpu *\emph{ dev\_id}}{}
allocate a percpu interrupt line

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt line to allocate

\item[{\code{irq\_handler\_t handler}}] \leavevmode
Function to be called when the IRQ occurs.

\item[{\code{unsigned long flags}}] \leavevmode
Interrupt type flags (IRQF\_TIMER only)

\item[{\code{const char * devname}}] \leavevmode
An ascii name for the claiming device

\item[{\code{void \_\_percpu * dev\_id}}] \leavevmode
A percpu cookie passed back to the handler function

\end{description}

\textbf{Description}
\begin{quote}

This call allocates interrupt resources and enables the
interrupt on the local CPU. If the interrupt is supposed to be
enabled on other CPUs, it has to be done on each CPU using
\code{enable\_percpu\_irq()}.

Dev\_id must be globally unique. It is a per-cpu variable, and
the handler gets called with the interrupted CPU's instance of
that variable.
\end{quote}
\index{irq\_get\_irqchip\_state (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_get_irqchip_state}\pysiglinewithargsret{int \bfcode{irq\_get\_irqchip\_state}}{unsigned int\emph{ irq}, enum irqchip\_irq\_state\emph{ which}, bool *\emph{ state}}{}
returns the irqchip state of a interrupt.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt line that is forwarded to a VM

\item[{\code{enum irqchip\_irq\_state which}}] \leavevmode
One of IRQCHIP\_STATE\_* the caller wants to know about

\item[{\code{bool * state}}] \leavevmode
a pointer to a boolean where the state is to be storeed

\end{description}

\textbf{Description}
\begin{quote}

This call snapshots the internal irqchip state of an
interrupt, returning into \textbf{state} the bit corresponding to
stage \textbf{which}

This function should be called with preemption disabled if the
interrupt controller has per-cpu registers.
\end{quote}
\index{irq\_set\_irqchip\_state (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_set_irqchip_state}\pysiglinewithargsret{int \bfcode{irq\_set\_irqchip\_state}}{unsigned int\emph{ irq}, enum irqchip\_irq\_state\emph{ which}, bool\emph{ val}}{}
set the state of a forwarded interrupt.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt line that is forwarded to a VM

\item[{\code{enum irqchip\_irq\_state which}}] \leavevmode
State to be restored (one of IRQCHIP\_STATE\_*)

\item[{\code{bool val}}] \leavevmode
Value corresponding to \textbf{which}

\end{description}

\textbf{Description}
\begin{quote}

This call sets the internal irqchip state of an interrupt,
depending on the value of \textbf{which}.

This function should be called with preemption disabled if the
interrupt controller has per-cpu registers.
\end{quote}
\index{irq\_set\_chip (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_set_chip}\pysiglinewithargsret{int \bfcode{irq\_set\_chip}}{unsigned int\emph{ irq}, struct {\hyperref[core\string-api/genericirq:c.irq_chip]{\emph{irq\_chip}}} *\emph{ chip}}{}
set the irq chip for an irq

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
irq number

\item[{\code{struct irq\_chip * chip}}] \leavevmode
pointer to irq chip description structure

\end{description}
\index{irq\_set\_irq\_type (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_set_irq_type}\pysiglinewithargsret{int \bfcode{irq\_set\_irq\_type}}{unsigned int\emph{ irq}, unsigned int\emph{ type}}{}
set the irq trigger type for an irq

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
irq number

\item[{\code{unsigned int type}}] \leavevmode
IRQ\_TYPE\_\{LEVEL,EDGE\}\_* value - see include/linux/irq.h

\end{description}
\index{irq\_set\_handler\_data (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_set_handler_data}\pysiglinewithargsret{int \bfcode{irq\_set\_handler\_data}}{unsigned int\emph{ irq}, void *\emph{ data}}{}
set irq handler data for an irq

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt number

\item[{\code{void * data}}] \leavevmode
Pointer to interrupt specific data

\end{description}

\textbf{Description}
\begin{quote}

Set the hardware irq controller data for an irq
\end{quote}
\index{irq\_set\_msi\_desc\_off (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_set_msi_desc_off}\pysiglinewithargsret{int \bfcode{irq\_set\_msi\_desc\_off}}{unsigned int\emph{ irq\_base}, unsigned int\emph{ irq\_offset}, struct msi\_desc *\emph{ entry}}{}
set MSI descriptor data for an irq at offset

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq\_base}}] \leavevmode
Interrupt number base

\item[{\code{unsigned int irq\_offset}}] \leavevmode
Interrupt number offset

\item[{\code{struct msi\_desc * entry}}] \leavevmode
Pointer to MSI descriptor data

\end{description}

\textbf{Description}
\begin{quote}

Set the MSI descriptor entry for an irq at offset
\end{quote}
\index{irq\_set\_msi\_desc (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_set_msi_desc}\pysiglinewithargsret{int \bfcode{irq\_set\_msi\_desc}}{unsigned int\emph{ irq}, struct msi\_desc *\emph{ entry}}{}
set MSI descriptor data for an irq

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt number

\item[{\code{struct msi\_desc * entry}}] \leavevmode
Pointer to MSI descriptor data

\end{description}

\textbf{Description}
\begin{quote}

Set the MSI descriptor entry for an irq
\end{quote}
\index{irq\_set\_chip\_data (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_set_chip_data}\pysiglinewithargsret{int \bfcode{irq\_set\_chip\_data}}{unsigned int\emph{ irq}, void *\emph{ data}}{}
set irq chip data for an irq

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt number

\item[{\code{void * data}}] \leavevmode
Pointer to chip specific data

\end{description}

\textbf{Description}
\begin{quote}

Set the hardware irq chip data for an irq
\end{quote}
\index{irq\_disable (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_disable}\pysiglinewithargsret{void \bfcode{irq\_disable}}{struct irq\_desc *\emph{ desc}}{}
Mark interrupt disabled

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_desc * desc}}] \leavevmode
irq descriptor which should be disabled

\end{description}

\textbf{Description}

If the chip does not implement the irq\_disable callback, we
use a lazy disable approach. That means we mark the interrupt
disabled, but leave the hardware unmasked. That's an
optimization because we avoid the hardware access for the
common case where no interrupt happens after we marked it
disabled. If an interrupt happens, then the interrupt flow
handler masks the line at the hardware level and marks it
pending.

If the interrupt chip does not implement the irq\_disable callback,
a driver can disable the lazy approach for a particular irq line by
calling `irq\_set\_status\_flags(irq, IRQ\_DISABLE\_UNLAZY)'. This can
be used for devices which cannot disable the interrupt at the
device level under certain circumstances and have to use
disable\_irq{[}\_nosync{]} instead.
\index{handle\_simple\_irq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.handle_simple_irq}\pysiglinewithargsret{void \bfcode{handle\_simple\_irq}}{struct irq\_desc *\emph{ desc}}{}
Simple and software-decoded IRQs.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_desc * desc}}] \leavevmode
the interrupt description structure for this irq

\end{description}

\textbf{Description}
\begin{quote}

Simple interrupts are either sent from a demultiplexing interrupt
handler or come from hardware, where no interrupt hardware control
is necessary.
\end{quote}

\textbf{Note}
\begin{description}
\item[{The caller is expected to handle the ack, clear, mask and}] \leavevmode
unmask issues if necessary.

\end{description}
\index{handle\_untracked\_irq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.handle_untracked_irq}\pysiglinewithargsret{void \bfcode{handle\_untracked\_irq}}{struct irq\_desc *\emph{ desc}}{}
Simple and software-decoded IRQs.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_desc * desc}}] \leavevmode
the interrupt description structure for this irq

\end{description}

\textbf{Description}
\begin{quote}

Untracked interrupts are sent from a demultiplexing interrupt
handler when the demultiplexer does not know which device it its
multiplexed irq domain generated the interrupt. IRQ's handled
through here are not subjected to stats tracking, randomness, or
spurious interrupt detection.
\end{quote}

\textbf{Note}
\begin{description}
\item[{Like handle\_simple\_irq, the caller is expected to handle}] \leavevmode
the ack, clear, mask and unmask issues if necessary.

\end{description}
\index{handle\_level\_irq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.handle_level_irq}\pysiglinewithargsret{void \bfcode{handle\_level\_irq}}{struct irq\_desc *\emph{ desc}}{}
Level type irq handler

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_desc * desc}}] \leavevmode
the interrupt description structure for this irq

\end{description}

\textbf{Description}
\begin{quote}

Level type interrupts are active as long as the hardware line has
the active level. This may require to mask the interrupt and unmask
it after the associated handler has acknowledged the device, so the
interrupt line is back to inactive.
\end{quote}
\index{handle\_fasteoi\_irq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.handle_fasteoi_irq}\pysiglinewithargsret{void \bfcode{handle\_fasteoi\_irq}}{struct irq\_desc *\emph{ desc}}{}
irq handler for transparent controllers

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_desc * desc}}] \leavevmode
the interrupt description structure for this irq

\end{description}

\textbf{Description}
\begin{quote}

Only a single callback will be issued to the chip: an -\textgreater{}:c:func:\emph{eoi()}
call when the interrupt has been serviced. This enables support
for modern forms of interrupt handlers, which handle the flow
details in hardware, transparently.
\end{quote}
\index{handle\_edge\_irq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.handle_edge_irq}\pysiglinewithargsret{void \bfcode{handle\_edge\_irq}}{struct irq\_desc *\emph{ desc}}{}
edge type IRQ handler

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_desc * desc}}] \leavevmode
the interrupt description structure for this irq

\end{description}

\textbf{Description}
\begin{quote}

Interrupt occures on the falling and/or rising edge of a hardware
signal. The occurrence is latched into the irq controller hardware
and must be acked in order to be reenabled. After the ack another
interrupt can happen on the same source even before the first one
is handled by the associated event handler. If this happens it
might be necessary to disable (mask) the interrupt depending on the
controller hardware. This requires to reenable the interrupt inside
of the loop which handles the interrupts which have arrived while
the handler was running. If all pending interrupts are handled, the
loop is left.
\end{quote}
\index{handle\_edge\_eoi\_irq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.handle_edge_eoi_irq}\pysiglinewithargsret{void \bfcode{handle\_edge\_eoi\_irq}}{struct irq\_desc *\emph{ desc}}{}
edge eoi type IRQ handler

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_desc * desc}}] \leavevmode
the interrupt description structure for this irq

\end{description}

\textbf{Description}

Similar as the above handle\_edge\_irq, but using eoi and w/o the
mask/unmask logic.
\index{handle\_percpu\_irq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.handle_percpu_irq}\pysiglinewithargsret{void \bfcode{handle\_percpu\_irq}}{struct irq\_desc *\emph{ desc}}{}
Per CPU local irq handler

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_desc * desc}}] \leavevmode
the interrupt description structure for this irq

\end{description}

\textbf{Description}
\begin{quote}

Per CPU interrupts on SMP machines without locking requirements
\end{quote}
\index{handle\_percpu\_devid\_irq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.handle_percpu_devid_irq}\pysiglinewithargsret{void \bfcode{handle\_percpu\_devid\_irq}}{struct irq\_desc *\emph{ desc}}{}
Per CPU local irq handler with per cpu dev ids

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_desc * desc}}] \leavevmode
the interrupt description structure for this irq

\end{description}

\textbf{Description}

Per CPU interrupts on SMP machines without locking requirements. Same as
{\hyperref[core\string-api/genericirq:c.handle_percpu_irq]{\emph{\code{handle\_percpu\_irq()}}}} above but with the following extras:

action-\textgreater{}percpu\_dev\_id is a pointer to percpu variables which
contain the real device id for the cpu on which this handler is
called
\index{irq\_cpu\_online (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_cpu_online}\pysiglinewithargsret{void \bfcode{irq\_cpu\_online}}{void}{}
Invoke all irq\_cpu\_online functions.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}
\begin{quote}

Iterate through all irqs and invoke the chip.:c:func:\emph{irq\_cpu\_online()}
for each.
\end{quote}
\index{irq\_cpu\_offline (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_cpu_offline}\pysiglinewithargsret{void \bfcode{irq\_cpu\_offline}}{void}{}
Invoke all irq\_cpu\_offline functions.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}
\begin{quote}

Iterate through all irqs and invoke the chip.:c:func:\emph{irq\_cpu\_offline()}
for each.
\end{quote}
\index{handle\_fasteoi\_ack\_irq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.handle_fasteoi_ack_irq}\pysiglinewithargsret{void \bfcode{handle\_fasteoi\_ack\_irq}}{struct irq\_desc *\emph{ desc}}{}
irq handler for edge hierarchy stacked on transparent controllers

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_desc * desc}}] \leavevmode
the interrupt description structure for this irq

\end{description}

\textbf{Description}
\begin{quote}

Like {\hyperref[core\string-api/genericirq:c.handle_fasteoi_irq]{\emph{\code{handle\_fasteoi\_irq()}}}}, but for use with hierarchy where
the irq\_chip also needs to have its -\textgreater{}:c:func:\emph{irq\_ack()} function
called.
\end{quote}
\index{handle\_fasteoi\_mask\_irq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.handle_fasteoi_mask_irq}\pysiglinewithargsret{void \bfcode{handle\_fasteoi\_mask\_irq}}{struct irq\_desc *\emph{ desc}}{}
irq handler for level hierarchy stacked on transparent controllers

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_desc * desc}}] \leavevmode
the interrupt description structure for this irq

\end{description}

\textbf{Description}
\begin{quote}

Like {\hyperref[core\string-api/genericirq:c.handle_fasteoi_irq]{\emph{\code{handle\_fasteoi\_irq()}}}}, but for use with hierarchy where
the irq\_chip also needs to have its -\textgreater{}:c:func:\emph{irq\_mask\_ack()} function
called.
\end{quote}
\index{irq\_chip\_enable\_parent (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_chip_enable_parent}\pysiglinewithargsret{void \bfcode{irq\_chip\_enable\_parent}}{struct {\hyperref[core\string-api/genericirq:c.irq_data]{\emph{irq\_data}}} *\emph{ data}}{}
Enable the parent interrupt (defaults to unmask if NULL)

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_data * data}}] \leavevmode
Pointer to interrupt specific data

\end{description}
\index{irq\_chip\_disable\_parent (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_chip_disable_parent}\pysiglinewithargsret{void \bfcode{irq\_chip\_disable\_parent}}{struct {\hyperref[core\string-api/genericirq:c.irq_data]{\emph{irq\_data}}} *\emph{ data}}{}
Disable the parent interrupt (defaults to mask if NULL)

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_data * data}}] \leavevmode
Pointer to interrupt specific data

\end{description}
\index{irq\_chip\_ack\_parent (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_chip_ack_parent}\pysiglinewithargsret{void \bfcode{irq\_chip\_ack\_parent}}{struct {\hyperref[core\string-api/genericirq:c.irq_data]{\emph{irq\_data}}} *\emph{ data}}{}
Acknowledge the parent interrupt

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_data * data}}] \leavevmode
Pointer to interrupt specific data

\end{description}
\index{irq\_chip\_mask\_parent (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_chip_mask_parent}\pysiglinewithargsret{void \bfcode{irq\_chip\_mask\_parent}}{struct {\hyperref[core\string-api/genericirq:c.irq_data]{\emph{irq\_data}}} *\emph{ data}}{}
Mask the parent interrupt

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_data * data}}] \leavevmode
Pointer to interrupt specific data

\end{description}
\index{irq\_chip\_unmask\_parent (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_chip_unmask_parent}\pysiglinewithargsret{void \bfcode{irq\_chip\_unmask\_parent}}{struct {\hyperref[core\string-api/genericirq:c.irq_data]{\emph{irq\_data}}} *\emph{ data}}{}
Unmask the parent interrupt

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_data * data}}] \leavevmode
Pointer to interrupt specific data

\end{description}
\index{irq\_chip\_eoi\_parent (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_chip_eoi_parent}\pysiglinewithargsret{void \bfcode{irq\_chip\_eoi\_parent}}{struct {\hyperref[core\string-api/genericirq:c.irq_data]{\emph{irq\_data}}} *\emph{ data}}{}
Invoke EOI on the parent interrupt

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_data * data}}] \leavevmode
Pointer to interrupt specific data

\end{description}
\index{irq\_chip\_set\_affinity\_parent (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_chip_set_affinity_parent}\pysiglinewithargsret{int \bfcode{irq\_chip\_set\_affinity\_parent}}{struct {\hyperref[core\string-api/genericirq:c.irq_data]{\emph{irq\_data}}} *\emph{ data}, const struct cpumask *\emph{ dest}, bool\emph{ force}}{}
Set affinity on the parent interrupt

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_data * data}}] \leavevmode
Pointer to interrupt specific data

\item[{\code{const struct cpumask * dest}}] \leavevmode
The affinity mask to set

\item[{\code{bool force}}] \leavevmode
Flag to enforce setting (disable online checks)

\end{description}

\textbf{Description}

Conditinal, as the underlying parent chip might not implement it.
\index{irq\_chip\_set\_type\_parent (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_chip_set_type_parent}\pysiglinewithargsret{int \bfcode{irq\_chip\_set\_type\_parent}}{struct {\hyperref[core\string-api/genericirq:c.irq_data]{\emph{irq\_data}}} *\emph{ data}, unsigned int\emph{ type}}{}
Set IRQ type on the parent interrupt

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_data * data}}] \leavevmode
Pointer to interrupt specific data

\item[{\code{unsigned int type}}] \leavevmode
IRQ\_TYPE\_\{LEVEL,EDGE\}\_* value - see include/linux/irq.h

\end{description}

\textbf{Description}

Conditional, as the underlying parent chip might not implement it.
\index{irq\_chip\_retrigger\_hierarchy (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_chip_retrigger_hierarchy}\pysiglinewithargsret{int \bfcode{irq\_chip\_retrigger\_hierarchy}}{struct {\hyperref[core\string-api/genericirq:c.irq_data]{\emph{irq\_data}}} *\emph{ data}}{}
Retrigger an interrupt in hardware

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_data * data}}] \leavevmode
Pointer to interrupt specific data

\end{description}

\textbf{Description}

Iterate through the domain hierarchy of the interrupt and check
whether a hw retrigger function exists. If yes, invoke it.
\index{irq\_chip\_set\_vcpu\_affinity\_parent (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_chip_set_vcpu_affinity_parent}\pysiglinewithargsret{int \bfcode{irq\_chip\_set\_vcpu\_affinity\_parent}}{struct {\hyperref[core\string-api/genericirq:c.irq_data]{\emph{irq\_data}}} *\emph{ data}, void *\emph{ vcpu\_info}}{}
Set vcpu affinity on the parent interrupt

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_data * data}}] \leavevmode
Pointer to interrupt specific data

\item[{\code{void * vcpu\_info}}] \leavevmode
The vcpu affinity information

\end{description}
\index{irq\_chip\_set\_wake\_parent (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_chip_set_wake_parent}\pysiglinewithargsret{int \bfcode{irq\_chip\_set\_wake\_parent}}{struct {\hyperref[core\string-api/genericirq:c.irq_data]{\emph{irq\_data}}} *\emph{ data}, unsigned int\emph{ on}}{}
Set/reset wake-up on the parent interrupt

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_data * data}}] \leavevmode
Pointer to interrupt specific data

\item[{\code{unsigned int on}}] \leavevmode
Whether to set or reset the wake-up capability of this irq

\end{description}

\textbf{Description}

Conditional, as the underlying parent chip might not implement it.
\index{irq\_chip\_compose\_msi\_msg (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_chip_compose_msi_msg}\pysiglinewithargsret{int \bfcode{irq\_chip\_compose\_msi\_msg}}{struct {\hyperref[core\string-api/genericirq:c.irq_data]{\emph{irq\_data}}} *\emph{ data}, struct msi\_msg *\emph{ msg}}{}
Componse msi message for a irq chip

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_data * data}}] \leavevmode
Pointer to interrupt specific data

\item[{\code{struct msi\_msg * msg}}] \leavevmode
Pointer to the MSI message

\end{description}

\textbf{Description}

For hierarchical domains we find the first chip in the hierarchy
which implements the irq\_compose\_msi\_msg callback. For non
hierarchical we use the top level chip.
\index{irq\_chip\_pm\_get (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_chip_pm_get}\pysiglinewithargsret{int \bfcode{irq\_chip\_pm\_get}}{struct {\hyperref[core\string-api/genericirq:c.irq_data]{\emph{irq\_data}}} *\emph{ data}}{}
Enable power for an IRQ chip

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_data * data}}] \leavevmode
Pointer to interrupt specific data

\end{description}

\textbf{Description}

Enable the power to the IRQ chip referenced by the interrupt data
structure.
\index{irq\_chip\_pm\_put (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_chip_pm_put}\pysiglinewithargsret{int \bfcode{irq\_chip\_pm\_put}}{struct {\hyperref[core\string-api/genericirq:c.irq_data]{\emph{irq\_data}}} *\emph{ data}}{}
Disable power for an IRQ chip

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_data * data}}] \leavevmode
Pointer to interrupt specific data

\end{description}

\textbf{Description}

Disable the power to the IRQ chip referenced by the interrupt data
structure, belongs. Note that power will only be disabled, once this
function has been called for all IRQs that have called {\hyperref[core\string-api/genericirq:c.irq_chip_pm_get]{\emph{\code{irq\_chip\_pm\_get()}}}}.


\subsection{Internal Functions Provided}
\label{core-api/genericirq:internal-functions-provided}
This chapter contains the autogenerated documentation of the internal
functions.
\index{generic\_handle\_irq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.generic_handle_irq}\pysiglinewithargsret{int \bfcode{generic\_handle\_irq}}{unsigned int\emph{ irq}}{}
Invoke the handler for a particular irq

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
The irq number to handle

\end{description}
\index{\_\_handle\_domain\_irq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.__handle_domain_irq}\pysiglinewithargsret{int \bfcode{\_\_handle\_domain\_irq}}{struct irq\_domain *\emph{ domain}, unsigned int\emph{ hwirq}, bool\emph{ lookup}, struct pt\_regs *\emph{ regs}}{}
Invoke the handler for a HW irq belonging to a domain

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_domain * domain}}] \leavevmode
The domain where to perform the lookup

\item[{\code{unsigned int hwirq}}] \leavevmode
The HW irq number to convert to a logical one

\item[{\code{bool lookup}}] \leavevmode
Whether to perform the domain lookup or not

\item[{\code{struct pt\_regs * regs}}] \leavevmode
Register file coming from the low-level handling code

\end{description}

\textbf{Return}

0 on success, or -EINVAL if conversion has failed
\index{irq\_free\_descs (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_free_descs}\pysiglinewithargsret{void \bfcode{irq\_free\_descs}}{unsigned int\emph{ from}, unsigned int\emph{ cnt}}{}
free irq descriptors

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int from}}] \leavevmode
Start of descriptor range

\item[{\code{unsigned int cnt}}] \leavevmode
Number of consecutive irqs to free

\end{description}
\index{\_\_irq\_alloc\_descs (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.__irq_alloc_descs}\pysiglinewithargsret{int \_\_ref \bfcode{\_\_irq\_alloc\_descs}}{int\emph{ irq}, unsigned int\emph{ from}, unsigned int\emph{ cnt}, int\emph{ node}, struct module *\emph{ owner}, const struct cpumask *\emph{ affinity}}{}
allocate and initialize a range of irq descriptors

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{int irq}}] \leavevmode
Allocate for specific irq number if irq \textgreater{}= 0

\item[{\code{unsigned int from}}] \leavevmode
Start the search from this irq number

\item[{\code{unsigned int cnt}}] \leavevmode
Number of consecutive irqs to allocate.

\item[{\code{int node}}] \leavevmode
Preferred node on which the irq descriptor should be allocated

\item[{\code{struct module * owner}}] \leavevmode
Owning module (can be NULL)

\item[{\code{const struct cpumask * affinity}}] \leavevmode
Optional pointer to an affinity mask array of size \textbf{cnt} which
hints where the irq descriptors should be allocated and which
default affinities to use

\end{description}

\textbf{Description}

Returns the first irq number or error code
\index{irq\_alloc\_hwirqs (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_alloc_hwirqs}\pysiglinewithargsret{unsigned int \bfcode{irq\_alloc\_hwirqs}}{int\emph{ cnt}, int\emph{ node}}{}
Allocate an irq descriptor and initialize the hardware

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{int cnt}}] \leavevmode
number of interrupts to allocate

\item[{\code{int node}}] \leavevmode
node on which to allocate

\end{description}

\textbf{Description}

Returns an interrupt number \textgreater{} 0 or 0, if the allocation fails.
\index{irq\_free\_hwirqs (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_free_hwirqs}\pysiglinewithargsret{void \bfcode{irq\_free\_hwirqs}}{unsigned int\emph{ from}, int\emph{ cnt}}{}
Free irq descriptor and cleanup the hardware

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int from}}] \leavevmode
Free from irq number

\item[{\code{int cnt}}] \leavevmode
number of interrupts to free

\end{description}
\index{irq\_get\_next\_irq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.irq_get_next_irq}\pysiglinewithargsret{unsigned int \bfcode{irq\_get\_next\_irq}}{unsigned int\emph{ offset}}{}
get next allocated irq number

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int offset}}] \leavevmode
where to start the search

\end{description}

\textbf{Description}

Returns next irq number after offset or nr\_irqs if none is found.
\index{kstat\_irqs\_cpu (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.kstat_irqs_cpu}\pysiglinewithargsret{unsigned int \bfcode{kstat\_irqs\_cpu}}{unsigned int\emph{ irq}, int\emph{ cpu}}{}
Get the statistics for an interrupt on a cpu

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
The interrupt number

\item[{\code{int cpu}}] \leavevmode
The cpu number

\end{description}

\textbf{Description}

Returns the sum of interrupt counts on \textbf{cpu} since boot for
\textbf{irq}. The caller must ensure that the interrupt is not removed
concurrently.
\index{kstat\_irqs (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.kstat_irqs}\pysiglinewithargsret{unsigned int \bfcode{kstat\_irqs}}{unsigned int\emph{ irq}}{}
Get the statistics for an interrupt

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
The interrupt number

\end{description}

\textbf{Description}

Returns the sum of interrupt counts on all cpus since boot for
\textbf{irq}. The caller must ensure that the interrupt is not removed
concurrently.
\index{kstat\_irqs\_usr (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.kstat_irqs_usr}\pysiglinewithargsret{unsigned int \bfcode{kstat\_irqs\_usr}}{unsigned int\emph{ irq}}{}
Get the statistics for an interrupt

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
The interrupt number

\end{description}

\textbf{Description}

Returns the sum of interrupt counts on all cpus since boot for
\textbf{irq}. Contrary to {\hyperref[core\string-api/genericirq:c.kstat_irqs]{\emph{\code{kstat\_irqs()}}}} this can be called from any
preemptible context. It's protected against concurrent removal of
an interrupt descriptor when sparse irqs are enabled.
\index{handle\_bad\_irq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genericirq:c.handle_bad_irq}\pysiglinewithargsret{void \bfcode{handle\_bad\_irq}}{struct irq\_desc *\emph{ desc}}{}
handle spurious and unhandled irqs

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_desc * desc}}] \leavevmode
description of the interrupt

\end{description}

\textbf{Description}

Handles spurious and unhandled IRQ's. It also prints a debugmessage.
\index{irq\_set\_chip (C function)}

\begin{fulllineitems}
\pysiglinewithargsret{int \bfcode{irq\_set\_chip}}{unsigned int\emph{ irq}, struct {\hyperref[core\string-api/genericirq:c.irq_chip]{\emph{irq\_chip}}} *\emph{ chip}}{}
set the irq chip for an irq

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
irq number

\item[{\code{struct irq\_chip * chip}}] \leavevmode
pointer to irq chip description structure

\end{description}
\index{irq\_set\_irq\_type (C function)}

\begin{fulllineitems}
\pysiglinewithargsret{int \bfcode{irq\_set\_irq\_type}}{unsigned int\emph{ irq}, unsigned int\emph{ type}}{}
set the irq trigger type for an irq

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
irq number

\item[{\code{unsigned int type}}] \leavevmode
IRQ\_TYPE\_\{LEVEL,EDGE\}\_* value - see include/linux/irq.h

\end{description}
\index{irq\_set\_handler\_data (C function)}

\begin{fulllineitems}
\pysiglinewithargsret{int \bfcode{irq\_set\_handler\_data}}{unsigned int\emph{ irq}, void *\emph{ data}}{}
set irq handler data for an irq

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt number

\item[{\code{void * data}}] \leavevmode
Pointer to interrupt specific data

\end{description}

\textbf{Description}
\begin{quote}

Set the hardware irq controller data for an irq
\end{quote}
\index{irq\_set\_msi\_desc\_off (C function)}

\begin{fulllineitems}
\pysiglinewithargsret{int \bfcode{irq\_set\_msi\_desc\_off}}{unsigned int\emph{ irq\_base}, unsigned int\emph{ irq\_offset}, struct msi\_desc *\emph{ entry}}{}
set MSI descriptor data for an irq at offset

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq\_base}}] \leavevmode
Interrupt number base

\item[{\code{unsigned int irq\_offset}}] \leavevmode
Interrupt number offset

\item[{\code{struct msi\_desc * entry}}] \leavevmode
Pointer to MSI descriptor data

\end{description}

\textbf{Description}
\begin{quote}

Set the MSI descriptor entry for an irq at offset
\end{quote}
\index{irq\_set\_msi\_desc (C function)}

\begin{fulllineitems}
\pysiglinewithargsret{int \bfcode{irq\_set\_msi\_desc}}{unsigned int\emph{ irq}, struct msi\_desc *\emph{ entry}}{}
set MSI descriptor data for an irq

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt number

\item[{\code{struct msi\_desc * entry}}] \leavevmode
Pointer to MSI descriptor data

\end{description}

\textbf{Description}
\begin{quote}

Set the MSI descriptor entry for an irq
\end{quote}
\index{irq\_set\_chip\_data (C function)}

\begin{fulllineitems}
\pysiglinewithargsret{int \bfcode{irq\_set\_chip\_data}}{unsigned int\emph{ irq}, void *\emph{ data}}{}
set irq chip data for an irq

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int irq}}] \leavevmode
Interrupt number

\item[{\code{void * data}}] \leavevmode
Pointer to chip specific data

\end{description}

\textbf{Description}
\begin{quote}

Set the hardware irq chip data for an irq
\end{quote}
\index{irq\_disable (C function)}

\begin{fulllineitems}
\pysiglinewithargsret{void \bfcode{irq\_disable}}{struct irq\_desc *\emph{ desc}}{}
Mark interrupt disabled

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_desc * desc}}] \leavevmode
irq descriptor which should be disabled

\end{description}

\textbf{Description}

If the chip does not implement the irq\_disable callback, we
use a lazy disable approach. That means we mark the interrupt
disabled, but leave the hardware unmasked. That's an
optimization because we avoid the hardware access for the
common case where no interrupt happens after we marked it
disabled. If an interrupt happens, then the interrupt flow
handler masks the line at the hardware level and marks it
pending.

If the interrupt chip does not implement the irq\_disable callback,
a driver can disable the lazy approach for a particular irq line by
calling `irq\_set\_status\_flags(irq, IRQ\_DISABLE\_UNLAZY)'. This can
be used for devices which cannot disable the interrupt at the
device level under certain circumstances and have to use
disable\_irq{[}\_nosync{]} instead.
\index{handle\_simple\_irq (C function)}

\begin{fulllineitems}
\pysiglinewithargsret{void \bfcode{handle\_simple\_irq}}{struct irq\_desc *\emph{ desc}}{}
Simple and software-decoded IRQs.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_desc * desc}}] \leavevmode
the interrupt description structure for this irq

\end{description}

\textbf{Description}
\begin{quote}

Simple interrupts are either sent from a demultiplexing interrupt
handler or come from hardware, where no interrupt hardware control
is necessary.
\end{quote}

\textbf{Note}
\begin{description}
\item[{The caller is expected to handle the ack, clear, mask and}] \leavevmode
unmask issues if necessary.

\end{description}
\index{handle\_untracked\_irq (C function)}

\begin{fulllineitems}
\pysiglinewithargsret{void \bfcode{handle\_untracked\_irq}}{struct irq\_desc *\emph{ desc}}{}
Simple and software-decoded IRQs.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_desc * desc}}] \leavevmode
the interrupt description structure for this irq

\end{description}

\textbf{Description}
\begin{quote}

Untracked interrupts are sent from a demultiplexing interrupt
handler when the demultiplexer does not know which device it its
multiplexed irq domain generated the interrupt. IRQ's handled
through here are not subjected to stats tracking, randomness, or
spurious interrupt detection.
\end{quote}

\textbf{Note}
\begin{description}
\item[{Like handle\_simple\_irq, the caller is expected to handle}] \leavevmode
the ack, clear, mask and unmask issues if necessary.

\end{description}
\index{handle\_level\_irq (C function)}

\begin{fulllineitems}
\pysiglinewithargsret{void \bfcode{handle\_level\_irq}}{struct irq\_desc *\emph{ desc}}{}
Level type irq handler

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_desc * desc}}] \leavevmode
the interrupt description structure for this irq

\end{description}

\textbf{Description}
\begin{quote}

Level type interrupts are active as long as the hardware line has
the active level. This may require to mask the interrupt and unmask
it after the associated handler has acknowledged the device, so the
interrupt line is back to inactive.
\end{quote}
\index{handle\_fasteoi\_irq (C function)}

\begin{fulllineitems}
\pysiglinewithargsret{void \bfcode{handle\_fasteoi\_irq}}{struct irq\_desc *\emph{ desc}}{}
irq handler for transparent controllers

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_desc * desc}}] \leavevmode
the interrupt description structure for this irq

\end{description}

\textbf{Description}
\begin{quote}

Only a single callback will be issued to the chip: an -\textgreater{}:c:func:\emph{eoi()}
call when the interrupt has been serviced. This enables support
for modern forms of interrupt handlers, which handle the flow
details in hardware, transparently.
\end{quote}
\index{handle\_edge\_irq (C function)}

\begin{fulllineitems}
\pysiglinewithargsret{void \bfcode{handle\_edge\_irq}}{struct irq\_desc *\emph{ desc}}{}
edge type IRQ handler

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_desc * desc}}] \leavevmode
the interrupt description structure for this irq

\end{description}

\textbf{Description}
\begin{quote}

Interrupt occures on the falling and/or rising edge of a hardware
signal. The occurrence is latched into the irq controller hardware
and must be acked in order to be reenabled. After the ack another
interrupt can happen on the same source even before the first one
is handled by the associated event handler. If this happens it
might be necessary to disable (mask) the interrupt depending on the
controller hardware. This requires to reenable the interrupt inside
of the loop which handles the interrupts which have arrived while
the handler was running. If all pending interrupts are handled, the
loop is left.
\end{quote}
\index{handle\_edge\_eoi\_irq (C function)}

\begin{fulllineitems}
\pysiglinewithargsret{void \bfcode{handle\_edge\_eoi\_irq}}{struct irq\_desc *\emph{ desc}}{}
edge eoi type IRQ handler

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_desc * desc}}] \leavevmode
the interrupt description structure for this irq

\end{description}

\textbf{Description}

Similar as the above handle\_edge\_irq, but using eoi and w/o the
mask/unmask logic.
\index{handle\_percpu\_irq (C function)}

\begin{fulllineitems}
\pysiglinewithargsret{void \bfcode{handle\_percpu\_irq}}{struct irq\_desc *\emph{ desc}}{}
Per CPU local irq handler

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_desc * desc}}] \leavevmode
the interrupt description structure for this irq

\end{description}

\textbf{Description}
\begin{quote}

Per CPU interrupts on SMP machines without locking requirements
\end{quote}
\index{handle\_percpu\_devid\_irq (C function)}

\begin{fulllineitems}
\pysiglinewithargsret{void \bfcode{handle\_percpu\_devid\_irq}}{struct irq\_desc *\emph{ desc}}{}
Per CPU local irq handler with per cpu dev ids

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_desc * desc}}] \leavevmode
the interrupt description structure for this irq

\end{description}

\textbf{Description}

Per CPU interrupts on SMP machines without locking requirements. Same as
{\hyperref[core\string-api/genericirq:c.handle_percpu_irq]{\emph{\code{handle\_percpu\_irq()}}}} above but with the following extras:

action-\textgreater{}percpu\_dev\_id is a pointer to percpu variables which
contain the real device id for the cpu on which this handler is
called
\index{irq\_cpu\_online (C function)}

\begin{fulllineitems}
\pysiglinewithargsret{void \bfcode{irq\_cpu\_online}}{void}{}
Invoke all irq\_cpu\_online functions.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}
\begin{quote}

Iterate through all irqs and invoke the chip.:c:func:\emph{irq\_cpu\_online()}
for each.
\end{quote}
\index{irq\_cpu\_offline (C function)}

\begin{fulllineitems}
\pysiglinewithargsret{void \bfcode{irq\_cpu\_offline}}{void}{}
Invoke all irq\_cpu\_offline functions.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void}}] \leavevmode
no arguments

\end{description}

\textbf{Description}
\begin{quote}

Iterate through all irqs and invoke the chip.:c:func:\emph{irq\_cpu\_offline()}
for each.
\end{quote}
\index{handle\_fasteoi\_ack\_irq (C function)}

\begin{fulllineitems}
\pysiglinewithargsret{void \bfcode{handle\_fasteoi\_ack\_irq}}{struct irq\_desc *\emph{ desc}}{}
irq handler for edge hierarchy stacked on transparent controllers

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_desc * desc}}] \leavevmode
the interrupt description structure for this irq

\end{description}

\textbf{Description}
\begin{quote}

Like {\hyperref[core\string-api/genericirq:c.handle_fasteoi_irq]{\emph{\code{handle\_fasteoi\_irq()}}}}, but for use with hierarchy where
the irq\_chip also needs to have its -\textgreater{}:c:func:\emph{irq\_ack()} function
called.
\end{quote}
\index{handle\_fasteoi\_mask\_irq (C function)}

\begin{fulllineitems}
\pysiglinewithargsret{void \bfcode{handle\_fasteoi\_mask\_irq}}{struct irq\_desc *\emph{ desc}}{}
irq handler for level hierarchy stacked on transparent controllers

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_desc * desc}}] \leavevmode
the interrupt description structure for this irq

\end{description}

\textbf{Description}
\begin{quote}

Like {\hyperref[core\string-api/genericirq:c.handle_fasteoi_irq]{\emph{\code{handle\_fasteoi\_irq()}}}}, but for use with hierarchy where
the irq\_chip also needs to have its -\textgreater{}:c:func:\emph{irq\_mask\_ack()} function
called.
\end{quote}
\index{irq\_chip\_enable\_parent (C function)}

\begin{fulllineitems}
\pysiglinewithargsret{void \bfcode{irq\_chip\_enable\_parent}}{struct {\hyperref[core\string-api/genericirq:c.irq_data]{\emph{irq\_data}}} *\emph{ data}}{}
Enable the parent interrupt (defaults to unmask if NULL)

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_data * data}}] \leavevmode
Pointer to interrupt specific data

\end{description}
\index{irq\_chip\_disable\_parent (C function)}

\begin{fulllineitems}
\pysiglinewithargsret{void \bfcode{irq\_chip\_disable\_parent}}{struct {\hyperref[core\string-api/genericirq:c.irq_data]{\emph{irq\_data}}} *\emph{ data}}{}
Disable the parent interrupt (defaults to mask if NULL)

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_data * data}}] \leavevmode
Pointer to interrupt specific data

\end{description}
\index{irq\_chip\_ack\_parent (C function)}

\begin{fulllineitems}
\pysiglinewithargsret{void \bfcode{irq\_chip\_ack\_parent}}{struct {\hyperref[core\string-api/genericirq:c.irq_data]{\emph{irq\_data}}} *\emph{ data}}{}
Acknowledge the parent interrupt

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_data * data}}] \leavevmode
Pointer to interrupt specific data

\end{description}
\index{irq\_chip\_mask\_parent (C function)}

\begin{fulllineitems}
\pysiglinewithargsret{void \bfcode{irq\_chip\_mask\_parent}}{struct {\hyperref[core\string-api/genericirq:c.irq_data]{\emph{irq\_data}}} *\emph{ data}}{}
Mask the parent interrupt

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_data * data}}] \leavevmode
Pointer to interrupt specific data

\end{description}
\index{irq\_chip\_unmask\_parent (C function)}

\begin{fulllineitems}
\pysiglinewithargsret{void \bfcode{irq\_chip\_unmask\_parent}}{struct {\hyperref[core\string-api/genericirq:c.irq_data]{\emph{irq\_data}}} *\emph{ data}}{}
Unmask the parent interrupt

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_data * data}}] \leavevmode
Pointer to interrupt specific data

\end{description}
\index{irq\_chip\_eoi\_parent (C function)}

\begin{fulllineitems}
\pysiglinewithargsret{void \bfcode{irq\_chip\_eoi\_parent}}{struct {\hyperref[core\string-api/genericirq:c.irq_data]{\emph{irq\_data}}} *\emph{ data}}{}
Invoke EOI on the parent interrupt

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_data * data}}] \leavevmode
Pointer to interrupt specific data

\end{description}
\index{irq\_chip\_set\_affinity\_parent (C function)}

\begin{fulllineitems}
\pysiglinewithargsret{int \bfcode{irq\_chip\_set\_affinity\_parent}}{struct {\hyperref[core\string-api/genericirq:c.irq_data]{\emph{irq\_data}}} *\emph{ data}, const struct cpumask *\emph{ dest}, bool\emph{ force}}{}
Set affinity on the parent interrupt

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_data * data}}] \leavevmode
Pointer to interrupt specific data

\item[{\code{const struct cpumask * dest}}] \leavevmode
The affinity mask to set

\item[{\code{bool force}}] \leavevmode
Flag to enforce setting (disable online checks)

\end{description}

\textbf{Description}

Conditinal, as the underlying parent chip might not implement it.
\index{irq\_chip\_set\_type\_parent (C function)}

\begin{fulllineitems}
\pysiglinewithargsret{int \bfcode{irq\_chip\_set\_type\_parent}}{struct {\hyperref[core\string-api/genericirq:c.irq_data]{\emph{irq\_data}}} *\emph{ data}, unsigned int\emph{ type}}{}
Set IRQ type on the parent interrupt

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_data * data}}] \leavevmode
Pointer to interrupt specific data

\item[{\code{unsigned int type}}] \leavevmode
IRQ\_TYPE\_\{LEVEL,EDGE\}\_* value - see include/linux/irq.h

\end{description}

\textbf{Description}

Conditional, as the underlying parent chip might not implement it.
\index{irq\_chip\_retrigger\_hierarchy (C function)}

\begin{fulllineitems}
\pysiglinewithargsret{int \bfcode{irq\_chip\_retrigger\_hierarchy}}{struct {\hyperref[core\string-api/genericirq:c.irq_data]{\emph{irq\_data}}} *\emph{ data}}{}
Retrigger an interrupt in hardware

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_data * data}}] \leavevmode
Pointer to interrupt specific data

\end{description}

\textbf{Description}

Iterate through the domain hierarchy of the interrupt and check
whether a hw retrigger function exists. If yes, invoke it.
\index{irq\_chip\_set\_vcpu\_affinity\_parent (C function)}

\begin{fulllineitems}
\pysiglinewithargsret{int \bfcode{irq\_chip\_set\_vcpu\_affinity\_parent}}{struct {\hyperref[core\string-api/genericirq:c.irq_data]{\emph{irq\_data}}} *\emph{ data}, void *\emph{ vcpu\_info}}{}
Set vcpu affinity on the parent interrupt

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_data * data}}] \leavevmode
Pointer to interrupt specific data

\item[{\code{void * vcpu\_info}}] \leavevmode
The vcpu affinity information

\end{description}
\index{irq\_chip\_set\_wake\_parent (C function)}

\begin{fulllineitems}
\pysiglinewithargsret{int \bfcode{irq\_chip\_set\_wake\_parent}}{struct {\hyperref[core\string-api/genericirq:c.irq_data]{\emph{irq\_data}}} *\emph{ data}, unsigned int\emph{ on}}{}
Set/reset wake-up on the parent interrupt

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_data * data}}] \leavevmode
Pointer to interrupt specific data

\item[{\code{unsigned int on}}] \leavevmode
Whether to set or reset the wake-up capability of this irq

\end{description}

\textbf{Description}

Conditional, as the underlying parent chip might not implement it.
\index{irq\_chip\_compose\_msi\_msg (C function)}

\begin{fulllineitems}
\pysiglinewithargsret{int \bfcode{irq\_chip\_compose\_msi\_msg}}{struct {\hyperref[core\string-api/genericirq:c.irq_data]{\emph{irq\_data}}} *\emph{ data}, struct msi\_msg *\emph{ msg}}{}
Componse msi message for a irq chip

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_data * data}}] \leavevmode
Pointer to interrupt specific data

\item[{\code{struct msi\_msg * msg}}] \leavevmode
Pointer to the MSI message

\end{description}

\textbf{Description}

For hierarchical domains we find the first chip in the hierarchy
which implements the irq\_compose\_msi\_msg callback. For non
hierarchical we use the top level chip.
\index{irq\_chip\_pm\_get (C function)}

\begin{fulllineitems}
\pysiglinewithargsret{int \bfcode{irq\_chip\_pm\_get}}{struct {\hyperref[core\string-api/genericirq:c.irq_data]{\emph{irq\_data}}} *\emph{ data}}{}
Enable power for an IRQ chip

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_data * data}}] \leavevmode
Pointer to interrupt specific data

\end{description}

\textbf{Description}

Enable the power to the IRQ chip referenced by the interrupt data
structure.
\index{irq\_chip\_pm\_put (C function)}

\begin{fulllineitems}
\pysiglinewithargsret{int \bfcode{irq\_chip\_pm\_put}}{struct {\hyperref[core\string-api/genericirq:c.irq_data]{\emph{irq\_data}}} *\emph{ data}}{}
Disable power for an IRQ chip

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct irq\_data * data}}] \leavevmode
Pointer to interrupt specific data

\end{description}

\textbf{Description}

Disable the power to the IRQ chip referenced by the interrupt data
structure, belongs. Note that power will only be disabled, once this
function has been called for all IRQs that have called {\hyperref[core\string-api/genericirq:c.irq_chip_pm_get]{\emph{\code{irq\_chip\_pm\_get()}}}}.


\subsection{Credits}
\label{core-api/genericirq:credits}
The following people have contributed to this document:
\begin{enumerate}
\item {} 
Thomas Gleixner \href{mailto:tglx@linutronix.de}{tglx@linutronix.de}

\item {} 
Ingo Molnar \href{mailto:mingo@elte.hu}{mingo@elte.hu}

\end{enumerate}


\section{Using flexible arrays in the kernel}
\label{core-api/flexible-arrays:using-flexible-arrays-in-the-kernel}\label{core-api/flexible-arrays::doc}
Large contiguous memory allocations can be unreliable in the Linux kernel.
Kernel programmers will sometimes respond to this problem by allocating
pages with {\hyperref[core\string-api/kernel\string-api:c.vmalloc]{\emph{\code{vmalloc()}}}}.  This solution not ideal, though.  On 32-bit
systems, memory from vmalloc() must be mapped into a relatively small address
space; it's easy to run out.  On SMP systems, the page table changes required
by vmalloc() allocations can require expensive cross-processor interrupts on
all CPUs.  And, on all systems, use of space in the vmalloc() range increases
pressure on the translation lookaside buffer (TLB), reducing the performance
of the system.

In many cases, the need for memory from vmalloc() can be eliminated by piecing
together an array from smaller parts; the flexible array library exists to make
this task easier.

A flexible array holds an arbitrary (within limits) number of fixed-sized
objects, accessed via an integer index.  Sparse arrays are handled
reasonably well.  Only single-page allocations are made, so memory
allocation failures should be relatively rare.  The down sides are that the
arrays cannot be indexed directly, individual object size cannot exceed the
system page size, and putting data into a flexible array requires a copy
operation.  It's also worth noting that flexible arrays do no internal
locking at all; if concurrent access to an array is possible, then the
caller must arrange for appropriate mutual exclusion.

The creation of a flexible array is done with {\hyperref[core\string-api/flexible\string-arrays:c.flex_array_alloc]{\emph{\code{flex\_array\_alloc()}}}}:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZsh{}include \PYGZlt{}linux/flex\PYGZus{}array.h\PYGZgt{}

struct flex\PYGZus{}array *flex\PYGZus{}array\PYGZus{}alloc(int element\PYGZus{}size,
                                    unsigned int total,
                                    gfp\PYGZus{}t flags);
\end{Verbatim}

The individual object size is provided by \code{element\_size}, while total is the
maximum number of objects which can be stored in the array.  The flags
argument is passed directly to the internal memory allocation calls.  With
the current code, using flags to ask for high memory is likely to lead to
notably unpleasant side effects.

It is also possible to define flexible arrays at compile time with:

\begin{Verbatim}[commandchars=\\\{\}]
DEFINE\PYGZus{}FLEX\PYGZus{}ARRAY(name, element\PYGZus{}size, total);
\end{Verbatim}

This macro will result in a definition of an array with the given name; the
element size and total will be checked for validity at compile time.

Storing data into a flexible array is accomplished with a call to
{\hyperref[core\string-api/flexible\string-arrays:c.flex_array_put]{\emph{\code{flex\_array\_put()}}}}:

\begin{Verbatim}[commandchars=\\\{\}]
int flex\PYGZus{}array\PYGZus{}put(struct flex\PYGZus{}array *array, unsigned int element\PYGZus{}nr,
                   void *src, gfp\PYGZus{}t flags);
\end{Verbatim}

This call will copy the data from src into the array, in the position
indicated by \code{element\_nr} (which must be less than the maximum specified when
the array was created).  If any memory allocations must be performed, flags
will be used.  The return value is zero on success, a negative error code
otherwise.

There might possibly be a need to store data into a flexible array while
running in some sort of atomic context; in this situation, sleeping in the
memory allocator would be a bad thing.  That can be avoided by using
\code{GFP\_ATOMIC} for the flags value, but, often, there is a better way.  The
trick is to ensure that any needed memory allocations are done before
entering atomic context, using {\hyperref[core\string-api/flexible\string-arrays:c.flex_array_prealloc]{\emph{\code{flex\_array\_prealloc()}}}}:

\begin{Verbatim}[commandchars=\\\{\}]
int flex\PYGZus{}array\PYGZus{}prealloc(struct flex\PYGZus{}array *array, unsigned int start,
                        unsigned int nr\PYGZus{}elements, gfp\PYGZus{}t flags);
\end{Verbatim}

This function will ensure that memory for the elements indexed in the range
defined by \code{start} and \code{nr\_elements} has been allocated.  Thereafter, a
\code{flex\_array\_put()} call on an element in that range is guaranteed not to
block.

Getting data back out of the array is done with {\hyperref[core\string-api/flexible\string-arrays:c.flex_array_get]{\emph{\code{flex\_array\_get()}}}}:

\begin{Verbatim}[commandchars=\\\{\}]
void *flex\PYGZus{}array\PYGZus{}get(struct flex\PYGZus{}array *fa, unsigned int element\PYGZus{}nr);
\end{Verbatim}

The return value is a pointer to the data element, or NULL if that
particular element has never been allocated.

Note that it is possible to get back a valid pointer for an element which
has never been stored in the array.  Memory for array elements is allocated
one page at a time; a single allocation could provide memory for several
adjacent elements.  Flexible array elements are normally initialized to the
value \code{FLEX\_ARRAY\_FREE} (defined as 0x6c in \textless{}linux/poison.h\textgreater{}), so errors
involving that number probably result from use of unstored array entries.
Note that, if array elements are allocated with \code{\_\_GFP\_ZERO}, they will be
initialized to zero and this poisoning will not happen.

Individual elements in the array can be cleared with
{\hyperref[core\string-api/flexible\string-arrays:c.flex_array_clear]{\emph{\code{flex\_array\_clear()}}}}:

\begin{Verbatim}[commandchars=\\\{\}]
int flex\PYGZus{}array\PYGZus{}clear(struct flex\PYGZus{}array *array, unsigned int element\PYGZus{}nr);
\end{Verbatim}

This function will set the given element to \code{FLEX\_ARRAY\_FREE} and return
zero.  If storage for the indicated element is not allocated for the array,
\code{flex\_array\_clear()} will return \code{-EINVAL} instead.  Note that clearing an
element does not release the storage associated with it; to reduce the
allocated size of an array, call {\hyperref[core\string-api/flexible\string-arrays:c.flex_array_shrink]{\emph{\code{flex\_array\_shrink()}}}}:

\begin{Verbatim}[commandchars=\\\{\}]
int flex\PYGZus{}array\PYGZus{}shrink(struct flex\PYGZus{}array *array);
\end{Verbatim}

The return value will be the number of pages of memory actually freed.
This function works by scanning the array for pages containing nothing but
\code{FLEX\_ARRAY\_FREE} bytes, so (1) it can be expensive, and (2) it will not work
if the array's pages are allocated with \code{\_\_GFP\_ZERO}.

It is possible to remove all elements of an array with a call to
{\hyperref[core\string-api/flexible\string-arrays:c.flex_array_free_parts]{\emph{\code{flex\_array\_free\_parts()}}}}:

\begin{Verbatim}[commandchars=\\\{\}]
void flex\PYGZus{}array\PYGZus{}free\PYGZus{}parts(struct flex\PYGZus{}array *array);
\end{Verbatim}

This call frees all elements, but leaves the array itself in place.
Freeing the entire array is done with {\hyperref[core\string-api/flexible\string-arrays:c.flex_array_free]{\emph{\code{flex\_array\_free()}}}}:

\begin{Verbatim}[commandchars=\\\{\}]
void flex\PYGZus{}array\PYGZus{}free(struct flex\PYGZus{}array *array);
\end{Verbatim}

As of this writing, there are no users of flexible arrays in the mainline
kernel.  The functions described here are also not exported to modules;
that will probably be fixed when somebody comes up with a need for it.


\subsection{Flexible array functions}
\label{core-api/flexible-arrays:flexible-array-functions}\index{flex\_array\_alloc (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/flexible-arrays:c.flex_array_alloc}\pysiglinewithargsret{struct flex\_array * \bfcode{flex\_array\_alloc}}{int\emph{ element\_size}, unsigned int\emph{ total}, gfp\_t\emph{ flags}}{}
Creates a flexible array.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{int element\_size}}] \leavevmode
individual object size.

\item[{\code{unsigned int total}}] \leavevmode
maximum number of objects which can be stored.

\item[{\code{gfp\_t flags}}] \leavevmode
GFP flags

\end{description}

\textbf{Return}

Returns an object of structure flex\_array.
\index{flex\_array\_prealloc (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/flexible-arrays:c.flex_array_prealloc}\pysiglinewithargsret{int \bfcode{flex\_array\_prealloc}}{struct flex\_array *\emph{ fa}, unsigned int\emph{ start}, unsigned int\emph{ nr\_elements}, gfp\_t\emph{ flags}}{}
Ensures that memory for the elements indexed in the range defined by start and nr\_elements has been allocated.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct flex\_array * fa}}] \leavevmode
array to allocate memory to.

\item[{\code{unsigned int start}}] \leavevmode
start address

\item[{\code{unsigned int nr\_elements}}] \leavevmode
number of elements to be allocated.

\item[{\code{gfp\_t flags}}] \leavevmode
GFP flags

\end{description}
\index{flex\_array\_free (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/flexible-arrays:c.flex_array_free}\pysiglinewithargsret{void \bfcode{flex\_array\_free}}{struct flex\_array *\emph{ fa}}{}
Removes all elements of a flexible array.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct flex\_array * fa}}] \leavevmode
array to be freed.

\end{description}
\index{flex\_array\_free\_parts (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/flexible-arrays:c.flex_array_free_parts}\pysiglinewithargsret{void \bfcode{flex\_array\_free\_parts}}{struct flex\_array *\emph{ fa}}{}
Removes all elements of a flexible array, but leaves the array itself in place.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct flex\_array * fa}}] \leavevmode
array to be emptied.

\end{description}
\index{flex\_array\_put (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/flexible-arrays:c.flex_array_put}\pysiglinewithargsret{int \bfcode{flex\_array\_put}}{struct flex\_array *\emph{ fa}, unsigned int\emph{ element\_nr}, void *\emph{ src}, gfp\_t\emph{ flags}}{}
Stores data into a flexible array.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct flex\_array * fa}}] \leavevmode
array where element is to be stored.

\item[{\code{unsigned int element\_nr}}] \leavevmode
position to copy, must be less than the maximum specified when
the array was created.

\item[{\code{void * src}}] \leavevmode
data source to be copied into the array.

\item[{\code{gfp\_t flags}}] \leavevmode
GFP flags

\end{description}

\textbf{Return}

Returns zero on success, a negative error code otherwise.
\index{flex\_array\_clear (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/flexible-arrays:c.flex_array_clear}\pysiglinewithargsret{int \bfcode{flex\_array\_clear}}{struct flex\_array *\emph{ fa}, unsigned int\emph{ element\_nr}}{}
Clears an individual element in the array, sets the given element to FLEX\_ARRAY\_FREE.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct flex\_array * fa}}] \leavevmode
array to which element to be cleared belongs.

\item[{\code{unsigned int element\_nr}}] \leavevmode
element position to clear.

\end{description}

\textbf{Return}

Returns zero on success, -EINVAL otherwise.
\index{flex\_array\_get (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/flexible-arrays:c.flex_array_get}\pysiglinewithargsret{void * \bfcode{flex\_array\_get}}{struct flex\_array *\emph{ fa}, unsigned int\emph{ element\_nr}}{}
Retrieves data into a flexible array.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct flex\_array * fa}}] \leavevmode
array from which data is to be retrieved.

\item[{\code{unsigned int element\_nr}}] \leavevmode
Element position to retrieve data from.

\end{description}

\textbf{Return}
\begin{description}
\item[{Returns a pointer to the data element, or NULL if that}] \leavevmode
particular element has never been allocated.

\end{description}
\index{flex\_array\_shrink (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/flexible-arrays:c.flex_array_shrink}\pysiglinewithargsret{int \bfcode{flex\_array\_shrink}}{struct flex\_array *\emph{ fa}}{}
Reduces the allocated size of an array.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct flex\_array * fa}}] \leavevmode
array to shrink.

\end{description}

\textbf{Return}

Returns number of pages of memory actually freed.


\section{Reed-Solomon Library Programming Interface}
\label{core-api/librs:reed-solomon-library-programming-interface}\label{core-api/librs::doc}\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Thomas Gleixner

\end{description}\end{quote}


\subsection{Introduction}
\label{core-api/librs:introduction}
The generic Reed-Solomon Library provides encoding, decoding and error
correction functions.

Reed-Solomon codes are used in communication and storage applications to
ensure data integrity.

This documentation is provided for developers who want to utilize the
functions provided by the library.


\subsection{Known Bugs And Assumptions}
\label{core-api/librs:known-bugs-and-assumptions}
None.


\subsection{Usage}
\label{core-api/librs:usage}
This chapter provides examples of how to use the library.


\subsubsection{Initializing}
\label{core-api/librs:initializing}
The init function init\_rs returns a pointer to an rs decoder structure,
which holds the necessary information for encoding, decoding and error
correction with the given polynomial. It either uses an existing
matching decoder or creates a new one. On creation all the lookup tables
for fast en/decoding are created. The function may take a while, so make
sure not to call it in critical code paths.

\begin{Verbatim}[commandchars=\\\{\}]
/* the Reed Solomon control structure */
static struct rs\PYGZus{}control *rs\PYGZus{}decoder;

/* Symbolsize is 10 (bits)
 * Primitive polynomial is x\PYGZca{}10+x\PYGZca{}3+1
 * first consecutive root is 0
 * primitive element to generate roots = 1
 * generator polynomial degree (number of roots) = 6
 */
rs\PYGZus{}decoder = init\PYGZus{}rs (10, 0x409, 0, 1, 6);
\end{Verbatim}


\subsubsection{Encoding}
\label{core-api/librs:encoding}
The encoder calculates the Reed-Solomon code over the given data length
and stores the result in the parity buffer. Note that the parity buffer
must be initialized before calling the encoder.

The expanded data can be inverted on the fly by providing a non-zero
inversion mask. The expanded data is XOR'ed with the mask. This is used
e.g. for FLASH ECC, where the all 0xFF is inverted to an all 0x00. The
Reed-Solomon code for all 0x00 is all 0x00. The code is inverted before
storing to FLASH so it is 0xFF too. This prevents that reading from an
erased FLASH results in ECC errors.

The databytes are expanded to the given symbol size on the fly. There is
no support for encoding continuous bitstreams with a symbol size != 8 at
the moment. If it is necessary it should be not a big deal to implement
such functionality.

\begin{Verbatim}[commandchars=\\\{\}]
/* Parity buffer. Size = number of roots */
uint16\PYGZus{}t par[6];
/* Initialize the parity buffer */
memset(par, 0, sizeof(par));
/* Encode 512 byte in data8. Store parity in buffer par */
encode\PYGZus{}rs8 (rs\PYGZus{}decoder, data8, 512, par, 0);
\end{Verbatim}


\subsubsection{Decoding}
\label{core-api/librs:decoding}
The decoder calculates the syndrome over the given data length and the
received parity symbols and corrects errors in the data.

If a syndrome is available from a hardware decoder then the syndrome
calculation is skipped.

The correction of the data buffer can be suppressed by providing a
correction pattern buffer and an error location buffer to the decoder.
The decoder stores the calculated error location and the correction
bitmask in the given buffers. This is useful for hardware decoders which
use a weird bit ordering scheme.

The databytes are expanded to the given symbol size on the fly. There is
no support for decoding continuous bitstreams with a symbolsize != 8 at
the moment. If it is necessary it should be not a big deal to implement
such functionality.


\paragraph{Decoding with syndrome calculation, direct data correction}
\label{core-api/librs:decoding-with-syndrome-calculation-direct-data-correction}
\begin{Verbatim}[commandchars=\\\{\}]
/* Parity buffer. Size = number of roots */
uint16\PYGZus{}t par[6];
uint8\PYGZus{}t  data[512];
int numerr;
/* Receive data */
.....
/* Receive parity */
.....
/* Decode 512 byte in data8.*/
numerr = decode\PYGZus{}rs8 (rs\PYGZus{}decoder, data8, par, 512, NULL, 0, NULL, 0, NULL);
\end{Verbatim}


\paragraph{Decoding with syndrome given by hardware decoder, direct data correction}
\label{core-api/librs:decoding-with-syndrome-given-by-hardware-decoder-direct-data-correction}
\begin{Verbatim}[commandchars=\\\{\}]
/* Parity buffer. Size = number of roots */
uint16\PYGZus{}t par[6], syn[6];
uint8\PYGZus{}t  data[512];
int numerr;
/* Receive data */
.....
/* Receive parity */
.....
/* Get syndrome from hardware decoder */
.....
/* Decode 512 byte in data8.*/
numerr = decode\PYGZus{}rs8 (rs\PYGZus{}decoder, data8, par, 512, syn, 0, NULL, 0, NULL);
\end{Verbatim}


\paragraph{Decoding with syndrome given by hardware decoder, no direct data correction.}
\label{core-api/librs:decoding-with-syndrome-given-by-hardware-decoder-no-direct-data-correction}
Note: It's not necessary to give data and received parity to the
decoder.

\begin{Verbatim}[commandchars=\\\{\}]
/* Parity buffer. Size = number of roots */
uint16\PYGZus{}t par[6], syn[6], corr[8];
uint8\PYGZus{}t  data[512];
int numerr, errpos[8];
/* Receive data */
.....
/* Receive parity */
.....
/* Get syndrome from hardware decoder */
.....
/* Decode 512 byte in data8.*/
numerr = decode\PYGZus{}rs8 (rs\PYGZus{}decoder, NULL, NULL, 512, syn, 0, errpos, 0, corr);
for (i = 0; i \PYGZlt{} numerr; i++) \PYGZob{}
    do\PYGZus{}error\PYGZus{}correction\PYGZus{}in\PYGZus{}your\PYGZus{}buffer(errpos[i], corr[i]);
\PYGZcb{}
\end{Verbatim}


\subsubsection{Cleanup}
\label{core-api/librs:cleanup}
The function free\_rs frees the allocated resources, if the caller is
the last user of the decoder.

\begin{Verbatim}[commandchars=\\\{\}]
/* Release resources */
free\PYGZus{}rs(rs\PYGZus{}decoder);
\end{Verbatim}


\subsection{Structures}
\label{core-api/librs:structures}
This chapter contains the autogenerated documentation of the structures
which are used in the Reed-Solomon Library and are relevant for a
developer.
\index{rs\_control (C type)}

\begin{fulllineitems}
\phantomsection\label{core-api/librs:c.rs_control}\pysigline{struct \bfcode{rs\_control}}
rs control structure

\end{fulllineitems}


\textbf{Definition}

\begin{Verbatim}[commandchars=\\\{\}]
struct rs\PYGZus{}control \PYGZob{}
  int mm;
  int nn;
  uint16\PYGZus{}t *alpha\PYGZus{}to;
  uint16\PYGZus{}t *index\PYGZus{}of;
  uint16\PYGZus{}t *genpoly;
  int nroots;
  int fcr;
  int prim;
  int iprim;
  int gfpoly;
  int (*gffunc)(int);
  int users;
  struct list\PYGZus{}head list;
\PYGZcb{};
\end{Verbatim}

\textbf{Members}
\begin{description}
\item[{\code{mm}}] \leavevmode
Bits per symbol

\item[{\code{nn}}] \leavevmode
Symbols per block (= (1\textless{}\textless{}mm)-1)

\item[{\code{alpha\_to}}] \leavevmode
log lookup table

\item[{\code{index\_of}}] \leavevmode
Antilog lookup table

\item[{\code{genpoly}}] \leavevmode
Generator polynomial

\item[{\code{nroots}}] \leavevmode
Number of generator roots = number of parity symbols

\item[{\code{fcr}}] \leavevmode
First consecutive root, index form

\item[{\code{prim}}] \leavevmode
Primitive element, index form

\item[{\code{iprim}}] \leavevmode
prim-th root of 1, index form

\item[{\code{gfpoly}}] \leavevmode
The primitive generator polynominal

\item[{\code{gffunc}}] \leavevmode
Function to generate the field, if non-canonical representation

\item[{\code{users}}] \leavevmode
Users of this structure

\item[{\code{list}}] \leavevmode
List entry for the rs control list

\end{description}


\subsection{Public Functions Provided}
\label{core-api/librs:public-functions-provided}
This chapter contains the autogenerated documentation of the
Reed-Solomon functions which are exported.
\index{free\_rs (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/librs:c.free_rs}\pysiglinewithargsret{void \bfcode{free\_rs}}{struct {\hyperref[core\string-api/librs:c.rs_control]{\emph{rs\_control}}} *\emph{ rs}}{}
Free the rs control structure, if it is no longer used

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct rs\_control * rs}}] \leavevmode
the control structure which is not longer used by the
caller

\end{description}
\index{init\_rs (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/librs:c.init_rs}\pysiglinewithargsret{struct {\hyperref[core\string-api/librs:c.rs_control]{\emph{rs\_control}}} * \bfcode{init\_rs}}{int\emph{ symsize}, int\emph{ gfpoly}, int\emph{ fcr}, int\emph{ prim}, int\emph{ nroots}}{}
Find a matching or allocate a new rs control structure

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{int symsize}}] \leavevmode
the symbol size (number of bits)

\item[{\code{int gfpoly}}] \leavevmode
the extended Galois field generator polynomial coefficients,
with the 0th coefficient in the low order bit. The polynomial
must be primitive;

\item[{\code{int fcr}}] \leavevmode
the first consecutive root of the rs code generator polynomial
in index form

\item[{\code{int prim}}] \leavevmode
primitive element to generate polynomial roots

\item[{\code{int nroots}}] \leavevmode
RS code generator polynomial degree (number of roots)

\end{description}
\index{init\_rs\_non\_canonical (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/librs:c.init_rs_non_canonical}\pysiglinewithargsret{struct {\hyperref[core\string-api/librs:c.rs_control]{\emph{rs\_control}}} * \bfcode{init\_rs\_non\_canonical}}{int\emph{ symsize}, int (*gffunc)\emph{ (int}, int\emph{ fcr}, int\emph{ prim}, int\emph{ nroots}}{}
Find a matching or allocate a new rs control structure, for fields with non-canonical representation

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{int symsize}}] \leavevmode
the symbol size (number of bits)

\item[{\code{int (*)(int) gffunc}}] \leavevmode
pointer to function to generate the next field element,
or the multiplicative identity element if given 0.  Used
instead of gfpoly if gfpoly is 0

\item[{\code{int fcr}}] \leavevmode
the first consecutive root of the rs code generator polynomial
in index form

\item[{\code{int prim}}] \leavevmode
primitive element to generate polynomial roots

\item[{\code{int nroots}}] \leavevmode
RS code generator polynomial degree (number of roots)

\end{description}
\index{encode\_rs8 (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/librs:c.encode_rs8}\pysiglinewithargsret{int \bfcode{encode\_rs8}}{struct {\hyperref[core\string-api/librs:c.rs_control]{\emph{rs\_control}}} *\emph{ rs}, uint8\_t *\emph{ data}, int\emph{ len}, uint16\_t *\emph{ par}, uint16\_t\emph{ invmsk}}{}
Calculate the parity for data values (8bit data width)

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct rs\_control * rs}}] \leavevmode
the rs control structure

\item[{\code{uint8\_t * data}}] \leavevmode
data field of a given type

\item[{\code{int len}}] \leavevmode
data length

\item[{\code{uint16\_t * par}}] \leavevmode
parity data, must be initialized by caller (usually all 0)

\item[{\code{uint16\_t invmsk}}] \leavevmode
invert data mask (will be xored on data)

\end{description}

\textbf{Description}
\begin{quote}

The parity uses a uint16\_t data type to enable
symbol size \textgreater{} 8. The calling code must take care of encoding of the
syndrome result for storage itself.
\end{quote}
\index{decode\_rs8 (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/librs:c.decode_rs8}\pysiglinewithargsret{int \bfcode{decode\_rs8}}{struct {\hyperref[core\string-api/librs:c.rs_control]{\emph{rs\_control}}} *\emph{ rs}, uint8\_t *\emph{ data}, uint16\_t *\emph{ par}, int\emph{ len}, uint16\_t *\emph{ s}, int\emph{ no\_eras}, int *\emph{ eras\_pos}, uint16\_t\emph{ invmsk}, uint16\_t *\emph{ corr}}{}
Decode codeword (8bit data width)

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct rs\_control * rs}}] \leavevmode
the rs control structure

\item[{\code{uint8\_t * data}}] \leavevmode
data field of a given type

\item[{\code{uint16\_t * par}}] \leavevmode
received parity data field

\item[{\code{int len}}] \leavevmode
data length

\item[{\code{uint16\_t * s}}] \leavevmode
syndrome data field (if NULL, syndrome is calculated)

\item[{\code{int no\_eras}}] \leavevmode
number of erasures

\item[{\code{int * eras\_pos}}] \leavevmode
position of erasures, can be NULL

\item[{\code{uint16\_t invmsk}}] \leavevmode
invert data mask (will be xored on data, not on parity!)

\item[{\code{uint16\_t * corr}}] \leavevmode
buffer to store correction bitmask on eras\_pos

\end{description}

\textbf{Description}
\begin{quote}

The syndrome and parity uses a uint16\_t data type to enable
symbol size \textgreater{} 8. The calling code must take care of decoding of the
syndrome result and the received parity before calling this code.
Returns the number of corrected bits or -EBADMSG for uncorrectable errors.
\end{quote}
\index{encode\_rs16 (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/librs:c.encode_rs16}\pysiglinewithargsret{int \bfcode{encode\_rs16}}{struct {\hyperref[core\string-api/librs:c.rs_control]{\emph{rs\_control}}} *\emph{ rs}, uint16\_t *\emph{ data}, int\emph{ len}, uint16\_t *\emph{ par}, uint16\_t\emph{ invmsk}}{}
Calculate the parity for data values (16bit data width)

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct rs\_control * rs}}] \leavevmode
the rs control structure

\item[{\code{uint16\_t * data}}] \leavevmode
data field of a given type

\item[{\code{int len}}] \leavevmode
data length

\item[{\code{uint16\_t * par}}] \leavevmode
parity data, must be initialized by caller (usually all 0)

\item[{\code{uint16\_t invmsk}}] \leavevmode
invert data mask (will be xored on data, not on parity!)

\end{description}

\textbf{Description}
\begin{quote}

Each field in the data array contains up to symbol size bits of valid data.
\end{quote}
\index{decode\_rs16 (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/librs:c.decode_rs16}\pysiglinewithargsret{int \bfcode{decode\_rs16}}{struct {\hyperref[core\string-api/librs:c.rs_control]{\emph{rs\_control}}} *\emph{ rs}, uint16\_t *\emph{ data}, uint16\_t *\emph{ par}, int\emph{ len}, uint16\_t *\emph{ s}, int\emph{ no\_eras}, int *\emph{ eras\_pos}, uint16\_t\emph{ invmsk}, uint16\_t *\emph{ corr}}{}
Decode codeword (16bit data width)

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct rs\_control * rs}}] \leavevmode
the rs control structure

\item[{\code{uint16\_t * data}}] \leavevmode
data field of a given type

\item[{\code{uint16\_t * par}}] \leavevmode
received parity data field

\item[{\code{int len}}] \leavevmode
data length

\item[{\code{uint16\_t * s}}] \leavevmode
syndrome data field (if NULL, syndrome is calculated)

\item[{\code{int no\_eras}}] \leavevmode
number of erasures

\item[{\code{int * eras\_pos}}] \leavevmode
position of erasures, can be NULL

\item[{\code{uint16\_t invmsk}}] \leavevmode
invert data mask (will be xored on data, not on parity!)

\item[{\code{uint16\_t * corr}}] \leavevmode
buffer to store correction bitmask on eras\_pos

\end{description}

\textbf{Description}
\begin{quote}

Each field in the data array contains up to symbol size bits of valid data.
Returns the number of corrected bits or -EBADMSG for uncorrectable errors.
\end{quote}


\subsection{Credits}
\label{core-api/librs:credits}
The library code for encoding and decoding was written by Phil Karn.

\begin{Verbatim}[commandchars=\\\{\}]
Copyright 2002, Phil Karn, KA9Q
May be used under the terms of the GNU General Public License (GPL)
\end{Verbatim}

The wrapper functions and interfaces are written by Thomas Gleixner.

Many users have provided bugfixes, improvements and helping hands for
testing. Thanks a lot.

The following people have contributed to this document:

Thomas Gleixner\href{mailto:tglx@linutronix.de}{tglx@linutronix.de}


\section{The genalloc/genpool subsystem}
\label{core-api/genalloc:the-genalloc-genpool-subsystem}\label{core-api/genalloc::doc}
There are a number of memory-allocation subsystems in the kernel, each
aimed at a specific need.  Sometimes, however, a kernel developer needs to
implement a new allocator for a specific range of special-purpose memory;
often that memory is located on a device somewhere.  The author of the
driver for that device can certainly write a little allocator to get the
job done, but that is the way to fill the kernel with dozens of poorly
tested allocators.  Back in 2005, Jes Sorensen lifted one of those
allocators from the sym53c8xx\_2 driver and \href{https://lwn.net/Articles/125842/}{posted} it as a generic module
for the creation of ad hoc memory allocators.  This code was merged
for the 2.6.13 release; it has been modified considerably since then.

Code using this allocator should include \textless{}linux/genalloc.h\textgreater{}.  The action
begins with the creation of a pool using one of:
\index{gen\_pool\_create (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genalloc:c.gen_pool_create}\pysiglinewithargsret{struct gen\_pool * \bfcode{gen\_pool\_create}}{int\emph{ min\_alloc\_order}, int\emph{ nid}}{}
create a new special memory pool

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{int min\_alloc\_order}}] \leavevmode
log base 2 of number of bytes each bitmap bit represents

\item[{\code{int nid}}] \leavevmode
node id of the node the pool structure should be allocated on, or -1

\end{description}

\textbf{Description}

Create a new special memory pool that can be used to manage special purpose
memory not managed by the regular kmalloc/kfree interface.
\index{devm\_gen\_pool\_create (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genalloc:c.devm_gen_pool_create}\pysiglinewithargsret{struct gen\_pool * \bfcode{devm\_gen\_pool\_create}}{struct device *\emph{ dev}, int\emph{ min\_alloc\_order}, int\emph{ nid}, const char *\emph{ name}}{}
managed gen\_pool\_create

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct device * dev}}] \leavevmode
device that provides the gen\_pool

\item[{\code{int min\_alloc\_order}}] \leavevmode
log base 2 of number of bytes each bitmap bit represents

\item[{\code{int nid}}] \leavevmode
node selector for allocated gen\_pool, \code{NUMA\_NO\_NODE} for all nodes

\item[{\code{const char * name}}] \leavevmode
name of a gen\_pool or NULL, identifies a particular gen\_pool on device

\end{description}

\textbf{Description}

Create a new special memory pool that can be used to manage special purpose
memory not managed by the regular kmalloc/kfree interface. The pool will be
automatically destroyed by the device management code.

A call to {\hyperref[core\string-api/genalloc:c.gen_pool_create]{\emph{\code{gen\_pool\_create()}}}} will create a pool.  The granularity of
allocations is set with min\_alloc\_order; it is a log-base-2 number like
those used by the page allocator, but it refers to bytes rather than pages.
So, if min\_alloc\_order is passed as 3, then all allocations will be a
multiple of eight bytes.  Increasing min\_alloc\_order decreases the memory
required to track the memory in the pool.  The nid parameter specifies
which NUMA node should be used for the allocation of the housekeeping
structures; it can be -1 if the caller doesn't care.

The ``managed'' interface {\hyperref[core\string-api/genalloc:c.devm_gen_pool_create]{\emph{\code{devm\_gen\_pool\_create()}}}} ties the pool to a
specific device.  Among other things, it will automatically clean up the
pool when the given device is destroyed.

A pool is shut down with:
\index{gen\_pool\_destroy (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genalloc:c.gen_pool_destroy}\pysiglinewithargsret{void \bfcode{gen\_pool\_destroy}}{struct gen\_pool *\emph{ pool}}{}
destroy a special memory pool

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct gen\_pool * pool}}] \leavevmode
pool to destroy

\end{description}

\textbf{Description}

Destroy the specified special memory pool. Verifies that there are no
outstanding allocations.

It's worth noting that, if there are still allocations outstanding from the
given pool, this function will take the rather extreme step of invoking
BUG(), crashing the entire system.  You have been warned.

A freshly created pool has no memory to allocate.  It is fairly useless in
that state, so one of the first orders of business is usually to add memory
to the pool.  That can be done with one of:
\index{gen\_pool\_add (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genalloc:c.gen_pool_add}\pysiglinewithargsret{int \bfcode{gen\_pool\_add}}{struct gen\_pool *\emph{ pool}, unsigned long\emph{ addr}, size\_t\emph{ size}, int\emph{ nid}}{}
add a new chunk of special memory to the pool

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct gen\_pool * pool}}] \leavevmode
pool to add new memory chunk to

\item[{\code{unsigned long addr}}] \leavevmode
starting address of memory chunk to add to pool

\item[{\code{size\_t size}}] \leavevmode
size in bytes of the memory chunk to add to pool

\item[{\code{int nid}}] \leavevmode
node id of the node the chunk structure and bitmap should be
allocated on, or -1

\end{description}

\textbf{Description}

Add a new chunk of special memory to the specified pool.

Returns 0 on success or a -ve errno on failure.
\index{gen\_pool\_add\_virt (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genalloc:c.gen_pool_add_virt}\pysiglinewithargsret{int \bfcode{gen\_pool\_add\_virt}}{struct gen\_pool *\emph{ pool}, unsigned long\emph{ virt}, phys\_addr\_t\emph{ phys}, size\_t\emph{ size}, int\emph{ nid}}{}
add a new chunk of special memory to the pool

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct gen\_pool * pool}}] \leavevmode
pool to add new memory chunk to

\item[{\code{unsigned long virt}}] \leavevmode
virtual starting address of memory chunk to add to pool

\item[{\code{phys\_addr\_t phys}}] \leavevmode
physical starting address of memory chunk to add to pool

\item[{\code{size\_t size}}] \leavevmode
size in bytes of the memory chunk to add to pool

\item[{\code{int nid}}] \leavevmode
node id of the node the chunk structure and bitmap should be
allocated on, or -1

\end{description}

\textbf{Description}

Add a new chunk of special memory to the specified pool.

Returns 0 on success or a -ve errno on failure.

A call to {\hyperref[core\string-api/genalloc:c.gen_pool_add]{\emph{\code{gen\_pool\_add()}}}} will place the size bytes of memory
starting at addr (in the kernel's virtual address space) into the given
pool, once again using nid as the node ID for ancillary memory allocations.
The {\hyperref[core\string-api/genalloc:c.gen_pool_add_virt]{\emph{\code{gen\_pool\_add\_virt()}}}} variant associates an explicit physical
address with the memory; this is only necessary if the pool will be used
for DMA allocations.

The functions for allocating memory from the pool (and putting it back)
are:
\index{gen\_pool\_alloc (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genalloc:c.gen_pool_alloc}\pysiglinewithargsret{unsigned long \bfcode{gen\_pool\_alloc}}{struct gen\_pool *\emph{ pool}, size\_t\emph{ size}}{}
allocate special memory from the pool

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct gen\_pool * pool}}] \leavevmode
pool to allocate from

\item[{\code{size\_t size}}] \leavevmode
number of bytes to allocate from the pool

\end{description}

\textbf{Description}

Allocate the requested number of bytes from the specified pool.
Uses the pool allocation function (with first-fit algorithm by default).
Can not be used in NMI handler on architectures without
NMI-safe cmpxchg implementation.
\index{gen\_pool\_dma\_alloc (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genalloc:c.gen_pool_dma_alloc}\pysiglinewithargsret{void * \bfcode{gen\_pool\_dma\_alloc}}{struct gen\_pool *\emph{ pool}, size\_t\emph{ size}, dma\_addr\_t *\emph{ dma}}{}
allocate special memory from the pool for DMA usage

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct gen\_pool * pool}}] \leavevmode
pool to allocate from

\item[{\code{size\_t size}}] \leavevmode
number of bytes to allocate from the pool

\item[{\code{dma\_addr\_t * dma}}] \leavevmode
dma-view physical address return value.  Use NULL if unneeded.

\end{description}

\textbf{Description}

Allocate the requested number of bytes from the specified pool.
Uses the pool allocation function (with first-fit algorithm by default).
Can not be used in NMI handler on architectures without
NMI-safe cmpxchg implementation.
\index{gen\_pool\_free (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genalloc:c.gen_pool_free}\pysiglinewithargsret{void \bfcode{gen\_pool\_free}}{struct gen\_pool *\emph{ pool}, unsigned long\emph{ addr}, size\_t\emph{ size}}{}
free allocated special memory back to the pool

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct gen\_pool * pool}}] \leavevmode
pool to free to

\item[{\code{unsigned long addr}}] \leavevmode
starting address of memory to free back to pool

\item[{\code{size\_t size}}] \leavevmode
size in bytes of memory to free

\end{description}

\textbf{Description}

Free previously allocated special memory back to the specified
pool.  Can not be used in NMI handler on architectures without
NMI-safe cmpxchg implementation.

As one would expect, {\hyperref[core\string-api/genalloc:c.gen_pool_alloc]{\emph{\code{gen\_pool\_alloc()}}}} will allocate size\textless{} bytes
from the given pool.  The {\hyperref[core\string-api/genalloc:c.gen_pool_dma_alloc]{\emph{\code{gen\_pool\_dma\_alloc()}}}} variant allocates
memory for use with DMA operations, returning the associated physical
address in the space pointed to by dma.  This will only work if the memory
was added with {\hyperref[core\string-api/genalloc:c.gen_pool_add_virt]{\emph{\code{gen\_pool\_add\_virt()}}}}.  Note that this function
departs from the usual genpool pattern of using unsigned long values to
represent kernel addresses; it returns a void * instead.

That all seems relatively simple; indeed, some developers clearly found it
to be too simple.  After all, the interface above provides no control over
how the allocation functions choose which specific piece of memory to
return.  If that sort of control is needed, the following functions will be
of interest:
\index{gen\_pool\_alloc\_algo (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genalloc:c.gen_pool_alloc_algo}\pysiglinewithargsret{unsigned long \bfcode{gen\_pool\_alloc\_algo}}{struct gen\_pool *\emph{ pool}, size\_t\emph{ size}, genpool\_algo\_t\emph{ algo}, void *\emph{ data}}{}
allocate special memory from the pool

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct gen\_pool * pool}}] \leavevmode
pool to allocate from

\item[{\code{size\_t size}}] \leavevmode
number of bytes to allocate from the pool

\item[{\code{genpool\_algo\_t algo}}] \leavevmode
algorithm passed from caller

\item[{\code{void * data}}] \leavevmode
data passed to algorithm

\end{description}

\textbf{Description}

Allocate the requested number of bytes from the specified pool.
Uses the pool allocation function (with first-fit algorithm by default).
Can not be used in NMI handler on architectures without
NMI-safe cmpxchg implementation.
\index{gen\_pool\_set\_algo (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genalloc:c.gen_pool_set_algo}\pysiglinewithargsret{void \bfcode{gen\_pool\_set\_algo}}{struct gen\_pool *\emph{ pool}, genpool\_algo\_t\emph{ algo}, void *\emph{ data}}{}
set the allocation algorithm

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct gen\_pool * pool}}] \leavevmode
pool to change allocation algorithm

\item[{\code{genpool\_algo\_t algo}}] \leavevmode
custom algorithm function

\item[{\code{void * data}}] \leavevmode
additional data used by \textbf{algo}

\end{description}

\textbf{Description}

Call \textbf{algo} for each memory allocation in the pool.
If \textbf{algo} is NULL use gen\_pool\_first\_fit as default
memory allocation function.

Allocations with {\hyperref[core\string-api/genalloc:c.gen_pool_alloc_algo]{\emph{\code{gen\_pool\_alloc\_algo()}}}} specify an algorithm to be
used to choose the memory to be allocated; the default algorithm can be set
with {\hyperref[core\string-api/genalloc:c.gen_pool_set_algo]{\emph{\code{gen\_pool\_set\_algo()}}}}.  The data value is passed to the
algorithm; most ignore it, but it is occasionally needed.  One can,
naturally, write a special-purpose algorithm, but there is a fair set
already available:
\begin{itemize}
\item {} 
gen\_pool\_first\_fit is a simple first-fit allocator; this is the default
algorithm if none other has been specified.

\item {} 
gen\_pool\_first\_fit\_align forces the allocation to have a specific
alignment (passed via data in a genpool\_data\_align structure).

\item {} 
gen\_pool\_first\_fit\_order\_align aligns the allocation to the order of the
size.  A 60-byte allocation will thus be 64-byte aligned, for example.

\item {} 
gen\_pool\_best\_fit, as one would expect, is a simple best-fit allocator.

\item {} 
gen\_pool\_fixed\_alloc allocates at a specific offset (passed in a
genpool\_data\_fixed structure via the data parameter) within the pool.
If the indicated memory is not available the allocation fails.

\end{itemize}

There is a handful of other functions, mostly for purposes like querying
the space available in the pool or iterating through chunks of memory.
Most users, however, should not need much beyond what has been described
above.  With luck, wider awareness of this module will help to prevent the
writing of special-purpose memory allocators in the future.
\index{gen\_pool\_virt\_to\_phys (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genalloc:c.gen_pool_virt_to_phys}\pysiglinewithargsret{phys\_addr\_t \bfcode{gen\_pool\_virt\_to\_phys}}{struct gen\_pool *\emph{ pool}, unsigned long\emph{ addr}}{}
return the physical address of memory

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct gen\_pool * pool}}] \leavevmode
pool to allocate from

\item[{\code{unsigned long addr}}] \leavevmode
starting address of memory

\end{description}

\textbf{Description}

Returns the physical address on success, or -1 on error.
\index{gen\_pool\_for\_each\_chunk (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genalloc:c.gen_pool_for_each_chunk}\pysiglinewithargsret{void \bfcode{gen\_pool\_for\_each\_chunk}}{struct gen\_pool *\emph{ pool}, void (*func) (struct gen\_pool\emph{ *pool}, struct gen\_pool\_chunk\emph{ *chunk}, void\emph{ *data}, void *\emph{ data}}{}
call func for every chunk of generic memory pool

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct gen\_pool * pool}}] \leavevmode
the generic memory pool

\item[{\code{void (*)(struct gen\_pool *pool, struct gen\_pool\_chunk *chunk, void *data) func}}] \leavevmode
func to call

\item[{\code{void * data}}] \leavevmode
additional data used by \textbf{func}

\end{description}

\textbf{Description}

Call \textbf{func} for every chunk of generic memory pool.  The \textbf{func} is
called with rcu\_read\_lock held.
\index{addr\_in\_gen\_pool (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genalloc:c.addr_in_gen_pool}\pysiglinewithargsret{bool \bfcode{addr\_in\_gen\_pool}}{struct gen\_pool *\emph{ pool}, unsigned long\emph{ start}, size\_t\emph{ size}}{}
checks if an address falls within the range of a pool

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct gen\_pool * pool}}] \leavevmode
the generic memory pool

\item[{\code{unsigned long start}}] \leavevmode
start address

\item[{\code{size\_t size}}] \leavevmode
size of the region

\end{description}

\textbf{Description}

Check if the range of addresses falls within the specified pool. Returns
true if the entire range is contained in the pool and false otherwise.
\index{gen\_pool\_avail (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genalloc:c.gen_pool_avail}\pysiglinewithargsret{size\_t \bfcode{gen\_pool\_avail}}{struct gen\_pool *\emph{ pool}}{}
get available free space of the pool

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct gen\_pool * pool}}] \leavevmode
pool to get available free space

\end{description}

\textbf{Description}

Return available free space of the specified pool.
\index{gen\_pool\_size (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genalloc:c.gen_pool_size}\pysiglinewithargsret{size\_t \bfcode{gen\_pool\_size}}{struct gen\_pool *\emph{ pool}}{}
get size in bytes of memory managed by the pool

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct gen\_pool * pool}}] \leavevmode
pool to get size

\end{description}

\textbf{Description}

Return size in bytes of memory managed by the pool.
\index{gen\_pool\_get (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genalloc:c.gen_pool_get}\pysiglinewithargsret{struct gen\_pool * \bfcode{gen\_pool\_get}}{struct device *\emph{ dev}, const char *\emph{ name}}{}
Obtain the gen\_pool (if any) for a device

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct device * dev}}] \leavevmode
device to retrieve the gen\_pool from

\item[{\code{const char * name}}] \leavevmode
name of a gen\_pool or NULL, identifies a particular gen\_pool on device

\end{description}

\textbf{Description}

Returns the gen\_pool for the device if one is present, or NULL.
\index{of\_gen\_pool\_get (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/genalloc:c.of_gen_pool_get}\pysiglinewithargsret{struct gen\_pool * \bfcode{of\_gen\_pool\_get}}{struct device\_node *\emph{ np}, const char *\emph{ propname}, int\emph{ index}}{}
find a pool by phandle property

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct device\_node * np}}] \leavevmode
device node

\item[{\code{const char * propname}}] \leavevmode
property name containing phandle(s)

\item[{\code{int index}}] \leavevmode
index into the phandle array

\end{description}

\textbf{Description}

Returns the pool that contains the chunk starting at the physical
address of the device tree node pointed at by the phandle property,
or NULL if not found.


\section{The errseq\_t datatype}
\label{core-api/errseq::doc}\label{core-api/errseq:the-errseq-t-datatype}
An errseq\_t is a way of recording errors in one place, and allowing any
number of ``subscribers'' to tell whether it has changed since a previous
point where it was sampled.

The initial use case for this is tracking errors for file
synchronization syscalls (fsync, fdatasync, msync and sync\_file\_range),
but it may be usable in other situations.

It's implemented as an unsigned 32-bit value.  The low order bits are
designated to hold an error code (between 1 and MAX\_ERRNO).  The upper bits
are used as a counter.  This is done with atomics instead of locking so that
these functions can be called from any context.

Note that there is a risk of collisions if new errors are being recorded
frequently, since we have so few bits to use as a counter.

To mitigate this, the bit between the error value and counter is used as
a flag to tell whether the value has been sampled since a new value was
recorded.  That allows us to avoid bumping the counter if no one has
sampled it since the last time an error was recorded.

Thus we end up with a value that looks something like this:

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline

31..13
 & 
12
 & 
11..0
\\
\hline
counter
 & 
SF
 & 
errno
\\
\hline\end{tabulary}


The general idea is for ``watchers'' to sample an errseq\_t value and keep
it as a running cursor.  That value can later be used to tell whether
any new errors have occurred since that sampling was done, and atomically
record the state at the time that it was checked.  This allows us to
record errors in one place, and then have a number of ``watchers'' that
can tell whether the value has changed since they last checked it.

A new errseq\_t should always be zeroed out.  An errseq\_t value of all zeroes
is the special (but common) case where there has never been an error. An all
zero value thus serves as the ``epoch'' if one wishes to know whether there
has ever been an error set since it was first initialized.


\subsection{API usage}
\label{core-api/errseq:api-usage}
Let me tell you a story about a worker drone.  Now, he's a good worker
overall, but the company is a little...management heavy.  He has to
report to 77 supervisors today, and tomorrow the ``big boss'' is coming in
from out of town and he's sure to test the poor fellow too.

They're all handing him work to do -- so much he can't keep track of who
handed him what, but that's not really a big problem.  The supervisors
just want to know when he's finished all of the work they've handed him so
far and whether he made any mistakes since they last asked.

He might have made the mistake on work they didn't actually hand him,
but he can't keep track of things at that level of detail, all he can
remember is the most recent mistake that he made.

Here's our worker\_drone representation:

\begin{Verbatim}[commandchars=\\\{\}]
struct worker\PYGZus{}drone \PYGZob{}
        errseq\PYGZus{}t        wd\PYGZus{}err; /* for recording errors */
\PYGZcb{};
\end{Verbatim}

Every day, the worker\_drone starts out with a blank slate:

\begin{Verbatim}[commandchars=\\\{\}]
struct worker\PYGZus{}drone wd;

wd.wd\PYGZus{}err = (errseq\PYGZus{}t)0;
\end{Verbatim}

The supervisors come in and get an initial read for the day.  They
don't care about anything that happened before their watch begins:

\begin{Verbatim}[commandchars=\\\{\}]
struct supervisor \PYGZob{}
        errseq\PYGZus{}t        s\PYGZus{}wd\PYGZus{}err; /* private \PYGZdq{}cursor\PYGZdq{} for wd\PYGZus{}err */
        spinlock\PYGZus{}t      s\PYGZus{}wd\PYGZus{}err\PYGZus{}lock; /* protects s\PYGZus{}wd\PYGZus{}err */
\PYGZcb{}

struct supervisor       su;

su.s\PYGZus{}wd\PYGZus{}err = errseq\PYGZus{}sample(\PYGZam{}wd.wd\PYGZus{}err);
spin\PYGZus{}lock\PYGZus{}init(\PYGZam{}su.s\PYGZus{}wd\PYGZus{}err\PYGZus{}lock);
\end{Verbatim}

Now they start handing him tasks to do.  Every few minutes they ask him to
finish up all of the work they've handed him so far.  Then they ask him
whether he made any mistakes on any of it:

\begin{Verbatim}[commandchars=\\\{\}]
spin\PYGZus{}lock(\PYGZam{}su.su\PYGZus{}wd\PYGZus{}err\PYGZus{}lock);
err = errseq\PYGZus{}check\PYGZus{}and\PYGZus{}advance(\PYGZam{}wd.wd\PYGZus{}err, \PYGZam{}su.s\PYGZus{}wd\PYGZus{}err);
spin\PYGZus{}unlock(\PYGZam{}su.su\PYGZus{}wd\PYGZus{}err\PYGZus{}lock);
\end{Verbatim}

Up to this point, that just keeps returning 0.

Now, the owners of this company are quite miserly and have given him
substandard equipment with which to do his job. Occasionally it
glitches and he makes a mistake.  He sighs a heavy sigh, and marks it
down:

\begin{Verbatim}[commandchars=\\\{\}]
errseq\PYGZus{}set(\PYGZam{}wd.wd\PYGZus{}err, \PYGZhy{}EIO);
\end{Verbatim}

...and then gets back to work.  The supervisors eventually poll again
and they each get the error when they next check.  Subsequent calls will
return 0, until another error is recorded, at which point it's reported
to each of them once.

Note that the supervisors can't tell how many mistakes he made, only
whether one was made since they last checked, and the latest value
recorded.

Occasionally the big boss comes in for a spot check and asks the worker
to do a one-off job for him. He's not really watching the worker
full-time like the supervisors, but he does need to know whether a
mistake occurred while his job was processing.

He can just sample the current errseq\_t in the worker, and then use that
to tell whether an error has occurred later:

\begin{Verbatim}[commandchars=\\\{\}]
errseq\PYGZus{}t since = errseq\PYGZus{}sample(\PYGZam{}wd.wd\PYGZus{}err);
/* submit some work and wait for it to complete */
err = errseq\PYGZus{}check(\PYGZam{}wd.wd\PYGZus{}err, since);
\end{Verbatim}

Since he's just going to discard ``since'' after that point, he doesn't
need to advance it here. He also doesn't need any locking since it's
not usable by anyone else.


\subsection{Serializing errseq\_t cursor updates}
\label{core-api/errseq:serializing-errseq-t-cursor-updates}
Note that the errseq\_t API does not protect the errseq\_t cursor during a
check\_and\_advance\_operation. Only the canonical error code is handled
atomically.  In a situation where more than one task might be using the
same errseq\_t cursor at the same time, it's important to serialize
updates to that cursor.

If that's not done, then it's possible for the cursor to go backward
in which case the same error could be reported more than once.

Because of this, it's often advantageous to first do an errseq\_check to
see if anything has changed, and only later do an
errseq\_check\_and\_advance after taking the lock. e.g.:

\begin{Verbatim}[commandchars=\\\{\}]
if (errseq\PYGZus{}check(\PYGZam{}wd.wd\PYGZus{}err, READ\PYGZus{}ONCE(su.s\PYGZus{}wd\PYGZus{}err)) \PYGZob{}
        /* su.s\PYGZus{}wd\PYGZus{}err is protected by s\PYGZus{}wd\PYGZus{}err\PYGZus{}lock */
        spin\PYGZus{}lock(\PYGZam{}su.s\PYGZus{}wd\PYGZus{}err\PYGZus{}lock);
        err = errseq\PYGZus{}check\PYGZus{}and\PYGZus{}advance(\PYGZam{}wd.wd\PYGZus{}err, \PYGZam{}su.s\PYGZus{}wd\PYGZus{}err);
        spin\PYGZus{}unlock(\PYGZam{}su.s\PYGZus{}wd\PYGZus{}err\PYGZus{}lock);
\PYGZcb{}
\end{Verbatim}

That avoids the spinlock in the common case where nothing has changed
since the last time it was checked.


\subsection{Functions}
\label{core-api/errseq:functions}\index{errseq\_set (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/errseq:c.errseq_set}\pysiglinewithargsret{errseq\_t \bfcode{errseq\_set}}{errseq\_t *\emph{ eseq}, int\emph{ err}}{}
set a errseq\_t for later reporting

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{errseq\_t * eseq}}] \leavevmode
errseq\_t field that should be set

\item[{\code{int err}}] \leavevmode
error to set (must be between -1 and -MAX\_ERRNO)

\end{description}

\textbf{Description}

This function sets the error in \textbf{eseq}, and increments the sequence counter
if the last sequence was sampled at some point in the past.

Any error set will always overwrite an existing error.

\textbf{Return}

The previous value, primarily for debugging purposes. The
return value should not be used as a previously sampled value in later
calls as it will not have the SEEN flag set.
\index{errseq\_sample (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/errseq:c.errseq_sample}\pysiglinewithargsret{errseq\_t \bfcode{errseq\_sample}}{errseq\_t *\emph{ eseq}}{}
Grab current errseq\_t value.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{errseq\_t * eseq}}] \leavevmode
Pointer to errseq\_t to be sampled.

\end{description}

\textbf{Description}

This function allows callers to sample an errseq\_t value, marking it as
``seen'' if required.

\textbf{Return}

The current errseq value.
\index{errseq\_check (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/errseq:c.errseq_check}\pysiglinewithargsret{int \bfcode{errseq\_check}}{errseq\_t *\emph{ eseq}, errseq\_t\emph{ since}}{}
Has an error occurred since a particular sample point?

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{errseq\_t * eseq}}] \leavevmode
Pointer to errseq\_t value to be checked.

\item[{\code{errseq\_t since}}] \leavevmode
Previously-sampled errseq\_t from which to check.

\end{description}

\textbf{Description}

Grab the value that eseq points to, and see if it has changed \textbf{since}
the given value was sampled. The \textbf{since} value is not advanced, so there
is no need to mark the value as seen.

\textbf{Return}

The latest error set in the errseq\_t or 0 if it hasn't changed.
\index{errseq\_check\_and\_advance (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/errseq:c.errseq_check_and_advance}\pysiglinewithargsret{int \bfcode{errseq\_check\_and\_advance}}{errseq\_t *\emph{ eseq}, errseq\_t *\emph{ since}}{}
Check an errseq\_t and advance to current value.

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{errseq\_t * eseq}}] \leavevmode
Pointer to value being checked and reported.

\item[{\code{errseq\_t * since}}] \leavevmode
Pointer to previously-sampled errseq\_t to check against and advance.

\end{description}

\textbf{Description}

Grab the eseq value, and see whether it matches the value that \textbf{since}
points to. If it does, then just return 0.

If it doesn't, then the value has changed. Set the ``seen'' flag, and try to
swap it into place as the new eseq value. Then, set that value as the new
``since'' value, and return whatever the error portion is set to.

Note that no locking is provided here for concurrent updates to the ``since''
value. The caller must provide that if necessary. Because of this, callers
may want to do a lockless errseq\_check before taking the lock and calling
this.

\textbf{Return}

Negative errno if one has been stored, or 0 if no new error has
occurred.


\section{How to get printk format specifiers right}
\label{core-api/printk-formats::doc}\label{core-api/printk-formats:how-to-get-printk-format-specifiers-right}\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Randy Dunlap \textless{}\href{mailto:rdunlap@infradead.org}{rdunlap@infradead.org}\textgreater{}

\item[{Author}] \leavevmode
Andrew Murray \textless{}\href{mailto:amurray@mpc-data.co.uk}{amurray@mpc-data.co.uk}\textgreater{}

\end{description}\end{quote}


\subsection{Integer types}
\label{core-api/printk-formats:integer-types}
\begin{Verbatim}[commandchars=\\\{\}]
If variable is of Type,         use printk format specifier:
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
        int                     \PYGZpc{}d or \PYGZpc{}x
        unsigned int            \PYGZpc{}u or \PYGZpc{}x
        long                    \PYGZpc{}ld or \PYGZpc{}lx
        unsigned long           \PYGZpc{}lu or \PYGZpc{}lx
        long long               \PYGZpc{}lld or \PYGZpc{}llx
        unsigned long long      \PYGZpc{}llu or \PYGZpc{}llx
        size\PYGZus{}t                  \PYGZpc{}zu or \PYGZpc{}zx
        ssize\PYGZus{}t                 \PYGZpc{}zd or \PYGZpc{}zx
        s32                     \PYGZpc{}d or \PYGZpc{}x
        u32                     \PYGZpc{}u or \PYGZpc{}x
        s64                     \PYGZpc{}lld or \PYGZpc{}llx
        u64                     \PYGZpc{}llu or \PYGZpc{}llx
\end{Verbatim}

If \textless{}type\textgreater{} is dependent on a config option for its size (e.g., sector\_t,
blkcnt\_t) or is architecture-dependent for its size (e.g., tcflag\_t), use a
format specifier of its largest possible type and explicitly cast to it.

Example:

\begin{Verbatim}[commandchars=\\\{\}]
printk(\PYGZdq{}test: sector number/total blocks: \PYGZpc{}llu/\PYGZpc{}llu\PYGZbs{}n\PYGZdq{},
        (unsigned long long)sector, (unsigned long long)blockcount);
\end{Verbatim}

Reminder: sizeof() returns type size\_t.

The kernel's printf does not support \%n. Floating point formats (\%e, \%f,
\%g, \%a) are also not recognized, for obvious reasons. Use of any
unsupported specifier or length qualifier results in a WARN and early
return from vsnprintf().


\subsection{Pointer types}
\label{core-api/printk-formats:pointer-types}
A raw pointer value may be printed with \%p which will hash the address
before printing. The kernel also supports extended specifiers for printing
pointers of different types.


\subsubsection{Plain Pointers}
\label{core-api/printk-formats:plain-pointers}
\begin{Verbatim}[commandchars=\\\{\}]
\PYGZpc{}p      abcdef12 or 00000000abcdef12
\end{Verbatim}

Pointers printed without a specifier extension (i.e unadorned \%p) are
hashed to prevent leaking information about the kernel memory layout. This
has the added benefit of providing a unique identifier. On 64-bit machines
the first 32 bits are zeroed. If you \emph{really} want the address see \%px
below.


\subsubsection{Symbols/Function Pointers}
\label{core-api/printk-formats:symbols-function-pointers}
\begin{Verbatim}[commandchars=\\\{\}]
\PYGZpc{}pS     versatile\PYGZus{}init+0x0/0x110
\PYGZpc{}ps     versatile\PYGZus{}init
\PYGZpc{}pF     versatile\PYGZus{}init+0x0/0x110
\PYGZpc{}pf     versatile\PYGZus{}init
\PYGZpc{}pSR    versatile\PYGZus{}init+0x9/0x110
        (with \PYGZus{}\PYGZus{}builtin\PYGZus{}extract\PYGZus{}return\PYGZus{}addr() translation)
\PYGZpc{}pB     prev\PYGZus{}fn\PYGZus{}of\PYGZus{}versatile\PYGZus{}init+0x88/0x88
\end{Verbatim}

The \code{S} and \code{s} specifiers are used for printing a pointer in symbolic
format. They result in the symbol name with (S) or without (s)
offsets. If KALLSYMS are disabled then the symbol address is printed instead.

Note, that the \code{F} and \code{f} specifiers are identical to \code{S} (\code{s})
and thus deprecated. We have \code{F} and \code{f} because on ia64, ppc64 and
parisc64 function pointers are indirect and, in fact, are function
descriptors, which require additional dereferencing before we can lookup
the symbol. As of now, \code{S} and \code{s} perform dereferencing on those
platforms (when needed), so \code{F} and \code{f} exist for compatibility
reasons only.

The \code{B} specifier results in the symbol name with offsets and should be
used when printing stack backtraces. The specifier takes into
consideration the effect of compiler optimisations which may occur
when tail-calls are used and marked with the noreturn GCC attribute.


\subsubsection{Kernel Pointers}
\label{core-api/printk-formats:kernel-pointers}
\begin{Verbatim}[commandchars=\\\{\}]
\PYGZpc{}pK     01234567 or 0123456789abcdef
\end{Verbatim}

For printing kernel pointers which should be hidden from unprivileged
users. The behaviour of \%pK depends on the kptr\_restrict sysctl - see
Documentation/sysctl/kernel.txt for more details.


\subsubsection{Unmodified Addresses}
\label{core-api/printk-formats:unmodified-addresses}
\begin{Verbatim}[commandchars=\\\{\}]
\PYGZpc{}px     01234567 or 0123456789abcdef
\end{Verbatim}

For printing pointers when you \emph{really} want to print the address. Please
consider whether or not you are leaking sensitive information about the
kernel memory layout before printing pointers with \%px. \%px is functionally
equivalent to \%lx (or \%lu). \%px is preferred because it is more uniquely
grep'able. If in the future we need to modify the way the kernel handles
printing pointers we will be better equipped to find the call sites.


\subsubsection{Struct Resources}
\label{core-api/printk-formats:struct-resources}
\begin{Verbatim}[commandchars=\\\{\}]
\PYGZpc{}pr     [mem 0x60000000\PYGZhy{}0x6fffffff flags 0x2200] or
        [mem 0x0000000060000000\PYGZhy{}0x000000006fffffff flags 0x2200]
\PYGZpc{}pR     [mem 0x60000000\PYGZhy{}0x6fffffff pref] or
        [mem 0x0000000060000000\PYGZhy{}0x000000006fffffff pref]
\end{Verbatim}

For printing struct resources. The \code{R} and \code{r} specifiers result in a
printed resource with (R) or without (r) a decoded flags member.

Passed by reference.


\subsubsection{Physical address types phys\_addr\_t}
\label{core-api/printk-formats:physical-address-types-phys-addr-t}
\begin{Verbatim}[commandchars=\\\{\}]
\PYGZpc{}pa[p]  0x01234567 or 0x0123456789abcdef
\end{Verbatim}

For printing a phys\_addr\_t type (and its derivatives, such as
resource\_size\_t) which can vary based on build options, regardless of the
width of the CPU data path.

Passed by reference.


\subsubsection{DMA address types dma\_addr\_t}
\label{core-api/printk-formats:dma-address-types-dma-addr-t}
\begin{Verbatim}[commandchars=\\\{\}]
\PYGZpc{}pad    0x01234567 or 0x0123456789abcdef
\end{Verbatim}

For printing a dma\_addr\_t type which can vary based on build options,
regardless of the width of the CPU data path.

Passed by reference.


\subsubsection{Raw buffer as an escaped string}
\label{core-api/printk-formats:raw-buffer-as-an-escaped-string}
\begin{Verbatim}[commandchars=\\\{\}]
\PYGZpc{}*pE[achnops]
\end{Verbatim}

For printing raw buffer as an escaped string. For the following buffer:

\begin{Verbatim}[commandchars=\\\{\}]
1b 62 20 5c 43 07 22 90 0d 5d
\end{Verbatim}

A few examples show how the conversion would be done (excluding surrounding
quotes):

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZpc{}*pE            \PYGZdq{}\PYGZbs{}eb \PYGZbs{}C\PYGZbs{}a\PYGZdq{}\PYGZbs{}220\PYGZbs{}r]\PYGZdq{}
\PYGZpc{}*pEhp          \PYGZdq{}\PYGZbs{}x1bb \PYGZbs{}C\PYGZbs{}x07\PYGZdq{}\PYGZbs{}x90\PYGZbs{}x0d]\PYGZdq{}
\PYGZpc{}*pEa           \PYGZdq{}\PYGZbs{}e\PYGZbs{}142\PYGZbs{}040\PYGZbs{}\PYGZbs{}\PYGZbs{}103\PYGZbs{}a\PYGZbs{}042\PYGZbs{}220\PYGZbs{}r\PYGZbs{}135\PYGZdq{}
\end{Verbatim}

The conversion rules are applied according to an optional combination
of flags (see \code{string\_escape\_mem()} kernel documentation for the
details):
\begin{itemize}
\item {} 
a - ESCAPE\_ANY

\item {} 
c - ESCAPE\_SPECIAL

\item {} 
h - ESCAPE\_HEX

\item {} 
n - ESCAPE\_NULL

\item {} 
o - ESCAPE\_OCTAL

\item {} 
p - ESCAPE\_NP

\item {} 
s - ESCAPE\_SPACE

\end{itemize}

By default ESCAPE\_ANY\_NP is used.

ESCAPE\_ANY\_NP is the sane choice for many cases, in particularly for
printing SSIDs.

If field width is omitted then 1 byte only will be escaped.


\subsubsection{Raw buffer as a hex string}
\label{core-api/printk-formats:raw-buffer-as-a-hex-string}
\begin{Verbatim}[commandchars=\\\{\}]
\PYGZpc{}*ph    00 01 02  ...  3f
\PYGZpc{}*phC   00:01:02: ... :3f
\PYGZpc{}*phD   00\PYGZhy{}01\PYGZhy{}02\PYGZhy{} ... \PYGZhy{}3f
\PYGZpc{}*phN   000102 ... 3f
\end{Verbatim}

For printing small buffers (up to 64 bytes long) as a hex string with a
certain separator. For larger buffers consider using
\code{print\_hex\_dump()}.


\subsubsection{MAC/FDDI addresses}
\label{core-api/printk-formats:mac-fddi-addresses}
\begin{Verbatim}[commandchars=\\\{\}]
\PYGZpc{}pM     00:01:02:03:04:05
\PYGZpc{}pMR    05:04:03:02:01:00
\PYGZpc{}pMF    00\PYGZhy{}01\PYGZhy{}02\PYGZhy{}03\PYGZhy{}04\PYGZhy{}05
\PYGZpc{}pm     000102030405
\PYGZpc{}pmR    050403020100
\end{Verbatim}

For printing 6-byte MAC/FDDI addresses in hex notation. The \code{M} and \code{m}
specifiers result in a printed address with (M) or without (m) byte
separators. The default byte separator is the colon (:).

Where FDDI addresses are concerned the \code{F} specifier can be used after
the \code{M} specifier to use dash (-) separators instead of the default
separator.

For Bluetooth addresses the \code{R} specifier shall be used after the \code{M}
specifier to use reversed byte order suitable for visual interpretation
of Bluetooth addresses which are in the little endian order.

Passed by reference.


\subsubsection{IPv4 addresses}
\label{core-api/printk-formats:ipv4-addresses}
\begin{Verbatim}[commandchars=\\\{\}]
\PYGZpc{}pI4    1.2.3.4
\PYGZpc{}pi4    001.002.003.004
\PYGZpc{}p[Ii]4[hnbl]
\end{Verbatim}

For printing IPv4 dot-separated decimal addresses. The \code{I4} and \code{i4}
specifiers result in a printed address with (i4) or without (I4) leading
zeros.

The additional \code{h}, \code{n}, \code{b}, and \code{l} specifiers are used to specify
host, network, big or little endian order addresses respectively. Where
no specifier is provided the default network/big endian order is used.

Passed by reference.


\subsubsection{IPv6 addresses}
\label{core-api/printk-formats:ipv6-addresses}
\begin{Verbatim}[commandchars=\\\{\}]
\PYGZpc{}pI6    0001:0002:0003:0004:0005:0006:0007:0008
\PYGZpc{}pi6    00010002000300040005000600070008
\PYGZpc{}pI6c   1:2:3:4:5:6:7:8
\end{Verbatim}

For printing IPv6 network-order 16-bit hex addresses. The \code{I6} and \code{i6}
specifiers result in a printed address with (I6) or without (i6)
colon-separators. Leading zeros are always used.

The additional \code{c} specifier can be used with the \code{I} specifier to
print a compressed IPv6 address as described by
\href{http://tools.ietf.org/html/rfc5952}{http://tools.ietf.org/html/rfc5952}

Passed by reference.


\subsubsection{IPv4/IPv6 addresses (generic, with port, flowinfo, scope)}
\label{core-api/printk-formats:ipv4-ipv6-addresses-generic-with-port-flowinfo-scope}
\begin{Verbatim}[commandchars=\\\{\}]
\PYGZpc{}pIS    1.2.3.4         or 0001:0002:0003:0004:0005:0006:0007:0008
\PYGZpc{}piS    001.002.003.004 or 00010002000300040005000600070008
\PYGZpc{}pISc   1.2.3.4         or 1:2:3:4:5:6:7:8
\PYGZpc{}pISpc  1.2.3.4:12345   or [1:2:3:4:5:6:7:8]:12345
\PYGZpc{}p[Ii]S[pfschnbl]
\end{Verbatim}

For printing an IP address without the need to distinguish whether it's of
type AF\_INET or AF\_INET6. A pointer to a valid struct sockaddr,
specified through \code{IS} or \code{iS}, can be passed to this format specifier.

The additional \code{p}, \code{f}, and \code{s} specifiers are used to specify port
(IPv4, IPv6), flowinfo (IPv6) and scope (IPv6). Ports have a \code{:} prefix,
flowinfo a \code{/} and scope a \code{\%}, each followed by the actual value.

In case of an IPv6 address the compressed IPv6 address as described by
\href{http://tools.ietf.org/html/rfc5952}{http://tools.ietf.org/html/rfc5952} is being used if the additional
specifier \code{c} is given. The IPv6 address is surrounded by \code{{[}}, \code{{]}} in
case of additional specifiers \code{p}, \code{f} or \code{s} as suggested by
\href{https://tools.ietf.org/html/draft-ietf-6man-text-addr-representation-07}{https://tools.ietf.org/html/draft-ietf-6man-text-addr-representation-07}

In case of IPv4 addresses, the additional \code{h}, \code{n}, \code{b}, and \code{l}
specifiers can be used as well and are ignored in case of an IPv6
address.

Passed by reference.

Further examples:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZpc{}pISfc          1.2.3.4         or [1:2:3:4:5:6:7:8]/123456789
\PYGZpc{}pISsc          1.2.3.4         or [1:2:3:4:5:6:7:8]\PYGZpc{}1234567890
\PYGZpc{}pISpfc         1.2.3.4:12345   or [1:2:3:4:5:6:7:8]:12345/123456789
\end{Verbatim}


\subsubsection{UUID/GUID addresses}
\label{core-api/printk-formats:uuid-guid-addresses}
\begin{Verbatim}[commandchars=\\\{\}]
\PYGZpc{}pUb    00010203\PYGZhy{}0405\PYGZhy{}0607\PYGZhy{}0809\PYGZhy{}0a0b0c0d0e0f
\PYGZpc{}pUB    00010203\PYGZhy{}0405\PYGZhy{}0607\PYGZhy{}0809\PYGZhy{}0A0B0C0D0E0F
\PYGZpc{}pUl    03020100\PYGZhy{}0504\PYGZhy{}0706\PYGZhy{}0809\PYGZhy{}0a0b0c0e0e0f
\PYGZpc{}pUL    03020100\PYGZhy{}0504\PYGZhy{}0706\PYGZhy{}0809\PYGZhy{}0A0B0C0E0E0F
\end{Verbatim}

For printing 16-byte UUID/GUIDs addresses. The additional \code{l}, \code{L},
\code{b} and \code{B} specifiers are used to specify a little endian order in
lower (l) or upper case (L) hex notation - and big endian order in lower (b)
or upper case (B) hex notation.

Where no additional specifiers are used the default big endian
order with lower case hex notation will be printed.

Passed by reference.


\subsubsection{dentry names}
\label{core-api/printk-formats:dentry-names}
\begin{Verbatim}[commandchars=\\\{\}]
\PYGZpc{}pd\PYGZob{},2,3,4\PYGZcb{}
\PYGZpc{}pD\PYGZob{},2,3,4\PYGZcb{}
\end{Verbatim}

For printing dentry name; if we race with \code{d\_move()}, the name might
be a mix of old and new ones, but it won't oops.  \%pd dentry is a safer
equivalent of \%s dentry-\textgreater{}d\_name.name we used to use, \%pd\textless{}n\textgreater{} prints \code{n}
last components.  \%pD does the same thing for struct file.

Passed by reference.


\subsubsection{block\_device names}
\label{core-api/printk-formats:block-device-names}
\begin{Verbatim}[commandchars=\\\{\}]
\PYGZpc{}pg     sda, sda1 or loop0p1
\end{Verbatim}

For printing name of block\_device pointers.


\subsubsection{struct va\_format}
\label{core-api/printk-formats:struct-va-format}
\begin{Verbatim}[commandchars=\\\{\}]
\PYGZpc{}pV
\end{Verbatim}

For printing struct va\_format structures. These contain a format string
and va\_list as follows:

\begin{Verbatim}[commandchars=\\\{\}]
struct va\PYGZus{}format \PYGZob{}
        const char *fmt;
        va\PYGZus{}list *va;
\PYGZcb{};
\end{Verbatim}

Implements a ``recursive vsnprintf''.

Do not use this feature without some mechanism to verify the
correctness of the format string and va\_list arguments.

Passed by reference.


\subsubsection{kobjects}
\label{core-api/printk-formats:kobjects}
\begin{Verbatim}[commandchars=\\\{\}]
\PYGZpc{}pOF[fnpPcCF]
\end{Verbatim}

For printing kobject based structs (device nodes). Default behaviour is
equivalent to \%pOFf.
\begin{itemize}
\item {} 
f - device node full\_name

\item {} 
n - device node name

\item {} 
p - device node phandle

\item {} 
P - device node path spec (name + @unit)

\item {} 
F - device node flags

\item {} 
c - major compatible string

\item {} 
C - full compatible string

\end{itemize}

The separator when using multiple arguments is `:'

Examples:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZpc{}pOF    /foo/bar@0                      \PYGZhy{} Node full name
\PYGZpc{}pOFf   /foo/bar@0                      \PYGZhy{} Same as above
\PYGZpc{}pOFfp  /foo/bar@0:10                   \PYGZhy{} Node full name + phandle
\PYGZpc{}pOFfcF /foo/bar@0:foo,device:\PYGZhy{}\PYGZhy{}P\PYGZhy{}      \PYGZhy{} Node full name +
                                          major compatible string +
                                          node flags
                                                D \PYGZhy{} dynamic
                                                d \PYGZhy{} detached
                                                P \PYGZhy{} Populated
                                                B \PYGZhy{} Populated bus
\end{Verbatim}

Passed by reference.


\subsubsection{struct clk}
\label{core-api/printk-formats:struct-clk}
\begin{Verbatim}[commandchars=\\\{\}]
\PYGZpc{}pC     pll1
\PYGZpc{}pCn    pll1
\PYGZpc{}pCr    1560000000
\end{Verbatim}

For printing struct clk structures. \%pC and \%pCn print the name
(Common Clock Framework) or address (legacy clock framework) of the
structure; \%pCr prints the current clock rate.

Passed by reference.


\subsubsection{bitmap and its derivatives such as cpumask and nodemask}
\label{core-api/printk-formats:bitmap-and-its-derivatives-such-as-cpumask-and-nodemask}
\begin{Verbatim}[commandchars=\\\{\}]
\PYGZpc{}*pb    0779
\PYGZpc{}*pbl   0,3\PYGZhy{}6,8\PYGZhy{}10
\end{Verbatim}

For printing bitmap and its derivatives such as cpumask and nodemask,
\%*pb outputs the bitmap with field width as the number of bits and \%*pbl
output the bitmap as range list with field width as the number of bits.

Passed by reference.


\subsubsection{Flags bitfields such as page flags, gfp\_flags}
\label{core-api/printk-formats:flags-bitfields-such-as-page-flags-gfp-flags}
\begin{Verbatim}[commandchars=\\\{\}]
\PYGZpc{}pGp    referenced\textbar{}uptodate\textbar{}lru\textbar{}active\textbar{}private
\PYGZpc{}pGg    GFP\PYGZus{}USER\textbar{}GFP\PYGZus{}DMA32\textbar{}GFP\PYGZus{}NOWARN
\PYGZpc{}pGv    read\textbar{}exec\textbar{}mayread\textbar{}maywrite\textbar{}mayexec\textbar{}denywrite
\end{Verbatim}

For printing flags bitfields as a collection of symbolic constants that
would construct the value. The type of flags is given by the third
character. Currently supported are {[}p{]}age flags, {[}v{]}ma\_flags (both
expect \code{unsigned long *}) and {[}g{]}fp\_flags (expects \code{gfp\_t *}). The flag
names and print order depends on the particular type.

Note that this format should not be used directly in the
\code{TP\_printk()} part of a tracepoint. Instead, use the show\_*\_flags()
functions from \textless{}trace/events/mmflags.h\textgreater{}.

Passed by reference.


\subsubsection{Network device features}
\label{core-api/printk-formats:network-device-features}
\begin{Verbatim}[commandchars=\\\{\}]
\PYGZpc{}pNF    0x000000000000c000
\end{Verbatim}

For printing netdev\_features\_t.

Passed by reference.


\subsection{Thanks}
\label{core-api/printk-formats:thanks}
If you add other \%p extensions, please extend \textless{}lib/test\_printf.c\textgreater{} with
one or more test cases, if at all feasible.

Thank you for your cooperation and attention.


\chapter{Interfaces for kernel debugging}
\label{core-api/index:interfaces-for-kernel-debugging}

\section{The object-lifetime debugging infrastructure}
\label{core-api/debug-objects:the-object-lifetime-debugging-infrastructure}\label{core-api/debug-objects::doc}\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Thomas Gleixner

\end{description}\end{quote}


\subsection{Introduction}
\label{core-api/debug-objects:introduction}
debugobjects is a generic infrastructure to track the life time of
kernel objects and validate the operations on those.

debugobjects is useful to check for the following error patterns:
\begin{itemize}
\item {} 
Activation of uninitialized objects

\item {} 
Initialization of active objects

\item {} 
Usage of freed/destroyed objects

\end{itemize}

debugobjects is not changing the data structure of the real object so it
can be compiled in with a minimal runtime impact and enabled on demand
with a kernel command line option.


\subsection{Howto use debugobjects}
\label{core-api/debug-objects:howto-use-debugobjects}
A kernel subsystem needs to provide a data structure which describes the
object type and add calls into the debug code at appropriate places. The
data structure to describe the object type needs at minimum the name of
the object type. Optional functions can and should be provided to fixup
detected problems so the kernel can continue to work and the debug
information can be retrieved from a live system instead of hard core
debugging with serial consoles and stack trace transcripts from the
monitor.

The debug calls provided by debugobjects are:
\begin{itemize}
\item {} 
debug\_object\_init

\item {} 
debug\_object\_init\_on\_stack

\item {} 
debug\_object\_activate

\item {} 
debug\_object\_deactivate

\item {} 
debug\_object\_destroy

\item {} 
debug\_object\_free

\item {} 
debug\_object\_assert\_init

\end{itemize}

Each of these functions takes the address of the real object and a
pointer to the object type specific debug description structure.

Each detected error is reported in the statistics and a limited number
of errors are printk'ed including a full stack trace.

The statistics are available via /sys/kernel/debug/debug\_objects/stats.
They provide information about the number of warnings and the number of
successful fixups along with information about the usage of the internal
tracking objects and the state of the internal tracking objects pool.


\subsection{Debug functions}
\label{core-api/debug-objects:debug-functions}\index{debug\_object\_init (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/debug-objects:c.debug_object_init}\pysiglinewithargsret{void \bfcode{debug\_object\_init}}{void *\emph{ addr}, struct {\hyperref[core\string-api/debug\string-objects:c.debug_obj_descr]{\emph{debug\_obj\_descr}}} *\emph{ descr}}{}
debug checks when an object is initialized

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void * addr}}] \leavevmode
address of the object

\item[{\code{struct debug\_obj\_descr * descr}}] \leavevmode
pointer to an object specific debug description structure

\end{description}

This function is called whenever the initialization function of a real
object is called.

When the real object is already tracked by debugobjects it is checked,
whether the object can be initialized. Initializing is not allowed for
active and destroyed objects. When debugobjects detects an error, then
it calls the fixup\_init function of the object type description
structure if provided by the caller. The fixup function can correct the
problem before the real initialization of the object happens. E.g. it
can deactivate an active object in order to prevent damage to the
subsystem.

When the real object is not yet tracked by debugobjects, debugobjects
allocates a tracker object for the real object and sets the tracker
object state to ODEBUG\_STATE\_INIT. It verifies that the object is not
on the callers stack. If it is on the callers stack then a limited
number of warnings including a full stack trace is printk'ed. The
calling code must use debug\_object\_init\_on\_stack() and remove the
object before leaving the function which allocated it. See next section.
\index{debug\_object\_init\_on\_stack (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/debug-objects:c.debug_object_init_on_stack}\pysiglinewithargsret{void \bfcode{debug\_object\_init\_on\_stack}}{void *\emph{ addr}, struct {\hyperref[core\string-api/debug\string-objects:c.debug_obj_descr]{\emph{debug\_obj\_descr}}} *\emph{ descr}}{}
debug checks when an object on stack is initialized

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void * addr}}] \leavevmode
address of the object

\item[{\code{struct debug\_obj\_descr * descr}}] \leavevmode
pointer to an object specific debug description structure

\end{description}

This function is called whenever the initialization function of a real
object which resides on the stack is called.

When the real object is already tracked by debugobjects it is checked,
whether the object can be initialized. Initializing is not allowed for
active and destroyed objects. When debugobjects detects an error, then
it calls the fixup\_init function of the object type description
structure if provided by the caller. The fixup function can correct the
problem before the real initialization of the object happens. E.g. it
can deactivate an active object in order to prevent damage to the
subsystem.

When the real object is not yet tracked by debugobjects debugobjects
allocates a tracker object for the real object and sets the tracker
object state to ODEBUG\_STATE\_INIT. It verifies that the object is on
the callers stack.

An object which is on the stack must be removed from the tracker by
calling debug\_object\_free() before the function which allocates the
object returns. Otherwise we keep track of stale objects.
\index{debug\_object\_activate (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/debug-objects:c.debug_object_activate}\pysiglinewithargsret{int \bfcode{debug\_object\_activate}}{void *\emph{ addr}, struct {\hyperref[core\string-api/debug\string-objects:c.debug_obj_descr]{\emph{debug\_obj\_descr}}} *\emph{ descr}}{}
debug checks when an object is activated

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void * addr}}] \leavevmode
address of the object

\item[{\code{struct debug\_obj\_descr * descr}}] \leavevmode
pointer to an object specific debug description structure
Returns 0 for success, -EINVAL for check failed.

\end{description}

This function is called whenever the activation function of a real
object is called.

When the real object is already tracked by debugobjects it is checked,
whether the object can be activated. Activating is not allowed for
active and destroyed objects. When debugobjects detects an error, then
it calls the fixup\_activate function of the object type description
structure if provided by the caller. The fixup function can correct the
problem before the real activation of the object happens. E.g. it can
deactivate an active object in order to prevent damage to the subsystem.

When the real object is not yet tracked by debugobjects then the
fixup\_activate function is called if available. This is necessary to
allow the legitimate activation of statically allocated and initialized
objects. The fixup function checks whether the object is valid and calls
the debug\_objects\_init() function to initialize the tracking of this
object.

When the activation is legitimate, then the state of the associated
tracker object is set to ODEBUG\_STATE\_ACTIVE.
\index{debug\_object\_deactivate (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/debug-objects:c.debug_object_deactivate}\pysiglinewithargsret{void \bfcode{debug\_object\_deactivate}}{void *\emph{ addr}, struct {\hyperref[core\string-api/debug\string-objects:c.debug_obj_descr]{\emph{debug\_obj\_descr}}} *\emph{ descr}}{}
debug checks when an object is deactivated

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void * addr}}] \leavevmode
address of the object

\item[{\code{struct debug\_obj\_descr * descr}}] \leavevmode
pointer to an object specific debug description structure

\end{description}

This function is called whenever the deactivation function of a real
object is called.

When the real object is tracked by debugobjects it is checked, whether
the object can be deactivated. Deactivating is not allowed for untracked
or destroyed objects.

When the deactivation is legitimate, then the state of the associated
tracker object is set to ODEBUG\_STATE\_INACTIVE.
\index{debug\_object\_destroy (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/debug-objects:c.debug_object_destroy}\pysiglinewithargsret{void \bfcode{debug\_object\_destroy}}{void *\emph{ addr}, struct {\hyperref[core\string-api/debug\string-objects:c.debug_obj_descr]{\emph{debug\_obj\_descr}}} *\emph{ descr}}{}
debug checks when an object is destroyed

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void * addr}}] \leavevmode
address of the object

\item[{\code{struct debug\_obj\_descr * descr}}] \leavevmode
pointer to an object specific debug description structure

\end{description}

This function is called to mark an object destroyed. This is useful to
prevent the usage of invalid objects, which are still available in
memory: either statically allocated objects or objects which are freed
later.

When the real object is tracked by debugobjects it is checked, whether
the object can be destroyed. Destruction is not allowed for active and
destroyed objects. When debugobjects detects an error, then it calls the
fixup\_destroy function of the object type description structure if
provided by the caller. The fixup function can correct the problem
before the real destruction of the object happens. E.g. it can
deactivate an active object in order to prevent damage to the subsystem.

When the destruction is legitimate, then the state of the associated
tracker object is set to ODEBUG\_STATE\_DESTROYED.
\index{debug\_object\_free (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/debug-objects:c.debug_object_free}\pysiglinewithargsret{void \bfcode{debug\_object\_free}}{void *\emph{ addr}, struct {\hyperref[core\string-api/debug\string-objects:c.debug_obj_descr]{\emph{debug\_obj\_descr}}} *\emph{ descr}}{}
debug checks when an object is freed

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void * addr}}] \leavevmode
address of the object

\item[{\code{struct debug\_obj\_descr * descr}}] \leavevmode
pointer to an object specific debug description structure

\end{description}

This function is called before an object is freed.

When the real object is tracked by debugobjects it is checked, whether
the object can be freed. Free is not allowed for active objects. When
debugobjects detects an error, then it calls the fixup\_free function of
the object type description structure if provided by the caller. The
fixup function can correct the problem before the real free of the
object happens. E.g. it can deactivate an active object in order to
prevent damage to the subsystem.

Note that debug\_object\_free removes the object from the tracker. Later
usage of the object is detected by the other debug checks.
\index{debug\_object\_assert\_init (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/debug-objects:c.debug_object_assert_init}\pysiglinewithargsret{void \bfcode{debug\_object\_assert\_init}}{void *\emph{ addr}, struct {\hyperref[core\string-api/debug\string-objects:c.debug_obj_descr]{\emph{debug\_obj\_descr}}} *\emph{ descr}}{}
debug checks when object should be init-ed

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{void * addr}}] \leavevmode
address of the object

\item[{\code{struct debug\_obj\_descr * descr}}] \leavevmode
pointer to an object specific debug description structure

\end{description}

This function is called to assert that an object has been initialized.

When the real object is not tracked by debugobjects, it calls
fixup\_assert\_init of the object type description structure provided by
the caller, with the hardcoded object state ODEBUG\_NOT\_AVAILABLE. The
fixup function can correct the problem by calling debug\_object\_init
and other specific initializing functions.

When the real object is already tracked by debugobjects it is ignored.


\subsection{Fixup functions}
\label{core-api/debug-objects:fixup-functions}

\subsubsection{Debug object type description structure}
\label{core-api/debug-objects:debug-object-type-description-structure}\index{debug\_obj (C type)}

\begin{fulllineitems}
\phantomsection\label{core-api/debug-objects:c.debug_obj}\pysigline{struct \bfcode{debug\_obj}}
representaion of an tracked object

\end{fulllineitems}


\textbf{Definition}

\begin{Verbatim}[commandchars=\\\{\}]
struct debug\PYGZus{}obj \PYGZob{}
  struct hlist\PYGZus{}node       node;
  enum debug\PYGZus{}obj\PYGZus{}state    state;
  unsigned int            astate;
  void *object;
  struct debug\PYGZus{}obj\PYGZus{}descr  *descr;
\PYGZcb{};
\end{Verbatim}

\textbf{Members}
\begin{description}
\item[{\code{node}}] \leavevmode
hlist node to link the object into the tracker list

\item[{\code{state}}] \leavevmode
tracked object state

\item[{\code{astate}}] \leavevmode
current active state

\item[{\code{object}}] \leavevmode
pointer to the real object

\item[{\code{descr}}] \leavevmode
pointer to an object type specific debug description structure

\end{description}
\index{debug\_obj\_descr (C type)}

\begin{fulllineitems}
\phantomsection\label{core-api/debug-objects:c.debug_obj_descr}\pysigline{struct \bfcode{debug\_obj\_descr}}
object type specific debug description structure

\end{fulllineitems}


\textbf{Definition}

\begin{Verbatim}[commandchars=\\\{\}]
struct debug\PYGZus{}obj\PYGZus{}descr \PYGZob{}
  const char              *name;
  void *(*debug\PYGZus{}hint)(void *addr);
  bool (*is\PYGZus{}static\PYGZus{}object)(void *addr);
  bool (*fixup\PYGZus{}init)(void *addr, enum debug\PYGZus{}obj\PYGZus{}state state);
  bool (*fixup\PYGZus{}activate)(void *addr, enum debug\PYGZus{}obj\PYGZus{}state state);
  bool (*fixup\PYGZus{}destroy)(void *addr, enum debug\PYGZus{}obj\PYGZus{}state state);
  bool (*fixup\PYGZus{}free)(void *addr, enum debug\PYGZus{}obj\PYGZus{}state state);
  bool (*fixup\PYGZus{}assert\PYGZus{}init)(void *addr, enum debug\PYGZus{}obj\PYGZus{}state state);
\PYGZcb{};
\end{Verbatim}

\textbf{Members}
\begin{description}
\item[{\code{name}}] \leavevmode
name of the object typee

\item[{\code{debug\_hint}}] \leavevmode
function returning address, which have associated
kernel symbol, to allow identify the object

\item[{\code{is\_static\_object}}] \leavevmode
return true if the obj is static, otherwise return false

\item[{\code{fixup\_init}}] \leavevmode
fixup function, which is called when the init check
fails. All fixup functions must return true if fixup
was successful, otherwise return false

\item[{\code{fixup\_activate}}] \leavevmode
fixup function, which is called when the activate check
fails

\item[{\code{fixup\_destroy}}] \leavevmode
fixup function, which is called when the destroy check
fails

\item[{\code{fixup\_free}}] \leavevmode
fixup function, which is called when the free check
fails

\item[{\code{fixup\_assert\_init}}] \leavevmode
fixup function, which is called when the assert\_init
check fails

\end{description}


\subsubsection{fixup\_init}
\label{core-api/debug-objects:fixup-init}
This function is called from the debug code whenever a problem in
debug\_object\_init is detected. The function takes the address of the
object and the state which is currently recorded in the tracker.

Called from debug\_object\_init when the object state is:
\begin{itemize}
\item {} 
ODEBUG\_STATE\_ACTIVE

\end{itemize}

The function returns true when the fixup was successful, otherwise
false. The return value is used to update the statistics.

Note, that the function needs to call the debug\_object\_init() function
again, after the damage has been repaired in order to keep the state
consistent.


\subsubsection{fixup\_activate}
\label{core-api/debug-objects:fixup-activate}
This function is called from the debug code whenever a problem in
debug\_object\_activate is detected.

Called from debug\_object\_activate when the object state is:
\begin{itemize}
\item {} 
ODEBUG\_STATE\_NOTAVAILABLE

\item {} 
ODEBUG\_STATE\_ACTIVE

\end{itemize}

The function returns true when the fixup was successful, otherwise
false. The return value is used to update the statistics.

Note that the function needs to call the debug\_object\_activate()
function again after the damage has been repaired in order to keep the
state consistent.

The activation of statically initialized objects is a special case. When
debug\_object\_activate() has no tracked object for this object address
then fixup\_activate() is called with object state
ODEBUG\_STATE\_NOTAVAILABLE. The fixup function needs to check whether
this is a legitimate case of a statically initialized object or not. In
case it is it calls debug\_object\_init() and debug\_object\_activate()
to make the object known to the tracker and marked active. In this case
the function should return false because this is not a real fixup.


\subsubsection{fixup\_destroy}
\label{core-api/debug-objects:fixup-destroy}
This function is called from the debug code whenever a problem in
debug\_object\_destroy is detected.

Called from debug\_object\_destroy when the object state is:
\begin{itemize}
\item {} 
ODEBUG\_STATE\_ACTIVE

\end{itemize}

The function returns true when the fixup was successful, otherwise
false. The return value is used to update the statistics.


\subsubsection{fixup\_free}
\label{core-api/debug-objects:fixup-free}
This function is called from the debug code whenever a problem in
debug\_object\_free is detected. Further it can be called from the debug
checks in kfree/vfree, when an active object is detected from the
debug\_check\_no\_obj\_freed() sanity checks.

Called from debug\_object\_free() or debug\_check\_no\_obj\_freed() when
the object state is:
\begin{itemize}
\item {} 
ODEBUG\_STATE\_ACTIVE

\end{itemize}

The function returns true when the fixup was successful, otherwise
false. The return value is used to update the statistics.


\subsubsection{fixup\_assert\_init}
\label{core-api/debug-objects:fixup-assert-init}
This function is called from the debug code whenever a problem in
debug\_object\_assert\_init is detected.

Called from debug\_object\_assert\_init() with a hardcoded state
ODEBUG\_STATE\_NOTAVAILABLE when the object is not found in the debug
bucket.

The function returns true when the fixup was successful, otherwise
false. The return value is used to update the statistics.

Note, this function should make sure debug\_object\_init() is called
before returning.

The handling of statically initialized objects is a special case. The
fixup function should check if this is a legitimate case of a statically
initialized object or not. In this case only debug\_object\_init()
should be called to make the object known to the tracker. Then the
function should return false because this is not a real fixup.


\subsection{Known Bugs And Assumptions}
\label{core-api/debug-objects:known-bugs-and-assumptions}
None (knock on wood).


\section{The Linux Kernel Tracepoint API}
\label{core-api/tracepoint:the-linux-kernel-tracepoint-api}\label{core-api/tracepoint::doc}\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Jason Baron

\item[{Author}] \leavevmode
William Cohen

\end{description}\end{quote}


\subsection{Introduction}
\label{core-api/tracepoint:introduction}
Tracepoints are static probe points that are located in strategic points
throughout the kernel. `Probes' register/unregister with tracepoints via
a callback mechanism. The `probes' are strictly typed functions that are
passed a unique set of parameters defined by each tracepoint.

From this simple callback mechanism, `probes' can be used to profile,
debug, and understand kernel behavior. There are a number of tools that
provide a framework for using `probes'. These tools include Systemtap,
ftrace, and LTTng.

Tracepoints are defined in a number of header files via various macros.
Thus, the purpose of this document is to provide a clear accounting of
the available tracepoints. The intention is to understand not only what
tracepoints are available but also to understand where future
tracepoints might be added.

The API presented has functions of the form:
\code{trace\_tracepointname(function parameters)}. These are the tracepoints
callbacks that are found throughout the code. Registering and
unregistering probes with these callback sites is covered in the
\code{Documentation/trace/*} directory.


\subsection{IRQ}
\label{core-api/tracepoint:irq}\index{trace\_irq\_handler\_entry (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/tracepoint:c.trace_irq_handler_entry}\pysiglinewithargsret{void \bfcode{trace\_irq\_handler\_entry}}{int\emph{ irq}, struct {\hyperref[core\string-api/genericirq:c.irqaction]{\emph{irqaction}}} *\emph{ action}}{}
called immediately before the irq action handler

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{int irq}}] \leavevmode
irq number

\item[{\code{struct irqaction * action}}] \leavevmode
pointer to struct irqaction

\end{description}

\textbf{Description}

The struct irqaction pointed to by \textbf{action} contains various
information about the handler, including the device name,
\textbf{action}-\textgreater{}name, and the device id, \textbf{action}-\textgreater{}dev\_id. When used in
conjunction with the irq\_handler\_exit tracepoint, we can figure
out irq handler latencies.
\index{trace\_irq\_handler\_exit (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/tracepoint:c.trace_irq_handler_exit}\pysiglinewithargsret{void \bfcode{trace\_irq\_handler\_exit}}{int\emph{ irq}, struct {\hyperref[core\string-api/genericirq:c.irqaction]{\emph{irqaction}}} *\emph{ action}, int\emph{ ret}}{}
called immediately after the irq action handler returns

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{int irq}}] \leavevmode
irq number

\item[{\code{struct irqaction * action}}] \leavevmode
pointer to struct irqaction

\item[{\code{int ret}}] \leavevmode
return value

\end{description}

\textbf{Description}

If the \textbf{ret} value is set to IRQ\_HANDLED, then we know that the corresponding
\textbf{action}-\textgreater{}handler successfully handled this irq. Otherwise, the irq might be
a shared irq line, or the irq was not handled successfully. Can be used in
conjunction with the irq\_handler\_entry to understand irq handler latencies.
\index{trace\_softirq\_entry (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/tracepoint:c.trace_softirq_entry}\pysiglinewithargsret{void \bfcode{trace\_softirq\_entry}}{unsigned int\emph{ vec\_nr}}{}
called immediately before the softirq handler

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int vec\_nr}}] \leavevmode
softirq vector number

\end{description}

\textbf{Description}

When used in combination with the softirq\_exit tracepoint
we can determine the softirq handler routine.
\index{trace\_softirq\_exit (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/tracepoint:c.trace_softirq_exit}\pysiglinewithargsret{void \bfcode{trace\_softirq\_exit}}{unsigned int\emph{ vec\_nr}}{}
called immediately after the softirq handler returns

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int vec\_nr}}] \leavevmode
softirq vector number

\end{description}

\textbf{Description}

When used in combination with the softirq\_entry tracepoint
we can determine the softirq handler routine.
\index{trace\_softirq\_raise (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/tracepoint:c.trace_softirq_raise}\pysiglinewithargsret{void \bfcode{trace\_softirq\_raise}}{unsigned int\emph{ vec\_nr}}{}
called immediately when a softirq is raised

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int vec\_nr}}] \leavevmode
softirq vector number

\end{description}

\textbf{Description}

When used in combination with the softirq\_entry tracepoint
we can determine the softirq raise to run latency.


\subsection{SIGNAL}
\label{core-api/tracepoint:signal}\index{trace\_signal\_generate (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/tracepoint:c.trace_signal_generate}\pysiglinewithargsret{void \bfcode{trace\_signal\_generate}}{int\emph{ sig}, struct siginfo *\emph{ info}, struct task\_struct *\emph{ task}, int\emph{ group}, int\emph{ result}}{}
called when a signal is generated

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{int sig}}] \leavevmode
signal number

\item[{\code{struct siginfo * info}}] \leavevmode
pointer to struct siginfo

\item[{\code{struct task\_struct * task}}] \leavevmode
pointer to struct task\_struct

\item[{\code{int group}}] \leavevmode
shared or private

\item[{\code{int result}}] \leavevmode
TRACE\_SIGNAL\_*

\end{description}

\textbf{Description}

Current process sends a `sig' signal to `task' process with
`info' siginfo. If `info' is SEND\_SIG\_NOINFO or SEND\_SIG\_PRIV,
`info' is not a pointer and you can't access its field. Instead,
SEND\_SIG\_NOINFO means that si\_code is SI\_USER, and SEND\_SIG\_PRIV
means that si\_code is SI\_KERNEL.
\index{trace\_signal\_deliver (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/tracepoint:c.trace_signal_deliver}\pysiglinewithargsret{void \bfcode{trace\_signal\_deliver}}{int\emph{ sig}, struct siginfo *\emph{ info}, struct k\_sigaction *\emph{ ka}}{}
called when a signal is delivered

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{int sig}}] \leavevmode
signal number

\item[{\code{struct siginfo * info}}] \leavevmode
pointer to struct siginfo

\item[{\code{struct k\_sigaction * ka}}] \leavevmode
pointer to struct k\_sigaction

\end{description}

\textbf{Description}

A `sig' signal is delivered to current process with `info' siginfo,
and it will be handled by `ka'. ka-\textgreater{}sa.sa\_handler can be SIG\_IGN or
SIG\_DFL.
Note that some signals reported by signal\_generate tracepoint can be
lost, ignored or modified (by debugger) before hitting this tracepoint.
This means, this can show which signals are actually delivered, but
matching generated signals and delivered signals may not be correct.


\subsection{Block IO}
\label{core-api/tracepoint:block-io}\index{trace\_block\_touch\_buffer (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/tracepoint:c.trace_block_touch_buffer}\pysiglinewithargsret{void \bfcode{trace\_block\_touch\_buffer}}{struct buffer\_head *\emph{ bh}}{}
mark a buffer accessed

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct buffer\_head * bh}}] \leavevmode
buffer\_head being touched

\end{description}

\textbf{Description}

Called from \code{touch\_buffer()}.
\index{trace\_block\_dirty\_buffer (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/tracepoint:c.trace_block_dirty_buffer}\pysiglinewithargsret{void \bfcode{trace\_block\_dirty\_buffer}}{struct buffer\_head *\emph{ bh}}{}
mark a buffer dirty

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct buffer\_head * bh}}] \leavevmode
buffer\_head being dirtied

\end{description}

\textbf{Description}

Called from \code{mark\_buffer\_dirty()}.
\index{trace\_block\_rq\_requeue (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/tracepoint:c.trace_block_rq_requeue}\pysiglinewithargsret{void \bfcode{trace\_block\_rq\_requeue}}{struct request\_queue *\emph{ q}, struct request *\emph{ rq}}{}
place block IO request back on a queue

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
queue holding operation

\item[{\code{struct request * rq}}] \leavevmode
block IO operation request

\end{description}

\textbf{Description}

The block operation request \textbf{rq} is being placed back into queue
\textbf{q}.  For some reason the request was not completed and needs to be
put back in the queue.
\index{trace\_block\_rq\_complete (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/tracepoint:c.trace_block_rq_complete}\pysiglinewithargsret{void \bfcode{trace\_block\_rq\_complete}}{struct request *\emph{ rq}, int\emph{ error}, unsigned int\emph{ nr\_bytes}}{}
block IO operation completed by device driver

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request * rq}}] \leavevmode
block operations request

\item[{\code{int error}}] \leavevmode
status code

\item[{\code{unsigned int nr\_bytes}}] \leavevmode
number of completed bytes

\end{description}

\textbf{Description}

The block\_rq\_complete tracepoint event indicates that some portion
of operation request has been completed by the device driver.  If
the \textbf{rq}-\textgreater{}bio is \code{NULL}, then there is absolutely no additional work to
do for the request. If \textbf{rq}-\textgreater{}bio is non-NULL then there is
additional work required to complete the request.
\index{trace\_block\_rq\_insert (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/tracepoint:c.trace_block_rq_insert}\pysiglinewithargsret{void \bfcode{trace\_block\_rq\_insert}}{struct request\_queue *\emph{ q}, struct request *\emph{ rq}}{}
insert block operation request into queue

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
target queue

\item[{\code{struct request * rq}}] \leavevmode
block IO operation request

\end{description}

\textbf{Description}

Called immediately before block operation request \textbf{rq} is inserted
into queue \textbf{q}.  The fields in the operation request \textbf{rq} struct can
be examined to determine which device and sectors the pending
operation would access.
\index{trace\_block\_rq\_issue (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/tracepoint:c.trace_block_rq_issue}\pysiglinewithargsret{void \bfcode{trace\_block\_rq\_issue}}{struct request\_queue *\emph{ q}, struct request *\emph{ rq}}{}
issue pending block IO request operation to device driver

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
queue holding operation

\item[{\code{struct request * rq}}] \leavevmode
block IO operation operation request

\end{description}

\textbf{Description}

Called when block operation request \textbf{rq} from queue \textbf{q} is sent to a
device driver for processing.
\index{trace\_block\_bio\_bounce (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/tracepoint:c.trace_block_bio_bounce}\pysiglinewithargsret{void \bfcode{trace\_block\_bio\_bounce}}{struct request\_queue *\emph{ q}, struct bio *\emph{ bio}}{}
used bounce buffer when processing block operation

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
queue holding the block operation

\item[{\code{struct bio * bio}}] \leavevmode
block operation

\end{description}

\textbf{Description}

A bounce buffer was used to handle the block operation \textbf{bio} in \textbf{q}.
This occurs when hardware limitations prevent a direct transfer of
data between the \textbf{bio} data memory area and the IO device.  Use of a
bounce buffer requires extra copying of data and decreases
performance.
\index{trace\_block\_bio\_complete (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/tracepoint:c.trace_block_bio_complete}\pysiglinewithargsret{void \bfcode{trace\_block\_bio\_complete}}{struct request\_queue *\emph{ q}, struct bio *\emph{ bio}, int\emph{ error}}{}
completed all work on the block operation

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
queue holding the block operation

\item[{\code{struct bio * bio}}] \leavevmode
block operation completed

\item[{\code{int error}}] \leavevmode
io error value

\end{description}

\textbf{Description}

This tracepoint indicates there is no further work to do on this
block IO operation \textbf{bio}.
\index{trace\_block\_bio\_backmerge (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/tracepoint:c.trace_block_bio_backmerge}\pysiglinewithargsret{void \bfcode{trace\_block\_bio\_backmerge}}{struct request\_queue *\emph{ q}, struct request *\emph{ rq}, struct bio *\emph{ bio}}{}
merging block operation to the end of an existing operation

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
queue holding operation

\item[{\code{struct request * rq}}] \leavevmode
request bio is being merged into

\item[{\code{struct bio * bio}}] \leavevmode
new block operation to merge

\end{description}

\textbf{Description}

Merging block request \textbf{bio} to the end of an existing block request
in queue \textbf{q}.
\index{trace\_block\_bio\_frontmerge (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/tracepoint:c.trace_block_bio_frontmerge}\pysiglinewithargsret{void \bfcode{trace\_block\_bio\_frontmerge}}{struct request\_queue *\emph{ q}, struct request *\emph{ rq}, struct bio *\emph{ bio}}{}
merging block operation to the beginning of an existing operation

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
queue holding operation

\item[{\code{struct request * rq}}] \leavevmode
request bio is being merged into

\item[{\code{struct bio * bio}}] \leavevmode
new block operation to merge

\end{description}

\textbf{Description}

Merging block IO operation \textbf{bio} to the beginning of an existing block
operation in queue \textbf{q}.
\index{trace\_block\_bio\_queue (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/tracepoint:c.trace_block_bio_queue}\pysiglinewithargsret{void \bfcode{trace\_block\_bio\_queue}}{struct request\_queue *\emph{ q}, struct bio *\emph{ bio}}{}
putting new block IO operation in queue

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
queue holding operation

\item[{\code{struct bio * bio}}] \leavevmode
new block operation

\end{description}

\textbf{Description}

About to place the block IO operation \textbf{bio} into queue \textbf{q}.
\index{trace\_block\_getrq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/tracepoint:c.trace_block_getrq}\pysiglinewithargsret{void \bfcode{trace\_block\_getrq}}{struct request\_queue *\emph{ q}, struct bio *\emph{ bio}, int\emph{ rw}}{}
get a free request entry in queue for block IO operations

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
queue for operations

\item[{\code{struct bio * bio}}] \leavevmode
pending block IO operation (can be \code{NULL})

\item[{\code{int rw}}] \leavevmode
low bit indicates a read (\code{0}) or a write (\code{1})

\end{description}

\textbf{Description}

A request struct for queue \textbf{q} has been allocated to handle the
block IO operation \textbf{bio}.
\index{trace\_block\_sleeprq (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/tracepoint:c.trace_block_sleeprq}\pysiglinewithargsret{void \bfcode{trace\_block\_sleeprq}}{struct request\_queue *\emph{ q}, struct bio *\emph{ bio}, int\emph{ rw}}{}
waiting to get a free request entry in queue for block IO operation

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
queue for operation

\item[{\code{struct bio * bio}}] \leavevmode
pending block IO operation (can be \code{NULL})

\item[{\code{int rw}}] \leavevmode
low bit indicates a read (\code{0}) or a write (\code{1})

\end{description}

\textbf{Description}

In the case where a request struct cannot be provided for queue \textbf{q}
the process needs to wait for an request struct to become
available.  This tracepoint event is generated each time the
process goes to sleep waiting for request struct become available.
\index{trace\_block\_plug (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/tracepoint:c.trace_block_plug}\pysiglinewithargsret{void \bfcode{trace\_block\_plug}}{struct request\_queue *\emph{ q}}{}
keep operations requests in request queue

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
request queue to plug

\end{description}

\textbf{Description}

Plug the request queue \textbf{q}.  Do not allow block operation requests
to be sent to the device driver. Instead, accumulate requests in
the queue to improve throughput performance of the block device.
\index{trace\_block\_unplug (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/tracepoint:c.trace_block_unplug}\pysiglinewithargsret{void \bfcode{trace\_block\_unplug}}{struct request\_queue *\emph{ q}, unsigned int\emph{ depth}, bool\emph{ explicit}}{}
release of operations requests in request queue

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
request queue to unplug

\item[{\code{unsigned int depth}}] \leavevmode
number of requests just added to the queue

\item[{\code{bool explicit}}] \leavevmode
whether this was an explicit unplug, or one from \code{schedule()}

\end{description}

\textbf{Description}

Unplug request queue \textbf{q} because device driver is scheduled to work
on elements in the request queue.
\index{trace\_block\_split (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/tracepoint:c.trace_block_split}\pysiglinewithargsret{void \bfcode{trace\_block\_split}}{struct request\_queue *\emph{ q}, struct bio *\emph{ bio}, unsigned int\emph{ new\_sector}}{}
split a single bio struct into two bio structs

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
queue containing the bio

\item[{\code{struct bio * bio}}] \leavevmode
block operation being split

\item[{\code{unsigned int new\_sector}}] \leavevmode
The starting sector for the new bio

\end{description}

\textbf{Description}

The bio request \textbf{bio} in request queue \textbf{q} needs to be split into two
bio requests. The newly created \textbf{bio} request starts at
\textbf{new\_sector}. This split may be required due to hardware limitation
such as operation crossing device boundaries in a RAID system.
\index{trace\_block\_bio\_remap (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/tracepoint:c.trace_block_bio_remap}\pysiglinewithargsret{void \bfcode{trace\_block\_bio\_remap}}{struct request\_queue *\emph{ q}, struct bio *\emph{ bio}, dev\_t\emph{ dev}, sector\_t\emph{ from}}{}
map request for a logical device to the raw device

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
queue holding the operation

\item[{\code{struct bio * bio}}] \leavevmode
revised operation

\item[{\code{dev\_t dev}}] \leavevmode
device for the operation

\item[{\code{sector\_t from}}] \leavevmode
original sector for the operation

\end{description}

\textbf{Description}

An operation for a logical device has been mapped to the
raw block device.
\index{trace\_block\_rq\_remap (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/tracepoint:c.trace_block_rq_remap}\pysiglinewithargsret{void \bfcode{trace\_block\_rq\_remap}}{struct request\_queue *\emph{ q}, struct request *\emph{ rq}, dev\_t\emph{ dev}, sector\_t\emph{ from}}{}
map request for a block operation request

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct request\_queue * q}}] \leavevmode
queue holding the operation

\item[{\code{struct request * rq}}] \leavevmode
block IO operation request

\item[{\code{dev\_t dev}}] \leavevmode
device for the operation

\item[{\code{sector\_t from}}] \leavevmode
original sector for the operation

\end{description}

\textbf{Description}

The block operation request \textbf{rq} in \textbf{q} has been remapped.  The block
operation request \textbf{rq} holds the current information and \textbf{from} hold
the original sector.


\subsection{Workqueue}
\label{core-api/tracepoint:workqueue}\index{trace\_workqueue\_queue\_work (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/tracepoint:c.trace_workqueue_queue_work}\pysiglinewithargsret{void \bfcode{trace\_workqueue\_queue\_work}}{unsigned int\emph{ req\_cpu}, struct pool\_workqueue *\emph{ pwq}, struct work\_struct *\emph{ work}}{}
called when a work gets queued

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{unsigned int req\_cpu}}] \leavevmode
the requested cpu

\item[{\code{struct pool\_workqueue * pwq}}] \leavevmode
pointer to struct pool\_workqueue

\item[{\code{struct work\_struct * work}}] \leavevmode
pointer to struct work\_struct

\end{description}

\textbf{Description}

This event occurs when a work is queued immediately or once a
delayed work is actually queued on a workqueue (ie: once the delay
has been reached).
\index{trace\_workqueue\_activate\_work (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/tracepoint:c.trace_workqueue_activate_work}\pysiglinewithargsret{void \bfcode{trace\_workqueue\_activate\_work}}{struct work\_struct *\emph{ work}}{}
called when a work gets activated

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct work\_struct * work}}] \leavevmode
pointer to struct work\_struct

\end{description}

\textbf{Description}

This event occurs when a queued work is put on the active queue,
which happens immediately after queueing unless \textbf{max\_active} limit
is reached.
\index{trace\_workqueue\_execute\_start (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/tracepoint:c.trace_workqueue_execute_start}\pysiglinewithargsret{void \bfcode{trace\_workqueue\_execute\_start}}{struct work\_struct *\emph{ work}}{}
called immediately before the workqueue callback

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct work\_struct * work}}] \leavevmode
pointer to struct work\_struct

\end{description}

\textbf{Description}

Allows to track workqueue execution.
\index{trace\_workqueue\_execute\_end (C function)}

\begin{fulllineitems}
\phantomsection\label{core-api/tracepoint:c.trace_workqueue_execute_end}\pysiglinewithargsret{void \bfcode{trace\_workqueue\_execute\_end}}{struct work\_struct *\emph{ work}}{}
called immediately after the workqueue callback

\end{fulllineitems}


\textbf{Parameters}
\begin{description}
\item[{\code{struct work\_struct * work}}] \leavevmode
pointer to struct work\_struct

\end{description}

\textbf{Description}

Allows to track workqueue execution.



\renewcommand{\indexname}{Index}
\printindex
\end{document}
