% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[a4paper,8pt,english]{sphinxmanual}


\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsfonts}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{longtable}
\usepackage{sphinx}
\usepackage{multirow}
\usepackage{eqparbox}


\addto\captionsenglish{\renewcommand{\figurename}{Fig. }}
\addto\captionsenglish{\renewcommand{\tablename}{Table }}
\SetupFloatingEnvironment{literal-block}{name=Listing }


	% Use some font with UTF-8 support with XeLaTeX
        \usepackage{fontspec}
        \setsansfont{DejaVu Serif}
        \setromanfont{DejaVu Sans}
        \setmonofont{DejaVu Sans Mono}

     \usepackage[margin=0.5in, top=1in, bottom=1in]{geometry}
        \usepackage{ifthen}

        % Put notes in color and let them be inside a table
	\definecolor{NoteColor}{RGB}{204,255,255}
	\definecolor{WarningColor}{RGB}{255,204,204}
	\definecolor{AttentionColor}{RGB}{255,255,204}
	\definecolor{ImportantColor}{RGB}{192,255,204}
	\definecolor{OtherColor}{RGB}{204,204,204}
        \newlength{\mynoticelength}
        \makeatletter\newenvironment{coloredbox}[1]{%
	   \setlength{\fboxrule}{1pt}
	   \setlength{\fboxsep}{7pt}
	   \setlength{\mynoticelength}{\linewidth}
	   \addtolength{\mynoticelength}{-2\fboxsep}
	   \addtolength{\mynoticelength}{-2\fboxrule}
           \begin{lrbox}{\@tempboxa}\begin{minipage}{\mynoticelength}}{\end{minipage}\end{lrbox}%
	   \ifthenelse%
	      {\equal{\py@noticetype}{note}}%
	      {\colorbox{NoteColor}{\usebox{\@tempboxa}}}%
	      {%
	         \ifthenelse%
	         {\equal{\py@noticetype}{warning}}%
	         {\colorbox{WarningColor}{\usebox{\@tempboxa}}}%
		 {%
	            \ifthenelse%
	            {\equal{\py@noticetype}{attention}}%
	            {\colorbox{AttentionColor}{\usebox{\@tempboxa}}}%
		    {%
	               \ifthenelse%
	               {\equal{\py@noticetype}{important}}%
	               {\colorbox{ImportantColor}{\usebox{\@tempboxa}}}%
	               {\colorbox{OtherColor}{\usebox{\@tempboxa}}}%
		    }%
		 }%
	      }%
        }\makeatother

        \makeatletter
        \renewenvironment{notice}[2]{%
          \def\py@noticetype{#1}
          \begin{coloredbox}{#1}
          \bf\it
          \par\strong{#2}
          \csname py@noticestart@#1\endcsname
        }
	{
          \csname py@noticeend@\py@noticetype\endcsname
          \end{coloredbox}
        }
	\makeatother

     

\title{Linux Kernel Development Documentation}
\date{March 08, 2018}
\release{4.16.0-rc4+}
\author{The kernel development community}
\newcommand{\sphinxlogo}{}
\renewcommand{\releasename}{Release}
\setcounter{tocdepth}{0}
\makeindex

\makeatletter
\def\PYG@reset{\let\PYG@it=\relax \let\PYG@bf=\relax%
    \let\PYG@ul=\relax \let\PYG@tc=\relax%
    \let\PYG@bc=\relax \let\PYG@ff=\relax}
\def\PYG@tok#1{\csname PYG@tok@#1\endcsname}
\def\PYG@toks#1+{\ifx\relax#1\empty\else%
    \PYG@tok{#1}\expandafter\PYG@toks\fi}
\def\PYG@do#1{\PYG@bc{\PYG@tc{\PYG@ul{%
    \PYG@it{\PYG@bf{\PYG@ff{#1}}}}}}}
\def\PYG#1#2{\PYG@reset\PYG@toks#1+\relax+\PYG@do{#2}}

\expandafter\def\csname PYG@tok@gd\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@gu\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@gt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PYG@tok@gs\endcsname{\let\PYG@bf=\textbf}
\expandafter\def\csname PYG@tok@gr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@cm\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@vg\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@vi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@mh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@cs\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\colorbox[rgb]{1.00,0.94,0.94}{\strut ##1}}}
\expandafter\def\csname PYG@tok@ge\endcsname{\let\PYG@it=\textit}
\expandafter\def\csname PYG@tok@vc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@il\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@go\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.20,0.20,0.20}{##1}}}
\expandafter\def\csname PYG@tok@cp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@gi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PYG@tok@gh\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@ni\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.84,0.33,0.22}{##1}}}
\expandafter\def\csname PYG@tok@nl\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.13,0.44}{##1}}}
\expandafter\def\csname PYG@tok@nn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@no\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.38,0.68,0.84}{##1}}}
\expandafter\def\csname PYG@tok@na\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@nb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@nc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@nd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.33,0.33,0.33}{##1}}}
\expandafter\def\csname PYG@tok@ne\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@nf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.49}{##1}}}
\expandafter\def\csname PYG@tok@si\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.44,0.63,0.82}{##1}}}
\expandafter\def\csname PYG@tok@s2\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@nt\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.45}{##1}}}
\expandafter\def\csname PYG@tok@nv\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@s1\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@ch\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@m\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@gp\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@sh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@ow\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@sx\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@bp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@c1\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@o\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PYG@tok@kc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@c\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@mf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@err\endcsname{\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PYG@tok@mb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@ss\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.32,0.47,0.09}{##1}}}
\expandafter\def\csname PYG@tok@sr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.14,0.33,0.53}{##1}}}
\expandafter\def\csname PYG@tok@mo\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@kd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@mi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@kn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@cpf\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@kr\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@s\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@kp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@w\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PYG@tok@kt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.56,0.13,0.00}{##1}}}
\expandafter\def\csname PYG@tok@sc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@sb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@k\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@se\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@sd\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}

\def\PYGZbs{\char`\\}
\def\PYGZus{\char`\_}
\def\PYGZob{\char`\{}
\def\PYGZcb{\char`\}}
\def\PYGZca{\char`\^}
\def\PYGZam{\char`\&}
\def\PYGZlt{\char`\<}
\def\PYGZgt{\char`\>}
\def\PYGZsh{\char`\#}
\def\PYGZpc{\char`\%}
\def\PYGZdl{\char`\$}
\def\PYGZhy{\char`\-}
\def\PYGZsq{\char`\'}
\def\PYGZdq{\char`\"}
\def\PYGZti{\char`\~}
% for compatibility with earlier versions
\def\PYGZat{@}
\def\PYGZlb{[}
\def\PYGZrb{]}
\makeatother

\renewcommand\PYGZsq{\textquotesingle}

\begin{document}

\maketitle
\tableofcontents
\phantomsection\label{process/index::doc}\renewcommand\thesection*
\renewcommand\thesubsection*


So you want to be a Linux kernel developer?  Welcome!  While there is a lot
to be learned about the kernel in a technical sense, it is also important
to learn about how our community works.  Reading these documents will make
it much easier for you to get your changes merged with a minimum of
trouble.

Below are the essential guides that every developer should read.


\chapter{HOWTO do Linux kernel development}
\label{process/howto:process-index}\label{process/howto:working-with-the-kernel-development-community}\label{process/howto::doc}\label{process/howto:howto-do-linux-kernel-development}
This is the be-all, end-all document on this topic.  It contains
instructions on how to become a Linux kernel developer and how to learn
to work with the Linux kernel development community.  It tries to not
contain anything related to the technical aspects of kernel programming,
but will help point you in the right direction for that.

If anything in this document becomes out of date, please send in patches
to the maintainer of this file, who is listed at the bottom of the
document.


\section{Introduction}
\label{process/howto:introduction}
So, you want to learn how to become a Linux kernel developer?  Or you
have been told by your manager, ``Go write a Linux driver for this
device.''  This document's goal is to teach you everything you need to
know to achieve this by describing the process you need to go through,
and hints on how to work with the community.  It will also try to
explain some of the reasons why the community works like it does.

The kernel is written mostly in C, with some architecture-dependent
parts written in assembly. A good understanding of C is required for
kernel development.  Assembly (any architecture) is not required unless
you plan to do low-level development for that architecture.  Though they
are not a good substitute for a solid C education and/or years of
experience, the following books are good for, if anything, reference:
\begin{itemize}
\item {} 
``The C Programming Language'' by Kernighan and Ritchie {[}Prentice Hall{]}

\item {} 
``Practical C Programming'' by Steve Oualline {[}O'Reilly{]}

\item {} 
``C:  A Reference Manual'' by Harbison and Steele {[}Prentice Hall{]}

\end{itemize}

The kernel is written using GNU C and the GNU toolchain.  While it
adheres to the ISO C89 standard, it uses a number of extensions that are
not featured in the standard.  The kernel is a freestanding C
environment, with no reliance on the standard C library, so some
portions of the C standard are not supported.  Arbitrary long long
divisions and floating point are not allowed.  It can sometimes be
difficult to understand the assumptions the kernel has on the toolchain
and the extensions that it uses, and unfortunately there is no
definitive reference for them.  Please check the gcc info pages (\emph{info
gcc}) for some information on them.

Please remember that you are trying to learn how to work with the
existing development community.  It is a diverse group of people, with
high standards for coding, style and procedure.  These standards have
been created over time based on what they have found to work best for
such a large and geographically dispersed team.  Try to learn as much as
possible about these standards ahead of time, as they are well
documented; do not expect people to adapt to you or your company's way
of doing things.


\section{Legal Issues}
\label{process/howto:legal-issues}
The Linux kernel source code is released under the GPL.  Please see the
file, COPYING, in the main directory of the source tree, for details on
the license.  If you have further questions about the license, please
contact a lawyer, and do not ask on the Linux kernel mailing list.  The
people on the mailing lists are not lawyers, and you should not rely on
their statements on legal matters.

For common questions and answers about the GPL, please see:
\begin{quote}

\href{https://www.gnu.org/licenses/gpl-faq.html}{https://www.gnu.org/licenses/gpl-faq.html}
\end{quote}


\section{Documentation}
\label{process/howto:documentation}
The Linux kernel source tree has a large range of documents that are
invaluable for learning how to interact with the kernel community.  When
new features are added to the kernel, it is recommended that new
documentation files are also added which explain how to use the feature.
When a kernel change causes the interface that the kernel exposes to
userspace to change, it is recommended that you send the information or
a patch to the manual pages explaining the change to the manual pages
maintainer at \href{mailto:mtk.manpages@gmail.com}{mtk.manpages@gmail.com}, and CC the list
\href{mailto:linux-api@vger.kernel.org}{linux-api@vger.kernel.org}.

Here is a list of files that are in the kernel source tree that are
required reading:
\begin{quote}
\begin{description}
\item[{README}] \leavevmode
This file gives a short background on the Linux kernel and describes
what is necessary to do to configure and build the kernel.  People
who are new to the kernel should start here.

\item[{{\hyperref[process/changes:changes]{\emph{Documentation/process/changes.rst}}}}] \leavevmode
This file gives a list of the minimum levels of various software
packages that are necessary to build and run the kernel
successfully.

\item[{{\hyperref[process/coding\string-style:codingstyle]{\emph{Documentation/process/coding-style.rst}}}}] \leavevmode
This describes the Linux kernel coding style, and some of the
rationale behind it. All new code is expected to follow the
guidelines in this document. Most maintainers will only accept
patches if these rules are followed, and many people will only
review code if it is in the proper style.

\item[{{\hyperref[process/submitting\string-patches:submittingpatches]{\emph{Documentation/process/submitting-patches.rst}}} and {\hyperref[process/submitting\string-drivers:submittingdrivers]{\emph{Documentation/process/submitting-drivers.rst}}}}] \leavevmode
These files describe in explicit detail how to successfully create
and send a patch, including (but not limited to):
\begin{itemize}
\item {} 
Email contents

\item {} 
Email format

\item {} 
Who to send it to

\end{itemize}

Following these rules will not guarantee success (as all patches are
subject to scrutiny for content and style), but not following them
will almost always prevent it.

Other excellent descriptions of how to create patches properly are:
\begin{quote}
\begin{description}
\item[{``The Perfect Patch''}] \leavevmode
\href{https://www.ozlabs.org/~akpm/stuff/tpp.txt}{https://www.ozlabs.org/\textasciitilde{}akpm/stuff/tpp.txt}

\item[{``Linux kernel patch submission format''}] \leavevmode
\href{http://linux.yyz.us/patch-format.html}{http://linux.yyz.us/patch-format.html}

\end{description}
\end{quote}

\item[{{\hyperref[process/stable\string-api\string-nonsense:stable\string-api\string-nonsense]{\emph{Documentation/process/stable-api-nonsense.rst}}}}] \leavevmode
This file describes the rationale behind the conscious decision to
not have a stable API within the kernel, including things like:
\begin{itemize}
\item {} 
Subsystem shim-layers (for compatibility?)

\item {} 
Driver portability between Operating Systems.

\item {} 
Mitigating rapid change within the kernel source tree (or
preventing rapid change)

\end{itemize}

This document is crucial for understanding the Linux development
philosophy and is very important for people moving to Linux from
development on other Operating Systems.

\item[{\DUspan{xref,std,std-ref}{Documentation/admin-guide/security-bugs.rst}}] \leavevmode
If you feel you have found a security problem in the Linux kernel,
please follow the steps in this document to help notify the kernel
developers, and help solve the issue.

\item[{{\hyperref[process/management\string-style:managementstyle]{\emph{Documentation/process/management-style.rst}}}}] \leavevmode
This document describes how Linux kernel maintainers operate and the
shared ethos behind their methodologies.  This is important reading
for anyone new to kernel development (or anyone simply curious about
it), as it resolves a lot of common misconceptions and confusion
about the unique behavior of kernel maintainers.

\item[{{\hyperref[process/stable\string-kernel\string-rules:stable\string-kernel\string-rules]{\emph{Documentation/process/stable-kernel-rules.rst}}}}] \leavevmode
This file describes the rules on how the stable kernel releases
happen, and what to do if you want to get a change into one of these
releases.

\item[{{\hyperref[process/kernel\string-docs:kernel\string-docs]{\emph{Documentation/process/kernel-docs.rst}}}}] \leavevmode
A list of external documentation that pertains to kernel
development.  Please consult this list if you do not find what you
are looking for within the in-kernel documentation.

\item[{{\hyperref[process/applying\string-patches:applying\string-patches]{\emph{Documentation/process/applying-patches.rst}}}}] \leavevmode
A good introduction describing exactly what a patch is and how to
apply it to the different development branches of the kernel.

\end{description}
\end{quote}

The kernel also has a large number of documents that can be
automatically generated from the source code itself or from
ReStructuredText markups (ReST), like this one. This includes a
full description of the in-kernel API, and rules on how to handle
locking properly.

All such documents can be generated as PDF or HTML by running:

\begin{Verbatim}[commandchars=\\\{\}]
make pdfdocs
make htmldocs
\end{Verbatim}

respectively from the main kernel source directory.

The documents that uses ReST markup will be generated at Documentation/output.
They can also be generated on LaTeX and ePub formats with:

\begin{Verbatim}[commandchars=\\\{\}]
make latexdocs
make epubdocs
\end{Verbatim}


\section{Becoming A Kernel Developer}
\label{process/howto:becoming-a-kernel-developer}
If you do not know anything about Linux kernel development, you should
look at the Linux KernelNewbies project:
\begin{quote}

\href{https://kernelnewbies.org}{https://kernelnewbies.org}
\end{quote}

It consists of a helpful mailing list where you can ask almost any type
of basic kernel development question (make sure to search the archives
first, before asking something that has already been answered in the
past.)  It also has an IRC channel that you can use to ask questions in
real-time, and a lot of helpful documentation that is useful for
learning about Linux kernel development.

The website has basic information about code organization, subsystems,
and current projects (both in-tree and out-of-tree). It also describes
some basic logistical information, like how to compile a kernel and
apply a patch.

If you do not know where you want to start, but you want to look for
some task to start doing to join into the kernel development community,
go to the Linux Kernel Janitor's project:
\begin{quote}

\href{https://kernelnewbies.org/KernelJanitors}{https://kernelnewbies.org/KernelJanitors}
\end{quote}

It is a great place to start.  It describes a list of relatively simple
problems that need to be cleaned up and fixed within the Linux kernel
source tree.  Working with the developers in charge of this project, you
will learn the basics of getting your patch into the Linux kernel tree,
and possibly be pointed in the direction of what to go work on next, if
you do not already have an idea.

If you already have a chunk of code that you want to put into the kernel
tree, but need some help getting it in the proper form, the
kernel-mentors project was created to help you out with this.  It is a
mailing list, and can be found at:
\begin{quote}

\href{https://selenic.com/mailman/listinfo/kernel-mentors}{https://selenic.com/mailman/listinfo/kernel-mentors}
\end{quote}

Before making any actual modifications to the Linux kernel code, it is
imperative to understand how the code in question works.  For this
purpose, nothing is better than reading through it directly (most tricky
bits are commented well), perhaps even with the help of specialized
tools.  One such tool that is particularly recommended is the Linux
Cross-Reference project, which is able to present source code in a
self-referential, indexed webpage format. An excellent up-to-date
repository of the kernel code may be found at:
\begin{quote}

\href{http://lxr.free-electrons.com/}{http://lxr.free-electrons.com/}
\end{quote}


\section{The development process}
\label{process/howto:the-development-process}
Linux kernel development process currently consists of a few different
main kernel ``branches'' and lots of different subsystem-specific kernel
branches.  These different branches are:
\begin{itemize}
\item {} 
main 4.x kernel tree

\item {} 
4.x.y -stable kernel tree

\item {} 
4.x -git kernel patches

\item {} 
subsystem specific kernel trees and patches

\item {} 
the 4.x -next kernel tree for integration tests

\end{itemize}


\subsection{4.x kernel tree}
\label{process/howto:x-kernel-tree}
4.x kernels are maintained by Linus Torvalds, and can be found on
\href{https://kernel.org}{https://kernel.org} in the pub/linux/kernel/v4.x/ directory.  Its development
process is as follows:
\begin{itemize}
\item {} 
As soon as a new kernel is released a two weeks window is open,
during this period of time maintainers can submit big diffs to
Linus, usually the patches that have already been included in the
-next kernel for a few weeks.  The preferred way to submit big changes
is using git (the kernel's source management tool, more information
can be found at \href{https://git-scm.com/}{https://git-scm.com/}) but plain patches are also just
fine.

\item {} 
After two weeks a -rc1 kernel is released and the focus is on making the
new kernel as rock solid as possible.  Most of the patches at this point
should fix a regression.  Bugs that have always existed are not
regressions, so only push these kinds of fixes if they are important.
Please note that a whole new driver (or filesystem) might be accepted
after -rc1 because there is no risk of causing regressions with such a
change as long as the change is self-contained and does not affect areas
outside of the code that is being added.  git can be used to send
patches to Linus after -rc1 is released, but the patches need to also be
sent to a public mailing list for review.

\item {} 
A new -rc is released whenever Linus deems the current git tree to
be in a reasonably sane state adequate for testing.  The goal is to
release a new -rc kernel every week.

\item {} 
Process continues until the kernel is considered ``ready'', the
process should last around 6 weeks.

\end{itemize}

It is worth mentioning what Andrew Morton wrote on the linux-kernel
mailing list about kernel releases:
\begin{quote}

\emph{``Nobody knows when a kernel will be released, because it's
released according to perceived bug status, not according to a
preconceived timeline.''}
\end{quote}


\subsection{4.x.y -stable kernel tree}
\label{process/howto:x-y-stable-kernel-tree}
Kernels with 3-part versions are -stable kernels. They contain
relatively small and critical fixes for security problems or significant
regressions discovered in a given 4.x kernel.

This is the recommended branch for users who want the most recent stable
kernel and are not interested in helping test development/experimental
versions.

If no 4.x.y kernel is available, then the highest numbered 4.x
kernel is the current stable kernel.

4.x.y are maintained by the ``stable'' team \textless{}\href{mailto:stable@vger.kernel.org}{stable@vger.kernel.org}\textgreater{}, and
are released as needs dictate.  The normal release period is approximately
two weeks, but it can be longer if there are no pressing problems.  A
security-related problem, instead, can cause a release to happen almost
instantly.

The file Documentation/process/stable-kernel-rules.rst in the kernel tree
documents what kinds of changes are acceptable for the -stable tree, and
how the release process works.


\subsection{4.x -git patches}
\label{process/howto:x-git-patches}
These are daily snapshots of Linus' kernel tree which are managed in a
git repository (hence the name.) These patches are usually released
daily and represent the current state of Linus' tree.  They are more
experimental than -rc kernels since they are generated automatically
without even a cursory glance to see if they are sane.


\subsection{Subsystem Specific kernel trees and patches}
\label{process/howto:subsystem-specific-kernel-trees-and-patches}
The maintainers of the various kernel subsystems --- and also many
kernel subsystem developers --- expose their current state of
development in source repositories.  That way, others can see what is
happening in the different areas of the kernel.  In areas where
development is rapid, a developer may be asked to base his submissions
onto such a subsystem kernel tree so that conflicts between the
submission and other already ongoing work are avoided.

Most of these repositories are git trees, but there are also other SCMs
in use, or patch queues being published as quilt series.  Addresses of
these subsystem repositories are listed in the MAINTAINERS file.  Many
of them can be browsed at \href{https://git.kernel.org/}{https://git.kernel.org/}.

Before a proposed patch is committed to such a subsystem tree, it is
subject to review which primarily happens on mailing lists (see the
respective section below).  For several kernel subsystems, this review
process is tracked with the tool patchwork.  Patchwork offers a web
interface which shows patch postings, any comments on a patch or
revisions to it, and maintainers can mark patches as under review,
accepted, or rejected.  Most of these patchwork sites are listed at
\href{https://patchwork.kernel.org/}{https://patchwork.kernel.org/}.


\subsection{4.x -next kernel tree for integration tests}
\label{process/howto:x-next-kernel-tree-for-integration-tests}
Before updates from subsystem trees are merged into the mainline 4.x
tree, they need to be integration-tested.  For this purpose, a special
testing repository exists into which virtually all subsystem trees are
pulled on an almost daily basis:
\begin{quote}

\href{https://git.kernel.org/?p=linux/kernel/git/next/linux-next.git}{https://git.kernel.org/?p=linux/kernel/git/next/linux-next.git}
\end{quote}

This way, the -next kernel gives a summary outlook onto what will be
expected to go into the mainline kernel at the next merge period.
Adventurous testers are very welcome to runtime-test the -next kernel.


\section{Bug Reporting}
\label{process/howto:bug-reporting}
\href{https://bugzilla.kernel.org}{https://bugzilla.kernel.org} is where the Linux kernel developers track kernel
bugs.  Users are encouraged to report all bugs that they find in this
tool.  For details on how to use the kernel bugzilla, please see:
\begin{quote}

\href{https://bugzilla.kernel.org/page.cgi?id=faq.html}{https://bugzilla.kernel.org/page.cgi?id=faq.html}
\end{quote}

The file admin-guide/reporting-bugs.rst in the main kernel source directory has a good
template for how to report a possible kernel bug, and details what kind
of information is needed by the kernel developers to help track down the
problem.


\section{Managing bug reports}
\label{process/howto:managing-bug-reports}
One of the best ways to put into practice your hacking skills is by fixing
bugs reported by other people. Not only you will help to make the kernel
more stable, you'll learn to fix real world problems and you will improve
your skills, and other developers will be aware of your presence. Fixing
bugs is one of the best ways to get merits among other developers, because
not many people like wasting time fixing other people's bugs.

To work in the already reported bug reports, go to \href{https://bugzilla.kernel.org}{https://bugzilla.kernel.org}.
If you want to be advised of the future bug reports, you can subscribe to the
bugme-new mailing list (only new bug reports are mailed here) or to the
bugme-janitor mailing list (every change in the bugzilla is mailed here)
\begin{quote}

\href{https://lists.linux-foundation.org/mailman/listinfo/bugme-new}{https://lists.linux-foundation.org/mailman/listinfo/bugme-new}

\href{https://lists.linux-foundation.org/mailman/listinfo/bugme-janitors}{https://lists.linux-foundation.org/mailman/listinfo/bugme-janitors}
\end{quote}


\section{Mailing lists}
\label{process/howto:mailing-lists}
As some of the above documents describe, the majority of the core kernel
developers participate on the Linux Kernel Mailing list.  Details on how
to subscribe and unsubscribe from the list can be found at:
\begin{quote}

\href{http://vger.kernel.org/vger-lists.html\#linux-kernel}{http://vger.kernel.org/vger-lists.html\#linux-kernel}
\end{quote}

There are archives of the mailing list on the web in many different
places.  Use a search engine to find these archives.  For example:
\begin{quote}

\href{http://dir.gmane.org/gmane.linux.kernel}{http://dir.gmane.org/gmane.linux.kernel}
\end{quote}

It is highly recommended that you search the archives about the topic
you want to bring up, before you post it to the list. A lot of things
already discussed in detail are only recorded at the mailing list
archives.

Most of the individual kernel subsystems also have their own separate
mailing list where they do their development efforts.  See the
MAINTAINERS file for a list of what these lists are for the different
groups.

Many of the lists are hosted on kernel.org. Information on them can be
found at:
\begin{quote}

\href{http://vger.kernel.org/vger-lists.html}{http://vger.kernel.org/vger-lists.html}
\end{quote}

Please remember to follow good behavioral habits when using the lists.
Though a bit cheesy, the following URL has some simple guidelines for
interacting with the list (or any list):
\begin{quote}

\href{http://www.albion.com/netiquette/}{http://www.albion.com/netiquette/}
\end{quote}

If multiple people respond to your mail, the CC: list of recipients may
get pretty large. Don't remove anybody from the CC: list without a good
reason, or don't reply only to the list address. Get used to receiving the
mail twice, one from the sender and the one from the list, and don't try
to tune that by adding fancy mail-headers, people will not like it.

Remember to keep the context and the attribution of your replies intact,
keep the ``John Kernelhacker wrote ...:'' lines at the top of your reply, and
add your statements between the individual quoted sections instead of
writing at the top of the mail.

If you add patches to your mail, make sure they are plain readable text
as stated in Documentation/process/submitting-patches.rst.
Kernel developers don't want to deal with
attachments or compressed patches; they may want to comment on
individual lines of your patch, which works only that way. Make sure you
use a mail program that does not mangle spaces and tab characters. A
good first test is to send the mail to yourself and try to apply your
own patch by yourself. If that doesn't work, get your mail program fixed
or change it until it works.

Above all, please remember to show respect to other subscribers.


\section{Working with the community}
\label{process/howto:working-with-the-community}
The goal of the kernel community is to provide the best possible kernel
there is.  When you submit a patch for acceptance, it will be reviewed
on its technical merits and those alone.  So, what should you be
expecting?
\begin{itemize}
\item {} 
criticism

\item {} 
comments

\item {} 
requests for change

\item {} 
requests for justification

\item {} 
silence

\end{itemize}

Remember, this is part of getting your patch into the kernel.  You have
to be able to take criticism and comments about your patches, evaluate
them at a technical level and either rework your patches or provide
clear and concise reasoning as to why those changes should not be made.
If there are no responses to your posting, wait a few days and try
again, sometimes things get lost in the huge volume.

What should you not do?
\begin{itemize}
\item {} 
expect your patch to be accepted without question

\item {} 
become defensive

\item {} 
ignore comments

\item {} 
resubmit the patch without making any of the requested changes

\end{itemize}

In a community that is looking for the best technical solution possible,
there will always be differing opinions on how beneficial a patch is.
You have to be cooperative, and willing to adapt your idea to fit within
the kernel.  Or at least be willing to prove your idea is worth it.
Remember, being wrong is acceptable as long as you are willing to work
toward a solution that is right.

It is normal that the answers to your first patch might simply be a list
of a dozen things you should correct.  This does \textbf{not} imply that your
patch will not be accepted, and it is \textbf{not} meant against you
personally.  Simply correct all issues raised against your patch and
resend it.


\section{Differences between the kernel community and corporate structures}
\label{process/howto:differences-between-the-kernel-community-and-corporate-structures}
The kernel community works differently than most traditional corporate
development environments.  Here are a list of things that you can try to
do to avoid problems:
\begin{quote}

Good things to say regarding your proposed changes:
\begin{itemize}
\item {} 
``This solves multiple problems.''

\item {} 
``This deletes 2000 lines of code.''

\item {} 
``Here is a patch that explains what I am trying to describe.''

\item {} 
``I tested it on 5 different architectures...''

\item {} 
``Here is a series of small patches that...''

\item {} 
``This increases performance on typical machines...''

\end{itemize}

Bad things you should avoid saying:
\begin{itemize}
\item {} 
``We did it this way in AIX/ptx/Solaris, so therefore it must be
good...''

\item {} 
``I've being doing this for 20 years, so...''

\item {} 
``This is required for my company to make money''

\item {} 
``This is for our Enterprise product line.''

\item {} 
``Here is my 1000 page design document that describes my idea''

\item {} 
``I've been working on this for 6 months...''

\item {} 
``Here's a 5000 line patch that...''

\item {} 
``I rewrote all of the current mess, and here it is...''

\item {} 
``I have a deadline, and this patch needs to be applied now.''

\end{itemize}
\end{quote}

Another way the kernel community is different than most traditional
software engineering work environments is the faceless nature of
interaction.  One benefit of using email and irc as the primary forms of
communication is the lack of discrimination based on gender or race.
The Linux kernel work environment is accepting of women and minorities
because all you are is an email address.  The international aspect also
helps to level the playing field because you can't guess gender based on
a person's name. A man may be named Andrea and a woman may be named Pat.
Most women who have worked in the Linux kernel and have expressed an
opinion have had positive experiences.

The language barrier can cause problems for some people who are not
comfortable with English.  A good grasp of the language can be needed in
order to get ideas across properly on mailing lists, so it is
recommended that you check your emails to make sure they make sense in
English before sending them.


\section{Break up your changes}
\label{process/howto:break-up-your-changes}
The Linux kernel community does not gladly accept large chunks of code
dropped on it all at once.  The changes need to be properly introduced,
discussed, and broken up into tiny, individual portions.  This is almost
the exact opposite of what companies are used to doing.  Your proposal
should also be introduced very early in the development process, so that
you can receive feedback on what you are doing.  It also lets the
community feel that you are working with them, and not simply using them
as a dumping ground for your feature.  However, don't send 50 emails at
one time to a mailing list, your patch series should be smaller than
that almost all of the time.

The reasons for breaking things up are the following:
\begin{enumerate}
\item {} 
Small patches increase the likelihood that your patches will be
applied, since they don't take much time or effort to verify for
correctness.  A 5 line patch can be applied by a maintainer with
barely a second glance. However, a 500 line patch may take hours to
review for correctness (the time it takes is exponentially
proportional to the size of the patch, or something).

Small patches also make it very easy to debug when something goes
wrong.  It's much easier to back out patches one by one than it is
to dissect a very large patch after it's been applied (and broken
something).

\item {} 
It's important not only to send small patches, but also to rewrite
and simplify (or simply re-order) patches before submitting them.

\end{enumerate}

Here is an analogy from kernel developer Al Viro:
\begin{quote}

\emph{``Think of a teacher grading homework from a math student.  The
teacher does not want to see the student's trials and errors
before they came up with the solution. They want to see the
cleanest, most elegant answer.  A good student knows this, and
would never submit her intermediate work before the final
solution.}

\emph{The same is true of kernel development. The maintainers and
reviewers do not want to see the thought process behind the
solution to the problem one is solving. They want to see a
simple and elegant solution.''}
\end{quote}

It may be challenging to keep the balance between presenting an elegant
solution and working together with the community and discussing your
unfinished work. Therefore it is good to get early in the process to
get feedback to improve your work, but also keep your changes in small
chunks that they may get already accepted, even when your whole task is
not ready for inclusion now.

Also realize that it is not acceptable to send patches for inclusion
that are unfinished and will be ``fixed up later.''


\section{Justify your change}
\label{process/howto:justify-your-change}
Along with breaking up your patches, it is very important for you to let
the Linux community know why they should add this change.  New features
must be justified as being needed and useful.


\section{Document your change}
\label{process/howto:document-your-change}
When sending in your patches, pay special attention to what you say in
the text in your email.  This information will become the ChangeLog
information for the patch, and will be preserved for everyone to see for
all time.  It should describe the patch completely, containing:
\begin{itemize}
\item {} 
why the change is necessary

\item {} 
the overall design approach in the patch

\item {} 
implementation details

\item {} 
testing results

\end{itemize}

For more details on what this should all look like, please see the
ChangeLog section of the document:
\begin{quote}
\begin{description}
\item[{``The Perfect Patch''}] \leavevmode
\href{http://www.ozlabs.org/~akpm/stuff/tpp.txt}{http://www.ozlabs.org/\textasciitilde{}akpm/stuff/tpp.txt}

\end{description}
\end{quote}

All of these things are sometimes very hard to do. It can take years to
perfect these practices (if at all). It's a continuous process of
improvement that requires a lot of patience and determination. But
don't give up, it's possible. Many have done it before, and each had to
start exactly where you are now.


\bigskip\hrule{}\bigskip


Thanks to Paolo Ciarrocchi who allowed the ``Development Process''
(\href{https://lwn.net/Articles/94386/}{https://lwn.net/Articles/94386/}) section
to be based on text he had written, and to Randy Dunlap and Gerrit
Huizenga for some of the list of things you should and should not say.
Also thanks to Pat Mochel, Hanna Linder, Randy Dunlap, Kay Sievers,
Vojtech Pavlik, Jan Kara, Josh Boyer, Kees Cook, Andrew Morton, Andi
Kleen, Vadim Lobanov, Jesper Juhl, Adrian Bunk, Keri Harris, Frans Pop,
David A. Wheeler, Junio Hamano, Michael Kerrisk, and Alex Shepard for
their review, comments, and contributions.  Without their help, this
document would not have been possible.

Maintainer: Greg Kroah-Hartman \textless{}\href{mailto:greg@kroah.com}{greg@kroah.com}\textgreater{}


\chapter{Code of Conflict}
\label{process/code-of-conflict:code-of-conflict}\label{process/code-of-conflict::doc}
The Linux kernel development effort is a very personal process compared
to ``traditional'' ways of developing software.  Your code and ideas
behind it will be carefully reviewed, often resulting in critique and
criticism.  The review will almost always require improvements to the
code before it can be included in the kernel.  Know that this happens
because everyone involved wants to see the best possible solution for
the overall success of Linux.  This development process has been proven
to create the most robust operating system kernel ever, and we do not
want to do anything to cause the quality of submission and eventual
result to ever decrease.

If however, anyone feels personally abused, threatened, or otherwise
uncomfortable due to this process, that is not acceptable.  If so,
please contact the Linux Foundation's Technical Advisory Board at
\textless{}\href{mailto:tab@lists.linux-foundation.org}{tab@lists.linux-foundation.org}\textgreater{}, or the individual members, and they
will work to resolve the issue to the best of their ability.  For more
information on who is on the Technical Advisory Board and what their
role is, please see:
\begin{itemize}
\item {} 
\href{http://www.linuxfoundation.org/projects/linux/tab}{http://www.linuxfoundation.org/projects/linux/tab}

\end{itemize}

As a reviewer of code, please strive to keep things civil and focused on
the technical issues involved.  We are all humans, and frustrations can
be high on both sides of the process.  Try to keep in mind the immortal
words of Bill and Ted, ``Be excellent to each other.''


\chapter{A guide to the Kernel Development Process}
\label{process/development-process:development-process-main}\label{process/development-process::doc}\label{process/development-process:a-guide-to-the-kernel-development-process}
Contents:


\section{Introduction}
\label{process/1.Intro:introduction}\label{process/1.Intro::doc}

\subsection{Executive summary}
\label{process/1.Intro:executive-summary}
The rest of this section covers the scope of the kernel development process
and the kinds of frustrations that developers and their employers can
encounter there.  There are a great many reasons why kernel code should be
merged into the official (``mainline'') kernel, including automatic
availability to users, community support in many forms, and the ability to
influence the direction of kernel development.  Code contributed to the
Linux kernel must be made available under a GPL-compatible license.

{\hyperref[process/2.Process:development\string-process]{\emph{How the development process works}}} introduces the development process, the kernel
release cycle, and the mechanics of the merge window.  The various phases in
the patch development, review, and merging cycle are covered.  There is some
discussion of tools and mailing lists.  Developers wanting to get started
with kernel development are encouraged to track down and fix bugs as an
initial exercise.

{\hyperref[process/3.Early\string-stage:development\string-early\string-stage]{\emph{Early-stage planning}}} covers early-stage project planning, with an
emphasis on involving the development community as soon as possible.

{\hyperref[process/4.Coding:development\string-coding]{\emph{Getting the code right}}} is about the coding process; several pitfalls which
have been encountered by other developers are discussed.  Some requirements for
patches are covered, and there is an introduction to some of the tools
which can help to ensure that kernel patches are correct.

{\hyperref[process/5.Posting:development\string-posting]{\emph{Posting patches}}} talks about the process of posting patches for
review. To be taken seriously by the development community, patches must be
properly formatted and described, and they must be sent to the right place.
Following the advice in this section should help to ensure the best
possible reception for your work.

{\hyperref[process/6.Followthrough:development\string-followthrough]{\emph{Followthrough}}} covers what happens after posting patches; the
job is far from done at that point.  Working with reviewers is a crucial part
of the development process; this section offers a number of tips on how to
avoid problems at this important stage.  Developers are cautioned against
assuming that the job is done when a patch is merged into the mainline.

{\hyperref[process/7.AdvancedTopics:development\string-advancedtopics]{\emph{Advanced topics}}} introduces a couple of ``advanced'' topics:
managing patches with git and reviewing patches posted by others.

{\hyperref[process/8.Conclusion:development\string-conclusion]{\emph{For more information}}} concludes the document with pointers to sources
for more information on kernel development.


\subsection{What this document is about}
\label{process/1.Intro:what-this-document-is-about}
The Linux kernel, at over 8 million lines of code and well over 1000
contributors to each release, is one of the largest and most active free
software projects in existence.  Since its humble beginning in 1991, this
kernel has evolved into a best-of-breed operating system component which
runs on pocket-sized digital music players, desktop PCs, the largest
supercomputers in existence, and all types of systems in between.  It is a
robust, efficient, and scalable solution for almost any situation.

With the growth of Linux has come an increase in the number of developers
(and companies) wishing to participate in its development.  Hardware
vendors want to ensure that Linux supports their products well, making
those products attractive to Linux users.  Embedded systems vendors, who
use Linux as a component in an integrated product, want Linux to be as
capable and well-suited to the task at hand as possible.  Distributors and
other software vendors who base their products on Linux have a clear
interest in the capabilities, performance, and reliability of the Linux
kernel.  And end users, too, will often wish to change Linux to make it
better suit their needs.

One of the most compelling features of Linux is that it is accessible to
these developers; anybody with the requisite skills can improve Linux and
influence the direction of its development.  Proprietary products cannot
offer this kind of openness, which is a characteristic of the free software
process.  But, if anything, the kernel is even more open than most other
free software projects.  A typical three-month kernel development cycle can
involve over 1000 developers working for more than 100 different companies
(or for no company at all).

Working with the kernel development community is not especially hard.  But,
that notwithstanding, many potential contributors have experienced
difficulties when trying to do kernel work.  The kernel community has
evolved its own distinct ways of operating which allow it to function
smoothly (and produce a high-quality product) in an environment where
thousands of lines of code are being changed every day.  So it is not
surprising that Linux kernel development process differs greatly from
proprietary development methods.

The kernel's development process may come across as strange and
intimidating to new developers, but there are good reasons and solid
experience behind it.  A developer who does not understand the kernel
community's ways (or, worse, who tries to flout or circumvent them) will
have a frustrating experience in store.  The development community, while
being helpful to those who are trying to learn, has little time for those
who will not listen or who do not care about the development process.

It is hoped that those who read this document will be able to avoid that
frustrating experience.  There is a lot of material here, but the effort
involved in reading it will be repaid in short order.  The development
community is always in need of developers who will help to make the kernel
better; the following text should help you - or those who work for you -
join our community.


\subsection{Credits}
\label{process/1.Intro:credits}
This document was written by Jonathan Corbet, \href{mailto:corbet@lwn.net}{corbet@lwn.net}.  It has been
improved by comments from Johannes Berg, James Berry, Alex Chiang, Roland
Dreier, Randy Dunlap, Jake Edge, Jiri Kosina, Matt Mackall, Arthur Marsh,
Amanda McPherson, Andrew Morton, Andrew Price, Tsugikazu Shibata, and
Jochen Voß.

This work was supported by the Linux Foundation; thanks especially to
Amanda McPherson, who saw the value of this effort and made it all happen.


\subsection{The importance of getting code into the mainline}
\label{process/1.Intro:the-importance-of-getting-code-into-the-mainline}
Some companies and developers occasionally wonder why they should bother
learning how to work with the kernel community and get their code into the
mainline kernel (the ``mainline'' being the kernel maintained by Linus
Torvalds and used as a base by Linux distributors).  In the short term,
contributing code can look like an avoidable expense; it seems easier to
just keep the code separate and support users directly.  The truth of the
matter is that keeping code separate (``out of tree'') is a false economy.

As a way of illustrating the costs of out-of-tree code, here are a few
relevant aspects of the kernel development process; most of these will be
discussed in greater detail later in this document.  Consider:
\begin{itemize}
\item {} 
Code which has been merged into the mainline kernel is available to all
Linux users.  It will automatically be present on all distributions which
enable it.  There is no need for driver disks, downloads, or the hassles
of supporting multiple versions of multiple distributions; it all just
works, for the developer and for the user.  Incorporation into the
mainline solves a large number of distribution and support problems.

\item {} 
While kernel developers strive to maintain a stable interface to user
space, the internal kernel API is in constant flux.  The lack of a stable
internal interface is a deliberate design decision; it allows fundamental
improvements to be made at any time and results in higher-quality code.
But one result of that policy is that any out-of-tree code requires
constant upkeep if it is to work with new kernels.  Maintaining
out-of-tree code requires significant amounts of work just to keep that
code working.

Code which is in the mainline, instead, does not require this work as the
result of a simple rule requiring any developer who makes an API change
to also fix any code that breaks as the result of that change.  So code
which has been merged into the mainline has significantly lower
maintenance costs.

\item {} 
Beyond that, code which is in the kernel will often be improved by other
developers.  Surprising results can come from empowering your user
community and customers to improve your product.

\item {} 
Kernel code is subjected to review, both before and after merging into
the mainline.  No matter how strong the original developer's skills are,
this review process invariably finds ways in which the code can be
improved.  Often review finds severe bugs and security problems.  This is
especially true for code which has been developed in a closed
environment; such code benefits strongly from review by outside
developers.  Out-of-tree code is lower-quality code.

\item {} 
Participation in the development process is your way to influence the
direction of kernel development.  Users who complain from the sidelines
are heard, but active developers have a stronger voice - and the ability
to implement changes which make the kernel work better for their needs.

\item {} 
When code is maintained separately, the possibility that a third party
will contribute a different implementation of a similar feature always
exists.  Should that happen, getting your code merged will become much
harder - to the point of impossibility.  Then you will be faced with the
unpleasant alternatives of either (1) maintaining a nonstandard feature
out of tree indefinitely, or (2) abandoning your code and migrating your
users over to the in-tree version.

\item {} 
Contribution of code is the fundamental action which makes the whole
process work.  By contributing your code you can add new functionality to
the kernel and provide capabilities and examples which are of use to
other kernel developers.  If you have developed code for Linux (or are
thinking about doing so), you clearly have an interest in the continued
success of this platform; contributing code is one of the best ways to
help ensure that success.

\end{itemize}

All of the reasoning above applies to any out-of-tree kernel code,
including code which is distributed in proprietary, binary-only form.
There are, however, additional factors which should be taken into account
before considering any sort of binary-only kernel code distribution.  These
include:
\begin{itemize}
\item {} 
The legal issues around the distribution of proprietary kernel modules
are cloudy at best; quite a few kernel copyright holders believe that
most binary-only modules are derived products of the kernel and that, as
a result, their distribution is a violation of the GNU General Public
license (about which more will be said below).  Your author is not a
lawyer, and nothing in this document can possibly be considered to be
legal advice.  The true legal status of closed-source modules can only be
determined by the courts.  But the uncertainty which haunts those modules
is there regardless.

\item {} 
Binary modules greatly increase the difficulty of debugging kernel
problems, to the point that most kernel developers will not even try.  So
the distribution of binary-only modules will make it harder for your
users to get support from the community.

\item {} 
Support is also harder for distributors of binary-only modules, who must
provide a version of the module for every distribution and every kernel
version they wish to support.  Dozens of builds of a single module can
be required to provide reasonably comprehensive coverage, and your users
will have to upgrade your module separately every time they upgrade their
kernel.

\item {} 
Everything that was said above about code review applies doubly to
closed-source code.  Since this code is not available at all, it cannot
have been reviewed by the community and will, beyond doubt, have serious
problems.

\end{itemize}

Makers of embedded systems, in particular, may be tempted to disregard much
of what has been said in this section in the belief that they are shipping
a self-contained product which uses a frozen kernel version and requires no
more development after its release.  This argument misses the value of
widespread code review and the value of allowing your users to add
capabilities to your product.  But these products, too, have a limited
commercial life, after which a new version must be released.  At that
point, vendors whose code is in the mainline and well maintained will be
much better positioned to get the new product ready for market quickly.


\subsection{Licensing}
\label{process/1.Intro:licensing}
Code is contributed to the Linux kernel under a number of licenses, but all
code must be compatible with version 2 of the GNU General Public License
(GPLv2), which is the license covering the kernel distribution as a whole.
In practice, that means that all code contributions are covered either by
GPLv2 (with, optionally, language allowing distribution under later
versions of the GPL) or the three-clause BSD license.  Any contributions
which are not covered by a compatible license will not be accepted into the
kernel.

Copyright assignments are not required (or requested) for code contributed
to the kernel.  All code merged into the mainline kernel retains its
original ownership; as a result, the kernel now has thousands of owners.

One implication of this ownership structure is that any attempt to change
the licensing of the kernel is doomed to almost certain failure.  There are
few practical scenarios where the agreement of all copyright holders could
be obtained (or their code removed from the kernel).  So, in particular,
there is no prospect of a migration to version 3 of the GPL in the
foreseeable future.

It is imperative that all code contributed to the kernel be legitimately
free software.  For that reason, code from anonymous (or pseudonymous)
contributors will not be accepted.  All contributors are required to ``sign
off'' on their code, stating that the code can be distributed with the
kernel under the GPL.  Code which has not been licensed as free software by
its owner, or which risks creating copyright-related problems for the
kernel (such as code which derives from reverse-engineering efforts lacking
proper safeguards) cannot be contributed.

Questions about copyright-related issues are common on Linux development
mailing lists.  Such questions will normally receive no shortage of
answers, but one should bear in mind that the people answering those
questions are not lawyers and cannot provide legal advice.  If you have
legal questions relating to Linux source code, there is no substitute for
talking with a lawyer who understands this field.  Relying on answers
obtained on technical mailing lists is a risky affair.


\section{How the development process works}
\label{process/2.Process:development-process}\label{process/2.Process:how-the-development-process-works}\label{process/2.Process::doc}
Linux kernel development in the early 1990's was a pretty loose affair,
with relatively small numbers of users and developers involved.  With a
user base in the millions and with some 2,000 developers involved over the
course of one year, the kernel has since had to evolve a number of
processes to keep development happening smoothly.  A solid understanding of
how the process works is required in order to be an effective part of it.


\subsection{The big picture}
\label{process/2.Process:the-big-picture}
The kernel developers use a loosely time-based release process, with a new
major kernel release happening every two or three months.  The recent
release history looks like this:
\begin{quote}

\begin{tabulary}{\linewidth}{|L|L|}
\hline

2.6.38
 & 
March 14, 2011
\\
\hline
2.6.37
 & 
January 4, 2011
\\
\hline
2.6.36
 & 
October 20, 2010
\\
\hline
2.6.35
 & 
August 1, 2010
\\
\hline
2.6.34
 & 
May 15, 2010
\\
\hline
2.6.33
 & 
February 24, 2010
\\
\hline\end{tabulary}

\end{quote}

Every 2.6.x release is a major kernel release with new features, internal
API changes, and more.  A typical 2.6 release can contain nearly 10,000
changesets with changes to several hundred thousand lines of code.  2.6 is
thus the leading edge of Linux kernel development; the kernel uses a
rolling development model which is continually integrating major changes.

A relatively straightforward discipline is followed with regard to the
merging of patches for each release.  At the beginning of each development
cycle, the ``merge window'' is said to be open.  At that time, code which is
deemed to be sufficiently stable (and which is accepted by the development
community) is merged into the mainline kernel.  The bulk of changes for a
new development cycle (and all of the major changes) will be merged during
this time, at a rate approaching 1,000 changes (``patches,'' or ``changesets'')
per day.

(As an aside, it is worth noting that the changes integrated during the
merge window do not come out of thin air; they have been collected, tested,
and staged ahead of time.  How that process works will be described in
detail later on).

The merge window lasts for approximately two weeks.  At the end of this
time, Linus Torvalds will declare that the window is closed and release the
first of the ``rc'' kernels.  For the kernel which is destined to be 2.6.40,
for example, the release which happens at the end of the merge window will
be called 2.6.40-rc1.  The -rc1 release is the signal that the time to
merge new features has passed, and that the time to stabilize the next
kernel has begun.

Over the next six to ten weeks, only patches which fix problems should be
submitted to the mainline.  On occasion a more significant change will be
allowed, but such occasions are rare; developers who try to merge new
features outside of the merge window tend to get an unfriendly reception.
As a general rule, if you miss the merge window for a given feature, the
best thing to do is to wait for the next development cycle.  (An occasional
exception is made for drivers for previously-unsupported hardware; if they
touch no in-tree code, they cannot cause regressions and should be safe to
add at any time).

As fixes make their way into the mainline, the patch rate will slow over
time.  Linus releases new -rc kernels about once a week; a normal series
will get up to somewhere between -rc6 and -rc9 before the kernel is
considered to be sufficiently stable and the final 2.6.x release is made.
At that point the whole process starts over again.

As an example, here is how the 2.6.38 development cycle went (all dates in
2011):
\begin{quote}

\begin{tabulary}{\linewidth}{|L|L|}
\hline

January 4
 & 
2.6.37 stable release
\\
\hline
January 18
 & 
2.6.38-rc1, merge window closes
\\
\hline
January 21
 & 
2.6.38-rc2
\\
\hline
February 1
 & 
2.6.38-rc3
\\
\hline
February 7
 & 
2.6.38-rc4
\\
\hline
February 15
 & 
2.6.38-rc5
\\
\hline
February 21
 & 
2.6.38-rc6
\\
\hline
March 1
 & 
2.6.38-rc7
\\
\hline
March 7
 & 
2.6.38-rc8
\\
\hline
March 14
 & 
2.6.38 stable release
\\
\hline\end{tabulary}

\end{quote}

How do the developers decide when to close the development cycle and create
the stable release?  The most significant metric used is the list of
regressions from previous releases.  No bugs are welcome, but those which
break systems which worked in the past are considered to be especially
serious.  For this reason, patches which cause regressions are looked upon
unfavorably and are quite likely to be reverted during the stabilization
period.

The developers' goal is to fix all known regressions before the stable
release is made.  In the real world, this kind of perfection is hard to
achieve; there are just too many variables in a project of this size.
There comes a point where delaying the final release just makes the problem
worse; the pile of changes waiting for the next merge window will grow
larger, creating even more regressions the next time around.  So most 2.6.x
kernels go out with a handful of known regressions though, hopefully, none
of them are serious.

Once a stable release is made, its ongoing maintenance is passed off to the
``stable team,'' currently consisting of Greg Kroah-Hartman.  The stable team
will release occasional updates to the stable release using the 2.6.x.y
numbering scheme.  To be considered for an update release, a patch must (1)
fix a significant bug, and (2) already be merged into the mainline for the
next development kernel.  Kernels will typically receive stable updates for
a little more than one development cycle past their initial release.  So,
for example, the 2.6.36 kernel's history looked like:
\begin{quote}

\begin{tabulary}{\linewidth}{|L|L|}
\hline

October 10
 & 
2.6.36 stable release
\\
\hline
November 22
 & 
2.6.36.1
\\
\hline
December 9
 & 
2.6.36.2
\\
\hline
January 7
 & 
2.6.36.3
\\
\hline
February 17
 & 
2.6.36.4
\\
\hline\end{tabulary}

\end{quote}

2.6.36.4 was the final stable update for the 2.6.36 release.

Some kernels are designated ``long term'' kernels; they will receive support
for a longer period.  As of this writing, the current long term kernels
and their maintainers are:
\begin{quote}

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline

2.6.27
 & 
Willy Tarreau
 & 
(Deep-frozen stable kernel)
\\
\hline
2.6.32
 & 
Greg Kroah-Hartman
 & \\
\hline
2.6.35
 & 
Andi Kleen
 & 
(Embedded flag kernel)
\\
\hline\end{tabulary}

\end{quote}

The selection of a kernel for long-term support is purely a matter of a
maintainer having the need and the time to maintain that release.  There
are no known plans for long-term support for any specific upcoming
release.


\subsection{The lifecycle of a patch}
\label{process/2.Process:the-lifecycle-of-a-patch}
Patches do not go directly from the developer's keyboard into the mainline
kernel.  There is, instead, a somewhat involved (if somewhat informal)
process designed to ensure that each patch is reviewed for quality and that
each patch implements a change which is desirable to have in the mainline.
This process can happen quickly for minor fixes, or, in the case of large
and controversial changes, go on for years.  Much developer frustration
comes from a lack of understanding of this process or from attempts to
circumvent it.

In the hopes of reducing that frustration, this document will describe how
a patch gets into the kernel.  What follows below is an introduction which
describes the process in a somewhat idealized way.  A much more detailed
treatment will come in later sections.

The stages that a patch goes through are, generally:
\begin{itemize}
\item {} 
Design.  This is where the real requirements for the patch - and the way
those requirements will be met - are laid out.  Design work is often
done without involving the community, but it is better to do this work
in the open if at all possible; it can save a lot of time redesigning
things later.

\item {} 
Early review.  Patches are posted to the relevant mailing list, and
developers on that list reply with any comments they may have.  This
process should turn up any major problems with a patch if all goes
well.

\item {} 
Wider review.  When the patch is getting close to ready for mainline
inclusion, it should be accepted by a relevant subsystem maintainer -
though this acceptance is not a guarantee that the patch will make it
all the way to the mainline.  The patch will show up in the maintainer's
subsystem tree and into the -next trees (described below).  When the
process works, this step leads to more extensive review of the patch and
the discovery of any problems resulting from the integration of this
patch with work being done by others.

\end{itemize}
\begin{itemize}
\item {} 
Please note that most maintainers also have day jobs, so merging
your patch may not be their highest priority.  If your patch is
getting feedback about changes that are needed, you should either
make those changes or justify why they should not be made.  If your
patch has no review complaints but is not being merged by its
appropriate subsystem or driver maintainer, you should be persistent
in updating the patch to the current kernel so that it applies cleanly
and keep sending it for review and merging.

\end{itemize}
\begin{itemize}
\item {} 
Merging into the mainline.  Eventually, a successful patch will be
merged into the mainline repository managed by Linus Torvalds.  More
comments and/or problems may surface at this time; it is important that
the developer be responsive to these and fix any issues which arise.

\item {} 
Stable release.  The number of users potentially affected by the patch
is now large, so, once again, new problems may arise.

\item {} 
Long-term maintenance.  While it is certainly possible for a developer
to forget about code after merging it, that sort of behavior tends to
leave a poor impression in the development community.  Merging code
eliminates some of the maintenance burden, in that others will fix
problems caused by API changes.  But the original developer should
continue to take responsibility for the code if it is to remain useful
in the longer term.

\end{itemize}

One of the largest mistakes made by kernel developers (or their employers)
is to try to cut the process down to a single ``merging into the mainline''
step.  This approach invariably leads to frustration for everybody
involved.


\subsection{How patches get into the Kernel}
\label{process/2.Process:how-patches-get-into-the-kernel}
There is exactly one person who can merge patches into the mainline kernel
repository: Linus Torvalds.  But, of the over 9,500 patches which went
into the 2.6.38 kernel, only 112 (around 1.3\%) were directly chosen by Linus
himself.  The kernel project has long since grown to a size where no single
developer could possibly inspect and select every patch unassisted.  The
way the kernel developers have addressed this growth is through the use of
a lieutenant system built around a chain of trust.

The kernel code base is logically broken down into a set of subsystems:
networking, specific architecture support, memory management, video
devices, etc.  Most subsystems have a designated maintainer, a developer
who has overall responsibility for the code within that subsystem.  These
subsystem maintainers are the gatekeepers (in a loose way) for the portion
of the kernel they manage; they are the ones who will (usually) accept a
patch for inclusion into the mainline kernel.

Subsystem maintainers each manage their own version of the kernel source
tree, usually (but certainly not always) using the git source management
tool.  Tools like git (and related tools like quilt or mercurial) allow
maintainers to track a list of patches, including authorship information
and other metadata.  At any given time, the maintainer can identify which
patches in his or her repository are not found in the mainline.

When the merge window opens, top-level maintainers will ask Linus to ``pull''
the patches they have selected for merging from their repositories.  If
Linus agrees, the stream of patches will flow up into his repository,
becoming part of the mainline kernel.  The amount of attention that Linus
pays to specific patches received in a pull operation varies.  It is clear
that, sometimes, he looks quite closely.  But, as a general rule, Linus
trusts the subsystem maintainers to not send bad patches upstream.

Subsystem maintainers, in turn, can pull patches from other maintainers.
For example, the networking tree is built from patches which accumulated
first in trees dedicated to network device drivers, wireless networking,
etc.  This chain of repositories can be arbitrarily long, though it rarely
exceeds two or three links.  Since each maintainer in the chain trusts
those managing lower-level trees, this process is known as the ``chain of
trust.''

Clearly, in a system like this, getting patches into the kernel depends on
finding the right maintainer.  Sending patches directly to Linus is not
normally the right way to go.


\subsection{Next trees}
\label{process/2.Process:next-trees}
The chain of subsystem trees guides the flow of patches into the kernel,
but it also raises an interesting question: what if somebody wants to look
at all of the patches which are being prepared for the next merge window?
Developers will be interested in what other changes are pending to see
whether there are any conflicts to worry about; a patch which changes a
core kernel function prototype, for example, will conflict with any other
patches which use the older form of that function.  Reviewers and testers
want access to the changes in their integrated form before all of those
changes land in the mainline kernel.  One could pull changes from all of
the interesting subsystem trees, but that would be a big and error-prone
job.

The answer comes in the form of -next trees, where subsystem trees are
collected for testing and review.  The older of these trees, maintained by
Andrew Morton, is called ``-mm'' (for memory management, which is how it got
started).  The -mm tree integrates patches from a long list of subsystem
trees; it also has some patches aimed at helping with debugging.

Beyond that, -mm contains a significant collection of patches which have
been selected by Andrew directly.  These patches may have been posted on a
mailing list, or they may apply to a part of the kernel for which there is
no designated subsystem tree.  As a result, -mm operates as a sort of
subsystem tree of last resort; if there is no other obvious path for a
patch into the mainline, it is likely to end up in -mm.  Miscellaneous
patches which accumulate in -mm will eventually either be forwarded on to
an appropriate subsystem tree or be sent directly to Linus.  In a typical
development cycle, approximately 5-10\% of the patches going into the
mainline get there via -mm.

The current -mm patch is available in the ``mmotm'' (-mm of the moment)
directory at:
\begin{quote}

\href{http://www.ozlabs.org/~akpm/mmotm/}{http://www.ozlabs.org/\textasciitilde{}akpm/mmotm/}
\end{quote}

Use of the MMOTM tree is likely to be a frustrating experience, though;
there is a definite chance that it will not even compile.

The primary tree for next-cycle patch merging is linux-next, maintained by
Stephen Rothwell.  The linux-next tree is, by design, a snapshot of what
the mainline is expected to look like after the next merge window closes.
Linux-next trees are announced on the linux-kernel and linux-next mailing
lists when they are assembled; they can be downloaded from:
\begin{quote}

\href{http://www.kernel.org/pub/linux/kernel/next/}{http://www.kernel.org/pub/linux/kernel/next/}
\end{quote}

Linux-next has become an integral part of the kernel development process;
all patches merged during a given merge window should really have found
their way into linux-next some time before the merge window opens.


\subsection{Staging trees}
\label{process/2.Process:staging-trees}
The kernel source tree contains the drivers/staging/ directory, where
many sub-directories for drivers or filesystems that are on their way to
being added to the kernel tree live.  They remain in drivers/staging while
they still need more work; once complete, they can be moved into the
kernel proper.  This is a way to keep track of drivers that aren't
up to Linux kernel coding or quality standards, but people may want to use
them and track development.

Greg Kroah-Hartman currently maintains the staging tree.  Drivers that
still need work are sent to him, with each driver having its own
subdirectory in drivers/staging/.  Along with the driver source files, a
TODO file should be present in the directory as well.  The TODO file lists
the pending work that the driver needs for acceptance into the kernel
proper, as well as a list of people that should be Cc'd for any patches to
the driver.  Current rules require that drivers contributed to staging
must, at a minimum, compile properly.

Staging can be a relatively easy way to get new drivers into the mainline
where, with luck, they will come to the attention of other developers and
improve quickly.  Entry into staging is not the end of the story, though;
code in staging which is not seeing regular progress will eventually be
removed.  Distributors also tend to be relatively reluctant to enable
staging drivers.  So staging is, at best, a stop on the way toward becoming
a proper mainline driver.


\subsection{Tools}
\label{process/2.Process:tools}
As can be seen from the above text, the kernel development process depends
heavily on the ability to herd collections of patches in various
directions.  The whole thing would not work anywhere near as well as it
does without suitably powerful tools.  Tutorials on how to use these tools
are well beyond the scope of this document, but there is space for a few
pointers.

By far the dominant source code management system used by the kernel
community is git.  Git is one of a number of distributed version control
systems being developed in the free software community.  It is well tuned
for kernel development, in that it performs quite well when dealing with
large repositories and large numbers of patches.  It also has a reputation
for being difficult to learn and use, though it has gotten better over
time.  Some sort of familiarity with git is almost a requirement for kernel
developers; even if they do not use it for their own work, they'll need git
to keep up with what other developers (and the mainline) are doing.

Git is now packaged by almost all Linux distributions.  There is a home
page at:
\begin{quote}

\href{http://git-scm.com/}{http://git-scm.com/}
\end{quote}

That page has pointers to documentation and tutorials.

Among the kernel developers who do not use git, the most popular choice is
almost certainly Mercurial:
\begin{quote}

\href{http://www.selenic.com/mercurial/}{http://www.selenic.com/mercurial/}
\end{quote}

Mercurial shares many features with git, but it provides an interface which
many find easier to use.

The other tool worth knowing about is Quilt:
\begin{quote}

\href{http://savannah.nongnu.org/projects/quilt/}{http://savannah.nongnu.org/projects/quilt/}
\end{quote}

Quilt is a patch management system, rather than a source code management
system.  It does not track history over time; it is, instead, oriented
toward tracking a specific set of changes against an evolving code base.
Some major subsystem maintainers use quilt to manage patches intended to go
upstream.  For the management of certain kinds of trees (-mm, for example),
quilt is the best tool for the job.


\subsection{Mailing lists}
\label{process/2.Process:mailing-lists}
A great deal of Linux kernel development work is done by way of mailing
lists.  It is hard to be a fully-functioning member of the community
without joining at least one list somewhere.  But Linux mailing lists also
represent a potential hazard to developers, who risk getting buried under a
load of electronic mail, running afoul of the conventions used on the Linux
lists, or both.

Most kernel mailing lists are run on vger.kernel.org; the master list can
be found at:
\begin{quote}

\href{http://vger.kernel.org/vger-lists.html}{http://vger.kernel.org/vger-lists.html}
\end{quote}

There are lists hosted elsewhere, though; a number of them are at
lists.redhat.com.

The core mailing list for kernel development is, of course, linux-kernel.
This list is an intimidating place to be; volume can reach 500 messages per
day, the amount of noise is high, the conversation can be severely
technical, and participants are not always concerned with showing a high
degree of politeness.  But there is no other place where the kernel
development community comes together as a whole; developers who avoid this
list will miss important information.

There are a few hints which can help with linux-kernel survival:
\begin{itemize}
\item {} 
Have the list delivered to a separate folder, rather than your main
mailbox.  One must be able to ignore the stream for sustained periods of
time.

\item {} 
Do not try to follow every conversation - nobody else does.  It is
important to filter on both the topic of interest (though note that
long-running conversations can drift away from the original subject
without changing the email subject line) and the people who are
participating.

\item {} 
Do not feed the trolls.  If somebody is trying to stir up an angry
response, ignore them.

\item {} 
When responding to linux-kernel email (or that on other lists) preserve
the Cc: header for all involved.  In the absence of a strong reason (such
as an explicit request), you should never remove recipients.  Always make
sure that the person you are responding to is in the Cc: list.  This
convention also makes it unnecessary to explicitly ask to be copied on
replies to your postings.

\item {} 
Search the list archives (and the net as a whole) before asking
questions.  Some developers can get impatient with people who clearly
have not done their homework.

\item {} 
Avoid top-posting (the practice of putting your answer above the quoted
text you are responding to).  It makes your response harder to read and
makes a poor impression.

\item {} 
Ask on the correct mailing list.  Linux-kernel may be the general meeting
point, but it is not the best place to find developers from all
subsystems.

\end{itemize}

The last point - finding the correct mailing list - is a common place for
beginning developers to go wrong.  Somebody who asks a networking-related
question on linux-kernel will almost certainly receive a polite suggestion
to ask on the netdev list instead, as that is the list frequented by most
networking developers.  Other lists exist for the SCSI, video4linux, IDE,
filesystem, etc. subsystems.  The best place to look for mailing lists is
in the MAINTAINERS file packaged with the kernel source.


\subsection{Getting started with Kernel development}
\label{process/2.Process:getting-started-with-kernel-development}
Questions about how to get started with the kernel development process are
common - from both individuals and companies.  Equally common are missteps
which make the beginning of the relationship harder than it has to be.

Companies often look to hire well-known developers to get a development
group started.  This can, in fact, be an effective technique.  But it also
tends to be expensive and does not do much to grow the pool of experienced
kernel developers.  It is possible to bring in-house developers up to speed
on Linux kernel development, given the investment of a bit of time.  Taking
this time can endow an employer with a group of developers who understand
the kernel and the company both, and who can help to train others as well.
Over the medium term, this is often the more profitable approach.

Individual developers are often, understandably, at a loss for a place to
start.  Beginning with a large project can be intimidating; one often wants
to test the waters with something smaller first.  This is the point where
some developers jump into the creation of patches fixing spelling errors or
minor coding style issues.  Unfortunately, such patches create a level of
noise which is distracting for the development community as a whole, so,
increasingly, they are looked down upon.  New developers wishing to
introduce themselves to the community will not get the sort of reception
they wish for by these means.

Andrew Morton gives this advice for aspiring kernel developers

\begin{Verbatim}[commandchars=\\\{\}]
The \PYGZsh{}1 project for all kernel beginners should surely be \PYGZdq{}make sure
that the kernel runs perfectly at all times on all machines which
you can lay your hands on\PYGZdq{}.  Usually the way to do this is to work
with others on getting things fixed up (this can require
persistence!) but that\PYGZsq{}s fine \PYGZhy{} it\PYGZsq{}s a part of kernel development.
\end{Verbatim}

(\href{http://lwn.net/Articles/283982/}{http://lwn.net/Articles/283982/}).

In the absence of obvious problems to fix, developers are advised to look
at the current lists of regressions and open bugs in general.  There is
never any shortage of issues in need of fixing; by addressing these issues,
developers will gain experience with the process while, at the same time,
building respect with the rest of the development community.


\section{Early-stage planning}
\label{process/3.Early-stage:development-early-stage}\label{process/3.Early-stage::doc}\label{process/3.Early-stage:early-stage-planning}
When contemplating a Linux kernel development project, it can be tempting
to jump right in and start coding.  As with any significant project,
though, much of the groundwork for success is best laid before the first
line of code is written.  Some time spent in early planning and
communication can save far more time later on.


\subsection{Specifying the problem}
\label{process/3.Early-stage:specifying-the-problem}
Like any engineering project, a successful kernel enhancement starts with a
clear description of the problem to be solved.  In some cases, this step is
easy: when a driver is needed for a specific piece of hardware, for
example.  In others, though, it is tempting to confuse the real problem
with the proposed solution, and that can lead to difficulties.

Consider an example: some years ago, developers working with Linux audio
sought a way to run applications without dropouts or other artifacts caused
by excessive latency in the system.  The solution they arrived at was a
kernel module intended to hook into the Linux Security Module (LSM)
framework; this module could be configured to give specific applications
access to the realtime scheduler.  This module was implemented and sent to
the linux-kernel mailing list, where it immediately ran into problems.

To the audio developers, this security module was sufficient to solve their
immediate problem.  To the wider kernel community, though, it was seen as a
misuse of the LSM framework (which is not intended to confer privileges
onto processes which they would not otherwise have) and a risk to system
stability.  Their preferred solutions involved realtime scheduling access
via the rlimit mechanism for the short term, and ongoing latency reduction
work in the long term.

The audio community, however, could not see past the particular solution
they had implemented; they were unwilling to accept alternatives.  The
resulting disagreement left those developers feeling disillusioned with the
entire kernel development process; one of them went back to an audio list
and posted this:
\begin{quote}

There are a number of very good Linux kernel developers, but they
tend to get outshouted by a large crowd of arrogant fools. Trying
to communicate user requirements to these people is a waste of
time. They are much too ``intelligent'' to listen to lesser mortals.
\end{quote}

(\href{http://lwn.net/Articles/131776/}{http://lwn.net/Articles/131776/}).

The reality of the situation was different; the kernel developers were far
more concerned about system stability, long-term maintenance, and finding
the right solution to the problem than they were with a specific module.
The moral of the story is to focus on the problem - not a specific solution
- and to discuss it with the development community before investing in the
creation of a body of code.

So, when contemplating a kernel development project, one should obtain
answers to a short set of questions:
\begin{itemize}
\item {} 
What, exactly, is the problem which needs to be solved?

\item {} 
Who are the users affected by this problem?  Which use cases should the
solution address?

\item {} 
How does the kernel fall short in addressing that problem now?

\end{itemize}

Only then does it make sense to start considering possible solutions.


\subsection{Early discussion}
\label{process/3.Early-stage:early-discussion}
When planning a kernel development project, it makes great sense to hold
discussions with the community before launching into implementation.  Early
communication can save time and trouble in a number of ways:
\begin{itemize}
\item {} 
It may well be that the problem is addressed by the kernel in ways which
you have not understood.  The Linux kernel is large and has a number of
features and capabilities which are not immediately obvious.  Not all
kernel capabilities are documented as well as one might like, and it is
easy to miss things.  Your author has seen the posting of a complete
driver which duplicated an existing driver that the new author had been
unaware of.  Code which reinvents existing wheels is not only wasteful;
it will also not be accepted into the mainline kernel.

\item {} 
There may be elements of the proposed solution which will not be
acceptable for mainline merging.  It is better to find out about
problems like this before writing the code.

\item {} 
It's entirely possible that other developers have thought about the
problem; they may have ideas for a better solution, and may be willing
to help in the creation of that solution.

\end{itemize}

Years of experience with the kernel development community have taught a
clear lesson: kernel code which is designed and developed behind closed
doors invariably has problems which are only revealed when the code is
released into the community.  Sometimes these problems are severe,
requiring months or years of effort before the code can be brought up to
the kernel community's standards.  Some examples include:
\begin{itemize}
\item {} 
The Devicescape network stack was designed and implemented for
single-processor systems.  It could not be merged into the mainline
until it was made suitable for multiprocessor systems.  Retrofitting
locking and such into code is a difficult task; as a result, the merging
of this code (now called mac80211) was delayed for over a year.

\item {} 
The Reiser4 filesystem included a number of capabilities which, in the
core kernel developers' opinion, should have been implemented in the
virtual filesystem layer instead.  It also included features which could
not easily be implemented without exposing the system to user-caused
deadlocks.  The late revelation of these problems - and refusal to
address some of them - has caused Reiser4 to stay out of the mainline
kernel.

\item {} 
The AppArmor security module made use of internal virtual filesystem
data structures in ways which were considered to be unsafe and
unreliable.  This concern (among others) kept AppArmor out of the
mainline for years.

\end{itemize}

In each of these cases, a great deal of pain and extra work could have been
avoided with some early discussion with the kernel developers.


\subsection{Who do you talk to?}
\label{process/3.Early-stage:who-do-you-talk-to}
When developers decide to take their plans public, the next question will
be: where do we start?  The answer is to find the right mailing list(s) and
the right maintainer.  For mailing lists, the best approach is to look in
the MAINTAINERS file for a relevant place to post.  If there is a suitable
subsystem list, posting there is often preferable to posting on
linux-kernel; you are more likely to reach developers with expertise in the
relevant subsystem and the environment may be more supportive.

Finding maintainers can be a bit harder.  Again, the MAINTAINERS file is
the place to start.  That file tends to not always be up to date, though,
and not all subsystems are represented there.  The person listed in the
MAINTAINERS file may, in fact, not be the person who is actually acting in
that role currently.  So, when there is doubt about who to contact, a
useful trick is to use git (and ``git log'' in particular) to see who is
currently active within the subsystem of interest.  Look at who is writing
patches, and who, if anybody, is attaching Signed-off-by lines to those
patches.  Those are the people who will be best placed to help with a new
development project.

The task of finding the right maintainer is sometimes challenging enough
that the kernel developers have added a script to ease the process:

\begin{Verbatim}[commandchars=\\\{\}]
.../scripts/get\PYGZus{}maintainer.pl
\end{Verbatim}

This script will return the current maintainer(s) for a given file or
directory when given the ``-f'' option.  If passed a patch on the
command line, it will list the maintainers who should probably receive
copies of the patch.  There are a number of options regulating how hard
get\_maintainer.pl will search for maintainers; please be careful about
using the more aggressive options as you may end up including developers
who have no real interest in the code you are modifying.

If all else fails, talking to Andrew Morton can be an effective way to
track down a maintainer for a specific piece of code.


\subsection{When to post?}
\label{process/3.Early-stage:when-to-post}
If possible, posting your plans during the early stages can only be
helpful.  Describe the problem being solved and any plans that have been
made on how the implementation will be done.  Any information you can
provide can help the development community provide useful input on the
project.

One discouraging thing which can happen at this stage is not a hostile
reaction, but, instead, little or no reaction at all.  The sad truth of the
matter is (1) kernel developers tend to be busy, (2) there is no shortage
of people with grand plans and little code (or even prospect of code) to
back them up, and (3) nobody is obligated to review or comment on ideas
posted by others.  Beyond that, high-level designs often hide problems
which are only revealed when somebody actually tries to implement those
designs; for that reason, kernel developers would rather see the code.

If a request-for-comments posting yields little in the way of comments, do
not assume that it means there is no interest in the project.
Unfortunately, you also cannot assume that there are no problems with your
idea.  The best thing to do in this situation is to proceed, keeping the
community informed as you go.


\subsection{Getting official buy-in}
\label{process/3.Early-stage:getting-official-buy-in}
If your work is being done in a corporate environment - as most Linux
kernel work is - you must, obviously, have permission from suitably
empowered managers before you can post your company's plans or code to a
public mailing list.  The posting of code which has not been cleared for
release under a GPL-compatible license can be especially problematic; the
sooner that a company's management and legal staff can agree on the posting
of a kernel development project, the better off everybody involved will be.

Some readers may be thinking at this point that their kernel work is
intended to support a product which does not yet have an officially
acknowledged existence.  Revealing their employer's plans on a public
mailing list may not be a viable option.  In cases like this, it is worth
considering whether the secrecy is really necessary; there is often no real
need to keep development plans behind closed doors.

That said, there are also cases where a company legitimately cannot
disclose its plans early in the development process.  Companies with
experienced kernel developers may choose to proceed in an open-loop manner
on the assumption that they will be able to avoid serious integration
problems later.  For companies without that sort of in-house expertise, the
best option is often to hire an outside developer to review the plans under
a non-disclosure agreement.  The Linux Foundation operates an NDA program
designed to help with this sort of situation; more information can be found
at:
\begin{quote}

\href{http://www.linuxfoundation.org/en/NDA\_program}{http://www.linuxfoundation.org/en/NDA\_program}
\end{quote}

This kind of review is often enough to avoid serious problems later on
without requiring public disclosure of the project.


\section{Getting the code right}
\label{process/4.Coding:development-coding}\label{process/4.Coding:getting-the-code-right}\label{process/4.Coding::doc}
While there is much to be said for a solid and community-oriented design
process, the proof of any kernel development project is in the resulting
code.  It is the code which will be examined by other developers and merged
(or not) into the mainline tree.  So it is the quality of this code which
will determine the ultimate success of the project.

This section will examine the coding process.  We'll start with a look at a
number of ways in which kernel developers can go wrong.  Then the focus
will shift toward doing things right and the tools which can help in that
quest.


\subsection{Pitfalls}
\label{process/4.Coding:pitfalls}

\subsubsection{Coding style}
\label{process/4.Coding:coding-style}
The kernel has long had a standard coding style, described in
{\hyperref[process/coding\string-style:codingstyle]{\emph{Documentation/process/coding-style.rst}}}.  For much of
that time, the policies described in that file were taken as being, at most,
advisory.  As a result, there is a substantial amount of code in the kernel
which does not meet the coding style guidelines.  The presence of that code
leads to two independent hazards for kernel developers.

The first of these is to believe that the kernel coding standards do not
matter and are not enforced.  The truth of the matter is that adding new
code to the kernel is very difficult if that code is not coded according to
the standard; many developers will request that the code be reformatted
before they will even review it.  A code base as large as the kernel
requires some uniformity of code to make it possible for developers to
quickly understand any part of it.  So there is no longer room for
strangely-formatted code.

Occasionally, the kernel's coding style will run into conflict with an
employer's mandated style.  In such cases, the kernel's style will have to
win before the code can be merged.  Putting code into the kernel means
giving up a degree of control in a number of ways - including control over
how the code is formatted.

The other trap is to assume that code which is already in the kernel is
urgently in need of coding style fixes.  Developers may start to generate
reformatting patches as a way of gaining familiarity with the process, or
as a way of getting their name into the kernel changelogs - or both.  But
pure coding style fixes are seen as noise by the development community;
they tend to get a chilly reception.  So this type of patch is best
avoided.  It is natural to fix the style of a piece of code while working
on it for other reasons, but coding style changes should not be made for
their own sake.

The coding style document also should not be read as an absolute law which
can never be transgressed.  If there is a good reason to go against the
style (a line which becomes far less readable if split to fit within the
80-column limit, for example), just do it.


\subsubsection{Abstraction layers}
\label{process/4.Coding:abstraction-layers}
Computer Science professors teach students to make extensive use of
abstraction layers in the name of flexibility and information hiding.
Certainly the kernel makes extensive use of abstraction; no project
involving several million lines of code could do otherwise and survive.
But experience has shown that excessive or premature abstraction can be
just as harmful as premature optimization.  Abstraction should be used to
the level required and no further.

At a simple level, consider a function which has an argument which is
always passed as zero by all callers.  One could retain that argument just
in case somebody eventually needs to use the extra flexibility that it
provides.  By that time, though, chances are good that the code which
implements this extra argument has been broken in some subtle way which was
never noticed - because it has never been used.  Or, when the need for
extra flexibility arises, it does not do so in a way which matches the
programmer's early expectation.  Kernel developers will routinely submit
patches to remove unused arguments; they should, in general, not be added
in the first place.

Abstraction layers which hide access to hardware - often to allow the bulk
of a driver to be used with multiple operating systems - are especially
frowned upon.  Such layers obscure the code and may impose a performance
penalty; they do not belong in the Linux kernel.

On the other hand, if you find yourself copying significant amounts of code
from another kernel subsystem, it is time to ask whether it would, in fact,
make sense to pull out some of that code into a separate library or to
implement that functionality at a higher level.  There is no value in
replicating the same code throughout the kernel.


\subsubsection{\#ifdef and preprocessor use in general}
\label{process/4.Coding:ifdef-and-preprocessor-use-in-general}
The C preprocessor seems to present a powerful temptation to some C
programmers, who see it as a way to efficiently encode a great deal of
flexibility into a source file.  But the preprocessor is not C, and heavy
use of it results in code which is much harder for others to read and
harder for the compiler to check for correctness.  Heavy preprocessor use
is almost always a sign of code which needs some cleanup work.

Conditional compilation with \#ifdef is, indeed, a powerful feature, and it
is used within the kernel.  But there is little desire to see code which is
sprinkled liberally with \#ifdef blocks.  As a general rule, \#ifdef use
should be confined to header files whenever possible.
Conditionally-compiled code can be confined to functions which, if the code
is not to be present, simply become empty.  The compiler will then quietly
optimize out the call to the empty function.  The result is far cleaner
code which is easier to follow.

C preprocessor macros present a number of hazards, including possible
multiple evaluation of expressions with side effects and no type safety.
If you are tempted to define a macro, consider creating an inline function
instead.  The code which results will be the same, but inline functions are
easier to read, do not evaluate their arguments multiple times, and allow
the compiler to perform type checking on the arguments and return value.


\subsubsection{Inline functions}
\label{process/4.Coding:inline-functions}
Inline functions present a hazard of their own, though.  Programmers can
become enamored of the perceived efficiency inherent in avoiding a function
call and fill a source file with inline functions.  Those functions,
however, can actually reduce performance.  Since their code is replicated
at each call site, they end up bloating the size of the compiled kernel.
That, in turn, creates pressure on the processor's memory caches, which can
slow execution dramatically.  Inline functions, as a rule, should be quite
small and relatively rare.  The cost of a function call, after all, is not
that high; the creation of large numbers of inline functions is a classic
example of premature optimization.

In general, kernel programmers ignore cache effects at their peril.  The
classic time/space tradeoff taught in beginning data structures classes
often does not apply to contemporary hardware.  Space \emph{is} time, in that a
larger program will run slower than one which is more compact.

More recent compilers take an increasingly active role in deciding whether
a given function should actually be inlined or not.  So the liberal
placement of ``inline'' keywords may not just be excessive; it could also be
irrelevant.


\subsubsection{Locking}
\label{process/4.Coding:locking}
In May, 2006, the ``Devicescape'' networking stack was, with great
fanfare, released under the GPL and made available for inclusion in the
mainline kernel.  This donation was welcome news; support for wireless
networking in Linux was considered substandard at best, and the Devicescape
stack offered the promise of fixing that situation.  Yet, this code did not
actually make it into the mainline until June, 2007 (2.6.22).  What
happened?

This code showed a number of signs of having been developed behind
corporate doors.  But one large problem in particular was that it was not
designed to work on multiprocessor systems.  Before this networking stack
(now called mac80211) could be merged, a locking scheme needed to be
retrofitted onto it.

Once upon a time, Linux kernel code could be developed without thinking
about the concurrency issues presented by multiprocessor systems.  Now,
however, this document is being written on a dual-core laptop.  Even on
single-processor systems, work being done to improve responsiveness will
raise the level of concurrency within the kernel.  The days when kernel
code could be written without thinking about locking are long past.

Any resource (data structures, hardware registers, etc.) which could be
accessed concurrently by more than one thread must be protected by a lock.
New code should be written with this requirement in mind; retrofitting
locking after the fact is a rather more difficult task.  Kernel developers
should take the time to understand the available locking primitives well
enough to pick the right tool for the job.  Code which shows a lack of
attention to concurrency will have a difficult path into the mainline.


\subsubsection{Regressions}
\label{process/4.Coding:regressions}
One final hazard worth mentioning is this: it can be tempting to make a
change (which may bring big improvements) which causes something to break
for existing users.  This kind of change is called a ``regression,'' and
regressions have become most unwelcome in the mainline kernel.  With few
exceptions, changes which cause regressions will be backed out if the
regression cannot be fixed in a timely manner.  Far better to avoid the
regression in the first place.

It is often argued that a regression can be justified if it causes things
to work for more people than it creates problems for.  Why not make a
change if it brings new functionality to ten systems for each one it
breaks?  The best answer to this question was expressed by Linus in July,
2007:

\begin{Verbatim}[commandchars=\\\{\}]
So we don\PYGZsq{}t fix bugs by introducing new problems.  That way lies
madness, and nobody ever knows if you actually make any real
progress at all. Is it two steps forwards, one step back, or one
step forward and two steps back?
\end{Verbatim}

(\href{http://lwn.net/Articles/243460/}{http://lwn.net/Articles/243460/}).

An especially unwelcome type of regression is any sort of change to the
user-space ABI.  Once an interface has been exported to user space, it must
be supported indefinitely.  This fact makes the creation of user-space
interfaces particularly challenging: since they cannot be changed in
incompatible ways, they must be done right the first time.  For this
reason, a great deal of thought, clear documentation, and wide review for
user-space interfaces is always required.


\subsection{Code checking tools}
\label{process/4.Coding:code-checking-tools}
For now, at least, the writing of error-free code remains an ideal that few
of us can reach.  What we can hope to do, though, is to catch and fix as
many of those errors as possible before our code goes into the mainline
kernel.  To that end, the kernel developers have put together an impressive
array of tools which can catch a wide variety of obscure problems in an
automated way.  Any problem caught by the computer is a problem which will
not afflict a user later on, so it stands to reason that the automated
tools should be used whenever possible.

The first step is simply to heed the warnings produced by the compiler.
Contemporary versions of gcc can detect (and warn about) a large number of
potential errors.  Quite often, these warnings point to real problems.
Code submitted for review should, as a rule, not produce any compiler
warnings.  When silencing warnings, take care to understand the real cause
and try to avoid ``fixes'' which make the warning go away without addressing
its cause.

Note that not all compiler warnings are enabled by default.  Build the
kernel with ``make EXTRA\_CFLAGS=-W'' to get the full set.

The kernel provides several configuration options which turn on debugging
features; most of these are found in the ``kernel hacking'' submenu.  Several
of these options should be turned on for any kernel used for development or
testing purposes.  In particular, you should turn on:
\begin{itemize}
\item {} 
ENABLE\_WARN\_DEPRECATED, ENABLE\_MUST\_CHECK, and FRAME\_WARN to get an
extra set of warnings for problems like the use of deprecated interfaces
or ignoring an important return value from a function.  The output
generated by these warnings can be verbose, but one need not worry about
warnings from other parts of the kernel.

\item {} 
DEBUG\_OBJECTS will add code to track the lifetime of various objects
created by the kernel and warn when things are done out of order.  If
you are adding a subsystem which creates (and exports) complex objects
of its own, consider adding support for the object debugging
infrastructure.

\item {} 
DEBUG\_SLAB can find a variety of memory allocation and use errors; it
should be used on most development kernels.

\item {} 
DEBUG\_SPINLOCK, DEBUG\_ATOMIC\_SLEEP, and DEBUG\_MUTEXES will find a
number of common locking errors.

\end{itemize}

There are quite a few other debugging options, some of which will be
discussed below.  Some of them have a significant performance impact and
should not be used all of the time.  But some time spent learning the
available options will likely be paid back many times over in short order.

One of the heavier debugging tools is the locking checker, or ``lockdep.''
This tool will track the acquisition and release of every lock (spinlock or
mutex) in the system, the order in which locks are acquired relative to
each other, the current interrupt environment, and more.  It can then
ensure that locks are always acquired in the same order, that the same
interrupt assumptions apply in all situations, and so on.  In other words,
lockdep can find a number of scenarios in which the system could, on rare
occasion, deadlock.  This kind of problem can be painful (for both
developers and users) in a deployed system; lockdep allows them to be found
in an automated manner ahead of time.  Code with any sort of non-trivial
locking should be run with lockdep enabled before being submitted for
inclusion.

As a diligent kernel programmer, you will, beyond doubt, check the return
status of any operation (such as a memory allocation) which can fail.  The
fact of the matter, though, is that the resulting failure recovery paths
are, probably, completely untested.  Untested code tends to be broken code;
you could be much more confident of your code if all those error-handling
paths had been exercised a few times.

The kernel provides a fault injection framework which can do exactly that,
especially where memory allocations are involved.  With fault injection
enabled, a configurable percentage of memory allocations will be made to
fail; these failures can be restricted to a specific range of code.
Running with fault injection enabled allows the programmer to see how the
code responds when things go badly.  See
Documentation/fault-injection/fault-injection.txt for more information on
how to use this facility.

Other kinds of errors can be found with the ``sparse'' static analysis tool.
With sparse, the programmer can be warned about confusion between
user-space and kernel-space addresses, mixture of big-endian and
small-endian quantities, the passing of integer values where a set of bit
flags is expected, and so on.  Sparse must be installed separately (it can
be found at \href{https://sparse.wiki.kernel.org/index.php/Main\_Page}{https://sparse.wiki.kernel.org/index.php/Main\_Page} if your
distributor does not package it); it can then be run on the code by adding
``C=1'' to your make command.

The ``Coccinelle'' tool (\href{http://coccinelle.lip6.fr/}{http://coccinelle.lip6.fr/}) is able to find a wide
variety of potential coding problems; it can also propose fixes for those
problems.  Quite a few ``semantic patches'' for the kernel have been packaged
under the scripts/coccinelle directory; running ``make coccicheck'' will run
through those semantic patches and report on any problems found.  See
Documentation/dev-tools/coccinelle.rst for more information.

Other kinds of portability errors are best found by compiling your code for
other architectures.  If you do not happen to have an S/390 system or a
Blackfin development board handy, you can still perform the compilation
step.  A large set of cross compilers for x86 systems can be found at
\begin{quote}

\href{http://www.kernel.org/pub/tools/crosstool/}{http://www.kernel.org/pub/tools/crosstool/}
\end{quote}

Some time spent installing and using these compilers will help avoid
embarrassment later.


\subsection{Documentation}
\label{process/4.Coding:documentation}
Documentation has often been more the exception than the rule with kernel
development.  Even so, adequate documentation will help to ease the merging
of new code into the kernel, make life easier for other developers, and
will be helpful for your users.  In many cases, the addition of
documentation has become essentially mandatory.

The first piece of documentation for any patch is its associated
changelog.  Log entries should describe the problem being solved, the form
of the solution, the people who worked on the patch, any relevant
effects on performance, and anything else that might be needed to
understand the patch.  Be sure that the changelog says \emph{why} the patch is
worth applying; a surprising number of developers fail to provide that
information.

Any code which adds a new user-space interface - including new sysfs or
/proc files - should include documentation of that interface which enables
user-space developers to know what they are working with.  See
Documentation/ABI/README for a description of how this documentation should
be formatted and what information needs to be provided.

The file \DUspan{xref,std,std-ref}{Documentation/admin-guide/kernel-parameters.rst} describes all of the kernel's boot-time parameters.
Any patch which adds new parameters should add the appropriate entries to
this file.

Any new configuration options must be accompanied by help text which
clearly explains the options and when the user might want to select them.

Internal API information for many subsystems is documented by way of
specially-formatted comments; these comments can be extracted and formatted
in a number of ways by the ``kernel-doc'' script.  If you are working within
a subsystem which has kerneldoc comments, you should maintain them and add
them, as appropriate, for externally-available functions.  Even in areas
which have not been so documented, there is no harm in adding kerneldoc
comments for the future; indeed, this can be a useful activity for
beginning kernel developers.  The format of these comments, along with some
information on how to create kerneldoc templates can be found at
\DUspan{xref,std,std-ref}{Documentation/doc-guide/}.

Anybody who reads through a significant amount of existing kernel code will
note that, often, comments are most notable by their absence.  Once again,
the expectations for new code are higher than they were in the past;
merging uncommented code will be harder.  That said, there is little desire
for verbosely-commented code.  The code should, itself, be readable, with
comments explaining the more subtle aspects.

Certain things should always be commented.  Uses of memory barriers should
be accompanied by a line explaining why the barrier is necessary.  The
locking rules for data structures generally need to be explained somewhere.
Major data structures need comprehensive documentation in general.
Non-obvious dependencies between separate bits of code should be pointed
out.  Anything which might tempt a code janitor to make an incorrect
``cleanup'' needs a comment saying why it is done the way it is.  And so on.


\subsection{Internal API changes}
\label{process/4.Coding:internal-api-changes}
The binary interface provided by the kernel to user space cannot be broken
except under the most severe circumstances.  The kernel's internal
programming interfaces, instead, are highly fluid and can be changed when
the need arises.  If you find yourself having to work around a kernel API,
or simply not using a specific functionality because it does not meet your
needs, that may be a sign that the API needs to change.  As a kernel
developer, you are empowered to make such changes.

There are, of course, some catches.  API changes can be made, but they need
to be well justified.  So any patch making an internal API change should be
accompanied by a description of what the change is and why it is
necessary.  This kind of change should also be broken out into a separate
patch, rather than buried within a larger patch.

The other catch is that a developer who changes an internal API is
generally charged with the task of fixing any code within the kernel tree
which is broken by the change.  For a widely-used function, this duty can
lead to literally hundreds or thousands of changes - many of which are
likely to conflict with work being done by other developers.  Needless to
say, this can be a large job, so it is best to be sure that the
justification is solid.  Note that the Coccinelle tool can help with
wide-ranging API changes.

When making an incompatible API change, one should, whenever possible,
ensure that code which has not been updated is caught by the compiler.
This will help you to be sure that you have found all in-tree uses of that
interface.  It will also alert developers of out-of-tree code that there is
a change that they need to respond to.  Supporting out-of-tree code is not
something that kernel developers need to be worried about, but we also do
not have to make life harder for out-of-tree developers than it needs to
be.


\section{Posting patches}
\label{process/5.Posting:posting-patches}\label{process/5.Posting::doc}\label{process/5.Posting:development-posting}
Sooner or later, the time comes when your work is ready to be presented to
the community for review and, eventually, inclusion into the mainline
kernel.  Unsurprisingly, the kernel development community has evolved a set
of conventions and procedures which are used in the posting of patches;
following them will make life much easier for everybody involved.  This
document will attempt to cover these expectations in reasonable detail;
more information can also be found in the files process/submitting-patches.rst,
process/submitting-drivers.rst, and process/submit-checklist.rst in the kernel documentation
directory.


\subsection{When to post}
\label{process/5.Posting:when-to-post}
There is a constant temptation to avoid posting patches before they are
completely ``ready.''  For simple patches, that is not a problem.  If the
work being done is complex, though, there is a lot to be gained by getting
feedback from the community before the work is complete.  So you should
consider posting in-progress work, or even making a git tree available so
that interested developers can catch up with your work at any time.

When posting code which is not yet considered ready for inclusion, it is a
good idea to say so in the posting itself.  Also mention any major work
which remains to be done and any known problems.  Fewer people will look at
patches which are known to be half-baked, but those who do will come in
with the idea that they can help you drive the work in the right direction.


\subsection{Before creating patches}
\label{process/5.Posting:before-creating-patches}
There are a number of things which should be done before you consider
sending patches to the development community.  These include:
\begin{itemize}
\item {} 
Test the code to the extent that you can.  Make use of the kernel's
debugging tools, ensure that the kernel will build with all reasonable
combinations of configuration options, use cross-compilers to build for
different architectures, etc.

\item {} 
Make sure your code is compliant with the kernel coding style
guidelines.

\item {} 
Does your change have performance implications?  If so, you should run
benchmarks showing what the impact (or benefit) of your change is; a
summary of the results should be included with the patch.

\item {} 
Be sure that you have the right to post the code.  If this work was done
for an employer, the employer likely has a right to the work and must be
agreeable with its release under the GPL.

\end{itemize}

As a general rule, putting in some extra thought before posting code almost
always pays back the effort in short order.


\subsection{Patch preparation}
\label{process/5.Posting:patch-preparation}
The preparation of patches for posting can be a surprising amount of work,
but, once again, attempting to save time here is not generally advisable
even in the short term.

Patches must be prepared against a specific version of the kernel.  As a
general rule, a patch should be based on the current mainline as found in
Linus's git tree.  When basing on mainline, start with a well-known release
point - a stable or -rc release - rather than branching off the mainline at
an arbitrary spot.

It may become necessary to make versions against -mm, linux-next, or a
subsystem tree, though, to facilitate wider testing and review.  Depending
on the area of your patch and what is going on elsewhere, basing a patch
against these other trees can require a significant amount of work
resolving conflicts and dealing with API changes.

Only the most simple changes should be formatted as a single patch;
everything else should be made as a logical series of changes.  Splitting
up patches is a bit of an art; some developers spend a long time figuring
out how to do it in the way that the community expects.  There are a few
rules of thumb, however, which can help considerably:
\begin{itemize}
\item {} 
The patch series you post will almost certainly not be the series of
changes found in your working revision control system.  Instead, the
changes you have made need to be considered in their final form, then
split apart in ways which make sense.  The developers are interested in
discrete, self-contained changes, not the path you took to get to those
changes.

\item {} 
Each logically independent change should be formatted as a separate
patch.  These changes can be small (``add a field to this structure'') or
large (adding a significant new driver, for example), but they should be
conceptually small and amenable to a one-line description.  Each patch
should make a specific change which can be reviewed on its own and
verified to do what it says it does.

\item {} 
As a way of restating the guideline above: do not mix different types of
changes in the same patch.  If a single patch fixes a critical security
bug, rearranges a few structures, and reformats the code, there is a
good chance that it will be passed over and the important fix will be
lost.

\item {} 
Each patch should yield a kernel which builds and runs properly; if your
patch series is interrupted in the middle, the result should still be a
working kernel.  Partial application of a patch series is a common
scenario when the ``git bisect'' tool is used to find regressions; if the
result is a broken kernel, you will make life harder for developers and
users who are engaging in the noble work of tracking down problems.

\item {} 
Do not overdo it, though.  One developer once posted a set of edits
to a single file as 500 separate patches - an act which did not make him
the most popular person on the kernel mailing list.  A single patch can
be reasonably large as long as it still contains a single \emph{logical}
change.

\item {} 
It can be tempting to add a whole new infrastructure with a series of
patches, but to leave that infrastructure unused until the final patch
in the series enables the whole thing.  This temptation should be
avoided if possible; if that series adds regressions, bisection will
finger the last patch as the one which caused the problem, even though
the real bug is elsewhere.  Whenever possible, a patch which adds new
code should make that code active immediately.

\end{itemize}

Working to create the perfect patch series can be a frustrating process
which takes quite a bit of time and thought after the ``real work'' has been
done.  When done properly, though, it is time well spent.


\subsection{Patch formatting and changelogs}
\label{process/5.Posting:patch-formatting-and-changelogs}
So now you have a perfect series of patches for posting, but the work is
not done quite yet.  Each patch needs to be formatted into a message which
quickly and clearly communicates its purpose to the rest of the world.  To
that end, each patch will be composed of the following:
\begin{itemize}
\item {} 
An optional ``From'' line naming the author of the patch.  This line is
only necessary if you are passing on somebody else's patch via email,
but it never hurts to add it when in doubt.

\item {} 
A one-line description of what the patch does.  This message should be
enough for a reader who sees it with no other context to figure out the
scope of the patch; it is the line that will show up in the ``short form''
changelogs.  This message is usually formatted with the relevant
subsystem name first, followed by the purpose of the patch.  For
example:

\begin{Verbatim}[commandchars=\\\{\}]
gpio: fix build on CONFIG\PYGZus{}GPIO\PYGZus{}SYSFS=n
\end{Verbatim}

\item {} 
A blank line followed by a detailed description of the contents of the
patch.  This description can be as long as is required; it should say
what the patch does and why it should be applied to the kernel.

\item {} 
One or more tag lines, with, at a minimum, one Signed-off-by: line from
the author of the patch.  Tags will be described in more detail below.

\end{itemize}

The items above, together, form the changelog for the patch.  Writing good
changelogs is a crucial but often-neglected art; it's worth spending
another moment discussing this issue.  When writing a changelog, you should
bear in mind that a number of different people will be reading your words.
These include subsystem maintainers and reviewers who need to decide
whether the patch should be included, distributors and other maintainers
trying to decide whether a patch should be backported to other kernels, bug
hunters wondering whether the patch is responsible for a problem they are
chasing, users who want to know how the kernel has changed, and more.  A
good changelog conveys the needed information to all of these people in the
most direct and concise way possible.

To that end, the summary line should describe the effects of and motivation
for the change as well as possible given the one-line constraint.  The
detailed description can then amplify on those topics and provide any
needed additional information.  If the patch fixes a bug, cite the commit
which introduced the bug if possible (and please provide both the commit ID
and the title when citing commits).  If a problem is associated with
specific log or compiler output, include that output to help others
searching for a solution to the same problem.  If the change is meant to
support other changes coming in later patch, say so.  If internal APIs are
changed, detail those changes and how other developers should respond.  In
general, the more you can put yourself into the shoes of everybody who will
be reading your changelog, the better that changelog (and the kernel as a
whole) will be.

Needless to say, the changelog should be the text used when committing the
change to a revision control system.  It will be followed by:
\begin{itemize}
\item {} 
The patch itself, in the unified (``-u'') patch format.  Using the ``-p''
option to diff will associate function names with changes, making the
resulting patch easier for others to read.

\end{itemize}

You should avoid including changes to irrelevant files (those generated by
the build process, for example, or editor backup files) in the patch.  The
file ``dontdiff'' in the Documentation directory can help in this regard;
pass it to diff with the ``-X'' option.

The tags mentioned above are used to describe how various developers have
been associated with the development of this patch.  They are described in
detail in the process/submitting-patches.rst document; what follows here is a brief
summary.  Each of these lines has the format:

\begin{Verbatim}[commandchars=\\\{\}]
tag: Full Name \PYGZlt{}email address\PYGZgt{}  optional\PYGZhy{}other\PYGZhy{}stuff
\end{Verbatim}

The tags in common use are:
\begin{itemize}
\item {} 
Signed-off-by: this is a developer's certification that he or she has
the right to submit the patch for inclusion into the kernel.  It is an
agreement to the Developer's Certificate of Origin, the full text of
which can be found in Documentation/process/submitting-patches.rst.  Code without a
proper signoff cannot be merged into the mainline.

\item {} 
Co-Developed-by: states that the patch was also created by another developer
along with the original author.  This is useful at times when multiple
people work on a single patch.  Note, this person also needs to have a
Signed-off-by: line in the patch as well.

\item {} 
Acked-by: indicates an agreement by another developer (often a
maintainer of the relevant code) that the patch is appropriate for
inclusion into the kernel.

\item {} 
Tested-by: states that the named person has tested the patch and found
it to work.

\item {} 
Reviewed-by: the named developer has reviewed the patch for correctness;
see the reviewer's statement in Documentation/process/submitting-patches.rst for more
detail.

\item {} 
Reported-by: names a user who reported a problem which is fixed by this
patch; this tag is used to give credit to the (often underappreciated)
people who test our code and let us know when things do not work
correctly.

\item {} 
Cc: the named person received a copy of the patch and had the
opportunity to comment on it.

\end{itemize}

Be careful in the addition of tags to your patches: only Cc: is appropriate
for addition without the explicit permission of the person named.


\subsection{Sending the patch}
\label{process/5.Posting:sending-the-patch}
Before you mail your patches, there are a couple of other things you should
take care of:
\begin{itemize}
\item {} 
Are you sure that your mailer will not corrupt the patches?  Patches
which have had gratuitous white-space changes or line wrapping performed
by the mail client will not apply at the other end, and often will not
be examined in any detail.  If there is any doubt at all, mail the patch
to yourself and convince yourself that it shows up intact.

Documentation/process/email-clients.rst has some helpful hints on making
specific mail clients work for sending patches.

\item {} 
Are you sure your patch is free of silly mistakes?  You should always
run patches through scripts/checkpatch.pl and address the complaints it
comes up with.  Please bear in mind that checkpatch.pl, while being the
embodiment of a fair amount of thought about what kernel patches should
look like, is not smarter than you.  If fixing a checkpatch.pl complaint
would make the code worse, don't do it.

\end{itemize}

Patches should always be sent as plain text.  Please do not send them as
attachments; that makes it much harder for reviewers to quote sections of
the patch in their replies.  Instead, just put the patch directly into your
message.

When mailing patches, it is important to send copies to anybody who might
be interested in it.  Unlike some other projects, the kernel encourages
people to err on the side of sending too many copies; don't assume that the
relevant people will see your posting on the mailing lists.  In particular,
copies should go to:
\begin{itemize}
\item {} 
The maintainer(s) of the affected subsystem(s).  As described earlier,
the MAINTAINERS file is the first place to look for these people.

\item {} 
Other developers who have been working in the same area - especially
those who might be working there now.  Using git to see who else has
modified the files you are working on can be helpful.

\item {} 
If you are responding to a bug report or a feature request, copy the
original poster as well.

\item {} 
Send a copy to the relevant mailing list, or, if nothing else applies,
the linux-kernel list.

\item {} 
If you are fixing a bug, think about whether the fix should go into the
next stable update.  If so, \href{mailto:stable@vger.kernel.org}{stable@vger.kernel.org} should get a copy of
the patch.  Also add a ``Cc: \href{mailto:stable@vger.kernel.org}{stable@vger.kernel.org}'' to the tags within
the patch itself; that will cause the stable team to get a notification
when your fix goes into the mainline.

\end{itemize}

When selecting recipients for a patch, it is good to have an idea of who
you think will eventually accept the patch and get it merged.  While it
is possible to send patches directly to Linus Torvalds and have him merge
them, things are not normally done that way.  Linus is busy, and there are
subsystem maintainers who watch over specific parts of the kernel.  Usually
you will be wanting that maintainer to merge your patches.  If there is no
obvious maintainer, Andrew Morton is often the patch target of last resort.

Patches need good subject lines.  The canonical format for a patch line is
something like:

\begin{Verbatim}[commandchars=\\\{\}]
[PATCH nn/mm] subsys: one\PYGZhy{}line description of the patch
\end{Verbatim}

where ``nn'' is the ordinal number of the patch, ``mm'' is the total number of
patches in the series, and ``subsys'' is the name of the affected subsystem.
Clearly, nn/mm can be omitted for a single, standalone patch.

If you have a significant series of patches, it is customary to send an
introductory description as part zero.  This convention is not universally
followed though; if you use it, remember that information in the
introduction does not make it into the kernel changelogs.  So please ensure
that the patches, themselves, have complete changelog information.

In general, the second and following parts of a multi-part patch should be
sent as a reply to the first part so that they all thread together at the
receiving end.  Tools like git and quilt have commands to mail out a set of
patches with the proper threading.  If you have a long series, though, and
are using git, please stay away from the --chain-reply-to option to avoid
creating exceptionally deep nesting.


\section{Followthrough}
\label{process/6.Followthrough:followthrough}\label{process/6.Followthrough:development-followthrough}\label{process/6.Followthrough::doc}
At this point, you have followed the guidelines given so far and, with the
addition of your own engineering skills, have posted a perfect series of
patches.  One of the biggest mistakes that even experienced kernel
developers can make is to conclude that their work is now done.  In truth,
posting patches indicates a transition into the next stage of the process,
with, possibly, quite a bit of work yet to be done.

It is a rare patch which is so good at its first posting that there is no
room for improvement.  The kernel development process recognizes this fact,
and, as a result, is heavily oriented toward the improvement of posted
code.  You, as the author of that code, will be expected to work with the
kernel community to ensure that your code is up to the kernel's quality
standards.  A failure to participate in this process is quite likely to
prevent the inclusion of your patches into the mainline.


\subsection{Working with reviewers}
\label{process/6.Followthrough:working-with-reviewers}
A patch of any significance will result in a number of comments from other
developers as they review the code.  Working with reviewers can be, for
many developers, the most intimidating part of the kernel development
process.  Life can be made much easier, though, if you keep a few things in
mind:
\begin{itemize}
\item {} 
If you have explained your patch well, reviewers will understand its
value and why you went to the trouble of writing it.  But that value
will not keep them from asking a fundamental question: what will it be
like to maintain a kernel with this code in it five or ten years later?
Many of the changes you may be asked to make - from coding style tweaks
to substantial rewrites - come from the understanding that Linux will
still be around and under development a decade from now.

\item {} 
Code review is hard work, and it is a relatively thankless occupation;
people remember who wrote kernel code, but there is little lasting fame
for those who reviewed it.  So reviewers can get grumpy, especially when
they see the same mistakes being made over and over again.  If you get a
review which seems angry, insulting, or outright offensive, resist the
impulse to respond in kind.  Code review is about the code, not about
the people, and code reviewers are not attacking you personally.

\item {} 
Similarly, code reviewers are not trying to promote their employers'
agendas at the expense of your own.  Kernel developers often expect to
be working on the kernel years from now, but they understand that their
employer could change.  They truly are, almost without exception,
working toward the creation of the best kernel they can; they are not
trying to create discomfort for their employers' competitors.

\end{itemize}

What all of this comes down to is that, when reviewers send you comments,
you need to pay attention to the technical observations that they are
making.  Do not let their form of expression or your own pride keep that
from happening.  When you get review comments on a patch, take the time to
understand what the reviewer is trying to say.  If possible, fix the things
that the reviewer is asking you to fix.  And respond back to the reviewer:
thank them, and describe how you will answer their questions.

Note that you do not have to agree with every change suggested by
reviewers.  If you believe that the reviewer has misunderstood your code,
explain what is really going on.  If you have a technical objection to a
suggested change, describe it and justify your solution to the problem.  If
your explanations make sense, the reviewer will accept them.  Should your
explanation not prove persuasive, though, especially if others start to
agree with the reviewer, take some time to think things over again.  It can
be easy to become blinded by your own solution to a problem to the point
that you don't realize that something is fundamentally wrong or, perhaps,
you're not even solving the right problem.

Andrew Morton has suggested that every review comment which does not result
in a code change should result in an additional code comment instead; that
can help future reviewers avoid the questions which came up the first time
around.

One fatal mistake is to ignore review comments in the hope that they will
go away.  They will not go away.  If you repost code without having
responded to the comments you got the time before, you're likely to find
that your patches go nowhere.

Speaking of reposting code: please bear in mind that reviewers are not
going to remember all the details of the code you posted the last time
around.  So it is always a good idea to remind reviewers of previously
raised issues and how you dealt with them; the patch changelog is a good
place for this kind of information.  Reviewers should not have to search
through list archives to familiarize themselves with what was said last
time; if you help them get a running start, they will be in a better mood
when they revisit your code.

What if you've tried to do everything right and things still aren't going
anywhere?  Most technical disagreements can be resolved through discussion,
but there are times when somebody simply has to make a decision.  If you
honestly believe that this decision is going against you wrongly, you can
always try appealing to a higher power.  As of this writing, that higher
power tends to be Andrew Morton.  Andrew has a great deal of respect in the
kernel development community; he can often unjam a situation which seems to
be hopelessly blocked.  Appealing to Andrew should not be done lightly,
though, and not before all other alternatives have been explored.  And bear
in mind, of course, that he may not agree with you either.


\subsection{What happens next}
\label{process/6.Followthrough:what-happens-next}
If a patch is considered to be a good thing to add to the kernel, and once
most of the review issues have been resolved, the next step is usually
entry into a subsystem maintainer's tree.  How that works varies from one
subsystem to the next; each maintainer has his or her own way of doing
things.  In particular, there may be more than one tree - one, perhaps,
dedicated to patches planned for the next merge window, and another for
longer-term work.

For patches applying to areas for which there is no obvious subsystem tree
(memory management patches, for example), the default tree often ends up
being -mm.  Patches which affect multiple subsystems can also end up going
through the -mm tree.

Inclusion into a subsystem tree can bring a higher level of visibility to a
patch.  Now other developers working with that tree will get the patch by
default.  Subsystem trees typically feed linux-next as well, making their
contents visible to the development community as a whole.  At this point,
there's a good chance that you will get more comments from a new set of
reviewers; these comments need to be answered as in the previous round.

What may also happen at this point, depending on the nature of your patch,
is that conflicts with work being done by others turn up.  In the worst
case, heavy patch conflicts can result in some work being put on the back
burner so that the remaining patches can be worked into shape and merged.
Other times, conflict resolution will involve working with the other
developers and, possibly, moving some patches between trees to ensure that
everything applies cleanly.  This work can be a pain, but count your
blessings: before the advent of the linux-next tree, these conflicts often
only turned up during the merge window and had to be addressed in a hurry.
Now they can be resolved at leisure, before the merge window opens.

Some day, if all goes well, you'll log on and see that your patch has been
merged into the mainline kernel.  Congratulations!  Once the celebration is
complete (and you have added yourself to the MAINTAINERS file), though, it
is worth remembering an important little fact: the job still is not done.
Merging into the mainline brings its own challenges.

To begin with, the visibility of your patch has increased yet again.  There
may be a new round of comments from developers who had not been aware of
the patch before.  It may be tempting to ignore them, since there is no
longer any question of your code being merged.  Resist that temptation,
though; you still need to be responsive to developers who have questions or
suggestions.

More importantly, though: inclusion into the mainline puts your code into
the hands of a much larger group of testers.  Even if you have contributed
a driver for hardware which is not yet available, you will be surprised by
how many people will build your code into their kernels.  And, of course,
where there are testers, there will be bug reports.

The worst sort of bug reports are regressions.  If your patch causes a
regression, you'll find an uncomfortable number of eyes upon you;
regressions need to be fixed as soon as possible.  If you are unwilling or
unable to fix the regression (and nobody else does it for you), your patch
will almost certainly be removed during the stabilization period.  Beyond
negating all of the work you have done to get your patch into the mainline,
having a patch pulled as the result of a failure to fix a regression could
well make it harder for you to get work merged in the future.

After any regressions have been dealt with, there may be other, ordinary
bugs to deal with.  The stabilization period is your best opportunity to
fix these bugs and ensure that your code's debut in a mainline kernel
release is as solid as possible.  So, please, answer bug reports, and fix
the problems if at all possible.  That's what the stabilization period is
for; you can start creating cool new patches once any problems with the old
ones have been taken care of.

And don't forget that there are other milestones which may also create bug
reports: the next mainline stable release, when prominent distributors pick
up a version of the kernel containing your patch, etc.  Continuing to
respond to these reports is a matter of basic pride in your work.  If that
is insufficient motivation, though, it's also worth considering that the
development community remembers developers who lose interest in their code
after it's merged.  The next time you post a patch, they will be evaluating
it with the assumption that you will not be around to maintain it
afterward.


\subsection{Other things that can happen}
\label{process/6.Followthrough:other-things-that-can-happen}
One day, you may open your mail client and see that somebody has mailed you
a patch to your code.  That is one of the advantages of having your code
out there in the open, after all.  If you agree with the patch, you can
either forward it on to the subsystem maintainer (be sure to include a
proper From: line so that the attribution is correct, and add a signoff of
your own), or send an Acked-by: response back and let the original poster
send it upward.

If you disagree with the patch, send a polite response explaining why.  If
possible, tell the author what changes need to be made to make the patch
acceptable to you.  There is a certain resistance to merging patches which
are opposed by the author and maintainer of the code, but it only goes so
far.  If you are seen as needlessly blocking good work, those patches will
eventually flow around you and get into the mainline anyway.  In the Linux
kernel, nobody has absolute veto power over any code.  Except maybe Linus.

On very rare occasion, you may see something completely different: another
developer posts a different solution to your problem.  At that point,
chances are that one of the two patches will not be merged, and ``mine was
here first'' is not considered to be a compelling technical argument.  If
somebody else's patch displaces yours and gets into the mainline, there is
really only one way to respond: be pleased that your problem got solved and
get on with your work.  Having one's work shoved aside in this manner can
be hurtful and discouraging, but the community will remember your reaction
long after they have forgotten whose patch actually got merged.


\section{Advanced topics}
\label{process/7.AdvancedTopics:development-advancedtopics}\label{process/7.AdvancedTopics:advanced-topics}\label{process/7.AdvancedTopics::doc}
At this point, hopefully, you have a handle on how the development process
works.  There is still more to learn, however!  This section will cover a
number of topics which can be helpful for developers wanting to become a
regular part of the Linux kernel development process.


\subsection{Managing patches with git}
\label{process/7.AdvancedTopics:managing-patches-with-git}
The use of distributed version control for the kernel began in early 2002,
when Linus first started playing with the proprietary BitKeeper
application.  While BitKeeper was controversial, the approach to software
version management it embodied most certainly was not.  Distributed version
control enabled an immediate acceleration of the kernel development
project.  In current times, there are several free alternatives to
BitKeeper.  For better or for worse, the kernel project has settled on git
as its tool of choice.

Managing patches with git can make life much easier for the developer,
especially as the volume of those patches grows.  Git also has its rough
edges and poses certain hazards; it is a young and powerful tool which is
still being civilized by its developers.  This document will not attempt to
teach the reader how to use git; that would be sufficient material for a
long document in its own right.  Instead, the focus here will be on how git
fits into the kernel development process in particular.  Developers who
wish to come up to speed with git will find more information at:
\begin{quote}

\href{http://git-scm.com/}{http://git-scm.com/}

\href{http://www.kernel.org/pub/software/scm/git/docs/user-manual.html}{http://www.kernel.org/pub/software/scm/git/docs/user-manual.html}
\end{quote}

and on various tutorials found on the web.

The first order of business is to read the above sites and get a solid
understanding of how git works before trying to use it to make patches
available to others.  A git-using developer should be able to obtain a copy
of the mainline repository, explore the revision history, commit changes to
the tree, use branches, etc.  An understanding of git's tools for the
rewriting of history (such as rebase) is also useful.  Git comes with its
own terminology and concepts; a new user of git should know about refs,
remote branches, the index, fast-forward merges, pushes and pulls, detached
heads, etc.  It can all be a little intimidating at the outset, but the
concepts are not that hard to grasp with a bit of study.

Using git to generate patches for submission by email can be a good
exercise while coming up to speed.

When you are ready to start putting up git trees for others to look at, you
will, of course, need a server that can be pulled from.  Setting up such a
server with git-daemon is relatively straightforward if you have a system
which is accessible to the Internet.  Otherwise, free, public hosting sites
(Github, for example) are starting to appear on the net.  Established
developers can get an account on kernel.org, but those are not easy to come
by; see \href{http://kernel.org/faq/}{http://kernel.org/faq/} for more information.

The normal git workflow involves the use of a lot of branches.  Each line
of development can be separated into a separate ``topic branch'' and
maintained independently.  Branches in git are cheap, there is no reason to
not make free use of them.  And, in any case, you should not do your
development in any branch which you intend to ask others to pull from.
Publicly-available branches should be created with care; merge in patches
from development branches when they are in complete form and ready to go -
not before.

Git provides some powerful tools which can allow you to rewrite your
development history.  An inconvenient patch (one which breaks bisection,
say, or which has some other sort of obvious bug) can be fixed in place or
made to disappear from the history entirely.  A patch series can be
rewritten as if it had been written on top of today's mainline, even though
you have been working on it for months.  Changes can be transparently
shifted from one branch to another.  And so on.  Judicious use of git's
ability to revise history can help in the creation of clean patch sets with
fewer problems.

Excessive use of this capability can lead to other problems, though, beyond
a simple obsession for the creation of the perfect project history.
Rewriting history will rewrite the changes contained in that history,
turning a tested (hopefully) kernel tree into an untested one.  But, beyond
that, developers cannot easily collaborate if they do not have a shared
view of the project history; if you rewrite history which other developers
have pulled into their repositories, you will make life much more difficult
for those developers.  So a simple rule of thumb applies here: history
which has been exported to others should generally be seen as immutable
thereafter.

So, once you push a set of changes to your publicly-available server, those
changes should not be rewritten.  Git will attempt to enforce this rule if
you try to push changes which do not result in a fast-forward merge
(i.e. changes which do not share the same history).  It is possible to
override this check, and there may be times when it is necessary to rewrite
an exported tree.  Moving changesets between trees to avoid conflicts in
linux-next is one example.  But such actions should be rare.  This is one
of the reasons why development should be done in private branches (which
can be rewritten if necessary) and only moved into public branches when
it's in a reasonably advanced state.

As the mainline (or other tree upon which a set of changes is based)
advances, it is tempting to merge with that tree to stay on the leading
edge.  For a private branch, rebasing can be an easy way to keep up with
another tree, but rebasing is not an option once a tree is exported to the
world.  Once that happens, a full merge must be done.  Merging occasionally
makes good sense, but overly frequent merges can clutter the history
needlessly.  Suggested technique in this case is to merge infrequently, and
generally only at specific release points (such as a mainline -rc
release).  If you are nervous about specific changes, you can always
perform test merges in a private branch.  The git ``rerere'' tool can be
useful in such situations; it remembers how merge conflicts were resolved
so that you don't have to do the same work twice.

One of the biggest recurring complaints about tools like git is this: the
mass movement of patches from one repository to another makes it easy to
slip in ill-advised changes which go into the mainline below the review
radar.  Kernel developers tend to get unhappy when they see that kind of
thing happening; putting up a git tree with unreviewed or off-topic patches
can affect your ability to get trees pulled in the future.  Quoting Linus:

\begin{Verbatim}[commandchars=\\\{\}]
You can send me patches, but for me to pull a git patch from you, I
need to know that you know what you\PYGZsq{}re doing, and I need to be able
to trust things *without* then having to go and check every
individual change by hand.
\end{Verbatim}

(\href{http://lwn.net/Articles/224135/}{http://lwn.net/Articles/224135/}).

To avoid this kind of situation, ensure that all patches within a given
branch stick closely to the associated topic; a ``driver fixes'' branch
should not be making changes to the core memory management code.  And, most
importantly, do not use a git tree to bypass the review process.  Post an
occasional summary of the tree to the relevant list, and, when the time is
right, request that the tree be included in linux-next.

If and when others start to send patches for inclusion into your tree,
don't forget to review them.  Also ensure that you maintain the correct
authorship information; the git ``am'' tool does its best in this regard, but
you may have to add a ``From:'' line to the patch if it has been relayed to
you via a third party.

When requesting a pull, be sure to give all the relevant information: where
your tree is, what branch to pull, and what changes will result from the
pull.  The git request-pull command can be helpful in this regard; it will
format the request as other developers expect, and will also check to be
sure that you have remembered to push those changes to the public server.


\subsection{Reviewing patches}
\label{process/7.AdvancedTopics:reviewing-patches}
Some readers will certainly object to putting this section with ``advanced
topics'' on the grounds that even beginning kernel developers should be
reviewing patches.  It is certainly true that there is no better way to
learn how to program in the kernel environment than by looking at code
posted by others.  In addition, reviewers are forever in short supply; by
looking at code you can make a significant contribution to the process as a
whole.

Reviewing code can be an intimidating prospect, especially for a new kernel
developer who may well feel nervous about questioning code - in public -
which has been posted by those with more experience.  Even code written by
the most experienced developers can be improved, though.  Perhaps the best
piece of advice for reviewers (all reviewers) is this: phrase review
comments as questions rather than criticisms.  Asking ``how does the lock
get released in this path?'' will always work better than stating ``the
locking here is wrong.''

Different developers will review code from different points of view.  Some
are mostly concerned with coding style and whether code lines have trailing
white space.  Others will focus primarily on whether the change implemented
by the patch as a whole is a good thing for the kernel or not.  Yet others
will check for problematic locking, excessive stack usage, possible
security issues, duplication of code found elsewhere, adequate
documentation, adverse effects on performance, user-space ABI changes, etc.
All types of review, if they lead to better code going into the kernel, are
welcome and worthwhile.


\section{For more information}
\label{process/8.Conclusion:for-more-information}\label{process/8.Conclusion::doc}\label{process/8.Conclusion:development-conclusion}
There are numerous sources of information on Linux kernel development and
related topics.  First among those will always be the Documentation
directory found in the kernel source distribution.  The top-level process/howto.rst
file is an important starting point; process/submitting-patches.rst and
process/submitting-drivers.rst are also something which all kernel developers should
read.  Many internal kernel APIs are documented using the kerneldoc
mechanism; ``make htmldocs'' or ``make pdfdocs'' can be used to generate those
documents in HTML or PDF format (though the version of TeX shipped by some
distributions runs into internal limits and fails to process the documents
properly).

Various web sites discuss kernel development at all levels of detail.  Your
author would like to humbly suggest \href{http://lwn.net/}{http://lwn.net/} as a source;
information on many specific kernel topics can be found via the LWN kernel
index at:
\begin{quote}

\href{http://lwn.net/Kernel/Index/}{http://lwn.net/Kernel/Index/}
\end{quote}

Beyond that, a valuable resource for kernel developers is:
\begin{quote}

\href{http://kernelnewbies.org/}{http://kernelnewbies.org/}
\end{quote}

And, of course, one should not forget \href{http://kernel.org/}{http://kernel.org/}, the definitive
location for kernel release information.

There are a number of books on kernel development:
\begin{quote}

Linux Device Drivers, 3rd Edition (Jonathan Corbet, Alessandro
Rubini, and Greg Kroah-Hartman).  Online at
\href{http://lwn.net/Kernel/LDD3/}{http://lwn.net/Kernel/LDD3/}.

Linux Kernel Development (Robert Love).

Understanding the Linux Kernel (Daniel Bovet and Marco Cesati).
\end{quote}

All of these books suffer from a common fault, though: they tend to be
somewhat obsolete by the time they hit the shelves, and they have been on
the shelves for a while now.  Still, there is quite a bit of good
information to be found there.

Documentation for git can be found at:
\begin{quote}

\href{http://www.kernel.org/pub/software/scm/git/docs/}{http://www.kernel.org/pub/software/scm/git/docs/}

\href{http://www.kernel.org/pub/software/scm/git/docs/user-manual.html}{http://www.kernel.org/pub/software/scm/git/docs/user-manual.html}
\end{quote}


\section{Conclusion}
\label{process/8.Conclusion:conclusion}
Congratulations to anybody who has made it through this long-winded
document.  Hopefully it has provided a helpful understanding of how the
Linux kernel is developed and how you can participate in that process.

In the end, it's the participation that matters.  Any open source software
project is no more than the sum of what its contributors put into it.  The
Linux kernel has progressed as quickly and as well as it has because it has
been helped by an impressively large group of developers, all of whom are
working to make it better.  The kernel is a premier example of what can be
done when thousands of people work together toward a common goal.

The kernel can always benefit from a larger developer base, though.  There
is always more work to do.  But, just as importantly, most other
participants in the Linux ecosystem can benefit through contributing to the
kernel.  Getting code into the mainline is the key to higher code quality,
lower maintenance and distribution costs, a higher level of influence over
the direction of kernel development, and more.  It is a situation where
everybody involved wins.  Fire up your editor and come join us; you will be
more than welcome.

The purpose of this document is to help developers (and their managers)
work with the development community with a minimum of frustration.  It is
an attempt to document how this community works in a way which is
accessible to those who are not intimately familiar with Linux kernel
development (or, indeed, free software development in general).  While
there is some technical material here, this is very much a process-oriented
discussion which does not require a deep knowledge of kernel programming to
understand.


\chapter{Submitting patches: the essential guide to getting your code into the kernel}
\label{process/submitting-patches:submitting-patches-the-essential-guide-to-getting-your-code-into-the-kernel}\label{process/submitting-patches::doc}\label{process/submitting-patches:submittingpatches}
For a person or company who wishes to submit a change to the Linux
kernel, the process can sometimes be daunting if you're not familiar
with ``the system.''  This text is a collection of suggestions which
can greatly increase the chances of your change being accepted.

This document contains a large number of suggestions in a relatively terse
format.  For detailed information on how the kernel development process
works, see {\hyperref[process/development\string-process:development\string-process\string-main]{\emph{Documentation/process}}}.
Also, read {\hyperref[process/submit\string-checklist:submitchecklist]{\emph{Documentation/process/submit-checklist.rst}}}
for a list of items to check before
submitting code.  If you are submitting a driver, also read
{\hyperref[process/submitting\string-drivers:submittingdrivers]{\emph{Documentation/process/submitting-drivers.rst}}};
for device tree binding patches, read
Documentation/devicetree/bindings/submitting-patches.txt.

Many of these steps describe the default behavior of the \code{git} version
control system; if you use \code{git} to prepare your patches, you'll find much
of the mechanical work done for you, though you'll still need to prepare
and document a sensible set of patches.  In general, use of \code{git} will make
your life as a kernel developer easier.


\section{0) Obtain a current source tree}
\label{process/submitting-patches:obtain-a-current-source-tree}
If you do not have a repository with the current kernel source handy, use
\code{git} to obtain one.  You'll want to start with the mainline repository,
which can be grabbed with:

\begin{Verbatim}[commandchars=\\\{\}]
git clone git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
\end{Verbatim}

Note, however, that you may not want to develop against the mainline tree
directly.  Most subsystem maintainers run their own trees and want to see
patches prepared against those trees.  See the \textbf{T:} entry for the subsystem
in the MAINTAINERS file to find that tree, or simply ask the maintainer if
the tree is not listed there.

It is still possible to download kernel releases via tarballs (as described
in the next section), but that is the hard way to do kernel development.


\section{1) \texttt{diff -up}}
\label{process/submitting-patches:diff-up}
If you must generate your patches by hand, use \code{diff -up} or \code{diff -uprN}
to create patches.  Git generates patches in this form by default; if
you're using \code{git}, you can skip this section entirely.

All changes to the Linux kernel occur in the form of patches, as
generated by \emph{\texttt{diff(1)}}.  When creating your patch, make sure to
create it in ``unified diff'' format, as supplied by the \code{-u} argument
to \emph{\texttt{diff(1)}}.
Also, please use the \code{-p} argument which shows which C function each
change is in - that makes the resultant \code{diff} a lot easier to read.
Patches should be based in the root kernel source directory,
not in any lower subdirectory.

To create a patch for a single file, it is often sufficient to do:

\begin{Verbatim}[commandchars=\\\{\}]
SRCTREE= linux
MYFILE=  drivers/net/mydriver.c

cd \PYGZdl{}SRCTREE
cp \PYGZdl{}MYFILE \PYGZdl{}MYFILE.orig
vi \PYGZdl{}MYFILE      \PYGZsh{} make your change
cd ..
diff \PYGZhy{}up \PYGZdl{}SRCTREE/\PYGZdl{}MYFILE\PYGZob{}.orig,\PYGZcb{} \PYGZgt{} /tmp/patch
\end{Verbatim}

To create a patch for multiple files, you should unpack a ``vanilla'',
or unmodified kernel source tree, and generate a \code{diff} against your
own source tree.  For example:

\begin{Verbatim}[commandchars=\\\{\}]
MYSRC= /devel/linux

tar xvfz linux\PYGZhy{}3.19.tar.gz
mv linux\PYGZhy{}3.19 linux\PYGZhy{}3.19\PYGZhy{}vanilla
diff \PYGZhy{}uprN \PYGZhy{}X linux\PYGZhy{}3.19\PYGZhy{}vanilla/Documentation/dontdiff \PYGZbs{}
        linux\PYGZhy{}3.19\PYGZhy{}vanilla \PYGZdl{}MYSRC \PYGZgt{} /tmp/patch
\end{Verbatim}

\code{dontdiff} is a list of files which are generated by the kernel during
the build process, and should be ignored in any \emph{\texttt{diff(1)}}-generated
patch.

Make sure your patch does not include any extra files which do not
belong in a patch submission.  Make sure to review your patch -after-
generating it with \emph{\texttt{diff(1)}}, to ensure accuracy.

If your changes produce a lot of deltas, you need to split them into
individual patches which modify things in logical stages; see
{\hyperref[process/submitting\string-patches:split\string-changes]{\emph{3) Separate your changes}}}.  This will facilitate review by other kernel developers,
very important if you want your patch accepted.

If you're using \code{git}, \code{git rebase -i} can help you with this process.  If
you're not using \code{git}, \code{quilt} \textless{}\href{http://savannah.nongnu.org/projects/quilt}{http://savannah.nongnu.org/projects/quilt}\textgreater{}
is another popular alternative.


\section{2) Describe your changes}
\label{process/submitting-patches:describe-your-changes}\label{process/submitting-patches:describe-changes}
Describe your problem.  Whether your patch is a one-line bug fix or
5000 lines of a new feature, there must be an underlying problem that
motivated you to do this work.  Convince the reviewer that there is a
problem worth fixing and that it makes sense for them to read past the
first paragraph.

Describe user-visible impact.  Straight up crashes and lockups are
pretty convincing, but not all bugs are that blatant.  Even if the
problem was spotted during code review, describe the impact you think
it can have on users.  Keep in mind that the majority of Linux
installations run kernels from secondary stable trees or
vendor/product-specific trees that cherry-pick only specific patches
from upstream, so include anything that could help route your change
downstream: provoking circumstances, excerpts from dmesg, crash
descriptions, performance regressions, latency spikes, lockups, etc.

Quantify optimizations and trade-offs.  If you claim improvements in
performance, memory consumption, stack footprint, or binary size,
include numbers that back them up.  But also describe non-obvious
costs.  Optimizations usually aren't free but trade-offs between CPU,
memory, and readability; or, when it comes to heuristics, between
different workloads.  Describe the expected downsides of your
optimization so that the reviewer can weigh costs against benefits.

Once the problem is established, describe what you are actually doing
about it in technical detail.  It's important to describe the change
in plain English for the reviewer to verify that the code is behaving
as you intend it to.

The maintainer will thank you if you write your patch description in a
form which can be easily pulled into Linux's source code management
system, \code{git}, as a ``commit log''.  See {\hyperref[process/submitting\string-patches:explicit\string-in\string-reply\string-to]{\emph{15) Explicit In-Reply-To headers}}}.

Solve only one problem per patch.  If your description starts to get
long, that's a sign that you probably need to split up your patch.
See {\hyperref[process/submitting\string-patches:split\string-changes]{\emph{3) Separate your changes}}}.

When you submit or resubmit a patch or patch series, include the
complete patch description and justification for it.  Don't just
say that this is version N of the patch (series).  Don't expect the
subsystem maintainer to refer back to earlier patch versions or referenced
URLs to find the patch description and put that into the patch.
I.e., the patch (series) and its description should be self-contained.
This benefits both the maintainers and reviewers.  Some reviewers
probably didn't even receive earlier versions of the patch.

Describe your changes in imperative mood, e.g. ``make xyzzy do frotz''
instead of ``{[}This patch{]} makes xyzzy do frotz'' or ``{[}I{]} changed xyzzy
to do frotz'', as if you are giving orders to the codebase to change
its behaviour.

If the patch fixes a logged bug entry, refer to that bug entry by
number and URL.  If the patch follows from a mailing list discussion,
give a URL to the mailing list archive; use the \href{https://lkml.kernel.org/}{https://lkml.kernel.org/}
redirector with a \code{Message-Id}, to ensure that the links cannot become
stale.

However, try to make your explanation understandable without external
resources.  In addition to giving a URL to a mailing list archive or
bug, summarize the relevant points of the discussion that led to the
patch as submitted.

If you want to refer to a specific commit, don't just refer to the
SHA-1 ID of the commit. Please also include the oneline summary of
the commit, to make it easier for reviewers to know what it is about.
Example:

\begin{Verbatim}[commandchars=\\\{\}]
Commit e21d2170f36602ae2708 (\PYGZdq{}video: remove unnecessary
platform\PYGZus{}set\PYGZus{}drvdata()\PYGZdq{}) removed the unnecessary
platform\PYGZus{}set\PYGZus{}drvdata(), but left the variable \PYGZdq{}dev\PYGZdq{} unused,
delete it.
\end{Verbatim}

You should also be sure to use at least the first twelve characters of the
SHA-1 ID.  The kernel repository holds a \emph{lot} of objects, making
collisions with shorter IDs a real possibility.  Bear in mind that, even if
there is no collision with your six-character ID now, that condition may
change five years from now.

If your patch fixes a bug in a specific commit, e.g. you found an issue using
\code{git bisect}, please use the `Fixes:' tag with the first 12 characters of
the SHA-1 ID, and the one line summary.  For example:

\begin{Verbatim}[commandchars=\\\{\}]
Fixes: e21d2170f366 (\PYGZdq{}video: remove unnecessary platform\PYGZus{}set\PYGZus{}drvdata()\PYGZdq{})
\end{Verbatim}

The following \code{git config} settings can be used to add a pretty format for
outputting the above style in the \code{git log} or \code{git show} commands:

\begin{Verbatim}[commandchars=\\\{\}]
[core]
        abbrev = 12
[pretty]
        fixes = Fixes: \PYGZpc{}h (\PYGZbs{}\PYGZdq{}\PYGZpc{}s\PYGZbs{}\PYGZdq{})
\end{Verbatim}


\section{3) Separate your changes}
\label{process/submitting-patches:separate-your-changes}\label{process/submitting-patches:split-changes}
Separate each \textbf{logical change} into a separate patch.

For example, if your changes include both bug fixes and performance
enhancements for a single driver, separate those changes into two
or more patches.  If your changes include an API update, and a new
driver which uses that new API, separate those into two patches.

On the other hand, if you make a single change to numerous files,
group those changes into a single patch.  Thus a single logical change
is contained within a single patch.

The point to remember is that each patch should make an easily understood
change that can be verified by reviewers.  Each patch should be justifiable
on its own merits.

If one patch depends on another patch in order for a change to be
complete, that is OK.  Simply note \textbf{``this patch depends on patch X''}
in your patch description.

When dividing your change into a series of patches, take special care to
ensure that the kernel builds and runs properly after each patch in the
series.  Developers using \code{git bisect} to track down a problem can end up
splitting your patch series at any point; they will not thank you if you
introduce bugs in the middle.

If you cannot condense your patch set into a smaller set of patches,
then only post say 15 or so at a time and wait for review and integration.


\section{4) Style-check your changes}
\label{process/submitting-patches:style-check-your-changes}
Check your patch for basic style violations, details of which can be
found in
{\hyperref[process/coding\string-style:codingstyle]{\emph{Documentation/process/coding-style.rst}}}.
Failure to do so simply wastes
the reviewers time and will get your patch rejected, probably
without even being read.

One significant exception is when moving code from one file to
another -- in this case you should not modify the moved code at all in
the same patch which moves it.  This clearly delineates the act of
moving the code and your changes.  This greatly aids review of the
actual differences and allows tools to better track the history of
the code itself.

Check your patches with the patch style checker prior to submission
(scripts/checkpatch.pl).  Note, though, that the style checker should be
viewed as a guide, not as a replacement for human judgment.  If your code
looks better with a violation then its probably best left alone.
\begin{description}
\item[{The checker reports at three levels:}] \leavevmode\begin{itemize}
\item {} 
ERROR: things that are very likely to be wrong

\item {} 
WARNING: things requiring careful review

\item {} 
CHECK: things requiring thought

\end{itemize}

\end{description}

You should be able to justify all violations that remain in your
patch.


\section{5) Select the recipients for your patch}
\label{process/submitting-patches:select-the-recipients-for-your-patch}
You should always copy the appropriate subsystem maintainer(s) on any patch
to code that they maintain; look through the MAINTAINERS file and the
source code revision history to see who those maintainers are.  The
script scripts/get\_maintainer.pl can be very useful at this step.  If you
cannot find a maintainer for the subsystem you are working on, Andrew
Morton (\href{mailto:akpm@linux-foundation.org}{akpm@linux-foundation.org}) serves as a maintainer of last resort.

You should also normally choose at least one mailing list to receive a copy
of your patch set.  \href{mailto:linux-kernel@vger.kernel.org}{linux-kernel@vger.kernel.org} functions as a list of
last resort, but the volume on that list has caused a number of developers
to tune it out.  Look in the MAINTAINERS file for a subsystem-specific
list; your patch will probably get more attention there.  Please do not
spam unrelated lists, though.

Many kernel-related lists are hosted on vger.kernel.org; you can find a
list of them at \href{http://vger.kernel.org/vger-lists.html}{http://vger.kernel.org/vger-lists.html}.  There are
kernel-related lists hosted elsewhere as well, though.

Do not send more than 15 patches at once to the vger mailing lists!!!

Linus Torvalds is the final arbiter of all changes accepted into the
Linux kernel.  His e-mail address is \textless{}\href{mailto:torvalds@linux-foundation.org}{torvalds@linux-foundation.org}\textgreater{}.
He gets a lot of e-mail, and, at this point, very few patches go through
Linus directly, so typically you should do your best to -avoid-
sending him e-mail.

If you have a patch that fixes an exploitable security bug, send that patch
to \href{mailto:security@kernel.org}{security@kernel.org}.  For severe bugs, a short embargo may be considered
to allow distributors to get the patch out to users; in such cases,
obviously, the patch should not be sent to any public lists.

Patches that fix a severe bug in a released kernel should be directed
toward the stable maintainers by putting a line like this:

\begin{Verbatim}[commandchars=\\\{\}]
Cc: stable@vger.kernel.org
\end{Verbatim}

into the sign-off area of your patch (note, NOT an email recipient).  You
should also read
{\hyperref[process/stable\string-kernel\string-rules:stable\string-kernel\string-rules]{\emph{Documentation/process/stable-kernel-rules.rst}}}
in addition to this file.

Note, however, that some subsystem maintainers want to come to their own
conclusions on which patches should go to the stable trees.  The networking
maintainer, in particular, would rather not see individual developers
adding lines like the above to their patches.

If changes affect userland-kernel interfaces, please send the MAN-PAGES
maintainer (as listed in the MAINTAINERS file) a man-pages patch, or at
least a notification of the change, so that some information makes its way
into the manual pages.  User-space API changes should also be copied to
\href{mailto:linux-api@vger.kernel.org}{linux-api@vger.kernel.org}.

For small patches you may want to CC the Trivial Patch Monkey
\href{mailto:trivial@kernel.org}{trivial@kernel.org} which collects ``trivial'' patches. Have a look
into the MAINTAINERS file for its current manager.

Trivial patches must qualify for one of the following rules:
\begin{itemize}
\item {} 
Spelling fixes in documentation

\item {} 
Spelling fixes for errors which could break \emph{\texttt{grep(1)}}

\item {} 
Warning fixes (cluttering with useless warnings is bad)

\item {} 
Compilation fixes (only if they are actually correct)

\item {} 
Runtime fixes (only if they actually fix things)

\item {} 
Removing use of deprecated functions/macros

\item {} 
Contact detail and documentation fixes

\item {} 
Non-portable code replaced by portable code (even in arch-specific,
since people copy, as long as it's trivial)

\item {} 
Any fix by the author/maintainer of the file (ie. patch monkey
in re-transmission mode)

\end{itemize}


\section{6) No MIME, no links, no compression, no attachments.  Just plain text}
\label{process/submitting-patches:no-mime-no-links-no-compression-no-attachments-just-plain-text}
Linus and other kernel developers need to be able to read and comment
on the changes you are submitting.  It is important for a kernel
developer to be able to ``quote'' your changes, using standard e-mail
tools, so that they may comment on specific portions of your code.

For this reason, all patches should be submitted by e-mail ``inline''.

\begin{notice}{warning}{Warning:}
Be wary of your editor's word-wrap corrupting your patch,
if you choose to cut-n-paste your patch.
\end{notice}

Do not attach the patch as a MIME attachment, compressed or not.
Many popular e-mail applications will not always transmit a MIME
attachment as plain text, making it impossible to comment on your
code.  A MIME attachment also takes Linus a bit more time to process,
decreasing the likelihood of your MIME-attached change being accepted.

Exception:  If your mailer is mangling patches then someone may ask
you to re-send them using MIME.

See {\hyperref[process/email\string-clients:email\string-clients]{\emph{Documentation/process/email-clients.rst}}}
for hints about configuring your e-mail client so that it sends your patches
untouched.


\section{7) E-mail size}
\label{process/submitting-patches:e-mail-size}
Large changes are not appropriate for mailing lists, and some
maintainers.  If your patch, uncompressed, exceeds 300 kB in size,
it is preferred that you store your patch on an Internet-accessible
server, and provide instead a URL (link) pointing to your patch.  But note
that if your patch exceeds 300 kB, it almost certainly needs to be broken up
anyway.


\section{8) Respond to review comments}
\label{process/submitting-patches:respond-to-review-comments}
Your patch will almost certainly get comments from reviewers on ways in
which the patch can be improved.  You must respond to those comments;
ignoring reviewers is a good way to get ignored in return.  Review comments
or questions that do not lead to a code change should almost certainly
bring about a comment or changelog entry so that the next reviewer better
understands what is going on.

Be sure to tell the reviewers what changes you are making and to thank them
for their time.  Code review is a tiring and time-consuming process, and
reviewers sometimes get grumpy.  Even in that case, though, respond
politely and address the problems they have pointed out.


\section{9) Don't get discouraged - or impatient}
\label{process/submitting-patches:don-t-get-discouraged-or-impatient}
After you have submitted your change, be patient and wait.  Reviewers are
busy people and may not get to your patch right away.

Once upon a time, patches used to disappear into the void without comment,
but the development process works more smoothly than that now.  You should
receive comments within a week or so; if that does not happen, make sure
that you have sent your patches to the right place.  Wait for a minimum of
one week before resubmitting or pinging reviewers - possibly longer during
busy times like merge windows.


\section{10) Include PATCH in the subject}
\label{process/submitting-patches:include-patch-in-the-subject}
Due to high e-mail traffic to Linus, and to linux-kernel, it is common
convention to prefix your subject line with {[}PATCH{]}.  This lets Linus
and other kernel developers more easily distinguish patches from other
e-mail discussions.


\section{11) Sign your work - the Developer's Certificate of Origin}
\label{process/submitting-patches:sign-your-work-the-developer-s-certificate-of-origin}
To improve tracking of who did what, especially with patches that can
percolate to their final resting place in the kernel through several
layers of maintainers, we've introduced a ``sign-off'' procedure on
patches that are being emailed around.

The sign-off is a simple line at the end of the explanation for the
patch, which certifies that you wrote it or otherwise have the right to
pass it on as an open-source patch.  The rules are pretty simple: if you
can certify the below:


\subsection{Developer's Certificate of Origin 1.1}
\label{process/submitting-patches:developer-s-certificate-of-origin-1-1}
By making a contribution to this project, I certify that:
\begin{enumerate}
\item {} 
The contribution was created in whole or in part by me and I
have the right to submit it under the open source license
indicated in the file; or

\item {} 
The contribution is based upon previous work that, to the best
of my knowledge, is covered under an appropriate open source
license and I have the right under that license to submit that
work with modifications, whether created in whole or in part
by me, under the same open source license (unless I am
permitted to submit under a different license), as indicated
in the file; or

\item {} 
The contribution was provided directly to me by some other
person who certified (a), (b) or (c) and I have not modified
it.

\item {} 
I understand and agree that this project and the contribution
are public and that a record of the contribution (including all
personal information I submit with it, including my sign-off) is
maintained indefinitely and may be redistributed consistent with
this project or the open source license(s) involved.

\end{enumerate}

then you just add a line saying:

\begin{Verbatim}[commandchars=\\\{\}]
Signed\PYGZhy{}off\PYGZhy{}by: Random J Developer \PYGZlt{}random@developer.example.org\PYGZgt{}
\end{Verbatim}

using your real name (sorry, no pseudonyms or anonymous contributions.)

Some people also put extra tags at the end.  They'll just be ignored for
now, but you can do this to mark internal company procedures or just
point out some special detail about the sign-off.

If you are a subsystem or branch maintainer, sometimes you need to slightly
modify patches you receive in order to merge them, because the code is not
exactly the same in your tree and the submitters'. If you stick strictly to
rule (c), you should ask the submitter to rediff, but this is a totally
counter-productive waste of time and energy. Rule (b) allows you to adjust
the code, but then it is very impolite to change one submitter's code and
make him endorse your bugs. To solve this problem, it is recommended that
you add a line between the last Signed-off-by header and yours, indicating
the nature of your changes. While there is nothing mandatory about this, it
seems like prepending the description with your mail and/or name, all
enclosed in square brackets, is noticeable enough to make it obvious that
you are responsible for last-minute changes. Example:

\begin{Verbatim}[commandchars=\\\{\}]
Signed\PYGZhy{}off\PYGZhy{}by: Random J Developer \PYGZlt{}random@developer.example.org\PYGZgt{}
[lucky@maintainer.example.org: struct foo moved from foo.c to foo.h]
Signed\PYGZhy{}off\PYGZhy{}by: Lucky K Maintainer \PYGZlt{}lucky@maintainer.example.org\PYGZgt{}
\end{Verbatim}

This practice is particularly helpful if you maintain a stable branch and
want at the same time to credit the author, track changes, merge the fix,
and protect the submitter from complaints. Note that under no circumstances
can you change the author's identity (the From header), as it is the one
which appears in the changelog.

Special note to back-porters: It seems to be a common and useful practice
to insert an indication of the origin of a patch at the top of the commit
message (just after the subject line) to facilitate tracking. For instance,
here's what we see in a 3.x-stable release:

\begin{Verbatim}[commandchars=\\\{\}]
Date:   Tue Oct 7 07:26:38 2014 \PYGZhy{}0400

  libata: Un\PYGZhy{}break ATA blacklist

  commit 1c40279960bcd7d52dbdf1d466b20d24b99176c8 upstream.
\end{Verbatim}

And here's what might appear in an older kernel once a patch is backported:

\begin{Verbatim}[commandchars=\\\{\}]
Date:   Tue May 13 22:12:27 2008 +0200

    wireless, airo: waitbusy() won\PYGZsq{}t delay

    [backport of 2.6 commit b7acbdfbd1f277c1eb23f344f899cfa4cd0bf36a]
\end{Verbatim}

Whatever the format, this information provides a valuable help to people
tracking your trees, and to people trying to troubleshoot bugs in your
tree.


\section{12) When to use Acked-by: and Cc:}
\label{process/submitting-patches:when-to-use-acked-by-and-cc}
The Signed-off-by: tag indicates that the signer was involved in the
development of the patch, or that he/she was in the patch's delivery path.

If a person was not directly involved in the preparation or handling of a
patch but wishes to signify and record their approval of it then they can
ask to have an Acked-by: line added to the patch's changelog.

Acked-by: is often used by the maintainer of the affected code when that
maintainer neither contributed to nor forwarded the patch.

Acked-by: is not as formal as Signed-off-by:.  It is a record that the acker
has at least reviewed the patch and has indicated acceptance.  Hence patch
mergers will sometimes manually convert an acker's ``yep, looks good to me''
into an Acked-by: (but note that it is usually better to ask for an
explicit ack).

Acked-by: does not necessarily indicate acknowledgement of the entire patch.
For example, if a patch affects multiple subsystems and has an Acked-by: from
one subsystem maintainer then this usually indicates acknowledgement of just
the part which affects that maintainer's code.  Judgement should be used here.
When in doubt people should refer to the original discussion in the mailing
list archives.

If a person has had the opportunity to comment on a patch, but has not
provided such comments, you may optionally add a \code{Cc:} tag to the patch.
This is the only tag which might be added without an explicit action by the
person it names - but it should indicate that this person was copied on the
patch.  This tag documents that potentially interested parties
have been included in the discussion.


\section{13) Using Reported-by:, Tested-by:, Reviewed-by:, Suggested-by: and Fixes:}
\label{process/submitting-patches:using-reported-by-tested-by-reviewed-by-suggested-by-and-fixes}
The Reported-by tag gives credit to people who find bugs and report them and it
hopefully inspires them to help us again in the future.  Please note that if
the bug was reported in private, then ask for permission first before using the
Reported-by tag.

A Tested-by: tag indicates that the patch has been successfully tested (in
some environment) by the person named.  This tag informs maintainers that
some testing has been performed, provides a means to locate testers for
future patches, and ensures credit for the testers.

Reviewed-by:, instead, indicates that the patch has been reviewed and found
acceptable according to the Reviewer's Statement:


\subsection{Reviewer's statement of oversight}
\label{process/submitting-patches:reviewer-s-statement-of-oversight}
By offering my Reviewed-by: tag, I state that:
\begin{enumerate}
\item {} 
I have carried out a technical review of this patch to
evaluate its appropriateness and readiness for inclusion into
the mainline kernel.

\item {} 
Any problems, concerns, or questions relating to the patch
have been communicated back to the submitter.  I am satisfied
with the submitter's response to my comments.

\item {} 
While there may be things that could be improved with this
submission, I believe that it is, at this time, (1) a
worthwhile modification to the kernel, and (2) free of known
issues which would argue against its inclusion.

\item {} 
While I have reviewed the patch and believe it to be sound, I
do not (unless explicitly stated elsewhere) make any
warranties or guarantees that it will achieve its stated
purpose or function properly in any given situation.

\end{enumerate}

A Reviewed-by tag is a statement of opinion that the patch is an
appropriate modification of the kernel without any remaining serious
technical issues.  Any interested reviewer (who has done the work) can
offer a Reviewed-by tag for a patch.  This tag serves to give credit to
reviewers and to inform maintainers of the degree of review which has been
done on the patch.  Reviewed-by: tags, when supplied by reviewers known to
understand the subject area and to perform thorough reviews, will normally
increase the likelihood of your patch getting into the kernel.

A Suggested-by: tag indicates that the patch idea is suggested by the person
named and ensures credit to the person for the idea. Please note that this
tag should not be added without the reporter's permission, especially if the
idea was not posted in a public forum. That said, if we diligently credit our
idea reporters, they will, hopefully, be inspired to help us again in the
future.

A Fixes: tag indicates that the patch fixes an issue in a previous commit. It
is used to make it easy to determine where a bug originated, which can help
review a bug fix. This tag also assists the stable kernel team in determining
which stable kernel versions should receive your fix. This is the preferred
method for indicating a bug fixed by the patch. See {\hyperref[process/submitting\string-patches:describe\string-changes]{\emph{2) Describe your changes}}}
for more details.


\section{14) The canonical patch format}
\label{process/submitting-patches:the-canonical-patch-format}
This section describes how the patch itself should be formatted.  Note
that, if you have your patches stored in a \code{git} repository, proper patch
formatting can be had with \code{git format-patch}.  The tools cannot create
the necessary text, though, so read the instructions below anyway.

The canonical patch subject line is:

\begin{Verbatim}[commandchars=\\\{\}]
Subject: [PATCH 001/123] subsystem: summary phrase
\end{Verbatim}

The canonical patch message body contains the following:
\begin{itemize}
\item {} 
A \code{from} line specifying the patch author, followed by an empty
line (only needed if the person sending the patch is not the author).

\item {} 
The body of the explanation, line wrapped at 75 columns, which will
be copied to the permanent changelog to describe this patch.

\item {} 
An empty line.

\item {} 
The \code{Signed-off-by:} lines, described above, which will
also go in the changelog.

\item {} 
A marker line containing simply \code{-{-}-}.

\item {} 
Any additional comments not suitable for the changelog.

\item {} 
The actual patch (\code{diff} output).

\end{itemize}

The Subject line format makes it very easy to sort the emails
alphabetically by subject line - pretty much any email reader will
support that - since because the sequence number is zero-padded,
the numerical and alphabetic sort is the same.

The \code{subsystem} in the email's Subject should identify which
area or subsystem of the kernel is being patched.

The \code{summary phrase} in the email's Subject should concisely
describe the patch which that email contains.  The \code{summary
phrase} should not be a filename.  Do not use the same \code{summary
phrase} for every patch in a whole patch series (where a \code{patch
series} is an ordered sequence of multiple, related patches).

Bear in mind that the \code{summary phrase} of your email becomes a
globally-unique identifier for that patch.  It propagates all the way
into the \code{git} changelog.  The \code{summary phrase} may later be used in
developer discussions which refer to the patch.  People will want to
google for the \code{summary phrase} to read discussion regarding that
patch.  It will also be the only thing that people may quickly see
when, two or three months later, they are going through perhaps
thousands of patches using tools such as \code{gitk} or \code{git log
-{-}oneline}.

For these reasons, the \code{summary} must be no more than 70-75
characters, and it must describe both what the patch changes, as well
as why the patch might be necessary.  It is challenging to be both
succinct and descriptive, but that is what a well-written summary
should do.

The \code{summary phrase} may be prefixed by tags enclosed in square
brackets: ``Subject: {[}PATCH \textless{}tag\textgreater{}...{]} \textless{}summary phrase\textgreater{}''.  The tags are
not considered part of the summary phrase, but describe how the patch
should be treated.  Common tags might include a version descriptor if
the multiple versions of the patch have been sent out in response to
comments (i.e., ``v1, v2, v3''), or ``RFC'' to indicate a request for
comments.  If there are four patches in a patch series the individual
patches may be numbered like this: 1/4, 2/4, 3/4, 4/4.  This assures
that developers understand the order in which the patches should be
applied and that they have reviewed or applied all of the patches in
the patch series.

A couple of example Subjects:

\begin{Verbatim}[commandchars=\\\{\}]
Subject: [PATCH 2/5] ext2: improve scalability of bitmap searching
Subject: [PATCH v2 01/27] x86: fix eflags tracking
\end{Verbatim}

The \code{from} line must be the very first line in the message body,
and has the form:
\begin{quote}

From: Original Author \textless{}\href{mailto:author@example.com}{author@example.com}\textgreater{}
\end{quote}

The \code{from} line specifies who will be credited as the author of the
patch in the permanent changelog.  If the \code{from} line is missing,
then the \code{From:} line from the email header will be used to determine
the patch author in the changelog.

The explanation body will be committed to the permanent source
changelog, so should make sense to a competent reader who has long
since forgotten the immediate details of the discussion that might
have led to this patch.  Including symptoms of the failure which the
patch addresses (kernel log messages, oops messages, etc.) is
especially useful for people who might be searching the commit logs
looking for the applicable patch.  If a patch fixes a compile failure,
it may not be necessary to include \_all\_ of the compile failures; just
enough that it is likely that someone searching for the patch can find
it.  As in the \code{summary phrase}, it is important to be both succinct as
well as descriptive.

The \code{-{-}-} marker line serves the essential purpose of marking for patch
handling tools where the changelog message ends.

One good use for the additional comments after the \code{-{-}-} marker is for
a \code{diffstat}, to show what files have changed, and the number of
inserted and deleted lines per file.  A \code{diffstat} is especially useful
on bigger patches.  Other comments relevant only to the moment or the
maintainer, not suitable for the permanent changelog, should also go
here.  A good example of such comments might be \code{patch changelogs}
which describe what has changed between the v1 and v2 version of the
patch.

If you are going to include a \code{diffstat} after the \code{-{-}-} marker, please
use \code{diffstat} options \code{-p 1 -w 70} so that filenames are listed from
the top of the kernel source tree and don't use too much horizontal
space (easily fit in 80 columns, maybe with some indentation).  (\code{git}
generates appropriate diffstats by default.)

See more details on the proper patch format in the following
references.


\section{15) Explicit In-Reply-To headers}
\label{process/submitting-patches:explicit-in-reply-to-headers}\label{process/submitting-patches:explicit-in-reply-to}
It can be helpful to manually add In-Reply-To: headers to a patch
(e.g., when using \code{git send-email}) to associate the patch with
previous relevant discussion, e.g. to link a bug fix to the email with
the bug report.  However, for a multi-patch series, it is generally
best to avoid using In-Reply-To: to link to older versions of the
series.  This way multiple versions of the patch don't become an
unmanageable forest of references in email clients.  If a link is
helpful, you can use the \href{https://lkml.kernel.org/}{https://lkml.kernel.org/} redirector (e.g., in
the cover email text) to link to an earlier version of the patch series.


\section{16) Sending \texttt{git pull} requests}
\label{process/submitting-patches:sending-git-pull-requests}
If you have a series of patches, it may be most convenient to have the
maintainer pull them directly into the subsystem repository with a
\code{git pull} operation.  Note, however, that pulling patches from a developer
requires a higher degree of trust than taking patches from a mailing list.
As a result, many subsystem maintainers are reluctant to take pull
requests, especially from new, unknown developers.  If in doubt you can use
the pull request as the cover letter for a normal posting of the patch
series, giving the maintainer the option of using either.

A pull request should have {[}GIT{]} or {[}PULL{]} in the subject line.  The
request itself should include the repository name and the branch of
interest on a single line; it should look something like:

\begin{Verbatim}[commandchars=\\\{\}]
Please pull from

    git://jdelvare.pck.nerim.net/jdelvare\PYGZhy{}2.6 i2c\PYGZhy{}for\PYGZhy{}linus

to get these changes:
\end{Verbatim}

A pull request should also include an overall message saying what will be
included in the request, a \code{git shortlog} listing of the patches
themselves, and a \code{diffstat} showing the overall effect of the patch series.
The easiest way to get all this information together is, of course, to let
\code{git} do it for you with the \code{git request-pull} command.

Some maintainers (including Linus) want to see pull requests from signed
commits; that increases their confidence that the request actually came
from you.  Linus, in particular, will not pull from public hosting sites
like GitHub in the absence of a signed tag.

The first step toward creating such tags is to make a GNUPG key and get it
signed by one or more core kernel developers.  This step can be hard for
new developers, but there is no way around it.  Attending conferences can
be a good way to find developers who can sign your key.

Once you have prepared a patch series in \code{git} that you wish to have somebody
pull, create a signed tag with \code{git tag -s}.  This will create a new tag
identifying the last commit in the series and containing a signature
created with your private key.  You will also have the opportunity to add a
changelog-style message to the tag; this is an ideal place to describe the
effects of the pull request as a whole.

If the tree the maintainer will be pulling from is not the repository you
are working from, don't forget to push the signed tag explicitly to the
public tree.

When generating your pull request, use the signed tag as the target.  A
command like this will do the trick:

\begin{Verbatim}[commandchars=\\\{\}]
git request\PYGZhy{}pull master git://my.public.tree/linux.git my\PYGZhy{}signed\PYGZhy{}tag
\end{Verbatim}


\section{References}
\label{process/submitting-patches:references}\begin{description}
\item[{Andrew Morton, ``The perfect patch'' (tpp).}] \leavevmode
\textless{}\href{http://www.ozlabs.org/~akpm/stuff/tpp.txt}{http://www.ozlabs.org/\textasciitilde{}akpm/stuff/tpp.txt}\textgreater{}

\item[{Jeff Garzik, ``Linux kernel patch submission format''.}] \leavevmode
\textless{}\href{http://linux.yyz.us/patch-format.html}{http://linux.yyz.us/patch-format.html}\textgreater{}

\item[{Greg Kroah-Hartman, ``How to piss off a kernel subsystem maintainer''.}] \leavevmode
\textless{}\href{http://www.kroah.com/log/linux/maintainer.html}{http://www.kroah.com/log/linux/maintainer.html}\textgreater{}

\textless{}\href{http://www.kroah.com/log/linux/maintainer-02.html}{http://www.kroah.com/log/linux/maintainer-02.html}\textgreater{}

\textless{}\href{http://www.kroah.com/log/linux/maintainer-03.html}{http://www.kroah.com/log/linux/maintainer-03.html}\textgreater{}

\textless{}\href{http://www.kroah.com/log/linux/maintainer-04.html}{http://www.kroah.com/log/linux/maintainer-04.html}\textgreater{}

\textless{}\href{http://www.kroah.com/log/linux/maintainer-05.html}{http://www.kroah.com/log/linux/maintainer-05.html}\textgreater{}

\textless{}\href{http://www.kroah.com/log/linux/maintainer-06.html}{http://www.kroah.com/log/linux/maintainer-06.html}\textgreater{}

\item[{NO!!!! No more huge patch bombs to \href{mailto:linux-kernel@vger.kernel.org}{linux-kernel@vger.kernel.org} people!}] \leavevmode
\textless{}\href{https://lkml.org/lkml/2005/7/11/336}{https://lkml.org/lkml/2005/7/11/336}\textgreater{}

\item[{Kernel Documentation/process/coding-style.rst:}] \leavevmode
{\hyperref[process/coding\string-style:codingstyle]{\emph{Documentation/process/coding-style.rst}}}

\item[{Linus Torvalds's mail on the canonical patch format:}] \leavevmode
\textless{}\href{http://lkml.org/lkml/2005/4/7/183}{http://lkml.org/lkml/2005/4/7/183}\textgreater{}

\item[{Andi Kleen, ``On submitting kernel patches''}] \leavevmode
Some strategies to get difficult or controversial changes in.

\href{http://halobates.de/on-submitting-patches.pdf}{http://halobates.de/on-submitting-patches.pdf}

\end{description}


\chapter{Linux kernel coding style}
\label{process/coding-style:codingstyle}\label{process/coding-style::doc}\label{process/coding-style:linux-kernel-coding-style}
This is a short document describing the preferred coding style for the
linux kernel.  Coding style is very personal, and I won't \textbf{force} my
views on anybody, but this is what goes for anything that I have to be
able to maintain, and I'd prefer it for most other things too.  Please
at least consider the points made here.

First off, I'd suggest printing out a copy of the GNU coding standards,
and NOT read it.  Burn them, it's a great symbolic gesture.

Anyway, here goes:


\section{1) Indentation}
\label{process/coding-style:indentation}
Tabs are 8 characters, and thus indentations are also 8 characters.
There are heretic movements that try to make indentations 4 (or even 2!)
characters deep, and that is akin to trying to define the value of PI to
be 3.

Rationale: The whole idea behind indentation is to clearly define where
a block of control starts and ends.  Especially when you've been looking
at your screen for 20 straight hours, you'll find it a lot easier to see
how the indentation works if you have large indentations.

Now, some people will claim that having 8-character indentations makes
the code move too far to the right, and makes it hard to read on a
80-character terminal screen.  The answer to that is that if you need
more than 3 levels of indentation, you're screwed anyway, and should fix
your program.

In short, 8-char indents make things easier to read, and have the added
benefit of warning you when you're nesting your functions too deep.
Heed that warning.

The preferred way to ease multiple indentation levels in a switch statement is
to align the \code{switch} and its subordinate \code{case} labels in the same column
instead of \code{double-indenting} the \code{case} labels.  E.g.:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k}{switch} \PYG{p}{(}\PYG{n}{suffix}\PYG{p}{)} \PYG{p}{\PYGZob{}}
\PYG{k}{case} \PYG{l+s+sc}{\PYGZsq{}G\PYGZsq{}}\PYG{o}{:}
\PYG{k}{case} \PYG{l+s+sc}{\PYGZsq{}g\PYGZsq{}}\PYG{o}{:}
        \PYG{n}{mem} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}}\PYG{o}{=} \PYG{l+m+mi}{30}\PYG{p}{;}
        \PYG{k}{break}\PYG{p}{;}
\PYG{k}{case} \PYG{l+s+sc}{\PYGZsq{}M\PYGZsq{}}\PYG{o}{:}
\PYG{k}{case} \PYG{l+s+sc}{\PYGZsq{}m\PYGZsq{}}\PYG{o}{:}
        \PYG{n}{mem} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}}\PYG{o}{=} \PYG{l+m+mi}{20}\PYG{p}{;}
        \PYG{k}{break}\PYG{p}{;}
\PYG{k}{case} \PYG{l+s+sc}{\PYGZsq{}K\PYGZsq{}}\PYG{o}{:}
\PYG{k}{case} \PYG{l+s+sc}{\PYGZsq{}k\PYGZsq{}}\PYG{o}{:}
        \PYG{n}{mem} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}}\PYG{o}{=} \PYG{l+m+mi}{10}\PYG{p}{;}
        \PYG{c+cm}{/* fall through */}
\PYG{k}{default}\PYG{o}{:}
        \PYG{k}{break}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}

Don't put multiple statements on a single line unless you have
something to hide:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k}{if} \PYG{p}{(}\PYG{n}{condition}\PYG{p}{)} \PYG{n}{do\PYGZus{}this}\PYG{p}{;}
  \PYG{n}{do\PYGZus{}something\PYGZus{}everytime}\PYG{p}{;}
\end{Verbatim}

Don't put multiple assignments on a single line either.  Kernel coding style
is super simple.  Avoid tricky expressions.

Outside of comments, documentation and except in Kconfig, spaces are never
used for indentation, and the above example is deliberately broken.

Get a decent editor and don't leave whitespace at the end of lines.


\section{2) Breaking long lines and strings}
\label{process/coding-style:breaking-long-lines-and-strings}
Coding style is all about readability and maintainability using commonly
available tools.

The limit on the length of lines is 80 columns and this is a strongly
preferred limit.

Statements longer than 80 columns will be broken into sensible chunks, unless
exceeding 80 columns significantly increases readability and does not hide
information. Descendants are always substantially shorter than the parent and
are placed substantially to the right. The same applies to function headers
with a long argument list. However, never break user-visible strings such as
printk messages, because that breaks the ability to grep for them.


\section{3) Placing Braces and Spaces}
\label{process/coding-style:placing-braces-and-spaces}
The other issue that always comes up in C styling is the placement of
braces.  Unlike the indent size, there are few technical reasons to
choose one placement strategy over the other, but the preferred way, as
shown to us by the prophets Kernighan and Ritchie, is to put the opening
brace last on the line, and put the closing brace first, thusly:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k}{if} \PYG{p}{(}\PYG{n}{x} \PYG{n}{is} \PYG{n+nb}{true}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{n}{we} \PYG{k}{do} \PYG{n}{y}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}

This applies to all non-function statement blocks (if, switch, for,
while, do).  E.g.:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k}{switch} \PYG{p}{(}\PYG{n}{action}\PYG{p}{)} \PYG{p}{\PYGZob{}}
\PYG{k}{case} \PYG{n+nl}{KOBJ\PYGZus{}ADD}\PYG{p}{:}
        \PYG{k}{return} \PYG{l+s}{\PYGZdq{}}\PYG{l+s}{add}\PYG{l+s}{\PYGZdq{}}\PYG{p}{;}
\PYG{k}{case} \PYG{n+nl}{KOBJ\PYGZus{}REMOVE}\PYG{p}{:}
        \PYG{k}{return} \PYG{l+s}{\PYGZdq{}}\PYG{l+s}{remove}\PYG{l+s}{\PYGZdq{}}\PYG{p}{;}
\PYG{k}{case} \PYG{n+nl}{KOBJ\PYGZus{}CHANGE}\PYG{p}{:}
        \PYG{k}{return} \PYG{l+s}{\PYGZdq{}}\PYG{l+s}{change}\PYG{l+s}{\PYGZdq{}}\PYG{p}{;}
\PYG{k}{default}\PYG{o}{:}
        \PYG{k}{return} \PYG{n+nb}{NULL}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}

However, there is one special case, namely functions: they have the
opening brace at the beginning of the next line, thus:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k+kt}{int} \PYG{n+nf}{function}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{x}\PYG{p}{)}
\PYG{p}{\PYGZob{}}
        \PYG{n}{body} \PYG{n}{of} \PYG{n}{function}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}

Heretic people all over the world have claimed that this inconsistency
is ...  well ...  inconsistent, but all right-thinking people know that
(a) K\&R are \textbf{right} and (b) K\&R are right.  Besides, functions are
special anyway (you can't nest them in C).

Note that the closing brace is empty on a line of its own, \textbf{except} in
the cases where it is followed by a continuation of the same statement,
ie a \code{while} in a do-statement or an \code{else} in an if-statement, like
this:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k}{do} \PYG{p}{\PYGZob{}}
        \PYG{n}{body} \PYG{n}{of} \PYG{k}{do}\PYG{o}{\PYGZhy{}}\PYG{n}{loop}
\PYG{p}{\PYGZcb{}} \PYG{k}{while} \PYG{p}{(}\PYG{n}{condition}\PYG{p}{)}\PYG{p}{;}
\end{Verbatim}

and

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k}{if} \PYG{p}{(}\PYG{n}{x} \PYG{o}{=}\PYG{o}{=} \PYG{n}{y}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{p}{.}\PYG{p}{.}
\PYG{p}{\PYGZcb{}} \PYG{k}{else} \PYG{k}{if} \PYG{p}{(}\PYG{n}{x} \PYG{o}{\PYGZgt{}} \PYG{n}{y}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{p}{.}\PYG{p}{.}\PYG{p}{.}
\PYG{p}{\PYGZcb{}} \PYG{k}{else} \PYG{p}{\PYGZob{}}
        \PYG{p}{.}\PYG{p}{.}\PYG{p}{.}\PYG{p}{.}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}

Rationale: K\&R.

Also, note that this brace-placement also minimizes the number of empty
(or almost empty) lines, without any loss of readability.  Thus, as the
supply of new-lines on your screen is not a renewable resource (think
25-line terminal screens here), you have more empty lines to put
comments on.

Do not unnecessarily use braces where a single statement will do.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k}{if} \PYG{p}{(}\PYG{n}{condition}\PYG{p}{)}
        \PYG{n}{action}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
\end{Verbatim}

and

\begin{Verbatim}[commandchars=\\\{\}]
if (condition)
        do\PYGZus{}this();
else
        do\PYGZus{}that();
\end{Verbatim}

This does not apply if only one branch of a conditional statement is a single
statement; in the latter case use braces in both branches:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k}{if} \PYG{p}{(}\PYG{n}{condition}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{n}{do\PYGZus{}this}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
        \PYG{n}{do\PYGZus{}that}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
\PYG{p}{\PYGZcb{}} \PYG{k}{else} \PYG{p}{\PYGZob{}}
        \PYG{n}{otherwise}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}


\subsection{3.1) Spaces}
\label{process/coding-style:spaces}
Linux kernel style for use of spaces depends (mostly) on
function-versus-keyword usage.  Use a space after (most) keywords.  The
notable exceptions are sizeof, typeof, alignof, and \_\_attribute\_\_, which look
somewhat like functions (and are usually used with parentheses in Linux,
although they are not required in the language, as in: \code{sizeof info} after
\code{struct fileinfo info;} is declared).

So use a space after these keywords:

\begin{Verbatim}[commandchars=\\\{\}]
if, switch, case, for, do, while
\end{Verbatim}

but not with sizeof, typeof, alignof, or \_\_attribute\_\_.  E.g.,

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{s} \PYG{o}{=} \PYG{k}{sizeof}\PYG{p}{(}\PYG{k}{struct} \PYG{n}{file}\PYG{p}{)}\PYG{p}{;}
\end{Verbatim}

Do not add spaces around (inside) parenthesized expressions.  This example is
\textbf{bad}:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{s} \PYG{o}{=} \PYG{k}{sizeof}\PYG{p}{(} \PYG{k}{struct} \PYG{n}{file} \PYG{p}{)}\PYG{p}{;}
\end{Verbatim}

When declaring pointer data or a function that returns a pointer type, the
preferred use of \code{*} is adjacent to the data name or function name and not
adjacent to the type name.  Examples:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k+kt}{char} \PYG{o}{*}\PYG{n}{linux\PYGZus{}banner}\PYG{p}{;}
\PYG{k+kt}{unsigned} \PYG{k+kt}{long} \PYG{k+kt}{long} \PYG{n+nf}{memparse}\PYG{p}{(}\PYG{k+kt}{char} \PYG{o}{*}\PYG{n}{ptr}\PYG{p}{,} \PYG{k+kt}{char} \PYG{o}{*}\PYG{o}{*}\PYG{n}{retptr}\PYG{p}{)}\PYG{p}{;}
\PYG{k+kt}{char} \PYG{o}{*}\PYG{n+nf}{match\PYGZus{}strdup}\PYG{p}{(}\PYG{n}{substring\PYGZus{}t} \PYG{o}{*}\PYG{n}{s}\PYG{p}{)}\PYG{p}{;}
\end{Verbatim}

Use one space around (on each side of) most binary and ternary operators,
such as any of these:

\begin{Verbatim}[commandchars=\\\{\}]
=  +  \PYGZhy{}  \PYGZlt{}  \PYGZgt{}  *  /  \PYGZpc{}  \textbar{}  \PYGZam{}  \PYGZca{}  \PYGZlt{}=  \PYGZgt{}=  ==  !=  ?  :
\end{Verbatim}

but no space after unary operators:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZam{}  *  +  \PYGZhy{}  \PYGZti{}  !  sizeof  typeof  alignof  \PYGZus{}\PYGZus{}attribute\PYGZus{}\PYGZus{}  defined
\end{Verbatim}

no space before the postfix increment \& decrement unary operators:

\begin{Verbatim}[commandchars=\\\{\}]
++  \PYGZhy{}\PYGZhy{}
\end{Verbatim}

no space after the prefix increment \& decrement unary operators:

\begin{Verbatim}[commandchars=\\\{\}]
++  \PYGZhy{}\PYGZhy{}
\end{Verbatim}

and no space around the \code{.} and \code{-\textgreater{}} structure member operators.

Do not leave trailing whitespace at the ends of lines.  Some editors with
\code{smart} indentation will insert whitespace at the beginning of new lines as
appropriate, so you can start typing the next line of code right away.
However, some such editors do not remove the whitespace if you end up not
putting a line of code there, such as if you leave a blank line.  As a result,
you end up with lines containing trailing whitespace.

Git will warn you about patches that introduce trailing whitespace, and can
optionally strip the trailing whitespace for you; however, if applying a series
of patches, this may make later patches in the series fail by changing their
context lines.


\section{4) Naming}
\label{process/coding-style:naming}
C is a Spartan language, and so should your naming be.  Unlike Modula-2
and Pascal programmers, C programmers do not use cute names like
ThisVariableIsATemporaryCounter.  A C programmer would call that
variable \code{tmp}, which is much easier to write, and not the least more
difficult to understand.

HOWEVER, while mixed-case names are frowned upon, descriptive names for
global variables are a must.  To call a global function \code{foo} is a
shooting offense.

GLOBAL variables (to be used only if you \textbf{really} need them) need to
have descriptive names, as do global functions.  If you have a function
that counts the number of active users, you should call that
\code{count\_active\_users()} or similar, you should \textbf{not} call it \code{cntusr()}.

Encoding the type of a function into the name (so-called Hungarian
notation) is brain damaged - the compiler knows the types anyway and can
check those, and it only confuses the programmer.  No wonder MicroSoft
makes buggy programs.

LOCAL variable names should be short, and to the point.  If you have
some random integer loop counter, it should probably be called \code{i}.
Calling it \code{loop\_counter} is non-productive, if there is no chance of it
being mis-understood.  Similarly, \code{tmp} can be just about any type of
variable that is used to hold a temporary value.

If you are afraid to mix up your local variable names, you have another
problem, which is called the function-growth-hormone-imbalance syndrome.
See chapter 6 (Functions).


\section{5) Typedefs}
\label{process/coding-style:typedefs}
Please don't use things like \code{vps\_t}.
It's a \textbf{mistake} to use typedef for structures and pointers. When you see a

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{vps\PYGZus{}t} \PYG{n}{a}\PYG{p}{;}
\end{Verbatim}

in the source, what does it mean?
In contrast, if it says

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k}{struct} \PYG{n}{virtual\PYGZus{}container} \PYG{o}{*}\PYG{n}{a}\PYG{p}{;}
\end{Verbatim}

you can actually tell what \code{a} is.

Lots of people think that typedefs \code{help readability}. Not so. They are
useful only for:
\begin{enumerate}
\item {} 
totally opaque objects (where the typedef is actively used to \textbf{hide}
what the object is).

Example: \code{pte\_t} etc. opaque objects that you can only access using
the proper accessor functions.

\begin{notice}{note}{Note:}
Opaqueness and \code{accessor functions} are not good in themselves.
The reason we have them for things like pte\_t etc. is that there
really is absolutely \textbf{zero} portably accessible information there.
\end{notice}

\item {} 
Clear integer types, where the abstraction \textbf{helps} avoid confusion
whether it is \code{int} or \code{long}.

u8/u16/u32 are perfectly fine typedefs, although they fit into
category (d) better than here.

\begin{notice}{note}{Note:}
Again - there needs to be a \textbf{reason} for this. If something is
\code{unsigned long}, then there's no reason to do
\begin{quote}

typedef unsigned long myflags\_t;
\end{quote}
\end{notice}

but if there is a clear reason for why it under certain circumstances
might be an \code{unsigned int} and under other configurations might be
\code{unsigned long}, then by all means go ahead and use a typedef.

\item {} 
when you use sparse to literally create a \textbf{new} type for
type-checking.

\item {} 
New types which are identical to standard C99 types, in certain
exceptional circumstances.

Although it would only take a short amount of time for the eyes and
brain to become accustomed to the standard types like \code{uint32\_t},
some people object to their use anyway.

Therefore, the Linux-specific \code{u8/u16/u32/u64} types and their
signed equivalents which are identical to standard types are
permitted -- although they are not mandatory in new code of your
own.

When editing existing code which already uses one or the other set
of types, you should conform to the existing choices in that code.

\item {} 
Types safe for use in userspace.

In certain structures which are visible to userspace, we cannot
require C99 types and cannot use the \code{u32} form above. Thus, we
use \_\_u32 and similar types in all structures which are shared
with userspace.

\end{enumerate}

Maybe there are other cases too, but the rule should basically be to NEVER
EVER use a typedef unless you can clearly match one of those rules.

In general, a pointer, or a struct that has elements that can reasonably
be directly accessed should \textbf{never} be a typedef.


\section{6) Functions}
\label{process/coding-style:functions}
Functions should be short and sweet, and do just one thing.  They should
fit on one or two screenfuls of text (the ISO/ANSI screen size is 80x24,
as we all know), and do one thing and do that well.

The maximum length of a function is inversely proportional to the
complexity and indentation level of that function.  So, if you have a
conceptually simple function that is just one long (but simple)
case-statement, where you have to do lots of small things for a lot of
different cases, it's OK to have a longer function.

However, if you have a complex function, and you suspect that a
less-than-gifted first-year high-school student might not even
understand what the function is all about, you should adhere to the
maximum limits all the more closely.  Use helper functions with
descriptive names (you can ask the compiler to in-line them if you think
it's performance-critical, and it will probably do a better job of it
than you would have done).

Another measure of the function is the number of local variables.  They
shouldn't exceed 5-10, or you're doing something wrong.  Re-think the
function, and split it into smaller pieces.  A human brain can
generally easily keep track of about 7 different things, anything more
and it gets confused.  You know you're brilliant, but maybe you'd like
to understand what you did 2 weeks from now.

In source files, separate functions with one blank line.  If the function is
exported, the \textbf{EXPORT} macro for it should follow immediately after the
closing function brace line.  E.g.:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k+kt}{int} \PYG{n+nf}{system\PYGZus{}is\PYGZus{}up}\PYG{p}{(}\PYG{k+kt}{void}\PYG{p}{)}
\PYG{p}{\PYGZob{}}
        \PYG{k}{return} \PYG{n}{system\PYGZus{}state} \PYG{o}{=}\PYG{o}{=} \PYG{n}{SYSTEM\PYGZus{}RUNNING}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\PYG{n}{EXPORT\PYGZus{}SYMBOL}\PYG{p}{(}\PYG{n}{system\PYGZus{}is\PYGZus{}up}\PYG{p}{)}\PYG{p}{;}
\end{Verbatim}

In function prototypes, include parameter names with their data types.
Although this is not required by the C language, it is preferred in Linux
because it is a simple way to add valuable information for the reader.


\section{7) Centralized exiting of functions}
\label{process/coding-style:centralized-exiting-of-functions}
Albeit deprecated by some people, the equivalent of the goto statement is
used frequently by compilers in form of the unconditional jump instruction.

The goto statement comes in handy when a function exits from multiple
locations and some common work such as cleanup has to be done.  If there is no
cleanup needed then just return directly.

Choose label names which say what the goto does or why the goto exists.  An
example of a good name could be \code{out\_free\_buffer:} if the goto frees \code{buffer}.
Avoid using GW-BASIC names like \code{err1:} and \code{err2:}, as you would have to
renumber them if you ever add or remove exit paths, and they make correctness
difficult to verify anyway.

The rationale for using gotos is:
\begin{itemize}
\item {} 
unconditional statements are easier to understand and follow

\item {} 
nesting is reduced

\item {} 
errors by not updating individual exit points when making
modifications are prevented

\item {} 
saves the compiler work to optimize redundant code away ;)

\end{itemize}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k+kt}{int} \PYG{n+nf}{fun}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{a}\PYG{p}{)}
\PYG{p}{\PYGZob{}}
        \PYG{k+kt}{int} \PYG{n}{result} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}
        \PYG{k+kt}{char} \PYG{o}{*}\PYG{n}{buffer}\PYG{p}{;}

        \PYG{n}{buffer} \PYG{o}{=} \PYG{n}{kmalloc}\PYG{p}{(}\PYG{n}{SIZE}\PYG{p}{,} \PYG{n}{GFP\PYGZus{}KERNEL}\PYG{p}{)}\PYG{p}{;}
        \PYG{k}{if} \PYG{p}{(}\PYG{o}{!}\PYG{n}{buffer}\PYG{p}{)}
                \PYG{k}{return} \PYG{o}{\PYGZhy{}}\PYG{n}{ENOMEM}\PYG{p}{;}

        \PYG{k}{if} \PYG{p}{(}\PYG{n}{condition1}\PYG{p}{)} \PYG{p}{\PYGZob{}}
                \PYG{k}{while} \PYG{p}{(}\PYG{n}{loop1}\PYG{p}{)} \PYG{p}{\PYGZob{}}
                        \PYG{p}{.}\PYG{p}{.}\PYG{p}{.}
                \PYG{p}{\PYGZcb{}}
                \PYG{n}{result} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{;}
                \PYG{k}{goto} \PYG{n}{out\PYGZus{}free\PYGZus{}buffer}\PYG{p}{;}
        \PYG{p}{\PYGZcb{}}
        \PYG{p}{.}\PYG{p}{.}\PYG{p}{.}
\PYG{n+nl}{out\PYGZus{}free\PYGZus{}buffer}\PYG{p}{:}
        \PYG{n}{kfree}\PYG{p}{(}\PYG{n}{buffer}\PYG{p}{)}\PYG{p}{;}
        \PYG{k}{return} \PYG{n}{result}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}

A common type of bug to be aware of is \code{one err bugs} which look like this:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n+nl}{err}\PYG{p}{:}
        \PYG{n}{kfree}\PYG{p}{(}\PYG{n}{foo}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}}\PYG{n}{bar}\PYG{p}{)}\PYG{p}{;}
        \PYG{n}{kfree}\PYG{p}{(}\PYG{n}{foo}\PYG{p}{)}\PYG{p}{;}
        \PYG{k}{return} \PYG{n}{ret}\PYG{p}{;}
\end{Verbatim}

The bug in this code is that on some exit paths \code{foo} is NULL.  Normally the
fix for this is to split it up into two error labels \code{err\_free\_bar:} and
\code{err\_free\_foo:}:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n+nl}{err\PYGZus{}free\PYGZus{}bar}\PYG{p}{:}
       \PYG{n}{kfree}\PYG{p}{(}\PYG{n}{foo}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}}\PYG{n}{bar}\PYG{p}{)}\PYG{p}{;}
\PYG{n+nl}{err\PYGZus{}free\PYGZus{}foo}\PYG{p}{:}
       \PYG{n}{kfree}\PYG{p}{(}\PYG{n}{foo}\PYG{p}{)}\PYG{p}{;}
       \PYG{k}{return} \PYG{n}{ret}\PYG{p}{;}
\end{Verbatim}

Ideally you should simulate errors to test all exit paths.


\section{8) Commenting}
\label{process/coding-style:commenting}
Comments are good, but there is also a danger of over-commenting.  NEVER
try to explain HOW your code works in a comment: it's much better to
write the code so that the \textbf{working} is obvious, and it's a waste of
time to explain badly written code.

Generally, you want your comments to tell WHAT your code does, not HOW.
Also, try to avoid putting comments inside a function body: if the
function is so complex that you need to separately comment parts of it,
you should probably go back to chapter 6 for a while.  You can make
small comments to note or warn about something particularly clever (or
ugly), but try to avoid excess.  Instead, put the comments at the head
of the function, telling people what it does, and possibly WHY it does
it.

When commenting the kernel API functions, please use the kernel-doc format.
See the files at \DUspan{xref,std,std-ref}{Documentation/doc-guide/} and
\code{scripts/kernel-doc} for details.

The preferred style for long (multi-line) comments is:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/*}
\PYG{c+cm}{ * This is the preferred style for multi\PYGZhy{}line}
\PYG{c+cm}{ * comments in the Linux kernel source code.}
\PYG{c+cm}{ * Please use it consistently.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Description:  A column of asterisks on the left side,}
\PYG{c+cm}{ * with beginning and ending almost\PYGZhy{}blank lines.}
\PYG{c+cm}{ */}
\end{Verbatim}

For files in net/ and drivers/net/ the preferred style for long (multi-line)
comments is a little different.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/* The preferred comment style for files in net/ and drivers/net}
\PYG{c+cm}{ * looks like this.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * It is nearly the same as the generally preferred comment style,}
\PYG{c+cm}{ * but there is no initial almost\PYGZhy{}blank line.}
\PYG{c+cm}{ */}
\end{Verbatim}

It's also important to comment data, whether they are basic types or derived
types.  To this end, use just one data declaration per line (no commas for
multiple data declarations).  This leaves you room for a small comment on each
item, explaining its use.


\section{9) You've made a mess of it}
\label{process/coding-style:you-ve-made-a-mess-of-it}
That's OK, we all do.  You've probably been told by your long-time Unix
user helper that \code{GNU emacs} automatically formats the C sources for
you, and you've noticed that yes, it does do that, but the defaults it
uses are less than desirable (in fact, they are worse than random
typing - an infinite number of monkeys typing into GNU emacs would never
make a good program).

So, you can either get rid of GNU emacs, or change it to use saner
values.  To do the latter, you can stick the following in your .emacs file:

\begin{Verbatim}[commandchars=\\\{\}]
(defun c\PYGZhy{}lineup\PYGZhy{}arglist\PYGZhy{}tabs\PYGZhy{}only (ignored)
  \PYGZdq{}Line up argument lists by tabs, not spaces\PYGZdq{}
  (let* ((anchor (c\PYGZhy{}langelem\PYGZhy{}pos c\PYGZhy{}syntactic\PYGZhy{}element))
         (column (c\PYGZhy{}langelem\PYGZhy{}2nd\PYGZhy{}pos c\PYGZhy{}syntactic\PYGZhy{}element))
         (offset (\PYGZhy{} (1+ column) anchor))
         (steps (floor offset c\PYGZhy{}basic\PYGZhy{}offset)))
    (* (max steps 1)
       c\PYGZhy{}basic\PYGZhy{}offset)))

(add\PYGZhy{}hook \PYGZsq{}c\PYGZhy{}mode\PYGZhy{}common\PYGZhy{}hook
          (lambda ()
            ;; Add kernel style
            (c\PYGZhy{}add\PYGZhy{}style
             \PYGZdq{}linux\PYGZhy{}tabs\PYGZhy{}only\PYGZdq{}
             \PYGZsq{}(\PYGZdq{}linux\PYGZdq{} (c\PYGZhy{}offsets\PYGZhy{}alist
                        (arglist\PYGZhy{}cont\PYGZhy{}nonempty
                         c\PYGZhy{}lineup\PYGZhy{}gcc\PYGZhy{}asm\PYGZhy{}reg
                         c\PYGZhy{}lineup\PYGZhy{}arglist\PYGZhy{}tabs\PYGZhy{}only))))))

(add\PYGZhy{}hook \PYGZsq{}c\PYGZhy{}mode\PYGZhy{}hook
          (lambda ()
            (let ((filename (buffer\PYGZhy{}file\PYGZhy{}name)))
              ;; Enable kernel mode for the appropriate files
              (when (and filename
                         (string\PYGZhy{}match (expand\PYGZhy{}file\PYGZhy{}name \PYGZdq{}\PYGZti{}/src/linux\PYGZhy{}trees\PYGZdq{})
                                       filename))
                (setq indent\PYGZhy{}tabs\PYGZhy{}mode t)
                (setq show\PYGZhy{}trailing\PYGZhy{}whitespace t)
                (c\PYGZhy{}set\PYGZhy{}style \PYGZdq{}linux\PYGZhy{}tabs\PYGZhy{}only\PYGZdq{})))))
\end{Verbatim}

This will make emacs go better with the kernel coding style for C
files below \code{\textasciitilde{}/src/linux-trees}.

But even if you fail in getting emacs to do sane formatting, not
everything is lost: use \code{indent}.

Now, again, GNU indent has the same brain-dead settings that GNU emacs
has, which is why you need to give it a few command line options.
However, that's not too bad, because even the makers of GNU indent
recognize the authority of K\&R (the GNU people aren't evil, they are
just severely misguided in this matter), so you just give indent the
options \code{-kr -i8} (stands for \code{K\&R, 8 character indents}), or use
\code{scripts/Lindent}, which indents in the latest style.

\code{indent} has a lot of options, and especially when it comes to comment
re-formatting you may want to take a look at the man page.  But
remember: \code{indent} is not a fix for bad programming.


\section{10) Kconfig configuration files}
\label{process/coding-style:kconfig-configuration-files}
For all of the Kconfig* configuration files throughout the source tree,
the indentation is somewhat different.  Lines under a \code{config} definition
are indented with one tab, while help text is indented an additional two
spaces.  Example:

\begin{Verbatim}[commandchars=\\\{\}]
config AUDIT
      bool \PYGZdq{}Auditing support\PYGZdq{}
      depends on NET
      help
        Enable auditing infrastructure that can be used with another
        kernel subsystem, such as SELinux (which requires this for
        logging of avc messages output).  Does not do system\PYGZhy{}call
        auditing without CONFIG\PYGZus{}AUDITSYSCALL.
\end{Verbatim}

Seriously dangerous features (such as write support for certain
filesystems) should advertise this prominently in their prompt string:

\begin{Verbatim}[commandchars=\\\{\}]
config ADFS\PYGZus{}FS\PYGZus{}RW
      bool \PYGZdq{}ADFS write support (DANGEROUS)\PYGZdq{}
      depends on ADFS\PYGZus{}FS
      ...
\end{Verbatim}

For full documentation on the configuration files, see the file
Documentation/kbuild/kconfig-language.txt.


\section{11) Data structures}
\label{process/coding-style:data-structures}
Data structures that have visibility outside the single-threaded
environment they are created and destroyed in should always have
reference counts.  In the kernel, garbage collection doesn't exist (and
outside the kernel garbage collection is slow and inefficient), which
means that you absolutely \textbf{have} to reference count all your uses.

Reference counting means that you can avoid locking, and allows multiple
users to have access to the data structure in parallel - and not having
to worry about the structure suddenly going away from under them just
because they slept or did something else for a while.

Note that locking is \textbf{not} a replacement for reference counting.
Locking is used to keep data structures coherent, while reference
counting is a memory management technique.  Usually both are needed, and
they are not to be confused with each other.

Many data structures can indeed have two levels of reference counting,
when there are users of different \code{classes}.  The subclass count counts
the number of subclass users, and decrements the global count just once
when the subclass count goes to zero.

Examples of this kind of \code{multi-level-reference-counting} can be found in
memory management (\code{struct mm\_struct}: mm\_users and mm\_count), and in
filesystem code (\code{struct super\_block}: s\_count and s\_active).

Remember: if another thread can find your data structure, and you don't
have a reference count on it, you almost certainly have a bug.


\section{12) Macros, Enums and RTL}
\label{process/coding-style:macros-enums-and-rtl}
Names of macros defining constants and labels in enums are capitalized.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{define CONSTANT 0x12345}
\end{Verbatim}

Enums are preferred when defining several related constants.

CAPITALIZED macro names are appreciated but macros resembling functions
may be named in lower case.

Generally, inline functions are preferable to macros resembling functions.

Macros with multiple statements should be enclosed in a do - while block:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{define macrofun(a, b, c)                       \PYGZbs{}}
\PYG{c+cp}{        do \PYGZob{}                                    \PYGZbs{}}
\PYG{c+cp}{                if (a == 5)                     \PYGZbs{}}
\PYG{c+cp}{                        do\PYGZus{}this(b, c);          \PYGZbs{}}
\PYG{c+cp}{        \PYGZcb{} while (0)}
\end{Verbatim}

Things to avoid when using macros:
\begin{enumerate}
\item {} 
macros that affect control flow:

\end{enumerate}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{define FOO(x)                                  \PYGZbs{}}
\PYG{c+cp}{        do \PYGZob{}                                    \PYGZbs{}}
\PYG{c+cp}{                if (blah(x) \PYGZlt{} 0)                \PYGZbs{}}
\PYG{c+cp}{                        return \PYGZhy{}EBUGGERED;      \PYGZbs{}}
\PYG{c+cp}{        \PYGZcb{} while (0)}
\end{Verbatim}

is a \textbf{very} bad idea.  It looks like a function call but exits the \code{calling}
function; don't break the internal parsers of those who will read the code.
\begin{enumerate}
\setcounter{enumi}{1}
\item {} 
macros that depend on having a local variable with a magic name:

\end{enumerate}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{define FOO(val) bar(index, val)}
\end{Verbatim}

might look like a good thing, but it's confusing as hell when one reads the
code and it's prone to breakage from seemingly innocent changes.

3) macros with arguments that are used as l-values: FOO(x) = y; will
bite you if somebody e.g. turns FOO into an inline function.

4) forgetting about precedence: macros defining constants using expressions
must enclose the expression in parentheses. Beware of similar issues with
macros using parameters.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{define CONSTANT 0x4000}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{define CONSTEXP (CONSTANT \textbar{} 3)}
\end{Verbatim}

5) namespace collisions when defining local variables in macros resembling
functions:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{define FOO(x)                          \PYGZbs{}}
\PYG{c+cp}{(\PYGZob{}                                      \PYGZbs{}}
\PYG{c+cp}{        typeof(x) ret;                  \PYGZbs{}}
\PYG{c+cp}{        ret = calc\PYGZus{}ret(x);              \PYGZbs{}}
\PYG{c+cp}{        (ret);                          \PYGZbs{}}
\PYG{c+cp}{\PYGZcb{})}
\end{Verbatim}

ret is a common name for a local variable - \_\_foo\_ret is less likely
to collide with an existing variable.

The cpp manual deals with macros exhaustively. The gcc internals manual also
covers RTL which is used frequently with assembly language in the kernel.


\section{13) Printing kernel messages}
\label{process/coding-style:printing-kernel-messages}
Kernel developers like to be seen as literate. Do mind the spelling
of kernel messages to make a good impression. Do not use crippled
words like \code{dont}; use \code{do not} or \code{don't} instead.  Make the messages
concise, clear, and unambiguous.

Kernel messages do not have to be terminated with a period.

Printing numbers in parentheses (\%d) adds no value and should be avoided.

There are a number of driver model diagnostic macros in \textless{}linux/device.h\textgreater{}
which you should use to make sure messages are matched to the right device
and driver, and are tagged with the right level:  dev\_err(), dev\_warn(),
dev\_info(), and so forth.  For messages that aren't associated with a
particular device, \textless{}linux/printk.h\textgreater{} defines pr\_notice(), pr\_info(),
pr\_warn(), pr\_err(), etc.

Coming up with good debugging messages can be quite a challenge; and once
you have them, they can be a huge help for remote troubleshooting.  However
debug message printing is handled differently than printing other non-debug
messages.  While the other pr\_XXX() functions print unconditionally,
pr\_debug() does not; it is compiled out by default, unless either DEBUG is
defined or CONFIG\_DYNAMIC\_DEBUG is set.  That is true for dev\_dbg() also,
and a related convention uses VERBOSE\_DEBUG to add dev\_vdbg() messages to
the ones already enabled by DEBUG.

Many subsystems have Kconfig debug options to turn on -DDEBUG in the
corresponding Makefile; in other cases specific files \#define DEBUG.  And
when a debug message should be unconditionally printed, such as if it is
already inside a debug-related \#ifdef section, printk(KERN\_DEBUG ...) can be
used.


\section{14) Allocating memory}
\label{process/coding-style:allocating-memory}
The kernel provides the following general purpose memory allocators:
kmalloc(), kzalloc(), kmalloc\_array(), kcalloc(), vmalloc(), and
vzalloc().  Please refer to the API documentation for further information
about them.

The preferred form for passing a size of a struct is the following:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{p} \PYG{o}{=} \PYG{n}{kmalloc}\PYG{p}{(}\PYG{k}{sizeof}\PYG{p}{(}\PYG{o}{*}\PYG{n}{p}\PYG{p}{)}\PYG{p}{,} \PYG{p}{.}\PYG{p}{.}\PYG{p}{.}\PYG{p}{)}\PYG{p}{;}
\end{Verbatim}

The alternative form where struct name is spelled out hurts readability and
introduces an opportunity for a bug when the pointer variable type is changed
but the corresponding sizeof that is passed to a memory allocator is not.

Casting the return value which is a void pointer is redundant. The conversion
from void pointer to any other pointer type is guaranteed by the C programming
language.

The preferred form for allocating an array is the following:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{p} \PYG{o}{=} \PYG{n}{kmalloc\PYGZus{}array}\PYG{p}{(}\PYG{n}{n}\PYG{p}{,} \PYG{k}{sizeof}\PYG{p}{(}\PYG{p}{.}\PYG{p}{.}\PYG{p}{.}\PYG{p}{)}\PYG{p}{,} \PYG{p}{.}\PYG{p}{.}\PYG{p}{.}\PYG{p}{)}\PYG{p}{;}
\end{Verbatim}

The preferred form for allocating a zeroed array is the following:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{p} \PYG{o}{=} \PYG{n}{kcalloc}\PYG{p}{(}\PYG{n}{n}\PYG{p}{,} \PYG{k}{sizeof}\PYG{p}{(}\PYG{p}{.}\PYG{p}{.}\PYG{p}{.}\PYG{p}{)}\PYG{p}{,} \PYG{p}{.}\PYG{p}{.}\PYG{p}{.}\PYG{p}{)}\PYG{p}{;}
\end{Verbatim}

Both forms check for overflow on the allocation size n * sizeof(...),
and return NULL if that occurred.


\section{15) The inline disease}
\label{process/coding-style:the-inline-disease}
There appears to be a common misperception that gcc has a magic ``make me
faster'' speedup option called \code{inline}. While the use of inlines can be
appropriate (for example as a means of replacing macros, see Chapter 12), it
very often is not. Abundant use of the inline keyword leads to a much bigger
kernel, which in turn slows the system as a whole down, due to a bigger
icache footprint for the CPU and simply because there is less memory
available for the pagecache. Just think about it; a pagecache miss causes a
disk seek, which easily takes 5 milliseconds. There are a LOT of cpu cycles
that can go into these 5 milliseconds.

A reasonable rule of thumb is to not put inline at functions that have more
than 3 lines of code in them. An exception to this rule are the cases where
a parameter is known to be a compiletime constant, and as a result of this
constantness you \emph{know} the compiler will be able to optimize most of your
function away at compile time. For a good example of this later case, see
the kmalloc() inline function.

Often people argue that adding inline to functions that are static and used
only once is always a win since there is no space tradeoff. While this is
technically correct, gcc is capable of inlining these automatically without
help, and the maintenance issue of removing the inline when a second user
appears outweighs the potential value of the hint that tells gcc to do
something it would have done anyway.


\section{16) Function return values and names}
\label{process/coding-style:function-return-values-and-names}
Functions can return values of many different kinds, and one of the
most common is a value indicating whether the function succeeded or
failed.  Such a value can be represented as an error-code integer
(-Exxx = failure, 0 = success) or a \code{succeeded} boolean (0 = failure,
non-zero = success).

Mixing up these two sorts of representations is a fertile source of
difficult-to-find bugs.  If the C language included a strong distinction
between integers and booleans then the compiler would find these mistakes
for us... but it doesn't.  To help prevent such bugs, always follow this
convention:

\begin{Verbatim}[commandchars=\\\{\}]
If the name of a function is an action or an imperative command,
the function should return an error\PYGZhy{}code integer.  If the name
is a predicate, the function should return a \PYGZdq{}succeeded\PYGZdq{} boolean.
\end{Verbatim}

For example, \code{add work} is a command, and the add\_work() function returns 0
for success or -EBUSY for failure.  In the same way, \code{PCI device present} is
a predicate, and the pci\_dev\_present() function returns 1 if it succeeds in
finding a matching device or 0 if it doesn't.

All EXPORTed functions must respect this convention, and so should all
public functions.  Private (static) functions need not, but it is
recommended that they do.

Functions whose return value is the actual result of a computation, rather
than an indication of whether the computation succeeded, are not subject to
this rule.  Generally they indicate failure by returning some out-of-range
result.  Typical examples would be functions that return pointers; they use
NULL or the ERR\_PTR mechanism to report failure.


\section{17) Don't re-invent the kernel macros}
\label{process/coding-style:don-t-re-invent-the-kernel-macros}
The header file include/linux/kernel.h contains a number of macros that
you should use, rather than explicitly coding some variant of them yourself.
For example, if you need to calculate the length of an array, take advantage
of the macro

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{define ARRAY\PYGZus{}SIZE(x) (sizeof(x) }\PYG{c+cp}{/}\PYG{c+cp}{ sizeof((x)[0]))}
\end{Verbatim}

Similarly, if you need to calculate the size of some structure member, use

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{define FIELD\PYGZus{}SIZEOF(t, f) (sizeof(((t*)0)\PYGZhy{}\PYGZgt{}f))}
\end{Verbatim}

There are also min() and max() macros that do strict type checking if you
need them.  Feel free to peruse that header file to see what else is already
defined that you shouldn't reproduce in your code.


\section{18) Editor modelines and other cruft}
\label{process/coding-style:editor-modelines-and-other-cruft}
Some editors can interpret configuration information embedded in source files,
indicated with special markers.  For example, emacs interprets lines marked
like this:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{*}\PYG{o}{\PYGZhy{}} \PYG{n+nl}{mode}\PYG{p}{:} \PYG{n}{c} \PYG{o}{\PYGZhy{}}\PYG{o}{*}\PYG{o}{\PYGZhy{}}
\end{Verbatim}

Or like this:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/*}
\PYG{c+cm}{Local Variables:}
\PYG{c+cm}{compile\PYGZhy{}command: \PYGZdq{}gcc \PYGZhy{}DMAGIC\PYGZus{}DEBUG\PYGZus{}FLAG foo.c\PYGZdq{}}
\PYG{c+cm}{End:}
\PYG{c+cm}{*/}
\end{Verbatim}

Vim interprets markers that look like this:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/* vim:set sw=8 noet */}
\end{Verbatim}

Do not include any of these in source files.  People have their own personal
editor configurations, and your source files should not override them.  This
includes markers for indentation and mode configuration.  People may use their
own custom mode, or may have some other magic method for making indentation
work correctly.


\section{19) Inline assembly}
\label{process/coding-style:inline-assembly}
In architecture-specific code, you may need to use inline assembly to interface
with CPU or platform functionality.  Don't hesitate to do so when necessary.
However, don't use inline assembly gratuitously when C can do the job.  You can
and should poke hardware from C when possible.

Consider writing simple helper functions that wrap common bits of inline
assembly, rather than repeatedly writing them with slight variations.  Remember
that inline assembly can use C parameters.

Large, non-trivial assembly functions should go in .S files, with corresponding
C prototypes defined in C header files.  The C prototypes for assembly
functions should use \code{asmlinkage}.

You may need to mark your asm statement as volatile, to prevent GCC from
removing it if GCC doesn't notice any side effects.  You don't always need to
do so, though, and doing so unnecessarily can limit optimization.

When writing a single inline assembly statement containing multiple
instructions, put each instruction on a separate line in a separate quoted
string, and end each string except the last with \code{\textbackslash{}n\textbackslash{}t} to properly indent
the next instruction in the assembly output:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{asm} \PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{magic \PYGZpc{}reg1, \PYGZsh{}42}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s}{\PYGZdq{}}
     \PYG{l+s}{\PYGZdq{}}\PYG{l+s}{more\PYGZus{}magic \PYGZpc{}reg2, \PYGZpc{}reg3}\PYG{l+s}{\PYGZdq{}}
     \PYG{o}{:} \PYG{c+cm}{/* outputs */} \PYG{o}{:} \PYG{c+cm}{/* inputs */} \PYG{o}{:} \PYG{c+cm}{/* clobbers */}\PYG{p}{)}\PYG{p}{;}
\end{Verbatim}


\section{20) Conditional Compilation}
\label{process/coding-style:conditional-compilation}
Wherever possible, don't use preprocessor conditionals (\#if, \#ifdef) in .c
files; doing so makes code harder to read and logic harder to follow.  Instead,
use such conditionals in a header file defining functions for use in those .c
files, providing no-op stub versions in the \#else case, and then call those
functions unconditionally from .c files.  The compiler will avoid generating
any code for the stub calls, producing identical results, but the logic will
remain easy to follow.

Prefer to compile out entire functions, rather than portions of functions or
portions of expressions.  Rather than putting an ifdef in an expression, factor
out part or all of the expression into a separate helper function and apply the
conditional to that function.

If you have a function or variable which may potentially go unused in a
particular configuration, and the compiler would warn about its definition
going unused, mark the definition as \_\_maybe\_unused rather than wrapping it in
a preprocessor conditional.  (However, if a function or variable \emph{always} goes
unused, delete it.)

Within code, where possible, use the IS\_ENABLED macro to convert a Kconfig
symbol into a C boolean expression, and use it in a normal C conditional:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k}{if} \PYG{p}{(}\PYG{n}{IS\PYGZus{}ENABLED}\PYG{p}{(}\PYG{n}{CONFIG\PYGZus{}SOMETHING}\PYG{p}{)}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{p}{.}\PYG{p}{.}\PYG{p}{.}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}

The compiler will constant-fold the conditional away, and include or exclude
the block of code just as with an \#ifdef, so this will not add any runtime
overhead.  However, this approach still allows the C compiler to see the code
inside the block, and check it for correctness (syntax, types, symbol
references, etc).  Thus, you still have to use an \#ifdef if the code inside the
block references symbols that will not exist if the condition is not met.

At the end of any non-trivial \#if or \#ifdef block (more than a few lines),
place a comment after the \#endif on the same line, noting the conditional
expression used.  For instance:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{ifdef CONFIG\PYGZus{}SOMETHING}
\PYG{p}{.}\PYG{p}{.}\PYG{p}{.}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{endif }\PYG{c+cm}{/* CONFIG\PYGZus{}SOMETHING */}
\end{Verbatim}


\section{Appendix I) References}
\label{process/coding-style:appendix-i-references}
The C Programming Language, Second Edition
by Brian W. Kernighan and Dennis M. Ritchie.
Prentice Hall, Inc., 1988.
ISBN 0-13-110362-8 (paperback), 0-13-110370-9 (hardback).

The Practice of Programming
by Brian W. Kernighan and Rob Pike.
Addison-Wesley, Inc., 1999.
ISBN 0-201-61586-X.

GNU manuals - where in compliance with K\&R and this text - for cpp, gcc,
gcc internals and indent, all available from \href{http://www.gnu.org/manual/}{http://www.gnu.org/manual/}

WG14 is the international standardization working group for the programming
language C, URL: \href{http://www.open-std.org/JTC1/SC22/WG14/}{http://www.open-std.org/JTC1/SC22/WG14/}

Kernel process/coding-style.rst, by \href{mailto:greg@kroah.com}{greg@kroah.com} at OLS 2002:
\href{http://www.kroah.com/linux/talks/ols\_2002\_kernel\_codingstyle\_talk/html/}{http://www.kroah.com/linux/talks/ols\_2002\_kernel\_codingstyle\_talk/html/}


\chapter{Kernel Maintainer PGP guide}
\label{process/maintainer-pgp-guide:kernel-maintainer-pgp-guide}\label{process/maintainer-pgp-guide::doc}\label{process/maintainer-pgp-guide:pgpguide}\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Konstantin Ryabitsev \textless{}\href{mailto:konstantin@linuxfoundation.org}{konstantin@linuxfoundation.org}\textgreater{}

\end{description}\end{quote}

This document is aimed at Linux kernel developers, and especially at
subsystem maintainers. It contains a subset of information discussed in
the more general ``\href{https://github.com/lfit/itpol/blob/master/protecting-code-integrity.md}{Protecting Code Integrity}'' guide published by the
Linux Foundation. Please read that document for more in-depth discussion
on some of the topics mentioned in this guide.


\section{The role of PGP in Linux Kernel development}
\label{process/maintainer-pgp-guide:protecting-code-integrity}\label{process/maintainer-pgp-guide:the-role-of-pgp-in-linux-kernel-development}
PGP helps ensure the integrity of the code that is produced by the Linux
kernel development community and, to a lesser degree, establish trusted
communication channels between developers via PGP-signed email exchange.

The Linux kernel source code is available in two main formats:
\begin{itemize}
\item {} 
Distributed source repositories (git)

\item {} 
Periodic release snapshots (tarballs)

\end{itemize}

Both git repositories and tarballs carry PGP signatures of the kernel
developers who create official kernel releases. These signatures offer a
cryptographic guarantee that downloadable versions made available via
kernel.org or any other mirrors are identical to what these developers
have on their workstations. To this end:
\begin{itemize}
\item {} 
git repositories provide PGP signatures on all tags

\item {} 
tarballs provide detached PGP signatures with all downloads

\end{itemize}


\subsection{Trusting the developers, not infrastructure}
\label{process/maintainer-pgp-guide:trusting-the-developers-not-infrastructure}\label{process/maintainer-pgp-guide:devs-not-infra}
Ever since the 2011 compromise of core kernel.org systems, the main
operating principle of the Kernel Archives project has been to assume
that any part of the infrastructure can be compromised at any time. For
this reason, the administrators have taken deliberate steps to emphasize
that trust must always be placed with developers and never with the code
hosting infrastructure, regardless of how good the security practices
for the latter may be.

The above guiding principle is the reason why this guide is needed. We
want to make sure that by placing trust into developers we do not simply
shift the blame for potential future security incidents to someone else.
The goal is to provide a set of guidelines developers can use to create
a secure working environment and safeguard the PGP keys used to
establish the integrity of the Linux kernel itself.


\section{PGP tools}
\label{process/maintainer-pgp-guide:pgp-tools}\label{process/maintainer-pgp-guide:id1}

\subsection{Use GnuPG v2}
\label{process/maintainer-pgp-guide:use-gnupg-v2}
Your distro should already have GnuPG installed by default, you just
need to verify that you are using version 2.x and not the legacy 1.4
release -- many distributions still package both, with the default
\code{gpg} command invoking GnuPG v.1. To check, run:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} gpg \PYGZhy{}\PYGZhy{}version \textbar{} head \PYGZhy{}n1
\end{Verbatim}

If you see \code{gpg (GnuPG) 1.4.x}, then you are using GnuPG v.1. Try the
\code{gpg2} command (if you don't have it, you may need to install the
gnupg2 package):

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} gpg2 \PYGZhy{}\PYGZhy{}version \textbar{} head \PYGZhy{}n1
\end{Verbatim}

If you see \code{gpg (GnuPG) 2.x.x}, then you are good to go. This guide
will assume you have the version 2.2 of GnuPG (or later). If you are
using version 2.0 of GnuPG, then some of the commands in this guide will
not work, and you should consider installing the latest 2.2 version of
GnuPG. Versions of gnupg-2.1.11 and later should be compatible for the
purposes of this guide as well.

If you have both \code{gpg} and \code{gpg2} commands, you should make sure you
are always using GnuPG v2, not the legacy version. You can enforce this
by setting the appropriate alias:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} alias gpg=gpg2
\end{Verbatim}

You can put that in your \code{.bashrc} to make sure it's always the case.


\subsubsection{Configure gpg-agent options}
\label{process/maintainer-pgp-guide:configure-gpg-agent-options}
The GnuPG agent is a helper tool that will start automatically whenever
you use the \code{gpg} command and run in the background with the purpose
of caching the private key passphrase. There are two options you should
know in order to tweak when the passphrase should be expired from cache:
\begin{itemize}
\item {} 
\code{default-cache-ttl} (seconds): If you use the same key again before
the time-to-live expires, the countdown will reset for another period.
The default is 600 (10 minutes).

\item {} 
\code{max-cache-ttl} (seconds): Regardless of how recently you've used
the key since initial passphrase entry, if the maximum time-to-live
countdown expires, you'll have to enter the passphrase again. The
default is 30 minutes.

\end{itemize}

If you find either of these defaults too short (or too long), you can
edit your \code{\textasciitilde{}/.gnupg/gpg-agent.conf} file to set your own values:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZsh{} set to 30 minutes for regular ttl, and 2 hours for max ttl
default\PYGZhy{}cache\PYGZhy{}ttl 1800
max\PYGZhy{}cache\PYGZhy{}ttl 7200
\end{Verbatim}

\begin{notice}{note}{Note:}
It is no longer necessary to start gpg-agent manually at the
beginning of your shell session. You may want to check your rc files
to remove anything you had in place for older versions of GnuPG, as
it may not be doing the right thing any more.
\end{notice}


\subsubsection{Set up a refresh cronjob}
\label{process/maintainer-pgp-guide:set-up-a-refresh-cronjob}
You will need to regularly refresh your keyring in order to get the
latest changes on other people's public keys, which is best done with a
daily cronjob:

\begin{Verbatim}[commandchars=\\\{\}]
@daily /usr/bin/gpg2 \PYGZhy{}\PYGZhy{}refresh \PYGZgt{}/dev/null 2\PYGZgt{}\PYGZam{}1
\end{Verbatim}

Check the full path to your \code{gpg} or \code{gpg2} command and use the
\code{gpg2} command if regular \code{gpg} for you is the legacy GnuPG v.1.


\section{Protect your master PGP key}
\label{process/maintainer-pgp-guide:protect-your-master-pgp-key}\label{process/maintainer-pgp-guide:master-key}
This guide assumes that you already have a PGP key that you use for Linux
kernel development purposes. If you do not yet have one, please see the
``\href{https://github.com/lfit/itpol/blob/master/protecting-code-integrity.md}{Protecting Code Integrity}'' document mentioned earlier for guidance
on how to create a new one.

You should also make a new key if your current one is weaker than 2048 bits
(RSA).


\subsection{Master key vs. Subkeys}
\label{process/maintainer-pgp-guide:master-key-vs-subkeys}
Subkeys are fully independent PGP keypairs that are tied to the ``master''
key using certifying key signatures (certificates). It is important to
understand the following:
\begin{enumerate}
\item {} 
There are no technical differences between the ``master key'' and ``subkeys.''

\item {} 
At creation time, we assign functional limitations to each key by
giving it specific capabilities.

\item {} 
A PGP key can have 4 capabilities:
\begin{itemize}
\item {} 
\textbf{{[}S{]}} key can be used for signing

\item {} 
\textbf{{[}E{]}} key can be used for encryption

\item {} 
\textbf{{[}A{]}} key can be used for authentication

\item {} 
\textbf{{[}C{]}} key can be used for certifying other keys

\end{itemize}

\item {} 
A single key may have multiple capabilities.

\item {} 
A subkey is fully independent from the master key. A message
encrypted to a subkey cannot be decrypted with the master key. If you
lose your private subkey, it cannot be recreated from the master key
in any way.

\end{enumerate}

The key carrying the \textbf{{[}C{]}} (certify) capability is considered the
``master'' key because it is the only key that can be used to indicate
relationship with other keys. Only the \textbf{{[}C{]}} key can be used to:
\begin{itemize}
\item {} 
add or revoke other keys (subkeys) with S/E/A capabilities

\item {} 
add, change or revoke identities (uids) associated with the key

\item {} 
add or change the expiration date on itself or any subkey

\item {} 
sign other people's keys for web of trust purposes

\end{itemize}

By default, GnuPG creates the following when generating new keys:
\begin{itemize}
\item {} 
A master key carrying both Certify and Sign capabilities (\textbf{{[}SC{]}})

\item {} 
A separate subkey with the Encryption capability (\textbf{{[}E{]}})

\end{itemize}

If you used the default parameters when generating your key, then that
is what you will have. You can verify by running \code{gpg -{-}list-secret-keys},
for example:

\begin{Verbatim}[commandchars=\\\{\}]
sec   rsa2048 2018\PYGZhy{}01\PYGZhy{}23 [SC] [expires: 2020\PYGZhy{}01\PYGZhy{}23]
      000000000000000000000000AAAABBBBCCCCDDDD
uid           [ultimate] Alice Dev \PYGZlt{}adev@kernel.org\PYGZgt{}
ssb   rsa2048 2018\PYGZhy{}01\PYGZhy{}23 [E] [expires: 2020\PYGZhy{}01\PYGZhy{}23]
\end{Verbatim}

Any key carrying the \textbf{{[}C{]}} capability is your master key, regardless
of any other capabilities it may have assigned to it.

The long line under the \code{sec} entry is your key fingerprint --
whenever you see \code{{[}fpr{]}} in the examples below, that 40-character
string is what it refers to.


\subsection{Ensure your passphrase is strong}
\label{process/maintainer-pgp-guide:ensure-your-passphrase-is-strong}
GnuPG uses passphrases to encrypt your private keys before storing them on
disk. This way, even if your \code{.gnupg} directory is leaked or stolen in
its entirety, the attackers cannot use your private keys without first
obtaining the passphrase to decrypt them.

It is absolutely essential that your private keys are protected by a
strong passphrase. To set it or change it, use:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} gpg \PYGZhy{}\PYGZhy{}change\PYGZhy{}passphrase [fpr]
\end{Verbatim}


\subsection{Create a separate Signing subkey}
\label{process/maintainer-pgp-guide:create-a-separate-signing-subkey}
Our goal is to protect your master key by moving it to offline media, so
if you only have a combined \textbf{{[}SC{]}} key, then you should create a separate
signing subkey:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} gpg \PYGZhy{}\PYGZhy{}quick\PYGZhy{}add\PYGZhy{}key [fpr] ed25519 sign
\end{Verbatim}

Remember to tell the keyservers about this change, so others can pull down
your new subkey:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} gpg \PYGZhy{}\PYGZhy{}send\PYGZhy{}key [fpr]
\end{Verbatim}

\begin{notice}{note}{Note:}
ECC support in GnuPG

GnuPG 2.1 and later has full support for Elliptic Curve
Cryptography, with ability to combine ECC subkeys with traditional
RSA master keys. The main upside of ECC cryptography is that it is
much faster computationally and creates much smaller signatures when
compared byte for byte with 2048+ bit RSA keys. Unless you plan on
using a smartcard device that does not support ECC operations, we
recommend that you create an ECC signing subkey for your kernel
work.

If for some reason you prefer to stay with RSA subkeys, just replace
``ed25519'' with ``rsa2048'' in the above command.
\end{notice}


\subsection{Back up your master key for disaster recovery}
\label{process/maintainer-pgp-guide:back-up-your-master-key-for-disaster-recovery}
The more signatures you have on your PGP key from other developers, the
more reasons you have to create a backup version that lives on something
other than digital media, for disaster recovery reasons.

The best way to create a printable hardcopy of your private key is by
using the \code{paperkey} software written for this very purpose. See \code{man
paperkey} for more details on the output format and its benefits over
other solutions. Paperkey should already be packaged for most
distributions.

Run the following command to create a hardcopy backup of your private
key:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} gpg \PYGZhy{}\PYGZhy{}export\PYGZhy{}secret\PYGZhy{}key [fpr] \textbar{} paperkey \PYGZhy{}o /tmp/key\PYGZhy{}backup.txt
\end{Verbatim}

Print out that file (or pipe the output straight to lpr), then take a
pen and write your passphrase on the margin of the paper. \textbf{This is
strongly recommended} because the key printout is still encrypted with
that passphrase, and if you ever change it you will not remember what it
used to be when you had created the backup -- \emph{guaranteed}.

Put the resulting printout and the hand-written passphrase into an envelope
and store in a secure and well-protected place, preferably away from your
home, such as your bank vault.

\begin{notice}{note}{Note:}
Your printer is probably no longer a simple dumb device connected to
your parallel port, but since the output is still encrypted with
your passphrase, printing out even to ``cloud-integrated'' modern
printers should remain a relatively safe operation. One option is to
change the passphrase on your master key immediately after you are
done with paperkey.
\end{notice}


\subsection{Back up your whole GnuPG directory}
\label{process/maintainer-pgp-guide:back-up-your-whole-gnupg-directory}
\begin{notice}{warning}{Warning:}
\textbf{!!!Do not skip this step!!!}
\end{notice}

It is important to have a readily available backup of your PGP keys
should you need to recover them. This is different from the
disaster-level preparedness we did with \code{paperkey}. You will also rely
on these external copies whenever you need to use your Certify key --
such as when making changes to your own key or signing other people's
keys after conferences and summits.

Start by getting a small USB ``thumb'' drive (preferably two!) that you
will use for backup purposes. You will need to encrypt them using LUKS
-- refer to your distro's documentation on how to accomplish this.

For the encryption passphrase, you can use the same one as on your
master key.

Once the encryption process is over, re-insert the USB drive and make
sure it gets properly mounted. Copy your entire \code{.gnupg} directory
over to the encrypted storage:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} cp \PYGZhy{}a \PYGZti{}/.gnupg /media/disk/foo/gnupg\PYGZhy{}backup
\end{Verbatim}

You should now test to make sure everything still works:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} gpg \PYGZhy{}\PYGZhy{}homedir=/media/disk/foo/gnupg\PYGZhy{}backup \PYGZhy{}\PYGZhy{}list\PYGZhy{}key [fpr]
\end{Verbatim}

If you don't get any errors, then you should be good to go. Unmount the
USB drive, distinctly label it so you don't blow it away next time you
need to use a random USB drive, and put in a safe place -- but not too
far away, because you'll need to use it every now and again for things
like editing identities, adding or revoking subkeys, or signing other
people's keys.


\subsection{Remove the master key from  your homedir}
\label{process/maintainer-pgp-guide:remove-the-master-key-from-your-homedir}
The files in our home directory are not as well protected as we like to
think.  They can be leaked or stolen via many different means:
\begin{itemize}
\item {} 
by accident when making quick homedir copies to set up a new workstation

\item {} 
by systems administrator negligence or malice

\item {} 
via poorly secured backups

\item {} 
via malware in desktop apps (browsers, pdf viewers, etc)

\item {} 
via coercion when crossing international borders

\end{itemize}

Protecting your key with a good passphrase greatly helps reduce the risk
of any of the above, but passphrases can be discovered via keyloggers,
shoulder-surfing, or any number of other means. For this reason, the
recommended setup is to remove your master key from your home directory
and store it on offline storage.

\begin{notice}{warning}{Warning:}
Please see the previous section and make sure you have backed up
your GnuPG directory in its entirety. What we are about to do will
render your key useless if you do not have a usable backup!
\end{notice}

First, identify the keygrip of your master key:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} gpg \PYGZhy{}\PYGZhy{}with\PYGZhy{}keygrip \PYGZhy{}\PYGZhy{}list\PYGZhy{}key [fpr]
\end{Verbatim}

The output will be something like this:

\begin{Verbatim}[commandchars=\\\{\}]
pub   rsa2048 2018\PYGZhy{}01\PYGZhy{}24 [SC] [expires: 2020\PYGZhy{}01\PYGZhy{}24]
      000000000000000000000000AAAABBBBCCCCDDDD
      Keygrip = 1111000000000000000000000000000000000000
uid           [ultimate] Alice Dev \PYGZlt{}adev@kernel.org\PYGZgt{}
sub   rsa2048 2018\PYGZhy{}01\PYGZhy{}24 [E] [expires: 2020\PYGZhy{}01\PYGZhy{}24]
      Keygrip = 2222000000000000000000000000000000000000
sub   ed25519 2018\PYGZhy{}01\PYGZhy{}24 [S]
      Keygrip = 3333000000000000000000000000000000000000
\end{Verbatim}

Find the keygrip entry that is beneath the \code{pub} line (right under the
master key fingerprint). This will correspond directly to a file in your
\code{\textasciitilde{}/.gnupg} directory:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} cd \PYGZti{}/.gnupg/private\PYGZhy{}keys\PYGZhy{}v1.d
\PYGZdl{} ls
1111000000000000000000000000000000000000.key
2222000000000000000000000000000000000000.key
3333000000000000000000000000000000000000.key
\end{Verbatim}

All you have to do is simply remove the .key file that corresponds to
the master keygrip:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} cd \PYGZti{}/.gnupg/private\PYGZhy{}keys\PYGZhy{}v1.d
\PYGZdl{} rm 1111000000000000000000000000000000000000.key
\end{Verbatim}

Now, if you issue the \code{-{-}list-secret-keys} command, it will show that
the master key is missing (the \code{\#} indicates it is not available):

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} gpg \PYGZhy{}\PYGZhy{}list\PYGZhy{}secret\PYGZhy{}keys
sec\PYGZsh{}  rsa2048 2018\PYGZhy{}01\PYGZhy{}24 [SC] [expires: 2020\PYGZhy{}01\PYGZhy{}24]
      000000000000000000000000AAAABBBBCCCCDDDD
uid           [ultimate] Alice Dev \PYGZlt{}adev@kernel.org\PYGZgt{}
ssb   rsa2048 2018\PYGZhy{}01\PYGZhy{}24 [E] [expires: 2020\PYGZhy{}01\PYGZhy{}24]
ssb   ed25519 2018\PYGZhy{}01\PYGZhy{}24 [S]
\end{Verbatim}

You should also remove any \code{secring.gpg} files in the \code{\textasciitilde{}/.gnupg}
directory, which are left over from earlier versions of GnuPG.


\subsubsection{If you don't have the ``private-keys-v1.d'' directory}
\label{process/maintainer-pgp-guide:if-you-don-t-have-the-private-keys-v1-d-directory}
If you do not have a \code{\textasciitilde{}/.gnupg/private-keys-v1.d} directory, then your
secret keys are still stored in the legacy \code{secring.gpg} file used by
GnuPG v1. Making any changes to your key, such as changing the
passphrase or adding a subkey, should automatically convert the old
\code{secring.gpg} format to use \code{private-keys-v1.d} instead.

Once you get that done, make sure to delete the obsolete \code{secring.gpg}
file, which still contains your private keys.


\section{Move the subkeys to a dedicated crypto device}
\label{process/maintainer-pgp-guide:smartcards}\label{process/maintainer-pgp-guide:move-the-subkeys-to-a-dedicated-crypto-device}
Even though the master key is now safe from being leaked or stolen, the
subkeys are still in your home directory. Anyone who manages to get
their hands on those will be able to decrypt your communication or fake
your signatures (if they know the passphrase). Furthermore, each time a
GnuPG operation is performed, the keys are loaded into system memory and
can be stolen from there by sufficiently advanced malware (think
Meltdown and Spectre).

The best way to completely protect your keys is to move them to a
specialized hardware device that is capable of smartcard operations.


\subsection{The benefits of smartcards}
\label{process/maintainer-pgp-guide:the-benefits-of-smartcards}
A smartcard contains a cryptographic chip that is capable of storing
private keys and performing crypto operations directly on the card
itself. Because the key contents never leave the smartcard, the
operating system of the computer into which you plug in the hardware
device is not able to retrieve the private keys themselves. This is very
different from the encrypted USB storage device we used earlier for
backup purposes -- while that USB device is plugged in and mounted, the
operating system is able to access the private key contents.

Using external encrypted USB media is not a substitute to having a
smartcard-capable device.


\subsection{Available smartcard devices}
\label{process/maintainer-pgp-guide:available-smartcard-devices}
Unless all your laptops and workstations have smartcard readers, the
easiest is to get a specialized USB device that implements smartcard
functionality.  There are several options available:
\begin{itemize}
\item {} 
\href{https://shop.nitrokey.com/shop/product/nitrokey-start-6}{Nitrokey Start}: Open hardware and Free Software, based on FSI
Japan's \href{http://www.fsij.org/doc-gnuk/}{Gnuk}. Offers support for ECC keys, but fewest security
features (such as resistance to tampering or some side-channel
attacks).

\item {} 
\href{https://shop.nitrokey.com/shop/product/nitrokey-pro-3}{Nitrokey Pro}: Similar to the Nitrokey Start, but more
tamper-resistant and offers more security features, but no ECC
support.

\item {} 
\href{https://www.yubico.com/product/yubikey-4-series/}{Yubikey 4}: proprietary hardware and software, but cheaper than
Nitrokey Pro and comes available in the USB-C form that is more useful
with newer laptops. Offers additional security features such as FIDO
U2F, but no ECC.

\end{itemize}

\href{https://lwn.net/Articles/736231/}{LWN has a good review} of some of the above models, as well as several
others. If you want to use ECC keys, your best bet among commercially
available devices is the Nitrokey Start.


\subsection{Configure your smartcard device}
\label{process/maintainer-pgp-guide:lwn-has-a-good-review}\label{process/maintainer-pgp-guide:configure-your-smartcard-device}
Your smartcard device should Just Work (TM) the moment you plug it into
any modern Linux workstation. You can verify it by running:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} gpg \PYGZhy{}\PYGZhy{}card\PYGZhy{}status
\end{Verbatim}

If you see full smartcard details, then you are good to go.
Unfortunately, troubleshooting all possible reasons why things may not
be working for you is way beyond the scope of this guide. If you are
having trouble getting the card to work with GnuPG, please seek help via
usual support channels.

To configure your smartcard, you will need to use the GnuPG menu system, as
there are no convenient command-line switches:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} gpg \PYGZhy{}\PYGZhy{}card\PYGZhy{}edit
[...omitted...]
gpg/card\PYGZgt{} admin
Admin commands are allowed
gpg/card\PYGZgt{} passwd
\end{Verbatim}

You should set the user PIN (1), Admin PIN (3), and the Reset Code (4).
Please make sure to record and store these in a safe place -- especially
the Admin PIN and the Reset Code (which allows you to completely wipe
the smartcard).  You so rarely need to use the Admin PIN, that you will
inevitably forget what it is if you do not record it.

Getting back to the main card menu, you can also set other values (such
as name, sex, login data, etc), but it's not necessary and will
additionally leak information about your smartcard should you lose it.

\begin{notice}{note}{Note:}
Despite having the name ``PIN'', neither the user PIN nor the admin
PIN on the card need to be numbers.
\end{notice}


\subsection{Move the subkeys to your smartcard}
\label{process/maintainer-pgp-guide:move-the-subkeys-to-your-smartcard}
Exit the card menu (using ``q'') and save all changes. Next, let's move
your subkeys onto the smartcard. You will need both your PGP key
passphrase and the admin PIN of the card for most operations:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} gpg \PYGZhy{}\PYGZhy{}edit\PYGZhy{}key [fpr]

Secret subkeys are available.

pub  rsa2048/AAAABBBBCCCCDDDD
     created: 2018\PYGZhy{}01\PYGZhy{}23  expires: 2020\PYGZhy{}01\PYGZhy{}23  usage: SC
     trust: ultimate      validity: ultimate
ssb  rsa2048/1111222233334444
     created: 2018\PYGZhy{}01\PYGZhy{}23  expires: never       usage: E
ssb  ed25519/5555666677778888
     created: 2017\PYGZhy{}12\PYGZhy{}07  expires: never       usage: S
[ultimate] (1). Alice Dev \PYGZlt{}adev@kernel.org\PYGZgt{}

gpg\PYGZgt{}
\end{Verbatim}

Using \code{-{-}edit-key} puts us into the menu mode again, and you will
notice that the key listing is a little different. From here on, all
commands are done from inside this menu mode, as indicated by \code{gpg\textgreater{}}.

First, let's select the key we'll be putting onto the card -- you do
this by typing \code{key 1} (it's the first one in the listing, the \textbf{{[}E{]}}
subkey):

\begin{Verbatim}[commandchars=\\\{\}]
gpg\PYGZgt{} key 1
\end{Verbatim}

In the output, you should now see \code{ssb*} on the \textbf{{[}E{]}} key. The \code{*}
indicates which key is currently ``selected.'' It works as a \emph{toggle},
meaning that if you type \code{key 1} again, the \code{*} will disappear and
the key will not be selected any more.

Now, let's move that key onto the smartcard:

\begin{Verbatim}[commandchars=\\\{\}]
gpg\PYGZgt{} keytocard
Please select where to store the key:
   (2) Encryption key
Your selection? 2
\end{Verbatim}

Since it's our \textbf{{[}E{]}} key, it makes sense to put it into the Encryption
slot.  When you submit your selection, you will be prompted first for
your PGP key passphrase, and then for the admin PIN. If the command
returns without an error, your key has been moved.

\textbf{Important}: Now type \code{key 1} again to unselect the first key, and
\code{key 2} to select the \textbf{{[}S{]}} key:

\begin{Verbatim}[commandchars=\\\{\}]
gpg\PYGZgt{} key 1
gpg\PYGZgt{} key 2
gpg\PYGZgt{} keytocard
Please select where to store the key:
   (1) Signature key
   (3) Authentication key
Your selection? 1
\end{Verbatim}

You can use the \textbf{{[}S{]}} key both for Signature and Authentication, but
we want to make sure it's in the Signature slot, so choose (1). Once
again, if your command returns without an error, then the operation was
successful:

\begin{Verbatim}[commandchars=\\\{\}]
gpg\PYGZgt{} q
Save changes? (y/N) y
\end{Verbatim}

Saving the changes will delete the keys you moved to the card from your
home directory (but it's okay, because we have them in our backups
should we need to do this again for a replacement smartcard).


\subsubsection{Verifying that the keys were moved}
\label{process/maintainer-pgp-guide:verifying-that-the-keys-were-moved}
If you perform \code{-{-}list-secret-keys} now, you will see a subtle
difference in the output:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} gpg \PYGZhy{}\PYGZhy{}list\PYGZhy{}secret\PYGZhy{}keys
sec\PYGZsh{}  rsa2048 2018\PYGZhy{}01\PYGZhy{}24 [SC] [expires: 2020\PYGZhy{}01\PYGZhy{}24]
      000000000000000000000000AAAABBBBCCCCDDDD
uid           [ultimate] Alice Dev \PYGZlt{}adev@kernel.org\PYGZgt{}
ssb\PYGZgt{}  rsa2048 2018\PYGZhy{}01\PYGZhy{}24 [E] [expires: 2020\PYGZhy{}01\PYGZhy{}24]
ssb\PYGZgt{}  ed25519 2018\PYGZhy{}01\PYGZhy{}24 [S]
\end{Verbatim}

The \code{\textgreater{}} in the \code{ssb\textgreater{}} output indicates that the subkey is only
available on the smartcard. If you go back into your secret keys
directory and look at the contents there, you will notice that the
\code{.key} files there have been replaced with stubs:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} cd \PYGZti{}/.gnupg/private\PYGZhy{}keys\PYGZhy{}v1.d
\PYGZdl{} strings *.key \textbar{} grep \PYGZsq{}private\PYGZhy{}key\PYGZsq{}
\end{Verbatim}

The output should contain \code{shadowed-private-key} to indicate that
these files are only stubs and the actual content is on the smartcard.


\subsubsection{Verifying that the smartcard is functioning}
\label{process/maintainer-pgp-guide:verifying-that-the-smartcard-is-functioning}
To verify that the smartcard is working as intended, you can create a
signature:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} echo \PYGZdq{}Hello world\PYGZdq{} \textbar{} gpg \PYGZhy{}\PYGZhy{}clearsign \PYGZgt{} /tmp/test.asc
\PYGZdl{} gpg \PYGZhy{}\PYGZhy{}verify /tmp/test.asc
\end{Verbatim}

This should ask for your smartcard PIN on your first command, and then
show ``Good signature'' after you run \code{gpg -{-}verify}.

Congratulations, you have successfully made it extremely difficult to
steal your digital developer identity!


\subsection{Other common GnuPG operations}
\label{process/maintainer-pgp-guide:other-common-gnupg-operations}
Here is a quick reference for some common operations you'll need to do
with your PGP key.


\subsubsection{Mounting your master key offline storage}
\label{process/maintainer-pgp-guide:mounting-your-master-key-offline-storage}
You will need your master key for any of the operations below, so you
will first need to mount your backup offline storage and tell GnuPG to
use it:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} export GNUPGHOME=/media/disk/foo/gnupg\PYGZhy{}backup
\PYGZdl{} gpg \PYGZhy{}\PYGZhy{}list\PYGZhy{}secret\PYGZhy{}keys
\end{Verbatim}

You want to make sure that you see \code{sec} and not \code{sec\#} in the
output (the \code{\#} means the key is not available and you're still using
your regular home directory location).


\subsubsection{Extending key expiration date}
\label{process/maintainer-pgp-guide:extending-key-expiration-date}
The master key has the default expiration date of 2 years from the date
of creation. This is done both for security reasons and to make obsolete
keys eventually disappear from keyservers.

To extend the expiration on your key by a year from current date, just
run:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} gpg \PYGZhy{}\PYGZhy{}quick\PYGZhy{}set\PYGZhy{}expire [fpr] 1y
\end{Verbatim}

You can also use a specific date if that is easier to remember (e.g.
your birthday, January 1st, or Canada Day):

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} gpg \PYGZhy{}\PYGZhy{}quick\PYGZhy{}set\PYGZhy{}expire [fpr] 2020\PYGZhy{}07\PYGZhy{}01
\end{Verbatim}

Remember to send the updated key back to keyservers:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} gpg \PYGZhy{}\PYGZhy{}send\PYGZhy{}key [fpr]
\end{Verbatim}


\subsubsection{Updating your work directory after any changes}
\label{process/maintainer-pgp-guide:updating-your-work-directory-after-any-changes}
After you make any changes to your key using the offline storage, you will
want to import these changes back into your regular working directory:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} gpg \PYGZhy{}\PYGZhy{}export \textbar{} gpg \PYGZhy{}\PYGZhy{}homedir \PYGZti{}/.gnupg \PYGZhy{}\PYGZhy{}import
\PYGZdl{} unset GNUPGHOME
\end{Verbatim}


\section{Using PGP with Git}
\label{process/maintainer-pgp-guide:using-pgp-with-git}
One of the core features of Git is its decentralized nature -- once a
repository is cloned to your system, you have full history of the
project, including all of its tags, commits and branches. However, with
hundreds of cloned repositories floating around, how does anyone verify
that their copy of linux.git has not been tampered with by a malicious
third party?

Or what happens if a backdoor is discovered in the code and the ``Author''
line in the commit says it was done by you, while you're pretty sure you
had \href{https://github.com/jayphelps/git-blame-someone-else}{nothing to do with it}?

To address both of these issues, Git introduced PGP integration. Signed
tags prove the repository integrity by assuring that its contents are
exactly the same as on the workstation of the developer who created the
tag, while signed commits make it nearly impossible for someone to
impersonate you without having access to your PGP keys.


\subsection{Configure git to use your PGP key}
\label{process/maintainer-pgp-guide:nothing-to-do-with-it}\label{process/maintainer-pgp-guide:configure-git-to-use-your-pgp-key}
If you only have one secret key in your keyring, then you don't really
need to do anything extra, as it becomes your default key.  However, if
you happen to have multiple secret keys, you can tell git which key
should be used (\code{{[}fpr{]}} is the fingerprint of your key):

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} git config \PYGZhy{}\PYGZhy{}global user.signingKey [fpr]
\end{Verbatim}

\textbf{IMPORTANT}: If you have a distinct \code{gpg2} command, then you should
tell git to always use it instead of the legacy \code{gpg} from version 1:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} git config \PYGZhy{}\PYGZhy{}global gpg.program gpg2
\end{Verbatim}


\subsection{How to work with signed tags}
\label{process/maintainer-pgp-guide:how-to-work-with-signed-tags}
To create a signed tag, simply pass the \code{-s} switch to the tag
command:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} git tag \PYGZhy{}s [tagname]
\end{Verbatim}

Our recommendation is to always sign git tags, as this allows other
developers to ensure that the git repository they are pulling from has
not been maliciously altered.


\subsubsection{How to verify signed tags}
\label{process/maintainer-pgp-guide:how-to-verify-signed-tags}
To verify a signed tag, simply use the \code{verify-tag} command:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} git verify\PYGZhy{}tag [tagname]
\end{Verbatim}

If you are pulling a tag from another fork of the project repository,
git should automatically verify the signature at the tip you're pulling
and show you the results during the merge operation:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} git pull [url] tags/sometag
\end{Verbatim}

The merge message will contain something like this:

\begin{Verbatim}[commandchars=\\\{\}]
Merge tag \PYGZsq{}sometag\PYGZsq{} of [url]

[Tag message]

\PYGZsh{} gpg: Signature made [...]
\PYGZsh{} gpg: Good signature from [...]
\end{Verbatim}

If you are verifying someone else's git tag, then you will need to
import their PGP key. Please refer to the
``{\hyperref[process/maintainer\string-pgp\string-guide:verify\string-identities]{\emph{How to verify kernel developer identities}}}'' section below.


\subsubsection{Configure git to always sign annotated tags}
\label{process/maintainer-pgp-guide:configure-git-to-always-sign-annotated-tags}
Chances are, if you're creating an annotated tag, you'll want to sign
it. To force git to always sign annotated tags, you can set a global
configuration option:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} git config \PYGZhy{}\PYGZhy{}global tag.forceSignAnnotated true
\end{Verbatim}


\subsection{How to work with signed commits}
\label{process/maintainer-pgp-guide:how-to-work-with-signed-commits}
It is easy to create signed commits, but it is much more difficult to
use them in Linux kernel development, since it relies on patches sent to
the mailing list, and this workflow does not preserve PGP commit
signatures. Furthermore, when rebasing your repository to match
upstream, even your own PGP commit signatures will end up discarded. For
this reason, most kernel developers don't bother signing their commits
and will ignore signed commits in any external repositories that they
rely upon in their work.

However, if you have your working git tree publicly available at some
git hosting service (kernel.org, infradead.org, ozlabs.org, or others),
then the recommendation is that you sign all your git commits even if
upstream developers do not directly benefit from this practice.

We recommend this for the following reasons:
\begin{enumerate}
\item {} 
Should there ever be a need to perform code forensics or track code
provenance, even externally maintained trees carrying PGP commit
signatures will be valuable for such purposes.

\item {} 
If you ever need to re-clone your local repository (for example,
after a disk failure), this lets you easily verify the repository
integrity before resuming your work.

\item {} 
If someone needs to cherry-pick your commits, this allows them to
quickly verify their integrity before applying them.

\end{enumerate}


\subsubsection{Creating signed commits}
\label{process/maintainer-pgp-guide:creating-signed-commits}
To create a signed commit, you just need to pass the \code{-S} flag to the
\code{git commit} command (it's capital \code{-S} due to collision with
another flag):

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} git commit \PYGZhy{}S
\end{Verbatim}


\subsubsection{Configure git to always sign commits}
\label{process/maintainer-pgp-guide:configure-git-to-always-sign-commits}
You can tell git to always sign commits:

\begin{Verbatim}[commandchars=\\\{\}]
git config \PYGZhy{}\PYGZhy{}global commit.gpgSign true
\end{Verbatim}

\begin{notice}{note}{Note:}
Make sure you configure \code{gpg-agent} before you turn this on.
\end{notice}


\section{How to verify kernel developer identities}
\label{process/maintainer-pgp-guide:how-to-verify-kernel-developer-identities}\label{process/maintainer-pgp-guide:verify-identities}
Signing tags and commits is easy, but how does one go about verifying
that the key used to sign something belongs to the actual kernel
developer and not to a malicious imposter?


\subsection{Configure auto-key-retrieval using WKD and DANE}
\label{process/maintainer-pgp-guide:configure-auto-key-retrieval-using-wkd-and-dane}
If you are not already someone with an extensive collection of other
developers' public keys, then you can jumpstart your keyring by relying
on key auto-discovery and auto-retrieval. GnuPG can piggyback on other
delegated trust technologies, namely DNSSEC and TLS, to get you going if
the prospect of starting your own Web of Trust from scratch is too
daunting.

Add the following to your \code{\textasciitilde{}/.gnupg/gpg.conf}:

\begin{Verbatim}[commandchars=\\\{\}]
auto\PYGZhy{}key\PYGZhy{}locate wkd,dane,local
auto\PYGZhy{}key\PYGZhy{}retrieve
\end{Verbatim}

DNS-Based Authentication of Named Entities (``DANE'') is a method for
publishing public keys in DNS and securing them using DNSSEC signed
zones. Web Key Directory (``WKD'') is the alternative method that uses
https lookups for the same purpose. When using either DANE or WKD for
looking up public keys, GnuPG will validate DNSSEC or TLS certificates,
respectively, before adding auto-retrieved public keys to your local
keyring.

Kernel.org publishes the WKD for all developers who have kernel.org
accounts. Once you have the above changes in your \code{gpg.conf}, you can
auto-retrieve the keys for Linus Torvalds and Greg Kroah-Hartman (if you
don't already have them):

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} gpg \PYGZhy{}\PYGZhy{}locate\PYGZhy{}keys torvalds@kernel.org gregkh@kernel.org
\end{Verbatim}

If you have a kernel.org account, then you should \href{https://korg.wiki.kernel.org/userdoc/mail\#adding\_a\_kernelorg\_uid\_to\_your\_pgp\_key}{add the kernel.org
UID to your key} to make WKD more useful to other kernel developers.


\subsection{Web of Trust (WOT) vs. Trust on First Use (TOFU)}
\label{process/maintainer-pgp-guide:web-of-trust-wot-vs-trust-on-first-use-tofu}\label{process/maintainer-pgp-guide:add-the-kernel-org-uid-to-your-key}
PGP incorporates a trust delegation mechanism known as the ``Web of
Trust.'' At its core, this is an attempt to replace the need for
centralized Certification Authorities of the HTTPS/TLS world. Instead of
various software makers dictating who should be your trusted certifying
entity, PGP leaves this responsibility to each user.

Unfortunately, very few people understand how the Web of Trust works.
While it remains an important aspect of the OpenPGP specification,
recent versions of GnuPG (2.2 and above) have implemented an alternative
mechanism called ``Trust on First Use'' (TOFU). You can think of TOFU as
``the SSH-like approach to trust.'' With SSH, the first time you connect
to a remote system, its key fingerprint is recorded and remembered. If
the key changes in the future, the SSH client will alert you and refuse
to connect, forcing you to make a decision on whether you choose to
trust the changed key or not. Similarly, the first time you import
someone's PGP key, it is assumed to be valid. If at any point in the
future GnuPG comes across another key with the same identity, both the
previously imported key and the new key will be marked as invalid and
you will need to manually figure out which one to keep.

We recommend that you use the combined TOFU+PGP trust model (which is
the new default in GnuPG v2). To set it, add (or modify) the
\code{trust-model} setting in \code{\textasciitilde{}/.gnupg/gpg.conf}:

\begin{Verbatim}[commandchars=\\\{\}]
trust\PYGZhy{}model tofu+pgp
\end{Verbatim}


\subsection{How to use keyservers (more) safely}
\label{process/maintainer-pgp-guide:how-to-use-keyservers-more-safely}
If you get a ``No public key'' error when trying to validate someone's
tag, then you should attempt to lookup that key using a keyserver. It is
important to keep in mind that there is absolutely no guarantee that the
key you retrieve from PGP keyservers belongs to the actual person --
that much is by design. You are supposed to use the Web of Trust to
establish key validity.

How to properly maintain the Web of Trust is beyond the scope of this
document, simply because doing it properly requires both effort and
dedication that tends to be beyond the caring threshold of most human
beings. Here are some shortcuts that will help you reduce the risk of
importing a malicious key.

First, let's say you've tried to run \code{git verify-tag} but it returned
an error saying the key is not found:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} git verify\PYGZhy{}tag sunxi\PYGZhy{}fixes\PYGZhy{}for\PYGZhy{}4.15\PYGZhy{}2
gpg: Signature made Sun 07 Jan 2018 10:51:55 PM EST
gpg:                using RSA key DA73759BF8619E484E5A3B47389A54219C0F2430
gpg:                issuer \PYGZdq{}wens@...org\PYGZdq{}
gpg: Can\PYGZsq{}t check signature: No public key
\end{Verbatim}

Let's query the keyserver for more info about that key fingerprint (the
fingerprint probably belongs to a subkey, so we can't use it directly
without finding out the ID of the master key it is associated with):

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} gpg \PYGZhy{}\PYGZhy{}search DA73759BF8619E484E5A3B47389A54219C0F2430
gpg: data source: hkp://keys.gnupg.net
(1) Chen\PYGZhy{}Yu Tsai \PYGZlt{}wens@...org\PYGZgt{}
      4096 bit RSA key C94035C21B4F2AEB, created: 2017\PYGZhy{}03\PYGZhy{}14, expires: 2019\PYGZhy{}03\PYGZhy{}15
Keys 1\PYGZhy{}1 of 1 for \PYGZdq{}DA73759BF8619E484E5A3B47389A54219C0F2430\PYGZdq{}.  Enter number(s), N)ext, or Q)uit \PYGZgt{} q
\end{Verbatim}

Locate the ID of the master key in the output, in our example
\code{C94035C21B4F2AEB}. Now display the key of Linus Torvalds that you
have on your keyring:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} gpg \PYGZhy{}\PYGZhy{}list\PYGZhy{}key torvalds@kernel.org
pub   rsa2048 2011\PYGZhy{}09\PYGZhy{}20 [SC]
      ABAF11C65A2970B130ABE3C479BE3E4300411886
uid           [ unknown] Linus Torvalds \PYGZlt{}torvalds@kernel.org\PYGZgt{}
sub   rsa2048 2011\PYGZhy{}09\PYGZhy{}20 [E]
\end{Verbatim}

Next, open the \href{https://pgp.cs.uu.nl/}{PGP pathfinder}. In the ``From'' field, paste the key
fingerprint of Linus Torvalds from the output above. In the ``To'' field,
paste they key-id you found via \code{gpg -{-}search} of the unknown key, and
check the results:
\begin{itemize}
\item {} 
\href{https://pgp.cs.uu.nl/paths/79BE3E4300411886/to/C94035C21B4F2AEB.html}{Finding paths to Linus}

\end{itemize}

If you get a few decent trust paths, then it's a pretty good indication
that it is a valid key. You can add it to your keyring from the
keyserver now:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} gpg \PYGZhy{}\PYGZhy{}recv\PYGZhy{}key C94035C21B4F2AEB
\end{Verbatim}

This process is not perfect, and you are obviously trusting the
administrators of the PGP Pathfinder service to not be malicious (in
fact, this goes against {\hyperref[process/maintainer\string-pgp\string-guide:devs\string-not\string-infra]{\emph{Trusting the developers, not infrastructure}}}). However, if you
do not carefully maintain your own web of trust, then it is a marked
improvement over blindly trusting keyservers.


\chapter{Email clients info for Linux}
\label{process/email-clients:email-clients-info-for-linux}\label{process/email-clients::doc}\label{process/email-clients:email-clients}\label{process/email-clients:finding-paths-to-linus}

\section{Git}
\label{process/email-clients:git}
These days most developers use \code{git send-email} instead of regular
email clients.  The man page for this is quite good.  On the receiving
end, maintainers use \code{git am} to apply the patches.

If you are new to \code{git} then send your first patch to yourself.  Save it
as raw text including all the headers.  Run \code{git am raw\_email.txt} and
then review the changelog with \code{git log}.  When that works then send
the patch to the appropriate mailing list(s).


\section{General Preferences}
\label{process/email-clients:general-preferences}
Patches for the Linux kernel are submitted via email, preferably as
inline text in the body of the email.  Some maintainers accept
attachments, but then the attachments should have content-type
\code{text/plain}.  However, attachments are generally frowned upon because
it makes quoting portions of the patch more difficult in the patch
review process.

Email clients that are used for Linux kernel patches should send the
patch text untouched.  For example, they should not modify or delete tabs
or spaces, even at the beginning or end of lines.

Don't send patches with \code{format=flowed}.  This can cause unexpected
and unwanted line breaks.

Don't let your email client do automatic word wrapping for you.
This can also corrupt your patch.

Email clients should not modify the character set encoding of the text.
Emailed patches should be in ASCII or UTF-8 encoding only.
If you configure your email client to send emails with UTF-8 encoding,
you avoid some possible charset problems.

Email clients should generate and maintain References: or In-Reply-To:
headers so that mail threading is not broken.

Copy-and-paste (or cut-and-paste) usually does not work for patches
because tabs are converted to spaces.  Using xclipboard, xclip, and/or
xcutsel may work, but it's best to test this for yourself or just avoid
copy-and-paste.

Don't use PGP/GPG signatures in mail that contains patches.
This breaks many scripts that read and apply the patches.
(This should be fixable.)

It's a good idea to send a patch to yourself, save the received message,
and successfully apply it with `patch' before sending patches to Linux
mailing lists.


\section{Some email client (MUA) hints}
\label{process/email-clients:some-email-client-mua-hints}
Here are some specific MUA configuration hints for editing and sending
patches for the Linux kernel.  These are not meant to be complete
software package configuration summaries.

Legend:
\begin{itemize}
\item {} 
TUI = text-based user interface

\item {} 
GUI = graphical user interface

\end{itemize}


\subsection{Alpine (TUI)}
\label{process/email-clients:alpine-tui}
Config options:

In the \emph{Sending Preferences} section:
\begin{itemize}
\item {} 
\emph{Do Not Send Flowed Text} must be \code{enabled}

\item {} 
\emph{Strip Whitespace Before Sending} must be \code{disabled}

\end{itemize}

When composing the message, the cursor should be placed where the patch
should appear, and then pressing \code{CTRL-R} let you specify the patch file
to insert into the message.


\subsection{Claws Mail (GUI)}
\label{process/email-clients:claws-mail-gui}
Works. Some people use this successfully for patches.

To insert a patch use \emph{Message\(\rightarrow\)Insert} File (\code{CTRL-I})
or an external editor.

If the inserted patch has to be edited in the Claws composition window
``Auto wrapping'' in
\emph{Configuration\(\rightarrow\)Preferences\(\rightarrow\)Compose\(\rightarrow\)Wrapping} should be
disabled.


\subsection{Evolution (GUI)}
\label{process/email-clients:evolution-gui}
Some people use this successfully for patches.
\begin{description}
\item[{When composing mail select: Preformat}] \leavevmode
from \emph{Format\(\rightarrow\)Paragraph Style\(\rightarrow\)Preformatted} (\code{CTRL-7})
or the toolbar

\end{description}

Then use:
\emph{Insert\(\rightarrow\)Text File...} (\code{ALT-N x})
to insert the patch.

You can also \code{diff -Nru old.c new.c \textbar{} xclip}, select
\emph{Preformat}, then paste with the middle button.


\subsection{Kmail (GUI)}
\label{process/email-clients:kmail-gui}
Some people use Kmail successfully for patches.

The default setting of not composing in HTML is appropriate; do not
enable it.

When composing an email, under options, uncheck ``word wrap''. The only
disadvantage is any text you type in the email will not be word-wrapped
so you will have to manually word wrap text before the patch. The easiest
way around this is to compose your email with word wrap enabled, then save
it as a draft. Once you pull it up again from your drafts it is now hard
word-wrapped and you can uncheck ``word wrap'' without losing the existing
wrapping.

At the bottom of your email, put the commonly-used patch delimiter before
inserting your patch:  three hyphens (\code{-{-}-}).

Then from the \emph{Message} menu item, select insert file and
choose your patch.
As an added bonus you can customise the message creation toolbar menu
and put the \emph{insert file} icon there.

Make the composer window wide enough so that no lines wrap. As of
KMail 1.13.5 (KDE 4.5.4), KMail will apply word wrapping when sending
the email if the lines wrap in the composer window. Having word wrapping
disabled in the Options menu isn't enough. Thus, if your patch has very
long lines, you must make the composer window very wide before sending
the email. See: \href{https://bugs.kde.org/show\_bug.cgi?id=174034}{https://bugs.kde.org/show\_bug.cgi?id=174034}

You can safely GPG sign attachments, but inlined text is preferred for
patches so do not GPG sign them.  Signing patches that have been inserted
as inlined text will make them tricky to extract from their 7-bit encoding.

If you absolutely must send patches as attachments instead of inlining
them as text, right click on the attachment and select properties, and
highlight \emph{Suggest automatic display} to make the attachment
inlined to make it more viewable.

When saving patches that are sent as inlined text, select the email that
contains the patch from the message list pane, right click and select
\emph{save as}.  You can use the whole email unmodified as a patch
if it was properly composed.  There is no option currently to save the email
when you are actually viewing it in its own window -- there has been a request
filed at kmail's bugzilla and hopefully this will be addressed.  Emails are
saved as read-write for user only so you will have to chmod them to make them
group and world readable if you copy them elsewhere.


\subsection{Lotus Notes (GUI)}
\label{process/email-clients:lotus-notes-gui}
Run away from it.


\subsection{IBM Verse (Web GUI)}
\label{process/email-clients:ibm-verse-web-gui}
See Lotus Notes.


\subsection{Mutt (TUI)}
\label{process/email-clients:mutt-tui}
Plenty of Linux developers use \code{mutt}, so it must work pretty well.

Mutt doesn't come with an editor, so whatever editor you use should be
used in a way that there are no automatic linebreaks.  Most editors have
an \emph{insert file} option that inserts the contents of a file
unaltered.

To use \code{vim} with mutt:

\begin{Verbatim}[commandchars=\\\{\}]
set editor=\PYGZdq{}vi\PYGZdq{}
\end{Verbatim}

If using xclip, type the command:

\begin{Verbatim}[commandchars=\\\{\}]
:set paste
\end{Verbatim}

before middle button or shift-insert or use:

\begin{Verbatim}[commandchars=\\\{\}]
:r filename
\end{Verbatim}

if you want to include the patch inline.
(a)ttach works fine without \code{set paste}.

You can also generate patches with \code{git format-patch} and then use Mutt
to send them:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} mutt \PYGZhy{}H 0001\PYGZhy{}some\PYGZhy{}bug\PYGZhy{}fix.patch
\end{Verbatim}

Config options:

It should work with default settings.
However, it's a good idea to set the \code{send\_charset} to:

\begin{Verbatim}[commandchars=\\\{\}]
set send\PYGZus{}charset=\PYGZdq{}us\PYGZhy{}ascii:utf\PYGZhy{}8\PYGZdq{}
\end{Verbatim}

Mutt is highly customizable. Here is a minimum configuration to start
using Mutt to send patches through Gmail:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZsh{} .muttrc
\PYGZsh{} ================  IMAP ====================
set imap\PYGZus{}user = \PYGZsq{}yourusername@gmail.com\PYGZsq{}
set imap\PYGZus{}pass = \PYGZsq{}yourpassword\PYGZsq{}
set spoolfile = imaps://imap.gmail.com/INBOX
set folder = imaps://imap.gmail.com/
set record=\PYGZdq{}imaps://imap.gmail.com/[Gmail]/Sent Mail\PYGZdq{}
set postponed=\PYGZdq{}imaps://imap.gmail.com/[Gmail]/Drafts\PYGZdq{}
set mbox=\PYGZdq{}imaps://imap.gmail.com/[Gmail]/All Mail\PYGZdq{}

\PYGZsh{} ================  SMTP  ====================
set smtp\PYGZus{}url = \PYGZdq{}smtp://username@smtp.gmail.com:587/\PYGZdq{}
set smtp\PYGZus{}pass = \PYGZdl{}imap\PYGZus{}pass
set ssl\PYGZus{}force\PYGZus{}tls = yes \PYGZsh{} Require encrypted connection

\PYGZsh{} ================  Composition  ====================
set editor = {}`echo \PYGZbs{}\PYGZdl{}EDITOR{}`
set edit\PYGZus{}headers = yes  \PYGZsh{} See the headers when editing
set charset = UTF\PYGZhy{}8     \PYGZsh{} value of \PYGZdl{}LANG; also fallback for send\PYGZus{}charset
\PYGZsh{} Sender, email address, and sign\PYGZhy{}off line must match
unset use\PYGZus{}domain        \PYGZsh{} because joe@localhost is just embarrassing
set realname = \PYGZdq{}YOUR NAME\PYGZdq{}
set from = \PYGZdq{}username@gmail.com\PYGZdq{}
set use\PYGZus{}from = yes
\end{Verbatim}

The Mutt docs have lots more information:
\begin{quote}

\href{http://dev.mutt.org/trac/wiki/UseCases/Gmail}{http://dev.mutt.org/trac/wiki/UseCases/Gmail}

\href{http://dev.mutt.org/doc/manual.html}{http://dev.mutt.org/doc/manual.html}
\end{quote}


\subsection{Pine (TUI)}
\label{process/email-clients:pine-tui}
Pine has had some whitespace truncation issues in the past, but these
should all be fixed now.

Use alpine (pine's successor) if you can.

Config options:
\begin{itemize}
\item {} 
\code{quell-flowed-text} is needed for recent versions

\item {} 
the \code{no-strip-whitespace-before-send} option is needed

\end{itemize}


\subsection{Sylpheed (GUI)}
\label{process/email-clients:sylpheed-gui}\begin{itemize}
\item {} 
Works well for inlining text (or using attachments).

\item {} 
Allows use of an external editor.

\item {} 
Is slow on large folders.

\item {} 
Won't do TLS SMTP auth over a non-SSL connection.

\item {} 
Has a helpful ruler bar in the compose window.

\item {} 
Adding addresses to address book doesn't understand the display name
properly.

\end{itemize}


\subsection{Thunderbird (GUI)}
\label{process/email-clients:thunderbird-gui}
Thunderbird is an Outlook clone that likes to mangle text, but there are ways
to coerce it into behaving.
\begin{itemize}
\item {} 
Allow use of an external editor:
The easiest thing to do with Thunderbird and patches is to use an
``external editor'' extension and then just use your favorite \code{\$EDITOR}
for reading/merging patches into the body text.  To do this, download
and install the extension, then add a button for it using
\emph{View\(\rightarrow\)Toolbars\(\rightarrow\)Customize...} and finally just click on it
when in the \emph{Compose} dialog.

Please note that ``external editor'' requires that your editor must not
fork, or in other words, the editor must not return before closing.
You may have to pass additional flags or change the settings of your
editor. Most notably if you are using gvim then you must pass the -f
option to gvim by putting \code{/usr/bin/gvim -f} (if the binary is in
\code{/usr/bin}) to the text editor field in \emph{external editor}
settings. If you are using some other editor then please read its manual
to find out how to do this.

\end{itemize}

To beat some sense out of the internal editor, do this:
\begin{itemize}
\item {} 
Edit your Thunderbird config settings so that it won't use \code{format=flowed}.
Go to \emph{edit\(\rightarrow\)preferences\(\rightarrow\)advanced\(\rightarrow\)config editor} to bring up
the thunderbird's registry editor.

\item {} 
Set \code{mailnews.send\_plaintext\_flowed} to \code{false}

\item {} 
Set \code{mailnews.wraplength} from \code{72} to \code{0}

\item {} 
\emph{View\(\rightarrow\)Message Body As\(\rightarrow\)Plain Text}

\item {} 
\emph{View\(\rightarrow\)Character Encoding\(\rightarrow\)Unicode (UTF-8)}

\end{itemize}


\subsection{TkRat (GUI)}
\label{process/email-clients:tkrat-gui}
Works.  Use ``Insert file...'' or external editor.


\subsection{Gmail (Web GUI)}
\label{process/email-clients:gmail-web-gui}
Does not work for sending patches.

Gmail web client converts tabs to spaces automatically.

At the same time it wraps lines every 78 chars with CRLF style line breaks
although tab2space problem can be solved with external editor.

Another problem is that Gmail will base64-encode any message that has a
non-ASCII character. That includes things like European names.


\chapter{Linux Kernel Enforcement Statement}
\label{process/kernel-enforcement-statement:linux-kernel-enforcement-statement}\label{process/kernel-enforcement-statement::doc}
As developers of the Linux kernel, we have a keen interest in how our software
is used and how the license for our software is enforced.  Compliance with the
reciprocal sharing obligations of GPL-2.0 is critical to the long-term
sustainability of our software and community.

Although there is a right to enforce the separate copyright interests in the
contributions made to our community, we share an interest in ensuring that
individual enforcement actions are conducted in a manner that benefits our
community and do not have an unintended negative impact on the health and
growth of our software ecosystem.  In order to deter unhelpful enforcement
actions, we agree that it is in the best interests of our development
community to undertake the following commitment to users of the Linux kernel
on behalf of ourselves and any successors to our copyright interests:
\begin{quote}

Notwithstanding the termination provisions of the GPL-2.0, we agree that
it is in the best interests of our development community to adopt the
following provisions of GPL-3.0 as additional permissions under our
license with respect to any non-defensive assertion of rights under the
license.
\begin{quote}

However, if you cease all violation of this License, then your license
from a particular copyright holder is reinstated (a) provisionally,
unless and until the copyright holder explicitly and finally
terminates your license, and (b) permanently, if the copyright holder
fails to notify you of the violation by some reasonable means prior to
60 days after the cessation.

Moreover, your license from a particular copyright holder is
reinstated permanently if the copyright holder notifies you of the
violation by some reasonable means, this is the first time you have
received notice of violation of this License (for any work) from that
copyright holder, and you cure the violation prior to 30 days after
your receipt of the notice.
\end{quote}
\end{quote}

Our intent in providing these assurances is to encourage more use of the
software.  We want companies and individuals to use, modify and distribute
this software.  We want to work with users in an open and transparent way to
eliminate any uncertainty about our expectations regarding compliance or
enforcement that might limit adoption of our software.  We view legal action
as a last resort, to be initiated only when other community efforts have
failed to resolve the problem.

Finally, once a non-compliance issue is resolved, we hope the user will feel
welcome to join us in our efforts on this project.  Working together, we will
be stronger.

Except where noted below, we speak only for ourselves, and not for any company
we might work for today, have in the past, or will in the future.
\begin{itemize}
\item {} 
Laura Abbott

\item {} 
Bjorn Andersson (Linaro)

\item {} 
Andrea Arcangeli

\item {} 
Neil Armstrong

\item {} 
Jens Axboe

\item {} 
Pablo Neira Ayuso

\item {} 
Khalid Aziz

\item {} 
Ralf Baechle

\item {} 
Felipe Balbi

\item {} 
Arnd Bergmann

\item {} 
Ard Biesheuvel

\item {} 
Tim Bird

\item {} 
Paolo Bonzini

\item {} 
Christian Borntraeger

\item {} 
Mark Brown (Linaro)

\item {} 
Paul Burton

\item {} 
Javier Martinez Canillas

\item {} 
Rob Clark

\item {} 
Kees Cook (Google)

\item {} 
Jonathan Corbet

\item {} 
Dennis Dalessandro

\item {} 
Vivien Didelot (Savoir-faire Linux)

\item {} 
Hans de Goede

\item {} 
Mel Gorman (SUSE)

\item {} 
Sven Eckelmann

\item {} 
Alex Elder (Linaro)

\item {} 
Fabio Estevam

\item {} 
Larry Finger

\item {} 
Bhumika Goyal

\item {} 
Andy Gross

\item {} 
Juergen Gross

\item {} 
Shawn Guo

\item {} 
Ulf Hansson

\item {} 
Stephen Hemminger (Microsoft)

\item {} 
Tejun Heo

\item {} 
Rob Herring

\item {} 
Masami Hiramatsu

\item {} 
Michal Hocko

\item {} 
Simon Horman

\item {} 
Johan Hovold (Hovold Consulting AB)

\item {} 
Christophe JAILLET

\item {} 
Olof Johansson

\item {} 
Lee Jones (Linaro)

\item {} 
Heiner Kallweit

\item {} 
Srinivas Kandagatla

\item {} 
Jan Kara

\item {} 
Shuah Khan (Samsung)

\item {} 
David Kershner

\item {} 
Jaegeuk Kim

\item {} 
Namhyung Kim

\item {} 
Colin Ian King

\item {} 
Jeff Kirsher

\item {} 
Greg Kroah-Hartman (Linux Foundation)

\item {} 
Christian König

\item {} 
Vinod Koul

\item {} 
Krzysztof Kozlowski

\item {} 
Viresh Kumar

\item {} 
Aneesh Kumar K.V

\item {} 
Julia Lawall

\item {} 
Doug Ledford

\item {} 
Chuck Lever (Oracle)

\item {} 
Daniel Lezcano

\item {} 
Shaohua Li

\item {} 
Xin Long

\item {} 
Tony Luck

\item {} 
Catalin Marinas (Arm Ltd)

\item {} 
Mike Marshall

\item {} 
Chris Mason

\item {} 
Paul E. McKenney

\item {} 
Arnaldo Carvalho de Melo

\item {} 
David S. Miller

\item {} 
Ingo Molnar

\item {} 
Kuninori Morimoto

\item {} 
Trond Myklebust

\item {} 
Martin K. Petersen (Oracle)

\item {} 
Borislav Petkov

\item {} 
Jiri Pirko

\item {} 
Josh Poimboeuf

\item {} 
Sebastian Reichel (Collabora)

\item {} 
Guenter Roeck

\item {} 
Joerg Roedel

\item {} 
Leon Romanovsky

\item {} 
Steven Rostedt (VMware)

\item {} 
Frank Rowand

\item {} 
Ivan Safonov

\item {} 
Anna Schumaker

\item {} 
Jes Sorensen

\item {} 
K.Y. Srinivasan

\item {} 
David Sterba (SUSE)

\item {} 
Heiko Stuebner

\item {} 
Jiri Kosina (SUSE)

\item {} 
Willy Tarreau

\item {} 
Dmitry Torokhov

\item {} 
Linus Torvalds

\item {} 
Thierry Reding

\item {} 
Rik van Riel

\item {} 
Luis R. Rodriguez

\item {} 
Geert Uytterhoeven (Glider bvba)

\item {} 
Eduardo Valentin (Amazon.com)

\item {} 
Daniel Vetter

\item {} 
Linus Walleij

\item {} 
Richard Weinberger

\item {} 
Dan Williams

\item {} 
Rafael J. Wysocki

\item {} 
Arvind Yadav

\item {} 
Masahiro Yamada

\item {} 
Wei Yongjun

\item {} 
Lv Zheng

\item {} 
Marc Zyngier (Arm Ltd)

\end{itemize}


\chapter{Kernel Driver Statement}
\label{process/kernel-driver-statement:kernel-driver-statement}\label{process/kernel-driver-statement::doc}

\section{Position Statement on Linux Kernel Modules}
\label{process/kernel-driver-statement:position-statement-on-linux-kernel-modules}
We, the undersigned Linux kernel developers, consider any closed-source
Linux kernel module or driver to be harmful and undesirable. We have
repeatedly found them to be detrimental to Linux users, businesses, and
the greater Linux ecosystem. Such modules negate the openness,
stability, flexibility, and maintainability of the Linux development
model and shut their users off from the expertise of the Linux
community. Vendors that provide closed-source kernel modules force their
customers to give up key Linux advantages or choose new vendors.
Therefore, in order to take full advantage of the cost savings and
shared support benefits open source has to offer, we urge vendors to
adopt a policy of supporting their customers on Linux with open-source
kernel code.

We speak only for ourselves, and not for any company we might work for
today, have in the past, or will in the future.
\begin{itemize}
\item {} 
Dave Airlie

\item {} 
Nick Andrew

\item {} 
Jens Axboe

\item {} 
Ralf Baechle

\item {} 
Felipe Balbi

\item {} 
Ohad Ben-Cohen

\item {} 
Muli Ben-Yehuda

\item {} 
Jiri Benc

\item {} 
Arnd Bergmann

\item {} 
Thomas Bogendoerfer

\item {} 
Vitaly Bordug

\item {} 
James Bottomley

\item {} 
Josh Boyer

\item {} 
Neil Brown

\item {} 
Mark Brown

\item {} 
David Brownell

\item {} 
Michael Buesch

\item {} 
Franck Bui-Huu

\item {} 
Adrian Bunk

\item {} 
François Cami

\item {} 
Ralph Campbell

\item {} 
Luiz Fernando N. Capitulino

\item {} 
Mauro Carvalho Chehab

\item {} 
Denis Cheng

\item {} 
Jonathan Corbet

\item {} 
Glauber Costa

\item {} 
Alan Cox

\item {} 
Magnus Damm

\item {} 
Ahmed S. Darwish

\item {} 
Robert P. J. Day

\item {} 
Hans de Goede

\item {} 
Arnaldo Carvalho de Melo

\item {} 
Helge Deller

\item {} 
Jean Delvare

\item {} 
Mathieu Desnoyers

\item {} 
Sven-Thorsten Dietrich

\item {} 
Alexey Dobriyan

\item {} 
Daniel Drake

\item {} 
Alex Dubov

\item {} 
Randy Dunlap

\item {} 
Michael Ellerman

\item {} 
Pekka Enberg

\item {} 
Jan Engelhardt

\item {} 
Mark Fasheh

\item {} \begin{enumerate}
\setcounter{enumi}{9}
\item {} 
Bruce Fields

\end{enumerate}

\item {} 
Larry Finger

\item {} 
Jeremy Fitzhardinge

\item {} 
Mike Frysinger

\item {} 
Kumar Gala

\item {} 
Robin Getz

\item {} 
Liam Girdwood

\item {} 
Jan-Benedict Glaw

\item {} 
Thomas Gleixner

\item {} 
Brice Goglin

\item {} 
Cyrill Gorcunov

\item {} 
Andy Gospodarek

\item {} 
Thomas Graf

\item {} 
Krzysztof Halasa

\item {} 
Harvey Harrison

\item {} 
Stephen Hemminger

\item {} 
Michael Hennerich

\item {} 
Tejun Heo

\item {} 
Benjamin Herrenschmidt

\item {} 
Kristian Høgsberg

\item {} 
Henrique de Moraes Holschuh

\item {} 
Marcel Holtmann

\item {} 
Mike Isely

\item {} 
Takashi Iwai

\item {} 
Olof Johansson

\item {} 
Dave Jones

\item {} 
Jesper Juhl

\item {} 
Matthias Kaehlcke

\item {} 
Kenji Kaneshige

\item {} 
Jan Kara

\item {} 
Jeremy Kerr

\item {} 
Russell King

\item {} 
Olaf Kirch

\item {} 
Roel Kluin

\item {} 
Hans-Jürgen Koch

\item {} 
Auke Kok

\item {} 
Peter Korsgaard

\item {} 
Jiri Kosina

\item {} 
Mariusz Kozlowski

\item {} 
Greg Kroah-Hartman

\item {} 
Michael Krufky

\item {} 
Aneesh Kumar

\item {} 
Clemens Ladisch

\item {} 
Christoph Lameter

\item {} 
Gunnar Larisch

\item {} 
Anders Larsen

\item {} 
Grant Likely

\item {} 
John W. Linville

\item {} 
Yinghai Lu

\item {} 
Tony Luck

\item {} 
Pavel Machek

\item {} 
Matt Mackall

\item {} 
Paul Mackerras

\item {} 
Roland McGrath

\item {} 
Patrick McHardy

\item {} 
Kyle McMartin

\item {} 
Paul Menage

\item {} 
Thierry Merle

\item {} 
Eric Miao

\item {} 
Akinobu Mita

\item {} 
Ingo Molnar

\item {} 
James Morris

\item {} 
Andrew Morton

\item {} 
Paul Mundt

\item {} 
Oleg Nesterov

\item {} 
Luca Olivetti

\item {} 
S.Çağlar Onur

\item {} 
Pierre Ossman

\item {} 
Keith Owens

\item {} 
Venkatesh Pallipadi

\item {} 
Nick Piggin

\item {} 
Nicolas Pitre

\item {} 
Evgeniy Polyakov

\item {} 
Richard Purdie

\item {} 
Mike Rapoport

\item {} 
Sam Ravnborg

\item {} 
Gerrit Renker

\item {} 
Stefan Richter

\item {} 
David Rientjes

\item {} 
Luis R. Rodriguez

\item {} 
Stefan Roese

\item {} 
Francois Romieu

\item {} 
Rami Rosen

\item {} 
Stephen Rothwell

\item {} 
Maciej W. Rozycki

\item {} 
Mark Salyzyn

\item {} 
Yoshinori Sato

\item {} 
Deepak Saxena

\item {} 
Holger Schurig

\item {} 
Amit Shah

\item {} 
Yoshihiro Shimoda

\item {} 
Sergei Shtylyov

\item {} 
Kay Sievers

\item {} 
Sebastian Siewior

\item {} 
Rik Snel

\item {} 
Jes Sorensen

\item {} 
Alexey Starikovskiy

\item {} 
Alan Stern

\item {} 
Timur Tabi

\item {} 
Hirokazu Takata

\item {} 
Eliezer Tamir

\item {} 
Eugene Teo

\item {} 
Doug Thompson

\item {} 
FUJITA Tomonori

\item {} 
Dmitry Torokhov

\item {} 
Marcelo Tosatti

\item {} 
Steven Toth

\item {} 
Theodore Tso

\item {} 
Matthias Urlichs

\item {} 
Geert Uytterhoeven

\item {} 
Arjan van de Ven

\item {} 
Ivo van Doorn

\item {} 
Rik van Riel

\item {} 
Wim Van Sebroeck

\item {} 
Hans Verkuil

\item {} 
Horst H. von Brand

\item {} 
Dmitri Vorobiev

\item {} 
Anton Vorontsov

\item {} 
Daniel Walker

\item {} 
Johannes Weiner

\item {} 
Harald Welte

\item {} 
Matthew Wilcox

\item {} 
Dan J. Williams

\item {} 
Darrick J. Wong

\item {} 
David Woodhouse

\item {} 
Chris Wright

\item {} 
Bryan Wu

\item {} 
Rafael J. Wysocki

\item {} 
Herbert Xu

\item {} 
Vlad Yasevich

\item {} 
Peter Zijlstra

\item {} 
Bartlomiej Zolnierkiewicz

\end{itemize}

Other guides to the community that are of interest to most developers are:


\chapter{Minimal requirements to compile the Kernel}
\label{process/changes:changes}\label{process/changes:minimal-requirements-to-compile-the-kernel}\label{process/changes::doc}

\section{Intro}
\label{process/changes:intro}
This document is designed to provide a list of the minimum levels of
software necessary to run the 4.x kernels.

This document is originally based on my ``Changes'' file for 2.0.x kernels
and therefore owes credit to the same people as that file (Jared Mauch,
Axel Boldt, Alessandro Sigala, and countless other users all over the
`net).


\subsection{Current Minimal Requirements}
\label{process/changes:current-minimal-requirements}
Upgrade to at \textbf{least} these software revisions before thinking you've
encountered a bug!  If you're unsure what version you're currently
running, the suggested command should tell you.

Again, keep in mind that this list assumes you are already functionally
running a Linux kernel.  Also, not all tools are necessary on all
systems; obviously, if you don't have any ISDN hardware, for example,
you probably needn't concern yourself with isdn4k-utils.

\begin{tabulary}{\linewidth}{|L|L|L|}
\hline
\textsf{\relax 
Program
} & \textsf{\relax 
Minimal version
} & \textsf{\relax 
Command to check the version
}\\
\hline
GNU C
 & 
3.2
 & 
gcc --version
\\
\hline
GNU make
 & 
3.81
 & 
make --version
\\
\hline
binutils
 & 
2.20
 & 
ld -v
\\
\hline
flex
 & 
2.5.35
 & 
flex --version
\\
\hline
bison
 & 
2.0
 & 
bison --version
\\
\hline
util-linux
 & 
2.10o
 & 
fdformat --version
\\
\hline
module-init-tools
 & 
0.9.10
 & 
depmod -V
\\
\hline
e2fsprogs
 & 
1.41.4
 & 
e2fsck -V
\\
\hline
jfsutils
 & 
1.1.3
 & 
fsck.jfs -V
\\
\hline
reiserfsprogs
 & 
3.6.3
 & 
reiserfsck -V
\\
\hline
xfsprogs
 & 
2.6.0
 & 
xfs\_db -V
\\
\hline
squashfs-tools
 & 
4.0
 & 
mksquashfs -version
\\
\hline
btrfs-progs
 & 
0.18
 & 
btrfsck
\\
\hline
pcmciautils
 & 
004
 & 
pccardctl -V
\\
\hline
quota-tools
 & 
3.09
 & 
quota -V
\\
\hline
PPP
 & 
2.4.0
 & 
pppd --version
\\
\hline
isdn4k-utils
 & 
3.1pre1
 & 
isdnctrl 2\textgreater{}\&1\textbar{}grep version
\\
\hline
nfs-utils
 & 
1.0.5
 & 
showmount --version
\\
\hline
procps
 & 
3.2.0
 & 
ps --version
\\
\hline
oprofile
 & 
0.9
 & 
oprofiled --version
\\
\hline
udev
 & 
081
 & 
udevd --version
\\
\hline
grub
 & 
0.93
 & 
grub --version \textbar{}\textbar{} grub-install --version
\\
\hline
mcelog
 & 
0.6
 & 
mcelog --version
\\
\hline
iptables
 & 
1.4.2
 & 
iptables -V
\\
\hline
openssl \& libcrypto
 & 
1.0.0
 & 
openssl version
\\
\hline
bc
 & 
1.06.95
 & 
bc --version
\\
\hline
Sphinx\protect\footnotemark[1]
 & 
1.3
 & 
sphinx-build --version
\\
\hline\end{tabulary}

\footnotetext[1]{
Sphinx is needed only to build the Kernel documentation
}

\subsection{Kernel compilation}
\label{process/changes:kernel-compilation}

\subsubsection{GCC}
\label{process/changes:gcc}
The gcc version requirements may vary depending on the type of CPU in your
computer.


\subsubsection{Make}
\label{process/changes:make}
You will need GNU make 3.81 or later to build the kernel.


\subsubsection{Binutils}
\label{process/changes:binutils}
The build system has, as of 4.13, switched to using thin archives (\emph{ar T})
rather than incremental linking (\emph{ld -r}) for built-in.o intermediate steps.
This requires binutils 2.20 or newer.


\subsubsection{Flex}
\label{process/changes:flex}
Since Linux 4.16, the build system generates lexical analyzers
during build.  This requires flex 2.5.35 or later.


\subsubsection{Bison}
\label{process/changes:bison}
Since Linux 4.16, the build system generates parsers
during build.  This requires bison 2.0 or later.


\subsubsection{Perl}
\label{process/changes:perl}
You will need perl 5 and the following modules: \code{Getopt::Long},
\code{Getopt::Std}, \code{File::Basename}, and \code{File::Find} to build the kernel.


\subsubsection{BC}
\label{process/changes:bc}
You will need bc to build kernels 3.10 and higher


\subsubsection{OpenSSL}
\label{process/changes:openssl}
Module signing and external certificate handling use the OpenSSL program and
crypto library to do key creation and signature generation.

You will need openssl to build kernels 3.7 and higher if module signing is
enabled.  You will also need openssl development packages to build kernels 4.3
and higher.


\subsection{System utilities}
\label{process/changes:system-utilities}

\subsubsection{Architectural changes}
\label{process/changes:architectural-changes}
DevFS has been obsoleted in favour of udev
(\href{http://www.kernel.org/pub/linux/utils/kernel/hotplug/}{http://www.kernel.org/pub/linux/utils/kernel/hotplug/})

32-bit UID support is now in place.  Have fun!

Linux documentation for functions is transitioning to inline
documentation via specially-formatted comments near their
definitions in the source.  These comments can be combined with ReST
files the Documentation/ directory to make enriched documentation, which can
then be converted to PostScript, HTML, LaTex, ePUB and PDF files.
In order to convert from ReST format to a format of your choice, you'll need
Sphinx.


\subsubsection{Util-linux}
\label{process/changes:util-linux}
New versions of util-linux provide \code{fdisk} support for larger disks,
support new options to mount, recognize more supported partition
types, have a fdformat which works with 2.4 kernels, and similar goodies.
You'll probably want to upgrade.


\subsubsection{Ksymoops}
\label{process/changes:ksymoops}
If the unthinkable happens and your kernel oopses, you may need the
ksymoops tool to decode it, but in most cases you don't.
It is generally preferred to build the kernel with \code{CONFIG\_KALLSYMS} so
that it produces readable dumps that can be used as-is (this also
produces better output than ksymoops).  If for some reason your kernel
is not build with \code{CONFIG\_KALLSYMS} and you have no way to rebuild and
reproduce the Oops with that option, then you can still decode that Oops
with ksymoops.


\subsubsection{Module-Init-Tools}
\label{process/changes:module-init-tools}
A new module loader is now in the kernel that requires \code{module-init-tools}
to use.  It is backward compatible with the 2.4.x series kernels.


\subsubsection{Mkinitrd}
\label{process/changes:mkinitrd}
These changes to the \code{/lib/modules} file tree layout also require that
mkinitrd be upgraded.


\subsubsection{E2fsprogs}
\label{process/changes:e2fsprogs}
The latest version of \code{e2fsprogs} fixes several bugs in fsck and
debugfs.  Obviously, it's a good idea to upgrade.


\subsubsection{JFSutils}
\label{process/changes:jfsutils}
The \code{jfsutils} package contains the utilities for the file system.
The following utilities are available:
\begin{itemize}
\item {} 
\code{fsck.jfs} - initiate replay of the transaction log, and check
and repair a JFS formatted partition.

\item {} 
\code{mkfs.jfs} - create a JFS formatted partition.

\item {} 
other file system utilities are also available in this package.

\end{itemize}


\subsubsection{Reiserfsprogs}
\label{process/changes:reiserfsprogs}
The reiserfsprogs package should be used for reiserfs-3.6.x
(Linux kernels 2.4.x). It is a combined package and contains working
versions of \code{mkreiserfs}, \code{resize\_reiserfs}, \code{debugreiserfs} and
\code{reiserfsck}. These utils work on both i386 and alpha platforms.


\subsubsection{Xfsprogs}
\label{process/changes:xfsprogs}
The latest version of \code{xfsprogs} contains \code{mkfs.xfs}, \code{xfs\_db}, and the
\code{xfs\_repair} utilities, among others, for the XFS filesystem.  It is
architecture independent and any version from 2.0.0 onward should
work correctly with this version of the XFS kernel code (2.6.0 or
later is recommended, due to some significant improvements).


\subsubsection{PCMCIAutils}
\label{process/changes:pcmciautils}
PCMCIAutils replaces \code{pcmcia-cs}. It properly sets up
PCMCIA sockets at system startup and loads the appropriate modules
for 16-bit PCMCIA devices if the kernel is modularized and the hotplug
subsystem is used.


\subsubsection{Quota-tools}
\label{process/changes:quota-tools}
Support for 32 bit uid's and gid's is required if you want to use
the newer version 2 quota format.  Quota-tools version 3.07 and
newer has this support.  Use the recommended version or newer
from the table above.


\subsubsection{Intel IA32 microcode}
\label{process/changes:intel-ia32-microcode}
A driver has been added to allow updating of Intel IA32 microcode,
accessible as a normal (misc) character device.  If you are not using
udev you may need to:

\begin{Verbatim}[commandchars=\\\{\}]
mkdir /dev/cpu
mknod /dev/cpu/microcode c 10 184
chmod 0644 /dev/cpu/microcode
\end{Verbatim}

as root before you can use this.  You'll probably also want to
get the user-space microcode\_ctl utility to use with this.


\subsubsection{udev}
\label{process/changes:udev}
\code{udev} is a userspace application for populating \code{/dev} dynamically with
only entries for devices actually present. \code{udev} replaces the basic
functionality of devfs, while allowing persistent device naming for
devices.


\subsubsection{FUSE}
\label{process/changes:fuse}
Needs libfuse 2.4.0 or later.  Absolute minimum is 2.3.0 but mount
options \code{direct\_io} and \code{kernel\_cache} won't work.


\subsection{Networking}
\label{process/changes:networking}

\subsubsection{General changes}
\label{process/changes:general-changes}
If you have advanced network configuration needs, you should probably
consider using the network tools from ip-route2.


\subsubsection{Packet Filter / NAT}
\label{process/changes:packet-filter-nat}
The packet filtering and NAT code uses the same tools like the previous 2.4.x
kernel series (iptables).  It still includes backwards-compatibility modules
for 2.2.x-style ipchains and 2.0.x-style ipfwadm.


\subsubsection{PPP}
\label{process/changes:ppp}
The PPP driver has been restructured to support multilink and to
enable it to operate over diverse media layers.  If you use PPP,
upgrade pppd to at least 2.4.0.

If you are not using udev, you must have the device file /dev/ppp
which can be made by:

\begin{Verbatim}[commandchars=\\\{\}]
mknod /dev/ppp c 108 0
\end{Verbatim}

as root.


\subsubsection{Isdn4k-utils}
\label{process/changes:isdn4k-utils}
Due to changes in the length of the phone number field, isdn4k-utils
needs to be recompiled or (preferably) upgraded.


\subsubsection{NFS-utils}
\label{process/changes:nfs-utils}
In ancient (2.4 and earlier) kernels, the nfs server needed to know
about any client that expected to be able to access files via NFS.  This
information would be given to the kernel by \code{mountd} when the client
mounted the filesystem, or by \code{exportfs} at system startup.  exportfs
would take information about active clients from \code{/var/lib/nfs/rmtab}.

This approach is quite fragile as it depends on rmtab being correct
which is not always easy, particularly when trying to implement
fail-over.  Even when the system is working well, \code{rmtab} suffers from
getting lots of old entries that never get removed.

With modern kernels we have the option of having the kernel tell mountd
when it gets a request from an unknown host, and mountd can give
appropriate export information to the kernel.  This removes the
dependency on \code{rmtab} and means that the kernel only needs to know about
currently active clients.

To enable this new functionality, you need to:

\begin{Verbatim}[commandchars=\\\{\}]
mount \PYGZhy{}t nfsd nfsd /proc/fs/nfsd
\end{Verbatim}

before running exportfs or mountd.  It is recommended that all NFS
services be protected from the internet-at-large by a firewall where
that is possible.


\subsubsection{mcelog}
\label{process/changes:mcelog}
On x86 kernels the mcelog utility is needed to process and log machine check
events when \code{CONFIG\_X86\_MCE} is enabled. Machine check events are errors
reported by the CPU. Processing them is strongly encouraged.


\subsection{Kernel documentation}
\label{process/changes:kernel-documentation}

\subsubsection{Sphinx}
\label{process/changes:sphinx}
Please see \DUspan{xref,std,std-ref}{sphinx\_install} in \code{Documentation/doc-guide/sphinx.rst}
for details about Sphinx requirements.


\section{Getting updated software}
\label{process/changes:getting-updated-software}

\subsection{Kernel compilation}
\label{process/changes:id2}

\subsubsection{gcc}
\label{process/changes:id3}\begin{itemize}
\item {} 
\textless{}\href{ftp://ftp.gnu.org/gnu/gcc/}{ftp://ftp.gnu.org/gnu/gcc/}\textgreater{}

\end{itemize}


\subsubsection{Make}
\label{process/changes:id4}\begin{itemize}
\item {} 
\textless{}\href{ftp://ftp.gnu.org/gnu/make/}{ftp://ftp.gnu.org/gnu/make/}\textgreater{}

\end{itemize}


\subsubsection{Binutils}
\label{process/changes:id5}\begin{itemize}
\item {} 
\textless{}\href{https://www.kernel.org/pub/linux/devel/binutils/}{https://www.kernel.org/pub/linux/devel/binutils/}\textgreater{}

\end{itemize}


\subsubsection{Flex}
\label{process/changes:id6}\begin{itemize}
\item {} 
\textless{}\href{https://github.com/westes/flex/releases}{https://github.com/westes/flex/releases}\textgreater{}

\end{itemize}


\subsubsection{Bison}
\label{process/changes:id7}\begin{itemize}
\item {} 
\textless{}\href{ftp://ftp.gnu.org/gnu/bison/}{ftp://ftp.gnu.org/gnu/bison/}\textgreater{}

\end{itemize}


\subsubsection{OpenSSL}
\label{process/changes:id8}\begin{itemize}
\item {} 
\textless{}\href{https://www.openssl.org/}{https://www.openssl.org/}\textgreater{}

\end{itemize}


\subsection{System utilities}
\label{process/changes:id9}

\subsubsection{Util-linux}
\label{process/changes:id10}\begin{itemize}
\item {} 
\textless{}\href{https://www.kernel.org/pub/linux/utils/util-linux/}{https://www.kernel.org/pub/linux/utils/util-linux/}\textgreater{}

\end{itemize}


\subsubsection{Ksymoops}
\label{process/changes:id11}\begin{itemize}
\item {} 
\textless{}\href{https://www.kernel.org/pub/linux/utils/kernel/ksymoops/v2.4/}{https://www.kernel.org/pub/linux/utils/kernel/ksymoops/v2.4/}\textgreater{}

\end{itemize}


\subsubsection{Module-Init-Tools}
\label{process/changes:id12}\begin{itemize}
\item {} 
\textless{}\href{https://www.kernel.org/pub/linux/utils/kernel/module-init-tools/}{https://www.kernel.org/pub/linux/utils/kernel/module-init-tools/}\textgreater{}

\end{itemize}


\subsubsection{Mkinitrd}
\label{process/changes:id13}\begin{itemize}
\item {} 
\textless{}\href{https://code.launchpad.net/initrd-tools/main}{https://code.launchpad.net/initrd-tools/main}\textgreater{}

\end{itemize}


\subsubsection{E2fsprogs}
\label{process/changes:id14}\begin{itemize}
\item {} 
\textless{}\href{http://prdownloads.sourceforge.net/e2fsprogs/e2fsprogs-1.29.tar.gz}{http://prdownloads.sourceforge.net/e2fsprogs/e2fsprogs-1.29.tar.gz}\textgreater{}

\end{itemize}


\subsubsection{JFSutils}
\label{process/changes:id15}\begin{itemize}
\item {} 
\textless{}\href{http://jfs.sourceforge.net/}{http://jfs.sourceforge.net/}\textgreater{}

\end{itemize}


\subsubsection{Reiserfsprogs}
\label{process/changes:id16}\begin{itemize}
\item {} 
\textless{}\href{http://www.kernel.org/pub/linux/utils/fs/reiserfs/}{http://www.kernel.org/pub/linux/utils/fs/reiserfs/}\textgreater{}

\end{itemize}


\subsubsection{Xfsprogs}
\label{process/changes:id17}\begin{itemize}
\item {} 
\textless{}\href{ftp://oss.sgi.com/projects/xfs/}{ftp://oss.sgi.com/projects/xfs/}\textgreater{}

\end{itemize}


\subsubsection{Pcmciautils}
\label{process/changes:id18}\begin{itemize}
\item {} 
\textless{}\href{https://www.kernel.org/pub/linux/utils/kernel/pcmcia/}{https://www.kernel.org/pub/linux/utils/kernel/pcmcia/}\textgreater{}

\end{itemize}


\subsubsection{Quota-tools}
\label{process/changes:id19}\begin{itemize}
\item {} 
\textless{}\href{http://sourceforge.net/projects/linuxquota/}{http://sourceforge.net/projects/linuxquota/}\textgreater{}

\end{itemize}


\subsubsection{Intel P6 microcode}
\label{process/changes:intel-p6-microcode}\begin{itemize}
\item {} 
\textless{}\href{https://downloadcenter.intel.com/}{https://downloadcenter.intel.com/}\textgreater{}

\end{itemize}


\subsubsection{udev}
\label{process/changes:id20}\begin{itemize}
\item {} 
\textless{}\href{http://www.freedesktop.org/software/systemd/man/udev.html}{http://www.freedesktop.org/software/systemd/man/udev.html}\textgreater{}

\end{itemize}


\subsubsection{FUSE}
\label{process/changes:id21}\begin{itemize}
\item {} 
\textless{}\href{http://sourceforge.net/projects/fuse}{http://sourceforge.net/projects/fuse}\textgreater{}

\end{itemize}


\subsubsection{mcelog}
\label{process/changes:id22}\begin{itemize}
\item {} 
\textless{}\href{http://www.mcelog.org/}{http://www.mcelog.org/}\textgreater{}

\end{itemize}


\subsection{Networking}
\label{process/changes:id23}

\subsubsection{PPP}
\label{process/changes:id24}\begin{itemize}
\item {} 
\textless{}\href{ftp://ftp.samba.org/pub/ppp/}{ftp://ftp.samba.org/pub/ppp/}\textgreater{}

\end{itemize}


\subsubsection{Isdn4k-utils}
\label{process/changes:id25}\begin{itemize}
\item {} 
\textless{}\href{ftp://ftp.isdn4linux.de/pub/isdn4linux/utils/}{ftp://ftp.isdn4linux.de/pub/isdn4linux/utils/}\textgreater{}

\end{itemize}


\subsubsection{NFS-utils}
\label{process/changes:id26}\begin{itemize}
\item {} 
\textless{}\href{http://sourceforge.net/project/showfiles.php?group\_id=14}{http://sourceforge.net/project/showfiles.php?group\_id=14}\textgreater{}

\end{itemize}


\subsubsection{Iptables}
\label{process/changes:iptables}\begin{itemize}
\item {} 
\textless{}\href{http://www.iptables.org/downloads.html}{http://www.iptables.org/downloads.html}\textgreater{}

\end{itemize}


\subsubsection{Ip-route2}
\label{process/changes:ip-route2}\begin{itemize}
\item {} 
\textless{}\href{https://www.kernel.org/pub/linux/utils/net/iproute2/}{https://www.kernel.org/pub/linux/utils/net/iproute2/}\textgreater{}

\end{itemize}


\subsubsection{OProfile}
\label{process/changes:oprofile}\begin{itemize}
\item {} 
\textless{}\href{http://oprofile.sf.net/download/}{http://oprofile.sf.net/download/}\textgreater{}

\end{itemize}


\subsubsection{NFS-Utils}
\label{process/changes:id27}\begin{itemize}
\item {} 
\textless{}\href{http://nfs.sourceforge.net/}{http://nfs.sourceforge.net/}\textgreater{}

\end{itemize}


\subsection{Kernel documentation}
\label{process/changes:id28}

\subsubsection{Sphinx}
\label{process/changes:id29}\begin{itemize}
\item {} 
\textless{}\href{http://www.sphinx-doc.org/}{http://www.sphinx-doc.org/}\textgreater{}

\end{itemize}


\chapter{Submitting Drivers For The Linux Kernel}
\label{process/submitting-drivers:submittingdrivers}\label{process/submitting-drivers:submitting-drivers-for-the-linux-kernel}\label{process/submitting-drivers::doc}
This document is intended to explain how to submit device drivers to the
various kernel trees. Note that if you are interested in video card drivers
you should probably talk to XFree86 (\href{http://www.xfree86.org/}{http://www.xfree86.org/}) and/or X.Org
(\href{http://x.org/}{http://x.org/}) instead.

\begin{notice}{note}{Note:}
This document is old and has seen little maintenance in recent years; it
should probably be updated or, perhaps better, just deleted.  Most of
what is here can be found in the other development documents anyway.

Oh, and we don't really recommend submitting changes to XFree86 :)
\end{notice}

Also read the Documentation/process/submitting-patches.rst document.


\section{Allocating Device Numbers}
\label{process/submitting-drivers:allocating-device-numbers}
Major and minor numbers for block and character devices are allocated
by the Linux assigned name and number authority (currently this is
Torben Mathiasen). The site is \href{http://www.lanana.org/}{http://www.lanana.org/}. This
also deals with allocating numbers for devices that are not going to
be submitted to the mainstream kernel.
See Documentation/admin-guide/devices.rst for more information on this.

If you don't use assigned numbers then when your device is submitted it will
be given an assigned number even if that is different from values you may
have shipped to customers before.


\section{Who To Submit Drivers To}
\label{process/submitting-drivers:who-to-submit-drivers-to}\begin{description}
\item[{Linux 2.0:}] \leavevmode
No new drivers are accepted for this kernel tree.

\item[{Linux 2.2:}] \leavevmode
No new drivers are accepted for this kernel tree.

\item[{Linux 2.4:}] \leavevmode
If the code area has a general maintainer then please submit it to
the maintainer listed in MAINTAINERS in the kernel file. If the
maintainer does not respond or you cannot find the appropriate
maintainer then please contact Willy Tarreau \textless{}\href{mailto:w@1wt.eu}{w@1wt.eu}\textgreater{}.

\item[{Linux 2.6 and upper:}] \leavevmode
The same rules apply as 2.4 except that you should follow linux-kernel
to track changes in API's. The final contact point for Linux 2.6+
submissions is Andrew Morton.

\end{description}


\section{What Criteria Determine Acceptance}
\label{process/submitting-drivers:what-criteria-determine-acceptance}\begin{description}
\item[{Licensing:}] \leavevmode
The code must be released to us under the
GNU General Public License. We don't insist on any kind
of exclusive GPL licensing, and if you wish the driver
to be useful to other communities such as BSD you may well
wish to release under multiple licenses.
See accepted licenses at include/linux/module.h

\item[{Copyright:}] \leavevmode
The copyright owner must agree to use of GPL.
It's best if the submitter and copyright owner
are the same person/entity. If not, the name of
the person/entity authorizing use of GPL should be
listed in case it's necessary to verify the will of
the copyright owner.

\item[{Interfaces:}] \leavevmode
If your driver uses existing interfaces and behaves like
other drivers in the same class it will be much more likely
to be accepted than if it invents gratuitous new ones.
If you need to implement a common API over Linux and NT
drivers do it in userspace.

\item[{Code:}] \leavevmode
Please use the Linux style of code formatting as documented
in {\hyperref[process/coding\string-style:codingstyle]{\emph{Documentation/process/coding-style.rst}}}.
If you have sections of code
that need to be in other formats, for example because they
are shared with a windows driver kit and you want to
maintain them just once separate them out nicely and note
this fact.

\item[{Portability:}] \leavevmode
Pointers are not always 32bits, not all computers are little
endian, people do not all have floating point and you
shouldn't use inline x86 assembler in your driver without
careful thought. Pure x86 drivers generally are not popular.
If you only have x86 hardware it is hard to test portability
but it is easy to make sure the code can easily be made
portable.

\item[{Clarity:}] \leavevmode
It helps if anyone can see how to fix the driver. It helps
you because you get patches not bug reports. If you submit a
driver that intentionally obfuscates how the hardware works
it will go in the bitbucket.

\item[{PM support:}] \leavevmode
Since Linux is used on many portable and desktop systems, your
driver is likely to be used on such a system and therefore it
should support basic power management by implementing, if
necessary, the .suspend and .resume methods used during the
system-wide suspend and resume transitions.  You should verify
that your driver correctly handles the suspend and resume, but
if you are unable to ensure that, please at least define the
.suspend method returning the -ENOSYS (``Function not
implemented'') error.  You should also try to make sure that your
driver uses as little power as possible when it's not doing
anything.  For the driver testing instructions see
Documentation/power/drivers-testing.txt and for a relatively
complete overview of the power management issues related to
drivers see Documentation/driver-api/pm/devices.rst.

\item[{Control:}] \leavevmode
In general if there is active maintenance of a driver by
the author then patches will be redirected to them unless
they are totally obvious and without need of checking.
If you want to be the contact and update point for the
driver it is a good idea to state this in the comments,
and include an entry in MAINTAINERS for your driver.

\end{description}


\section{What Criteria Do Not Determine Acceptance}
\label{process/submitting-drivers:what-criteria-do-not-determine-acceptance}\begin{description}
\item[{Vendor:}] \leavevmode
Being the hardware vendor and maintaining the driver is
often a good thing. If there is a stable working driver from
other people already in the tree don't expect `we are the
vendor' to get your driver chosen. Ideally work with the
existing driver author to build a single perfect driver.

\item[{Author:}] \leavevmode
It doesn't matter if a large Linux company wrote the driver,
or you did. Nobody has any special access to the kernel
tree. Anyone who tells you otherwise isn't telling the
whole story.

\end{description}


\section{Resources}
\label{process/submitting-drivers:resources}\begin{description}
\item[{Linux kernel master tree:}] \leavevmode
ftp.\emph{country\_code}.kernel.org:/pub/linux/kernel/...

where \emph{country\_code} == your country code, such as
\textbf{us}, \textbf{uk}, \textbf{fr}, etc.

\href{http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git}{http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git}

\item[{Linux kernel mailing list:}] \leavevmode
\href{mailto:linux-kernel@vger.kernel.org}{linux-kernel@vger.kernel.org}
{[}mail \href{mailto:majordomo@vger.kernel.org}{majordomo@vger.kernel.org} to subscribe{]}

\item[{Linux Device Drivers, Third Edition (covers 2.6.10):}] \leavevmode
\href{http://lwn.net/Kernel/LDD3/}{http://lwn.net/Kernel/LDD3/}  (free version)

\item[{LWN.net:}] \leavevmode
Weekly summary of kernel development activity - \href{http://lwn.net/}{http://lwn.net/}

2.6 API changes:
\begin{quote}

\href{http://lwn.net/Articles/2.6-kernel-api/}{http://lwn.net/Articles/2.6-kernel-api/}
\end{quote}

Porting drivers from prior kernels to 2.6:
\begin{quote}

\href{http://lwn.net/Articles/driver-porting/}{http://lwn.net/Articles/driver-porting/}
\end{quote}

\item[{KernelNewbies:}] \leavevmode
Documentation and assistance for new kernel programmers
\begin{quote}

\href{http://kernelnewbies.org/}{http://kernelnewbies.org/}
\end{quote}

\item[{Linux USB project:}] \leavevmode
\href{http://www.linux-usb.org/}{http://www.linux-usb.org/}

\item[{How to NOT write kernel driver by Arjan van de Ven:}] \leavevmode
\href{http://www.fenrus.org/how-to-not-write-a-device-driver-paper.pdf}{http://www.fenrus.org/how-to-not-write-a-device-driver-paper.pdf}

\item[{Kernel Janitor:}] \leavevmode
\href{http://kernelnewbies.org/KernelJanitors}{http://kernelnewbies.org/KernelJanitors}

\item[{GIT, Fast Version Control System:}] \leavevmode
\href{http://git-scm.com/}{http://git-scm.com/}

\end{description}


\chapter{The Linux Kernel Driver Interface}
\label{process/stable-api-nonsense:stable-api-nonsense}\label{process/stable-api-nonsense:the-linux-kernel-driver-interface}\label{process/stable-api-nonsense::doc}
(all of your questions answered and then some)

Greg Kroah-Hartman \textless{}\href{mailto:greg@kroah.com}{greg@kroah.com}\textgreater{}

This is being written to try to explain why Linux \textbf{does not have a binary
kernel interface, nor does it have a stable kernel interface}.

\begin{notice}{note}{Note:}
Please realize that this article describes the \textbf{in kernel} interfaces, not
the kernel to userspace interfaces.

The kernel to userspace interface is the one that application programs use,
the syscall interface.  That interface is \textbf{very} stable over time, and
will not break.  I have old programs that were built on a pre 0.9something
kernel that still work just fine on the latest 2.6 kernel release.
That interface is the one that users and application programmers can count
on being stable.
\end{notice}


\section{Executive Summary}
\label{process/stable-api-nonsense:executive-summary}
You think you want a stable kernel interface, but you really do not, and
you don't even know it.  What you want is a stable running driver, and
you get that only if your driver is in the main kernel tree.  You also
get lots of other good benefits if your driver is in the main kernel
tree, all of which has made Linux into such a strong, stable, and mature
operating system which is the reason you are using it in the first
place.


\section{Intro}
\label{process/stable-api-nonsense:intro}
It's only the odd person who wants to write a kernel driver that needs
to worry about the in-kernel interfaces changing.  For the majority of
the world, they neither see this interface, nor do they care about it at
all.

First off, I'm not going to address \textbf{any} legal issues about closed
source, hidden source, binary blobs, source wrappers, or any other term
that describes kernel drivers that do not have their source code
released under the GPL.  Please consult a lawyer if you have any legal
questions, I'm a programmer and hence, I'm just going to be describing
the technical issues here (not to make light of the legal issues, they
are real, and you do need to be aware of them at all times.)

So, there are two main topics here, binary kernel interfaces and stable
kernel source interfaces.  They both depend on each other, but we will
discuss the binary stuff first to get it out of the way.


\section{Binary Kernel Interface}
\label{process/stable-api-nonsense:binary-kernel-interface}
Assuming that we had a stable kernel source interface for the kernel, a
binary interface would naturally happen too, right?  Wrong.  Please
consider the following facts about the Linux kernel:
\begin{itemize}
\item {} 
Depending on the version of the C compiler you use, different kernel
data structures will contain different alignment of structures, and
possibly include different functions in different ways (putting
functions inline or not.)  The individual function organization
isn't that important, but the different data structure padding is
very important.

\item {} 
Depending on what kernel build options you select, a wide range of
different things can be assumed by the kernel:
\begin{itemize}
\item {} 
different structures can contain different fields

\item {} 
Some functions may not be implemented at all, (i.e. some locks
compile away to nothing for non-SMP builds.)

\item {} 
Memory within the kernel can be aligned in different ways,
depending on the build options.

\end{itemize}

\item {} 
Linux runs on a wide range of different processor architectures.
There is no way that binary drivers from one architecture will run
on another architecture properly.

\end{itemize}

Now a number of these issues can be addressed by simply compiling your
module for the exact specific kernel configuration, using the same exact
C compiler that the kernel was built with.  This is sufficient if you
want to provide a module for a specific release version of a specific
Linux distribution.  But multiply that single build by the number of
different Linux distributions and the number of different supported
releases of the Linux distribution and you quickly have a nightmare of
different build options on different releases.  Also realize that each
Linux distribution release contains a number of different kernels, all
tuned to different hardware types (different processor types and
different options), so for even a single release you will need to create
multiple versions of your module.

Trust me, you will go insane over time if you try to support this kind
of release, I learned this the hard way a long time ago...


\section{Stable Kernel Source Interfaces}
\label{process/stable-api-nonsense:stable-kernel-source-interfaces}
This is a much more ``volatile'' topic if you talk to people who try to
keep a Linux kernel driver that is not in the main kernel tree up to
date over time.

Linux kernel development is continuous and at a rapid pace, never
stopping to slow down.  As such, the kernel developers find bugs in
current interfaces, or figure out a better way to do things.  If they do
that, they then fix the current interfaces to work better.  When they do
so, function names may change, structures may grow or shrink, and
function parameters may be reworked.  If this happens, all of the
instances of where this interface is used within the kernel are fixed up
at the same time, ensuring that everything continues to work properly.

As a specific examples of this, the in-kernel USB interfaces have
undergone at least three different reworks over the lifetime of this
subsystem.  These reworks were done to address a number of different
issues:
\begin{itemize}
\item {} 
A change from a synchronous model of data streams to an asynchronous
one.  This reduced the complexity of a number of drivers and
increased the throughput of all USB drivers such that we are now
running almost all USB devices at their maximum speed possible.

\item {} 
A change was made in the way data packets were allocated from the
USB core by USB drivers so that all drivers now needed to provide
more information to the USB core to fix a number of documented
deadlocks.

\end{itemize}

This is in stark contrast to a number of closed source operating systems
which have had to maintain their older USB interfaces over time.  This
provides the ability for new developers to accidentally use the old
interfaces and do things in improper ways, causing the stability of the
operating system to suffer.

In both of these instances, all developers agreed that these were
important changes that needed to be made, and they were made, with
relatively little pain.  If Linux had to ensure that it will preserve a
stable source interface, a new interface would have been created, and
the older, broken one would have had to be maintained over time, leading
to extra work for the USB developers.  Since all Linux USB developers do
their work on their own time, asking programmers to do extra work for no
gain, for free, is not a possibility.

Security issues are also very important for Linux.  When a
security issue is found, it is fixed in a very short amount of time.  A
number of times this has caused internal kernel interfaces to be
reworked to prevent the security problem from occurring.  When this
happens, all drivers that use the interfaces were also fixed at the
same time, ensuring that the security problem was fixed and could not
come back at some future time accidentally.  If the internal interfaces
were not allowed to change, fixing this kind of security problem and
insuring that it could not happen again would not be possible.

Kernel interfaces are cleaned up over time.  If there is no one using a
current interface, it is deleted.  This ensures that the kernel remains
as small as possible, and that all potential interfaces are tested as
well as they can be (unused interfaces are pretty much impossible to
test for validity.)


\section{What to do}
\label{process/stable-api-nonsense:what-to-do}
So, if you have a Linux kernel driver that is not in the main kernel
tree, what are you, a developer, supposed to do?  Releasing a binary
driver for every different kernel version for every distribution is a
nightmare, and trying to keep up with an ever changing kernel interface
is also a rough job.

Simple, get your kernel driver into the main kernel tree (remember we
are talking about GPL released drivers here, if your code doesn't fall
under this category, good luck, you are on your own here, you leech
\textless{}insert link to leech comment from Andrew and Linus here\textgreater{}.)  If your
driver is in the tree, and a kernel interface changes, it will be fixed
up by the person who did the kernel change in the first place.  This
ensures that your driver is always buildable, and works over time, with
very little effort on your part.

The very good side effects of having your driver in the main kernel tree
are:
\begin{itemize}
\item {} 
The quality of the driver will rise as the maintenance costs (to the
original developer) will decrease.

\item {} 
Other developers will add features to your driver.

\item {} 
Other people will find and fix bugs in your driver.

\item {} 
Other people will find tuning opportunities in your driver.

\item {} 
Other people will update the driver for you when external interface
changes require it.

\item {} 
The driver automatically gets shipped in all Linux distributions
without having to ask the distros to add it.

\end{itemize}

As Linux supports a larger number of different devices ``out of the box''
than any other operating system, and it supports these devices on more
different processor architectures than any other operating system, this
proven type of development model must be doing something right :)


\bigskip\hrule{}\bigskip


Thanks to Randy Dunlap, Andrew Morton, David Brownell, Hanna Linder,
Robert Love, and Nishanth Aravamudan for their review and comments on
early drafts of this paper.


\chapter{Linux kernel management style}
\label{process/management-style:managementstyle}\label{process/management-style::doc}\label{process/management-style:linux-kernel-management-style}
This is a short document describing the preferred (or made up, depending
on who you ask) management style for the linux kernel.  It's meant to
mirror the process/coding-style.rst document to some degree, and mainly written to
avoid answering \footnote[1]{
This document does so not so much by answering the question, but by
making it painfully obvious to the questioner that we don't have a clue
to what the answer is.
}  the same (or similar) questions over and over again.

Management style is very personal and much harder to quantify than
simple coding style rules, so this document may or may not have anything
to do with reality.  It started as a lark, but that doesn't mean that it
might not actually be true. You'll have to decide for yourself.

Btw, when talking about ``kernel manager'', it's all about the technical
lead persons, not the people who do traditional management inside
companies.  If you sign purchase orders or you have any clue about the
budget of your group, you're almost certainly not a kernel manager.
These suggestions may or may not apply to you.

First off, I'd suggest buying ``Seven Habits of Highly Effective
People'', and NOT read it.  Burn it, it's a great symbolic gesture.

Anyway, here goes:


\section{1) Decisions}
\label{process/management-style:id2}\label{process/management-style:decisions}
Everybody thinks managers make decisions, and that decision-making is
important.  The bigger and more painful the decision, the bigger the
manager must be to make it.  That's very deep and obvious, but it's not
actually true.

The name of the game is to \textbf{avoid} having to make a decision.  In
particular, if somebody tells you ``choose (a) or (b), we really need you
to decide on this'', you're in trouble as a manager.  The people you
manage had better know the details better than you, so if they come to
you for a technical decision, you're screwed.  You're clearly not
competent to make that decision for them.

(Corollary:if the people you manage don't know the details better than
you, you're also screwed, although for a totally different reason.
Namely that you are in the wrong job, and that \textbf{they} should be managing
your brilliance instead).

So the name of the game is to \textbf{avoid} decisions, at least the big and
painful ones.  Making small and non-consequential decisions is fine, and
makes you look like you know what you're doing, so what a kernel manager
needs to do is to turn the big and painful ones into small things where
nobody really cares.

It helps to realize that the key difference between a big decision and a
small one is whether you can fix your decision afterwards.  Any decision
can be made small by just always making sure that if you were wrong (and
you \textbf{will} be wrong), you can always undo the damage later by
backtracking.  Suddenly, you get to be doubly managerial for making
\textbf{two} inconsequential decisions - the wrong one \textbf{and} the right one.

And people will even see that as true leadership (\emph{cough} bullshit
\emph{cough}).

Thus the key to avoiding big decisions becomes to just avoiding to do
things that can't be undone.  Don't get ushered into a corner from which
you cannot escape.  A cornered rat may be dangerous - a cornered manager
is just pitiful.

It turns out that since nobody would be stupid enough to ever really let
a kernel manager have huge fiscal responsibility \textbf{anyway}, it's usually
fairly easy to backtrack.  Since you're not going to be able to waste
huge amounts of money that you might not be able to repay, the only
thing you can backtrack on is a technical decision, and there
back-tracking is very easy: just tell everybody that you were an
incompetent nincompoop, say you're sorry, and undo all the worthless
work you had people work on for the last year.  Suddenly the decision
you made a year ago wasn't a big decision after all, since it could be
easily undone.

It turns out that some people have trouble with this approach, for two
reasons:
\begin{itemize}
\item {} 
admitting you were an idiot is harder than it looks.  We all like to
maintain appearances, and coming out in public to say that you were
wrong is sometimes very hard indeed.

\item {} 
having somebody tell you that what you worked on for the last year
wasn't worthwhile after all can be hard on the poor lowly engineers
too, and while the actual \textbf{work} was easy enough to undo by just
deleting it, you may have irrevocably lost the trust of that
engineer.  And remember: ``irrevocable'' was what we tried to avoid in
the first place, and your decision ended up being a big one after
all.

\end{itemize}

Happily, both of these reasons can be mitigated effectively by just
admitting up-front that you don't have a friggin' clue, and telling
people ahead of the fact that your decision is purely preliminary, and
might be the wrong thing.  You should always reserve the right to change
your mind, and make people very \textbf{aware} of that.  And it's much easier
to admit that you are stupid when you haven't \textbf{yet} done the really
stupid thing.

Then, when it really does turn out to be stupid, people just roll their
eyes and say ``Oops, he did it again''.

This preemptive admission of incompetence might also make the people who
actually do the work also think twice about whether it's worth doing or
not.  After all, if \textbf{they} aren't certain whether it's a good idea, you
sure as hell shouldn't encourage them by promising them that what they
work on will be included.  Make them at least think twice before they
embark on a big endeavor.

Remember: they'd better know more about the details than you do, and
they usually already think they have the answer to everything.  The best
thing you can do as a manager is not to instill confidence, but rather a
healthy dose of critical thinking on what they do.

Btw, another way to avoid a decision is to plaintively just whine ``can't
we just do both?'' and look pitiful.  Trust me, it works.  If it's not
clear which approach is better, they'll eventually figure it out.  The
answer may end up being that both teams get so frustrated by the
situation that they just give up.

That may sound like a failure, but it's usually a sign that there was
something wrong with both projects, and the reason the people involved
couldn't decide was that they were both wrong.  You end up coming up
smelling like roses, and you avoided yet another decision that you could
have screwed up on.


\section{2) People}
\label{process/management-style:people}
Most people are idiots, and being a manager means you'll have to deal
with it, and perhaps more importantly, that \textbf{they} have to deal with
\textbf{you}.

It turns out that while it's easy to undo technical mistakes, it's not
as easy to undo personality disorders.  You just have to live with
theirs - and yours.

However, in order to prepare yourself as a kernel manager, it's best to
remember not to burn any bridges, bomb any innocent villagers, or
alienate too many kernel developers. It turns out that alienating people
is fairly easy, and un-alienating them is hard. Thus ``alienating''
immediately falls under the heading of ``not reversible'', and becomes a
no-no according to {\hyperref[process/management\string-style:decisions]{\emph{1) Decisions}}}.

There's just a few simple rules here:
\begin{enumerate}
\item {} 
don't call people d*ckheads (at least not in public)

\item {} 
learn how to apologize when you forgot rule (1)

\end{enumerate}

The problem with \#1 is that it's very easy to do, since you can say
``you're a d*ckhead'' in millions of different ways \footnote[2]{
Paul Simon sang ``Fifty Ways to Leave Your Lover'', because quite
frankly, ``A Million Ways to Tell a Developer He Is a D*ckhead'' doesn't
scan nearly as well.  But I'm sure he thought about it.
}, sometimes without
even realizing it, and almost always with a white-hot conviction that
you are right.

And the more convinced you are that you are right (and let's face it,
you can call just about \textbf{anybody} a d*ckhead, and you often \textbf{will} be
right), the harder it ends up being to apologize afterwards.

To solve this problem, you really only have two options:
\begin{itemize}
\item {} 
get really good at apologies

\item {} 
spread the ``love'' out so evenly that nobody really ends up feeling
like they get unfairly targeted.  Make it inventive enough, and they
might even be amused.

\end{itemize}

The option of being unfailingly polite really doesn't exist. Nobody will
trust somebody who is so clearly hiding his true character.


\section{3) People II - the Good Kind}
\label{process/management-style:people-ii-the-good-kind}
While it turns out that most people are idiots, the corollary to that is
sadly that you are one too, and that while we can all bask in the secure
knowledge that we're better than the average person (let's face it,
nobody ever believes that they're average or below-average), we should
also admit that we're not the sharpest knife around, and there will be
other people that are less of an idiot than you are.

Some people react badly to smart people.  Others take advantage of them.

Make sure that you, as a kernel maintainer, are in the second group.
Suck up to them, because they are the people who will make your job
easier. In particular, they'll be able to make your decisions for you,
which is what the game is all about.

So when you find somebody smarter than you are, just coast along.  Your
management responsibilities largely become ones of saying ``Sounds like a
good idea - go wild'', or ``That sounds good, but what about xxx?''.  The
second version in particular is a great way to either learn something
new about ``xxx'' or seem \textbf{extra} managerial by pointing out something the
smarter person hadn't thought about.  In either case, you win.

One thing to look out for is to realize that greatness in one area does
not necessarily translate to other areas.  So you might prod people in
specific directions, but let's face it, they might be good at what they
do, and suck at everything else.  The good news is that people tend to
naturally gravitate back to what they are good at, so it's not like you
are doing something irreversible when you \textbf{do} prod them in some
direction, just don't push too hard.


\section{4) Placing blame}
\label{process/management-style:placing-blame}
Things will go wrong, and people want somebody to blame. Tag, you're it.

It's not actually that hard to accept the blame, especially if people
kind of realize that it wasn't \textbf{all} your fault.  Which brings us to the
best way of taking the blame: do it for another guy. You'll feel good
for taking the fall, he'll feel good about not getting blamed, and the
guy who lost his whole 36GB porn-collection because of your incompetence
will grudgingly admit that you at least didn't try to weasel out of it.

Then make the developer who really screwed up (if you can find him) know
\textbf{in\_private} that he screwed up.  Not just so he can avoid it in the
future, but so that he knows he owes you one.  And, perhaps even more
importantly, he's also likely the person who can fix it.  Because, let's
face it, it sure ain't you.

Taking the blame is also why you get to be manager in the first place.
It's part of what makes people trust you, and allow you the potential
glory, because you're the one who gets to say ``I screwed up''.  And if
you've followed the previous rules, you'll be pretty good at saying that
by now.


\section{5) Things to avoid}
\label{process/management-style:things-to-avoid}
There's one thing people hate even more than being called ``d*ckhead'',
and that is being called a ``d*ckhead'' in a sanctimonious voice.  The
first you can apologize for, the second one you won't really get the
chance.  They likely will no longer be listening even if you otherwise
do a good job.

We all think we're better than anybody else, which means that when
somebody else puts on airs, it \textbf{really} rubs us the wrong way.  You may
be morally and intellectually superior to everybody around you, but
don't try to make it too obvious unless you really \textbf{intend} to irritate
somebody \footnote[3]{
Hint: internet newsgroups that are not directly related to your work
are great ways to take out your frustrations at other people. Write
insulting posts with a sneer just to get into a good flame every once in
a while, and you'll feel cleansed. Just don't crap too close to home.
}.

Similarly, don't be too polite or subtle about things. Politeness easily
ends up going overboard and hiding the problem, and as they say, ``On the
internet, nobody can hear you being subtle''. Use a big blunt object to
hammer the point in, because you can't really depend on people getting
your point otherwise.

Some humor can help pad both the bluntness and the moralizing.  Going
overboard to the point of being ridiculous can drive a point home
without making it painful to the recipient, who just thinks you're being
silly.  It can thus help get through the personal mental block we all
have about criticism.


\section{6) Why me?}
\label{process/management-style:why-me}
Since your main responsibility seems to be to take the blame for other
peoples mistakes, and make it painfully obvious to everybody else that
you're incompetent, the obvious question becomes one of why do it in the
first place?

First off, while you may or may not get screaming teenage girls (or
boys, let's not be judgmental or sexist here) knocking on your dressing
room door, you \textbf{will} get an immense feeling of personal accomplishment
for being ``in charge''.  Never mind the fact that you're really leading
by trying to keep up with everybody else and running after them as fast
as you can.  Everybody will still think you're the person in charge.

It's a great job if you can hack it.


\chapter{Everything you ever wanted to know about Linux -stable releases}
\label{process/stable-kernel-rules:everything-you-ever-wanted-to-know-about-linux-stable-releases}\label{process/stable-kernel-rules:stable-kernel-rules}\label{process/stable-kernel-rules::doc}
Rules on what kind of patches are accepted, and which ones are not, into the
``-stable'' tree:
\begin{itemize}
\item {} 
It must be obviously correct and tested.

\item {} 
It cannot be bigger than 100 lines, with context.

\item {} 
It must fix only one thing.

\item {} 
It must fix a real bug that bothers people (not a, ``This could be a
problem...'' type thing).

\item {} 
It must fix a problem that causes a build error (but not for things
marked CONFIG\_BROKEN), an oops, a hang, data corruption, a real
security issue, or some ``oh, that's not good'' issue.  In short, something
critical.

\item {} 
Serious issues as reported by a user of a distribution kernel may also
be considered if they fix a notable performance or interactivity issue.
As these fixes are not as obvious and have a higher risk of a subtle
regression they should only be submitted by a distribution kernel
maintainer and include an addendum linking to a bugzilla entry if it
exists and additional information on the user-visible impact.

\item {} 
New device IDs and quirks are also accepted.

\item {} 
No ``theoretical race condition'' issues, unless an explanation of how the
race can be exploited is also provided.

\item {} 
It cannot contain any ``trivial'' fixes in it (spelling changes,
whitespace cleanups, etc).

\item {} 
It must follow the
{\hyperref[process/submitting\string-patches:submittingpatches]{\emph{Documentation/process/submitting-patches.rst}}}
rules.

\item {} 
It or an equivalent fix must already exist in Linus' tree (upstream).

\end{itemize}


\section{Procedure for submitting patches to the -stable tree}
\label{process/stable-kernel-rules:procedure-for-submitting-patches-to-the-stable-tree}\begin{itemize}
\item {} 
If the patch covers files in net/ or drivers/net please follow netdev stable
submission guidelines as described in
Documentation/networking/netdev-FAQ.txt

\item {} 
Security patches should not be handled (solely) by the -stable review
process but should follow the procedures in
\DUspan{xref,std,std-ref}{Documentation/admin-guide/security-bugs.rst}.

\end{itemize}


\section{For all other submissions, choose one of the following procedures}
\label{process/stable-kernel-rules:for-all-other-submissions-choose-one-of-the-following-procedures}

\subsection{Option 1}
\label{process/stable-kernel-rules:option-1}\label{process/stable-kernel-rules:id1}
To have the patch automatically included in the stable tree, add the tag

\begin{Verbatim}[commandchars=\\\{\}]
Cc: stable@vger.kernel.org
\end{Verbatim}

in the sign-off area. Once the patch is merged it will be applied to
the stable tree without anything else needing to be done by the author
or subsystem maintainer.


\subsection{Option 2}
\label{process/stable-kernel-rules:id2}\label{process/stable-kernel-rules:option-2}
After the patch has been merged to Linus' tree, send an email to
\href{mailto:stable@vger.kernel.org}{stable@vger.kernel.org} containing the subject of the patch, the commit ID,
why you think it should be applied, and what kernel version you wish it to
be applied to.


\subsection{Option 3}
\label{process/stable-kernel-rules:id3}\label{process/stable-kernel-rules:option-3}
Send the patch, after verifying that it follows the above rules, to
\href{mailto:stable@vger.kernel.org}{stable@vger.kernel.org}.  You must note the upstream commit ID in the
changelog of your submission, as well as the kernel version you wish
it to be applied to.

{\hyperref[process/stable\string-kernel\string-rules:option\string-1]{\emph{Option 1}}} is \textbf{strongly} preferred, is the easiest and most common.
{\hyperref[process/stable\string-kernel\string-rules:option\string-2]{\emph{Option 2}}} and {\hyperref[process/stable\string-kernel\string-rules:option\string-3]{\emph{Option 3}}} are more useful if the patch isn't deemed
worthy at the time it is applied to a public git tree (for instance, because
it deserves more regression testing first).  {\hyperref[process/stable\string-kernel\string-rules:option\string-3]{\emph{Option 3}}} is especially
useful if the patch needs some special handling to apply to an older kernel
(e.g., if API's have changed in the meantime).

Note that for {\hyperref[process/stable\string-kernel\string-rules:option\string-3]{\emph{Option 3}}}, if the patch deviates from the original
upstream patch (for example because it had to be backported) this must be very
clearly documented and justified in the patch description.

The upstream commit ID must be specified with a separate line above the commit
text, like this:

\begin{Verbatim}[commandchars=\\\{\}]
commit \PYGZlt{}sha1\PYGZgt{} upstream.
\end{Verbatim}

Additionally, some patches submitted via Option 1 may have additional patch
prerequisites which can be cherry-picked. This can be specified in the following
format in the sign-off area:

\begin{Verbatim}[commandchars=\\\{\}]
Cc: \PYGZlt{}stable@vger.kernel.org\PYGZgt{} \PYGZsh{} 3.3.x: a1f84a3: sched: Check for idle
Cc: \PYGZlt{}stable@vger.kernel.org\PYGZgt{} \PYGZsh{} 3.3.x: 1b9508f: sched: Rate\PYGZhy{}limit newidle
Cc: \PYGZlt{}stable@vger.kernel.org\PYGZgt{} \PYGZsh{} 3.3.x: fd21073: sched: Fix affinity logic
Cc: \PYGZlt{}stable@vger.kernel.org\PYGZgt{} \PYGZsh{} 3.3.x
Signed\PYGZhy{}off\PYGZhy{}by: Ingo Molnar \PYGZlt{}mingo@elte.hu\PYGZgt{}
\end{Verbatim}

The tag sequence has the meaning of:

\begin{Verbatim}[commandchars=\\\{\}]
git cherry\PYGZhy{}pick a1f84a3
git cherry\PYGZhy{}pick 1b9508f
git cherry\PYGZhy{}pick fd21073
git cherry\PYGZhy{}pick \PYGZlt{}this commit\PYGZgt{}
\end{Verbatim}

Also, some patches may have kernel version prerequisites.  This can be
specified in the following format in the sign-off area:

\begin{Verbatim}[commandchars=\\\{\}]
Cc: \PYGZlt{}stable@vger.kernel.org\PYGZgt{} \PYGZsh{} 3.3.x
\end{Verbatim}

The tag has the meaning of:

\begin{Verbatim}[commandchars=\\\{\}]
git cherry\PYGZhy{}pick \PYGZlt{}this commit\PYGZgt{}
\end{Verbatim}

For each ``-stable'' tree starting with the specified version.

Following the submission:
\begin{itemize}
\item {} 
The sender will receive an ACK when the patch has been accepted into the
queue, or a NAK if the patch is rejected.  This response might take a few
days, according to the developer's schedules.

\item {} 
If accepted, the patch will be added to the -stable queue, for review by
other developers and by the relevant subsystem maintainer.

\end{itemize}


\section{Review cycle}
\label{process/stable-kernel-rules:review-cycle}\begin{itemize}
\item {} 
When the -stable maintainers decide for a review cycle, the patches will be
sent to the review committee, and the maintainer of the affected area of
the patch (unless the submitter is the maintainer of the area) and CC: to
the linux-kernel mailing list.

\item {} 
The review committee has 48 hours in which to ACK or NAK the patch.

\item {} 
If the patch is rejected by a member of the committee, or linux-kernel
members object to the patch, bringing up issues that the maintainers and
members did not realize, the patch will be dropped from the queue.

\item {} 
At the end of the review cycle, the ACKed patches will be added to the
latest -stable release, and a new -stable release will happen.

\item {} 
Security patches will be accepted into the -stable tree directly from the
security kernel team, and not go through the normal review cycle.
Contact the kernel security team for more details on this procedure.

\end{itemize}


\section{Trees}
\label{process/stable-kernel-rules:trees}\begin{itemize}
\item {} 
The queues of patches, for both completed versions and in progress
versions can be found at:
\begin{quote}

\href{https://git.kernel.org/pub/scm/linux/kernel/git/stable/stable-queue.git}{https://git.kernel.org/pub/scm/linux/kernel/git/stable/stable-queue.git}
\end{quote}

\item {} 
The finalized and tagged releases of all stable kernels can be found
in separate branches per version at:
\begin{quote}

\href{https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux-stable.git}{https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux-stable.git}
\end{quote}

\end{itemize}


\section{Review committee}
\label{process/stable-kernel-rules:review-committee}\begin{itemize}
\item {} 
This is made up of a number of kernel developers who have volunteered for
this task, and a few that haven't.

\end{itemize}


\chapter{Linux Kernel patch submission checklist}
\label{process/submit-checklist:linux-kernel-patch-submission-checklist}\label{process/submit-checklist::doc}\label{process/submit-checklist:submitchecklist}
Here are some basic things that developers should do if they want to see their
kernel patch submissions accepted more quickly.

These are all above and beyond the documentation that is provided in
{\hyperref[process/submitting\string-patches:submittingpatches]{\emph{Documentation/process/submitting-patches.rst}}}
and elsewhere regarding submitting Linux kernel patches.
\begin{enumerate}
\item {} 
If you use a facility then \#include the file that defines/declares
that facility.  Don't depend on other header files pulling in ones
that you use.

\item {} 
Builds cleanly:

\end{enumerate}
\begin{enumerate}
\item {} 
with applicable or modified \code{CONFIG} options \code{=y}, \code{=m}, and
\code{=n}.  No \code{gcc} warnings/errors, no linker warnings/errors.

\item {} 
Passes \code{allnoconfig}, \code{allmodconfig}

\item {} 
Builds successfully when using \code{O=builddir}

\end{enumerate}
\begin{enumerate}
\setcounter{enumi}{2}
\item {} 
Builds on multiple CPU architectures by using local cross-compile tools
or some other build farm.

\item {} 
ppc64 is a good architecture for cross-compilation checking because it
tends to use \code{unsigned long} for 64-bit quantities.

\item {} 
Check your patch for general style as detailed in
{\hyperref[process/coding\string-style:codingstyle]{\emph{Documentation/process/coding-style.rst}}}.
Check for trivial violations with the patch style checker prior to
submission (\code{scripts/checkpatch.pl}).
You should be able to justify all violations that remain in
your patch.

\item {} 
Any new or modified \code{CONFIG} options do not muck up the config menu and
default to off unless they meet the exception criteria documented in
\code{Documentation/kbuild/kconfig-language.txt} Menu attributes: default value.

\item {} 
All new \code{Kconfig} options have help text.

\item {} 
Has been carefully reviewed with respect to relevant \code{Kconfig}
combinations.  This is very hard to get right with testing -- brainpower
pays off here.

\item {} 
Check cleanly with sparse.

\item {} 
Use \code{make checkstack} and \code{make namespacecheck} and fix any problems
that they find.

\begin{notice}{note}{Note:}
\code{checkstack} does not point out problems explicitly,
but any one function that uses more than 512 bytes on the stack is a
candidate for change.
\end{notice}

\item {} 
Include \DUspan{xref,std,std-ref}{kernel-doc} to document global  kernel APIs.
(Not required for static functions, but OK there also.) Use
\code{make htmldocs} or \code{make pdfdocs} to check the
\DUspan{xref,std,std-ref}{kernel-doc} and fix any issues.

\item {} 
Has been tested with \code{CONFIG\_PREEMPT}, \code{CONFIG\_DEBUG\_PREEMPT},
\code{CONFIG\_DEBUG\_SLAB}, \code{CONFIG\_DEBUG\_PAGEALLOC}, \code{CONFIG\_DEBUG\_MUTEXES},
\code{CONFIG\_DEBUG\_SPINLOCK}, \code{CONFIG\_DEBUG\_ATOMIC\_SLEEP},
\code{CONFIG\_PROVE\_RCU} and \code{CONFIG\_DEBUG\_OBJECTS\_RCU\_HEAD} all
simultaneously enabled.

\item {} 
Has been build- and runtime tested with and without \code{CONFIG\_SMP} and
\code{CONFIG\_PREEMPT.}

\item {} 
If the patch affects IO/Disk, etc: has been tested with and without
\code{CONFIG\_LBDAF.}

\item {} 
All codepaths have been exercised with all lockdep features enabled.

\item {} 
All new \code{/proc} entries are documented under \code{Documentation/}

\item {} 
All new kernel boot parameters are documented in
\code{Documentation/admin-guide/kernel-parameters.rst}.

\item {} 
All new module parameters are documented with \code{MODULE\_PARM\_DESC()}

\item {} 
All new userspace interfaces are documented in \code{Documentation/ABI/}.
See \code{Documentation/ABI/README} for more information.
Patches that change userspace interfaces should be CCed to
\href{mailto:linux-api@vger.kernel.org}{linux-api@vger.kernel.org}.

\item {} 
Check that it all passes \code{make headers\_check}.

\item {} 
Has been checked with injection of at least slab and page-allocation
failures.  See \code{Documentation/fault-injection/}.

If the new code is substantial, addition of subsystem-specific fault
injection might be appropriate.

\item {} 
Newly-added code has been compiled with \code{gcc -W} (use
\code{make EXTRA\_CFLAGS=-W}).  This will generate lots of noise, but is good
for finding bugs like ``warning: comparison between signed and unsigned''.

\item {} 
Tested after it has been merged into the -mm patchset to make sure
that it still works with all of the other queued patches and various
changes in the VM, VFS, and other subsystems.

\item {} 
All memory barriers \{e.g., \code{barrier()}, \code{rmb()}, \code{wmb()}\} need a
comment in the source code that explains the logic of what they are doing
and why.

\item {} 
If any ioctl's are added by the patch, then also update
\code{Documentation/ioctl/ioctl-number.txt}.

\item {} 
If your modified source code depends on or uses any of the kernel
APIs or features that are related to the following \code{Kconfig} symbols,
then test multiple builds with the related \code{Kconfig} symbols disabled
and/or \code{=m} (if that option is available) {[}not all of these at the
same time, just various/random combinations of them{]}:

\code{CONFIG\_SMP}, \code{CONFIG\_SYSFS}, \code{CONFIG\_PROC\_FS}, \code{CONFIG\_INPUT}, \code{CONFIG\_PCI}, \code{CONFIG\_BLOCK}, \code{CONFIG\_PM}, \code{CONFIG\_MAGIC\_SYSRQ},
\code{CONFIG\_NET}, \code{CONFIG\_INET=n} (but latter with \code{CONFIG\_NET=y}).

\end{enumerate}


\chapter{Index of Documentation for People Interested in Writing and/or Understanding the Linux Kernel}
\label{process/kernel-docs:index-of-documentation-for-people-interested-in-writing-and-or-understanding-the-linux-kernel}\label{process/kernel-docs::doc}\label{process/kernel-docs:kernel-docs}\begin{quote}

Juan-Mariano de Goyeneche \textless{}\href{mailto:jmseyas@dit.upm.es}{jmseyas@dit.upm.es}\textgreater{}
\end{quote}

The need for a document like this one became apparent in the
linux-kernel mailing list as the same questions, asking for pointers
to information, appeared again and again.

Fortunately, as more and more people get to GNU/Linux, more and more
get interested in the Kernel. But reading the sources is not always
enough. It is easy to understand the code, but miss the concepts, the
philosophy and design decisions behind this code.

Unfortunately, not many documents are available for beginners to
start. And, even if they exist, there was no ``well-known'' place which
kept track of them. These lines try to cover this lack. All documents
available on line known by the author are listed, while some reference
books are also mentioned.

PLEASE, if you know any paper not listed here or write a new document,
send me an e-mail, and I'll include a reference to it here. Any
corrections, ideas or comments are also welcomed.

The papers that follow are listed in no particular order. All are
cataloged with the following fields: the document's ``Title'', the
``Author''/s, the ``URL'' where they can be found, some ``Keywords'' helpful
when searching for specific topics, and a brief ``Description'' of the
Document.

Enjoy!

\begin{notice}{note}{Note:}
The documents on each section of this document are ordered by its
published date, from the newest to the oldest.
\end{notice}


\section{Docs at the Linux Kernel tree}
\label{process/kernel-docs:docs-at-the-linux-kernel-tree}
The Sphinx books should be built with \code{make \{htmldocs \textbar{} pdfdocs \textbar{} epubdocs\}}.
\begin{itemize}
\item {} 
Name: \textbf{linux/Documentation}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Many.

\item[{Location}] \leavevmode
Documentation/

\item[{Keywords}] \leavevmode
text files, Sphinx.

\item[{Description}] \leavevmode
Documentation that comes with the kernel sources,
inside the Documentation directory. Some pages from this document
(including this document itself) have been moved there, and might
be more up to date than the web version.

\end{description}\end{quote}

\end{itemize}


\section{On-line docs}
\label{process/kernel-docs:on-line-docs}\begin{itemize}
\item {} 
Title: \textbf{Linux Kernel Mailing List Glossary}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
various

\item[{URL}] \leavevmode
\href{https://kernelnewbies.org/KernelGlossary}{https://kernelnewbies.org/KernelGlossary}

\item[{Date}] \leavevmode
rolling version

\item[{Keywords}] \leavevmode
glossary, terms, linux-kernel.

\item[{Description}] \leavevmode
From the introduction: ``This glossary is intended as
a brief description of some of the acronyms and terms you may hear
during discussion of the Linux kernel''.

\end{description}\end{quote}

\item {} 
Title: \textbf{Tracing the Way of Data in a TCP Connection through the Linux Kernel}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Richard Sailer

\item[{URL}] \leavevmode
\href{https://archive.org/details/linux\_kernel\_data\_flow\_short\_paper}{https://archive.org/details/linux\_kernel\_data\_flow\_short\_paper}

\item[{Date}] \leavevmode
2016

\item[{Keywords}] \leavevmode
Linux Kernel Networking, TCP, tracing, ftrace

\item[{Description}] \leavevmode
A seminar paper explaining ftrace and how to use it for
understanding linux kernel internals,
illustrated at tracing the way of a TCP packet through the kernel.

\item[{Abstract}] \leavevmode
\emph{This short paper outlines the usage of ftrace a tracing framework
as a tool to understand a running Linux system.
Having obtained a trace-log a kernel hacker can read and understand
source code more determined and with context.
In a detailed example this approach is demonstrated in tracing
and the way of data in a TCP Connection through the kernel.
Finally this trace-log is used as base for more a exact conceptual
exploration and description of the Linux TCP/IP implementation.}

\end{description}\end{quote}

\item {} 
Title: \textbf{On submitting kernel Patches}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Andi Kleen

\item[{URL}] \leavevmode
\href{http://halobates.de/on-submitting-kernel-patches.pdf}{http://halobates.de/on-submitting-kernel-patches.pdf}

\item[{Date}] \leavevmode
2008

\item[{Keywords}] \leavevmode
patches, review process, types of submissions, basic rules, case studies

\item[{Description}] \leavevmode
This paper gives several experience values on what types of patches
there are and how likley they get merged.

\item[{Abstract}] \leavevmode
{[}...{]}. This paper examines some common problems for
submitting larger changes and some strategies to avoid problems.

\end{description}\end{quote}

\item {} 
Title: \textbf{Overview of the Virtual File System}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Richard Gooch.

\item[{URL}] \leavevmode
\href{http://www.mjmwired.net/kernel/Documentation/filesystems/vfs.txt}{http://www.mjmwired.net/kernel/Documentation/filesystems/vfs.txt}

\item[{Date}] \leavevmode
2007

\item[{Keywords}] \leavevmode
VFS, File System, mounting filesystems, opening files,
dentries, dcache.

\item[{Description}] \leavevmode
Brief introduction to the Linux Virtual File System.
What is it, how it works, operations taken when opening a file or
mounting a file system and description of important data
structures explaining the purpose of each of their entries.

\end{description}\end{quote}

\item {} 
Title: \textbf{Linux Device Drivers, Third Edition}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Jonathan Corbet, Alessandro Rubini, Greg Kroah-Hartman

\item[{URL}] \leavevmode
\href{http://lwn.net/Kernel/LDD3/}{http://lwn.net/Kernel/LDD3/}

\item[{Date}] \leavevmode
2005

\item[{Description}] \leavevmode
A 600-page book covering the (2.6.10) driver
programming API and kernel hacking in general.  Available under the
Creative Commons Attribution-ShareAlike 2.0 license.

\item[{note}] \leavevmode
You can also {\hyperref[process/kernel\string-docs:ldd3\string-published]{\emph{purchase a copy from O'Reilly or elsewhere}}}.

\end{description}\end{quote}

\item {} 
Title: \textbf{Writing an ALSA Driver}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Takashi Iwai \textless{}\href{mailto:tiwai@suse.de}{tiwai@suse.de}\textgreater{}

\item[{URL}] \leavevmode
\href{http://www.alsa-project.org/~iwai/writing-an-alsa-driver/index.html}{http://www.alsa-project.org/\textasciitilde{}iwai/writing-an-alsa-driver/index.html}

\item[{Date}] \leavevmode
2005

\item[{Keywords}] \leavevmode
ALSA, sound, soundcard, driver, lowlevel, hardware.

\item[{Description}] \leavevmode
Advanced Linux Sound Architecture for developers,
both at kernel and user-level sides. ALSA is the Linux kernel
sound architecture in the 2.6 kernel version.

\end{description}\end{quote}

\item {} 
Title: \textbf{Linux PCMCIA Programmer's Guide}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
David Hinds.

\item[{URL}] \leavevmode
\href{http://pcmcia-cs.sourceforge.net/ftp/doc/PCMCIA-PROG.html}{http://pcmcia-cs.sourceforge.net/ftp/doc/PCMCIA-PROG.html}

\item[{Date}] \leavevmode
2003

\item[{Keywords}] \leavevmode
PCMCIA.

\item[{Description}] \leavevmode
``This document describes how to write kernel device
drivers for the Linux PCMCIA Card Services interface. It also
describes how to write user-mode utilities for communicating with
Card Services.

\end{description}\end{quote}

\item {} 
Title: \textbf{Linux Kernel Module Programming Guide}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Ori Pomerantz.

\item[{URL}] \leavevmode
\href{http://tldp.org/LDP/lkmpg/2.6/html/index.html}{http://tldp.org/LDP/lkmpg/2.6/html/index.html}

\item[{Date}] \leavevmode
2001

\item[{Keywords}] \leavevmode
modules, GPL book, /proc, ioctls, system calls,
interrupt handlers .

\item[{Description}] \leavevmode
Very nice 92 pages GPL book on the topic of modules
programming. Lots of examples.

\end{description}\end{quote}

\item {} 
Title: \textbf{Global spinlock list and usage}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Rick Lindsley.

\item[{URL}] \leavevmode
\href{http://lse.sourceforge.net/lockhier/global-spin-lock}{http://lse.sourceforge.net/lockhier/global-spin-lock}

\item[{Date}] \leavevmode
2001

\item[{Keywords}] \leavevmode
spinlock.

\item[{Description}] \leavevmode
This is an attempt to document both the existence and
usage of the spinlocks in the Linux 2.4.5 kernel. Comprehensive
list of spinlocks showing when they are used, which functions
access them, how each lock is acquired, under what conditions it
is held, whether interrupts can occur or not while it is held...

\end{description}\end{quote}

\item {} 
Title: \textbf{A Linux vm README}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Kanoj Sarcar.

\item[{URL}] \leavevmode
\href{http://kos.enix.org/pub/linux-vmm.html}{http://kos.enix.org/pub/linux-vmm.html}

\item[{Date}] \leavevmode
2001

\item[{Keywords}] \leavevmode
virtual memory, mm, pgd, vma, page, page flags, page
cache, swap cache, kswapd.

\item[{Description}] \leavevmode
Telegraphic, short descriptions and definitions
relating the Linux virtual memory implementation.

\end{description}\end{quote}

\item {} 
Title: \textbf{Video4linux Drivers, Part 1: Video-Capture Device}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Alan Cox.

\item[{URL}] \leavevmode
\href{http://www.linux-mag.com/id/406}{http://www.linux-mag.com/id/406}

\item[{Date}] \leavevmode
2000

\item[{Keywords}] \leavevmode
video4linux, driver, video capture, capture devices,
camera driver.

\item[{Description}] \leavevmode
The title says it all.

\end{description}\end{quote}

\item {} 
Title: \textbf{Video4linux Drivers, Part 2: Video-capture Devices}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Alan Cox.

\item[{URL}] \leavevmode
\href{http://www.linux-mag.com/id/429}{http://www.linux-mag.com/id/429}

\item[{Date}] \leavevmode
2000

\item[{Keywords}] \leavevmode
video4linux, driver, video capture, capture devices,
camera driver, control, query capabilities, capability, facility.

\item[{Description}] \leavevmode
The title says it all.

\end{description}\end{quote}

\item {} 
Title: \textbf{Linux IP Networking. A Guide to the Implementation and Modification of the Linux Protocol Stack.}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Glenn Herrin.

\item[{URL}] \leavevmode
\href{http://www.cs.unh.edu/cnrg/gherrin}{http://www.cs.unh.edu/cnrg/gherrin}

\item[{Date}] \leavevmode
2000

\item[{Keywords}] \leavevmode
network, networking, protocol, IP, UDP, TCP, connection,
socket, receiving, transmitting, forwarding, routing, packets,
modules, /proc, sk\_buff, FIB, tags.

\item[{Description}] \leavevmode
Excellent paper devoted to the Linux IP Networking,
explaining anything from the kernel's to the user space
configuration tools' code. Very good to get a general overview of
the kernel networking implementation and understand all steps
packets follow from the time they are received at the network
device till they are delivered to applications. The studied kernel
code is from 2.2.14 version. Provides code for a working packet
dropper example.

\end{description}\end{quote}

\item {} 
Title: \textbf{How To Make Sure Your Driver Will Work On The Power Macintosh}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Paul Mackerras.

\item[{URL}] \leavevmode
\href{http://www.linux-mag.com/id/261}{http://www.linux-mag.com/id/261}

\item[{Date}] \leavevmode
1999

\item[{Keywords}] \leavevmode
Mac, Power Macintosh, porting, drivers, compatibility.

\item[{Description}] \leavevmode
The title says it all.

\end{description}\end{quote}

\item {} 
Title: \textbf{An Introduction to SCSI Drivers}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Alan Cox.

\item[{URL}] \leavevmode
\href{http://www.linux-mag.com/id/284}{http://www.linux-mag.com/id/284}

\item[{Date}] \leavevmode
1999

\item[{Keywords}] \leavevmode
SCSI, device, driver.

\item[{Description}] \leavevmode
The title says it all.

\end{description}\end{quote}

\item {} 
Title: \textbf{Advanced SCSI Drivers And Other Tales}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Alan Cox.

\item[{URL}] \leavevmode
\href{http://www.linux-mag.com/id/307}{http://www.linux-mag.com/id/307}

\item[{Date}] \leavevmode
1999

\item[{Keywords}] \leavevmode
SCSI, device, driver, advanced.

\item[{Description}] \leavevmode
The title says it all.

\end{description}\end{quote}

\item {} 
Title: \textbf{Writing Linux Mouse Drivers}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Alan Cox.

\item[{URL}] \leavevmode
\href{http://www.linux-mag.com/id/330}{http://www.linux-mag.com/id/330}

\item[{Date}] \leavevmode
1999

\item[{Keywords}] \leavevmode
mouse, driver, gpm.

\item[{Description}] \leavevmode
The title says it all.

\end{description}\end{quote}

\item {} 
Title: \textbf{More on Mouse Drivers}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Alan Cox.

\item[{URL}] \leavevmode
\href{http://www.linux-mag.com/id/356}{http://www.linux-mag.com/id/356}

\item[{Date}] \leavevmode
1999

\item[{Keywords}] \leavevmode
mouse, driver, gpm, races, asynchronous I/O.

\item[{Description}] \leavevmode
The title still says it all.

\end{description}\end{quote}

\item {} 
Title: \textbf{Writing Video4linux Radio Driver}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Alan Cox.

\item[{URL}] \leavevmode
\href{http://www.linux-mag.com/id/381}{http://www.linux-mag.com/id/381}

\item[{Date}] \leavevmode
1999

\item[{Keywords}] \leavevmode
video4linux, driver, radio, radio devices.

\item[{Description}] \leavevmode
The title says it all.

\end{description}\end{quote}

\item {} 
Title: \textbf{I/O Event Handling Under Linux}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Richard Gooch.

\item[{URL}] \leavevmode
\href{http://web.mit.edu/~yandros/doc/io-events.html}{http://web.mit.edu/\textasciitilde{}yandros/doc/io-events.html}

\item[{Date}] \leavevmode
1999

\item[{Keywords}] \leavevmode
IO, I/O, select(2), poll(2), FDs, aio\_read(2), readiness
event queues.

\item[{Description}] \leavevmode
From the Introduction: ``I/O Event handling is about
how your Operating System allows you to manage a large number of
open files (file descriptors in UNIX/POSIX, or FDs) in your
application. You want the OS to notify you when FDs become active
(have data ready to be read or are ready for writing). Ideally you
want a mechanism that is scalable. This means a large number of
inactive FDs cost very little in memory and CPU time to manage''.

\end{description}\end{quote}

\item {} 
Title: \textbf{(nearly) Complete Linux Loadable Kernel Modules. The definitive guide for hackers, virus coders and system administrators.}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
pragmatic/THC.

\item[{URL}] \leavevmode
\href{http://packetstormsecurity.org/docs/hack/LKM\_HACKING.html}{http://packetstormsecurity.org/docs/hack/LKM\_HACKING.html}

\item[{Date}] \leavevmode
1999

\item[{Keywords}] \leavevmode
syscalls, intercept, hide, abuse, symbol table.

\item[{Description}] \leavevmode
Interesting paper on how to abuse the Linux kernel in
order to intercept and modify syscalls, make
files/directories/processes invisible, become root, hijack ttys,
write kernel modules based virus... and solutions for admins to
avoid all those abuses.

\item[{Notes}] \leavevmode
For 2.0.x kernels. Gives guidances to port it to 2.2.x
kernels.

\end{description}\end{quote}

\item {} 
Name: \textbf{Linux Virtual File System}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Peter J. Braam.

\item[{URL}] \leavevmode
\href{http://www.coda.cs.cmu.edu/doc/talks/linuxvfs/}{http://www.coda.cs.cmu.edu/doc/talks/linuxvfs/}

\item[{Date}] \leavevmode
1998

\item[{Keywords}] \leavevmode
slides, VFS, inode, superblock, dentry, dcache.

\item[{Description}] \leavevmode
Set of slides, presumably from a presentation on the
Linux VFS layer. Covers version 2.1.x, with dentries and the
dcache.

\end{description}\end{quote}

\item {} 
Title: \textbf{The Venus kernel interface}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Peter J. Braam.

\item[{URL}] \leavevmode
\href{http://www.coda.cs.cmu.edu/doc/html/kernel-venus-protocol.html}{http://www.coda.cs.cmu.edu/doc/html/kernel-venus-protocol.html}

\item[{Date}] \leavevmode
1998

\item[{Keywords}] \leavevmode
coda, filesystem, venus, cache manager.

\item[{Description}] \leavevmode
``This document describes the communication between
Venus and kernel level file system code needed for the operation
of the Coda filesystem. This version document is meant to describe
the current interface (version 1.0) as well as improvements we
envisage''.

\end{description}\end{quote}

\item {} 
Title: \textbf{Design and Implementation of the Second Extended Filesystem}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Rémy Card, Theodore Ts'o, Stephen Tweedie.

\item[{URL}] \leavevmode
\href{http://web.mit.edu/tytso/www/linux/ext2intro.html}{http://web.mit.edu/tytso/www/linux/ext2intro.html}

\item[{Date}] \leavevmode
1998

\item[{Keywords}] \leavevmode
ext2, linux fs history, inode, directory, link, devices,
VFS, physical structure, performance, benchmarks, ext2fs library,
ext2fs tools, e2fsck.

\item[{Description}] \leavevmode
Paper written by three of the top ext2 hackers.
Covers Linux filesystems history, ext2 motivation, ext2 features,
design, physical structure on disk, performance, benchmarks,
e2fsck's passes description... A must read!

\item[{Notes}] \leavevmode
This paper was first published in the Proceedings of the
First Dutch International Symposium on Linux, ISBN 90-367-0385-9.

\end{description}\end{quote}

\item {} 
Title: \textbf{The Linux RAID-1, 4, 5 Code}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Ingo Molnar, Gadi Oxman and Miguel de Icaza.

\item[{URL}] \leavevmode
\href{http://www.linuxjournal.com/article.php?sid=2391}{http://www.linuxjournal.com/article.php?sid=2391}

\item[{Date}] \leavevmode
1997

\item[{Keywords}] \leavevmode
RAID, MD driver.

\item[{Description}] \leavevmode
Linux Journal Kernel Korner article. Here is its

\item[{Abstract}] \leavevmode
\emph{A description of the implementation of the RAID-1,
RAID-4 and RAID-5 personalities of the MD device driver in the
Linux kernel, providing users with high performance and reliable,
secondary-storage capability using software}.

\end{description}\end{quote}

\item {} 
Title: \textbf{Linux Kernel Hackers' Guide}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Michael K. Johnson.

\item[{URL}] \leavevmode
\href{http://www.tldp.org/LDP/khg/HyperNews/get/khg.html}{http://www.tldp.org/LDP/khg/HyperNews/get/khg.html}

\item[{Date}] \leavevmode
1997

\item[{Keywords}] \leavevmode
device drivers, files, VFS, kernel interface, character vs
block devices, hardware interrupts, scsi, DMA, access to user memory,
memory allocation, timers.

\item[{Description}] \leavevmode
A guide designed to help you get up to speed on the
concepts that are not intuitevly obvious, and to document the internal
structures of Linux.

\end{description}\end{quote}

\item {} 
Title: \textbf{Dynamic Kernels: Modularized Device Drivers}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Alessandro Rubini.

\item[{URL}] \leavevmode
\href{http://www.linuxjournal.com/article.php?sid=1219}{http://www.linuxjournal.com/article.php?sid=1219}

\item[{Date}] \leavevmode
1996

\item[{Keywords}] \leavevmode
device driver, module, loading/unloading modules,
allocating resources.

\item[{Description}] \leavevmode
Linux Journal Kernel Korner article. Here is its

\item[{Abstract}] \leavevmode
\emph{This is the first of a series of four articles
co-authored by Alessandro Rubini and Georg Zezchwitz which present
a practical approach to writing Linux device drivers as kernel
loadable modules. This installment presents an introduction to the
topic, preparing the reader to understand next month's
installment}.

\end{description}\end{quote}

\item {} 
Title: \textbf{Dynamic Kernels: Discovery}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Alessandro Rubini.

\item[{URL}] \leavevmode
\href{http://www.linuxjournal.com/article.php?sid=1220}{http://www.linuxjournal.com/article.php?sid=1220}

\item[{Date}] \leavevmode
1996

\item[{Keywords}] \leavevmode
character driver, init\_module, clean\_up module,
autodetection, mayor number, minor number, file operations,
open(), close().

\item[{Description}] \leavevmode
Linux Journal Kernel Korner article. Here is its

\item[{Abstract}] \leavevmode
\emph{This article, the second of four, introduces part of
the actual code to create custom module implementing a character
device driver. It describes the code for module initialization and
cleanup, as well as the open() and close() system calls}.

\end{description}\end{quote}

\item {} 
Title: \textbf{The Devil's in the Details}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Georg v. Zezschwitz and Alessandro Rubini.

\item[{URL}] \leavevmode
\href{http://www.linuxjournal.com/article.php?sid=1221}{http://www.linuxjournal.com/article.php?sid=1221}

\item[{Date}] \leavevmode
1996

\item[{Keywords}] \leavevmode
read(), write(), select(), ioctl(), blocking/non
blocking mode, interrupt handler.

\item[{Description}] \leavevmode
Linux Journal Kernel Korner article. Here is its

\item[{Abstract}] \leavevmode
\emph{This article, the third of four on writing character
device drivers, introduces concepts of reading, writing, and using
ioctl-calls}.

\end{description}\end{quote}

\item {} 
Title: \textbf{Dissecting Interrupts and Browsing DMA}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Alessandro Rubini and Georg v. Zezschwitz.

\item[{URL}] \leavevmode
\href{http://www.linuxjournal.com/article.php?sid=1222}{http://www.linuxjournal.com/article.php?sid=1222}

\item[{Date}] \leavevmode
1996

\item[{Keywords}] \leavevmode
interrupts, irqs, DMA, bottom halves, task queues.

\item[{Description}] \leavevmode
Linux Journal Kernel Korner article. Here is its

\item[{Abstract}] \leavevmode
\emph{This is the fourth in a series of articles about
writing character device drivers as loadable kernel modules. This
month, we further investigate the field of interrupt handling.
Though it is conceptually simple, practical limitations and
constraints make this an `'interesting'' part of device driver
writing, and several different facilities have been provided for
different situations. We also investigate the complex topic of
DMA}.

\end{description}\end{quote}

\item {} 
Title: \textbf{Device Drivers Concluded}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Georg v. Zezschwitz.

\item[{URL}] \leavevmode
\href{http://www.linuxjournal.com/article.php?sid=1287}{http://www.linuxjournal.com/article.php?sid=1287}

\item[{Date}] \leavevmode
1996

\item[{Keywords}] \leavevmode
address spaces, pages, pagination, page management,
demand loading, swapping, memory protection, memory mapping, mmap,
virtual memory areas (VMAs), vremap, PCI.

\item[{Description}] \leavevmode
Finally, the above turned out into a five articles
series. This latest one's introduction reads: ``This is the last of
five articles about character device drivers. In this final
section, Georg deals with memory mapping devices, beginning with
an overall description of the Linux memory management concepts''.

\end{description}\end{quote}

\item {} 
Title: \textbf{Network Buffers And Memory Management}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Alan Cox.

\item[{URL}] \leavevmode
\href{http://www.linuxjournal.com/article.php?sid=1312}{http://www.linuxjournal.com/article.php?sid=1312}

\item[{Date}] \leavevmode
1996

\item[{Keywords}] \leavevmode
sk\_buffs, network devices, protocol/link layer
variables, network devices flags, transmit, receive,
configuration, multicast.

\item[{Description}] \leavevmode
Linux Journal Kernel Korner.

\item[{Abstract}] \leavevmode
\emph{Writing a network device driver for Linux is fundamentally
simple---most of the complexity (other than talking to the
hardware) involves managing network packets in memory}.

\end{description}\end{quote}

\item {} 
Title: \textbf{Analysis of the Ext2fs structure}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Louis-Dominique Dubeau.

\item[{URL}] \leavevmode
\href{http://teaching.csse.uwa.edu.au/units/CITS2002/fs-ext2/}{http://teaching.csse.uwa.edu.au/units/CITS2002/fs-ext2/}

\item[{Date}] \leavevmode
1994

\item[{Keywords}] \leavevmode
ext2, filesystem, ext2fs.

\item[{Description}] \leavevmode
Description of ext2's blocks, directories, inodes,
bitmaps, invariants...

\end{description}\end{quote}

\end{itemize}


\section{Published books}
\label{process/kernel-docs:published-books}\begin{itemize}
\item {} 
Title: \textbf{Linux Treiber entwickeln}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Jürgen Quade, Eva-Katharina Kunst

\item[{Publisher}] \leavevmode
dpunkt.verlag

\item[{Date}] \leavevmode
Oct 2015 (4th edition)

\item[{Pages}] \leavevmode
688

\item[{ISBN}] \leavevmode
978-3-86490-288-8

\item[{Note}] \leavevmode
German. The third edition from 2011 is
much cheaper and still quite up-to-date.

\end{description}\end{quote}

\item {} 
Title: \textbf{Linux Kernel Networking: Implementation and Theory}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Rami Rosen

\item[{Publisher}] \leavevmode
Apress

\item[{Date}] \leavevmode
December 22, 2013

\item[{Pages}] \leavevmode
648

\item[{ISBN}] \leavevmode
978-1430261964

\end{description}\end{quote}

\item {} 
Title: \textbf{Embedded Linux Primer: A practical Real-World Approach, 2nd Edition}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Christopher Hallinan

\item[{Publisher}] \leavevmode
Pearson

\item[{Date}] \leavevmode
November, 2010

\item[{Pages}] \leavevmode
656

\item[{ISBN}] \leavevmode
978-0137017836

\end{description}\end{quote}

\item {} 
Title: \textbf{Linux Kernel Development, 3rd Edition}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Robert Love

\item[{Publisher}] \leavevmode
Addison-Wesley

\item[{Date}] \leavevmode
July, 2010

\item[{Pages}] \leavevmode
440

\item[{ISBN}] \leavevmode
978-0672329463

\end{description}\end{quote}

\item {} 
Title: \textbf{Essential Linux Device Drivers}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Sreekrishnan Venkateswaran

\item[{Published}] \leavevmode
Prentice Hall

\item[{Date}] \leavevmode
April, 2008

\item[{Pages}] \leavevmode
744

\item[{ISBN}] \leavevmode
978-0132396554

\end{description}\end{quote}

\end{itemize}
\phantomsection\label{process/kernel-docs:ldd3-published}\begin{itemize}
\item {} 
Title: \textbf{Linux Device Drivers, 3rd Edition}
\begin{quote}\begin{description}
\item[{Authors}] \leavevmode
Jonathan Corbet, Alessandro Rubini, and Greg Kroah-Hartman

\item[{Publisher}] \leavevmode
O'Reilly \& Associates

\item[{Date}] \leavevmode
2005

\item[{Pages}] \leavevmode
636

\item[{ISBN}] \leavevmode
0-596-00590-3

\item[{Notes}] \leavevmode
Further information in
\href{http://www.oreilly.com/catalog/linuxdrive3/}{http://www.oreilly.com/catalog/linuxdrive3/}
PDF format, URL: \href{http://lwn.net/Kernel/LDD3/}{http://lwn.net/Kernel/LDD3/}

\end{description}\end{quote}

\item {} 
Title: \textbf{Linux Kernel Internals}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Michael Beck

\item[{Publisher}] \leavevmode
Addison-Wesley

\item[{Date}] \leavevmode
1997

\item[{ISBN}] \leavevmode
0-201-33143-8 (second edition)

\end{description}\end{quote}

\item {} 
Title: \textbf{Programmation Linux 2.0 API systeme et fonctionnement du noyau}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Remy Card, Eric Dumas, Franck Mevel

\item[{Publisher}] \leavevmode
Eyrolles

\item[{Date}] \leavevmode
1997

\item[{Pages}] \leavevmode
520

\item[{ISBN}] \leavevmode
2-212-08932-5

\item[{Notes}] \leavevmode
French

\end{description}\end{quote}

\item {} 
Title: \textbf{The Design and Implementation of the 4.4 BSD UNIX Operating System}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Marshall Kirk McKusick, Keith Bostic, Michael J. Karels,
John S. Quarterman

\item[{Publisher}] \leavevmode
Addison-Wesley

\item[{Date}] \leavevmode
1996

\item[{ISBN}] \leavevmode
0-201-54979-4

\end{description}\end{quote}

\item {} 
Title: \textbf{Unix internals -- the new frontiers}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Uresh Vahalia

\item[{Publisher}] \leavevmode
Prentice Hall

\item[{Date}] \leavevmode
1996

\item[{Pages}] \leavevmode
600

\item[{ISBN}] \leavevmode
0-13-101908-2

\end{description}\end{quote}

\item {} 
Title: \textbf{Programming for the real world - POSIX.4}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Bill O. Gallmeister

\item[{Publisher}] \leavevmode
O'Reilly \& Associates, Inc

\item[{Date}] \leavevmode
1995

\item[{Pages}] \leavevmode
552

\item[{ISBN}] \leavevmode
I-56592-074-0

\item[{Notes}] \leavevmode
Though not being directly about Linux, Linux aims to be
POSIX. Good reference.

\end{description}\end{quote}

\item {} 
Title:  \textbf{UNIX  Systems  for  Modern Architectures: Symmetric Multiprocessing and Caching for Kernel Programmers}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Curt Schimmel

\item[{Publisher}] \leavevmode
Addison Wesley

\item[{Date}] \leavevmode
June, 1994

\item[{Pages}] \leavevmode
432

\item[{ISBN}] \leavevmode
0-201-63338-8

\end{description}\end{quote}

\item {} 
Title: \textbf{The Design and Implementation of the 4.3 BSD UNIX Operating System}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Samuel J. Leffler, Marshall Kirk McKusick, Michael J
Karels, John S. Quarterman

\item[{Publisher}] \leavevmode
Addison-Wesley

\item[{Date}] \leavevmode
1989 (reprinted with corrections on October, 1990)

\item[{ISBN}] \leavevmode
0-201-06196-1

\end{description}\end{quote}

\item {} 
Title: \textbf{The Design of the UNIX Operating System}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
Maurice J. Bach

\item[{Publisher}] \leavevmode
Prentice Hall

\item[{Date}] \leavevmode
1986

\item[{Pages}] \leavevmode
471

\item[{ISBN}] \leavevmode
0-13-201757-1

\end{description}\end{quote}

\end{itemize}


\section{Miscellaneous}
\label{process/kernel-docs:miscellaneous}\begin{itemize}
\item {} 
Name: \textbf{Cross-Referencing Linux}
\begin{quote}\begin{description}
\item[{URL}] \leavevmode
\href{http://lxr.free-electrons.com/}{http://lxr.free-electrons.com/}

\item[{Keywords}] \leavevmode
Browsing source code.

\item[{Description}] \leavevmode
Another web-based Linux kernel source code browser.
Lots of cross references to variables and functions. You can see
where they are defined and where they are used.

\end{description}\end{quote}

\item {} 
Name: \textbf{Linux Weekly News}
\begin{quote}\begin{description}
\item[{URL}] \leavevmode
\href{http://lwn.net}{http://lwn.net}

\item[{Keywords}] \leavevmode
latest kernel news.

\item[{Description}] \leavevmode
The title says it all. There's a fixed kernel section
summarizing developers' work, bug fixes, new features and versions
produced during the week. Published every Thursday.

\end{description}\end{quote}

\item {} 
Name: \textbf{The home page of Linux-MM}
\begin{quote}\begin{description}
\item[{Author}] \leavevmode
The Linux-MM team.

\item[{URL}] \leavevmode
\href{http://linux-mm.org/}{http://linux-mm.org/}

\item[{Keywords}] \leavevmode
memory management, Linux-MM, mm patches, TODO, docs,
mailing list.

\item[{Description}] \leavevmode
Site devoted to Linux Memory Management development.
Memory related patches, HOWTOs, links, mm developers... Don't miss
it if you are interested in memory management development!

\end{description}\end{quote}

\item {} 
Name: \textbf{Kernel Newbies IRC Channel and Website}
\begin{quote}\begin{description}
\item[{URL}] \leavevmode
\href{http://www.kernelnewbies.org}{http://www.kernelnewbies.org}

\item[{Keywords}] \leavevmode
IRC, newbies, channel, asking doubts.

\item[{Description}] \leavevmode
\#kernelnewbies on irc.oftc.net.
\#kernelnewbies is an IRC network dedicated to the `newbie'
kernel hacker. The audience mostly consists of people who are
learning about the kernel, working on kernel projects or
professional kernel hackers that want to help less seasoned kernel
people.
\#kernelnewbies is on the OFTC IRC Network.
Try irc.oftc.net as your server and then /join \#kernelnewbies.
The kernelnewbies website also hosts articles, documents, FAQs...

\end{description}\end{quote}

\item {} 
Name: \textbf{linux-kernel mailing list archives and search engines}
\begin{quote}\begin{description}
\item[{URL}] \leavevmode
\href{http://vger.kernel.org/vger-lists.html}{http://vger.kernel.org/vger-lists.html}

\item[{URL}] \leavevmode
\href{http://www.uwsg.indiana.edu/hypermail/linux/kernel/index.html}{http://www.uwsg.indiana.edu/hypermail/linux/kernel/index.html}

\item[{URL}] \leavevmode
\href{http://groups.google.com/group/mlist.linux.kernel}{http://groups.google.com/group/mlist.linux.kernel}

\item[{Keywords}] \leavevmode
linux-kernel, archives, search.

\item[{Description}] \leavevmode
Some of the linux-kernel mailing list archivers. If
you have a better/another one, please let me know.

\end{description}\end{quote}

\end{itemize}


\bigskip\hrule{}\bigskip


Document last updated on Tue 2016-Sep-20
\begin{description}
\item[{This document is based on:}] \leavevmode
\href{http://www.dit.upm.es/~jmseyas/linux/kernel/hackers-docs.html}{http://www.dit.upm.es/\textasciitilde{}jmseyas/linux/kernel/hackers-docs.html}

\end{description}

These are some overall technical guides that have been put here for now for
lack of a better place.


\chapter{Applying Patches To The Linux Kernel}
\label{process/applying-patches:applying-patches-to-the-linux-kernel}\label{process/applying-patches::doc}\label{process/applying-patches:applying-patches}\begin{description}
\item[{Original by:}] \leavevmode
Jesper Juhl, August 2005

\end{description}

\begin{notice}{note}{Note:}
This document is obsolete.  In most cases, rather than using \code{patch}
manually, you'll almost certainly want to look at using Git instead.
\end{notice}

A frequently asked question on the Linux Kernel Mailing List is how to apply
a patch to the kernel or, more specifically, what base kernel a patch for
one of the many trees/branches should be applied to. Hopefully this document
will explain this to you.

In addition to explaining how to apply and revert patches, a brief
description of the different kernel trees (and examples of how to apply
their specific patches) is also provided.


\section{What is a patch?}
\label{process/applying-patches:what-is-a-patch}
A patch is a small text document containing a delta of changes between two
different versions of a source tree. Patches are created with the \code{diff}
program.

To correctly apply a patch you need to know what base it was generated from
and what new version the patch will change the source tree into. These
should both be present in the patch file metadata or be possible to deduce
from the filename.


\section{How do I apply or revert a patch?}
\label{process/applying-patches:how-do-i-apply-or-revert-a-patch}
You apply a patch with the \code{patch} program. The patch program reads a diff
(or patch) file and makes the changes to the source tree described in it.

Patches for the Linux kernel are generated relative to the parent directory
holding the kernel source dir.

This means that paths to files inside the patch file contain the name of the
kernel source directories it was generated against (or some other directory
names like ``a/'' and ``b/'').

Since this is unlikely to match the name of the kernel source dir on your
local machine (but is often useful info to see what version an otherwise
unlabeled patch was generated against) you should change into your kernel
source directory and then strip the first element of the path from filenames
in the patch file when applying it (the \code{-p1} argument to \code{patch} does
this).

To revert a previously applied patch, use the -R argument to patch.
So, if you applied a patch like this:

\begin{Verbatim}[commandchars=\\\{\}]
patch \PYGZhy{}p1 \PYGZlt{} ../patch\PYGZhy{}x.y.z
\end{Verbatim}

You can revert (undo) it like this:

\begin{Verbatim}[commandchars=\\\{\}]
patch \PYGZhy{}R \PYGZhy{}p1 \PYGZlt{} ../patch\PYGZhy{}x.y.z
\end{Verbatim}


\section{How do I feed a patch/diff file to \texttt{patch}?}
\label{process/applying-patches:how-do-i-feed-a-patch-diff-file-to-patch}
This (as usual with Linux and other UNIX like operating systems) can be
done in several different ways.

In all the examples below I feed the file (in uncompressed form) to patch
via stdin using the following syntax:

\begin{Verbatim}[commandchars=\\\{\}]
patch \PYGZhy{}p1 \PYGZlt{} path/to/patch\PYGZhy{}x.y.z
\end{Verbatim}

If you just want to be able to follow the examples below and don't want to
know of more than one way to use patch, then you can stop reading this
section here.

Patch can also get the name of the file to use via the -i argument, like
this:

\begin{Verbatim}[commandchars=\\\{\}]
patch \PYGZhy{}p1 \PYGZhy{}i path/to/patch\PYGZhy{}x.y.z
\end{Verbatim}

If your patch file is compressed with gzip or xz and you don't want to
uncompress it before applying it, then you can feed it to patch like this
instead:

\begin{Verbatim}[commandchars=\\\{\}]
xzcat path/to/patch\PYGZhy{}x.y.z.xz \textbar{} patch \PYGZhy{}p1
bzcat path/to/patch\PYGZhy{}x.y.z.gz \textbar{} patch \PYGZhy{}p1
\end{Verbatim}

If you wish to uncompress the patch file by hand first before applying it
(what I assume you've done in the examples below), then you simply run
gunzip or xz on the file -- like this:

\begin{Verbatim}[commandchars=\\\{\}]
gunzip patch\PYGZhy{}x.y.z.gz
xz \PYGZhy{}d patch\PYGZhy{}x.y.z.xz
\end{Verbatim}

Which will leave you with a plain text patch-x.y.z file that you can feed to
patch via stdin or the \code{-i} argument, as you prefer.

A few other nice arguments for patch are \code{-s} which causes patch to be silent
except for errors which is nice to prevent errors from scrolling out of the
screen too fast, and \code{-{-}dry-run} which causes patch to just print a listing of
what would happen, but doesn't actually make any changes. Finally \code{-{-}verbose}
tells patch to print more information about the work being done.


\section{Common errors when patching}
\label{process/applying-patches:common-errors-when-patching}
When patch applies a patch file it attempts to verify the sanity of the
file in different ways.

Checking that the file looks like a valid patch file and checking the code
around the bits being modified matches the context provided in the patch are
just two of the basic sanity checks patch does.

If patch encounters something that doesn't look quite right it has two
options. It can either refuse to apply the changes and abort or it can try
to find a way to make the patch apply with a few minor changes.

One example of something that's not `quite right' that patch will attempt to
fix up is if all the context matches, the lines being changed match, but the
line numbers are different. This can happen, for example, if the patch makes
a change in the middle of the file but for some reasons a few lines have
been added or removed near the beginning of the file. In that case
everything looks good it has just moved up or down a bit, and patch will
usually adjust the line numbers and apply the patch.

Whenever patch applies a patch that it had to modify a bit to make it fit
it'll tell you about it by saying the patch applied with \textbf{fuzz}.
You should be wary of such changes since even though patch probably got it
right it doesn't /always/ get it right, and the result will sometimes be
wrong.

When patch encounters a change that it can't fix up with fuzz it rejects it
outright and leaves a file with a \code{.rej} extension (a reject file). You can
read this file to see exactly what change couldn't be applied, so you can
go fix it up by hand if you wish.

If you don't have any third-party patches applied to your kernel source, but
only patches from kernel.org and you apply the patches in the correct order,
and have made no modifications yourself to the source files, then you should
never see a fuzz or reject message from patch. If you do see such messages
anyway, then there's a high risk that either your local source tree or the
patch file is corrupted in some way. In that case you should probably try
re-downloading the patch and if things are still not OK then you'd be advised
to start with a fresh tree downloaded in full from kernel.org.

Let's look a bit more at some of the messages patch can produce.

If patch stops and presents a \code{File to patch:} prompt, then patch could not
find a file to be patched. Most likely you forgot to specify -p1 or you are
in the wrong directory. Less often, you'll find patches that need to be
applied with \code{-p0} instead of \code{-p1} (reading the patch file should reveal if
this is the case -- if so, then this is an error by the person who created
the patch but is not fatal).

If you get \code{Hunk \#2 succeeded at 1887 with fuzz 2 (offset 7 lines).} or a
message similar to that, then it means that patch had to adjust the location
of the change (in this example it needed to move 7 lines from where it
expected to make the change to make it fit).

The resulting file may or may not be OK, depending on the reason the file
was different than expected.

This often happens if you try to apply a patch that was generated against a
different kernel version than the one you are trying to patch.

If you get a message like \code{Hunk \#3 FAILED at 2387.}, then it means that the
patch could not be applied correctly and the patch program was unable to
fuzz its way through. This will generate a \code{.rej} file with the change that
caused the patch to fail and also a \code{.orig} file showing you the original
content that couldn't be changed.

If you get \code{Reversed (or previously applied) patch detected!  Assume -R? {[}n{]}}
then patch detected that the change contained in the patch seems to have
already been made.

If you actually did apply this patch previously and you just re-applied it
in error, then just say {[}n{]}o and abort this patch. If you applied this patch
previously and actually intended to revert it, but forgot to specify -R,
then you can say {[}\textbf{y}{]}es here to make patch revert it for you.

This can also happen if the creator of the patch reversed the source and
destination directories when creating the patch, and in that case reverting
the patch will in fact apply it.

A message similar to \code{patch: **** unexpected end of file in patch} or
\code{patch unexpectedly ends in middle of line} means that patch could make no
sense of the file you fed to it. Either your download is broken, you tried to
feed patch a compressed patch file without uncompressing it first, or the patch
file that you are using has been mangled by a mail client or mail transfer
agent along the way somewhere, e.g., by splitting a long line into two lines.
Often these warnings can easily be fixed by joining (concatenating) the
two lines that had been split.

As I already mentioned above, these errors should never happen if you apply
a patch from kernel.org to the correct version of an unmodified source tree.
So if you get these errors with kernel.org patches then you should probably
assume that either your patch file or your tree is broken and I'd advise you
to start over with a fresh download of a full kernel tree and the patch you
wish to apply.


\section{Are there any alternatives to \texttt{patch}?}
\label{process/applying-patches:are-there-any-alternatives-to-patch}
Yes there are alternatives.

You can use the \code{interdiff} program (\href{http://cyberelk.net/tim/patchutils/}{http://cyberelk.net/tim/patchutils/}) to
generate a patch representing the differences between two patches and then
apply the result.

This will let you move from something like 4.7.2 to 4.7.3 in a single
step. The -z flag to interdiff will even let you feed it patches in gzip or
bzip2 compressed form directly without the use of zcat or bzcat or manual
decompression.

Here's how you'd go from 4.7.2 to 4.7.3 in a single step:

\begin{Verbatim}[commandchars=\\\{\}]
interdiff \PYGZhy{}z ../patch\PYGZhy{}4.7.2.gz ../patch\PYGZhy{}4.7.3.gz \textbar{} patch \PYGZhy{}p1
\end{Verbatim}

Although interdiff may save you a step or two you are generally advised to
do the additional steps since interdiff can get things wrong in some cases.

Another alternative is \code{ketchup}, which is a python script for automatic
downloading and applying of patches (\href{http://www.selenic.com/ketchup/}{http://www.selenic.com/ketchup/}).

Other nice tools are diffstat, which shows a summary of changes made by a
patch; lsdiff, which displays a short listing of affected files in a patch
file, along with (optionally) the line numbers of the start of each patch;
and grepdiff, which displays a list of the files modified by a patch where
the patch contains a given regular expression.


\section{Where can I download the patches?}
\label{process/applying-patches:where-can-i-download-the-patches}
The patches are available at \href{http://kernel.org/}{http://kernel.org/}
Most recent patches are linked from the front page, but they also have
specific homes.

The 4.x.y (-stable) and 4.x patches live at
\begin{quote}

\href{https://www.kernel.org/pub/linux/kernel/v4.x/}{https://www.kernel.org/pub/linux/kernel/v4.x/}
\end{quote}

The -rc patches live at
\begin{quote}

\href{https://www.kernel.org/pub/linux/kernel/v4.x/testing/}{https://www.kernel.org/pub/linux/kernel/v4.x/testing/}
\end{quote}


\section{The 4.x kernels}
\label{process/applying-patches:the-4-x-kernels}
These are the base stable releases released by Linus. The highest numbered
release is the most recent.

If regressions or other serious flaws are found, then a -stable fix patch
will be released (see below) on top of this base. Once a new 4.x base
kernel is released, a patch is made available that is a delta between the
previous 4.x kernel and the new one.

To apply a patch moving from 4.6 to 4.7, you'd do the following (note
that such patches do \textbf{NOT} apply on top of 4.x.y kernels but on top of the
base 4.x kernel -- if you need to move from 4.x.y to 4.x+1 you need to
first revert the 4.x.y patch).

Here are some examples:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZsh{} moving from 4.6 to 4.7

\PYGZdl{} cd \PYGZti{}/linux\PYGZhy{}4.6                \PYGZsh{} change to kernel source dir
\PYGZdl{} patch \PYGZhy{}p1 \PYGZlt{} ../patch\PYGZhy{}4.7      \PYGZsh{} apply the 4.7 patch
\PYGZdl{} cd ..
\PYGZdl{} mv linux\PYGZhy{}4.6 linux\PYGZhy{}4.7        \PYGZsh{} rename source dir

\PYGZsh{} moving from 4.6.1 to 4.7

\PYGZdl{} cd \PYGZti{}/linux\PYGZhy{}4.6.1              \PYGZsh{} change to kernel source dir
\PYGZdl{} patch \PYGZhy{}p1 \PYGZhy{}R \PYGZlt{} ../patch\PYGZhy{}4.6.1 \PYGZsh{} revert the 4.6.1 patch
                                \PYGZsh{} source dir is now 4.6
\PYGZdl{} patch \PYGZhy{}p1 \PYGZlt{} ../patch\PYGZhy{}4.7      \PYGZsh{} apply new 4.7 patch
\PYGZdl{} cd ..
\PYGZdl{} mv linux\PYGZhy{}4.6.1 linux\PYGZhy{}4.7      \PYGZsh{} rename source dir
\end{Verbatim}


\section{The 4.x.y kernels}
\label{process/applying-patches:the-4-x-y-kernels}
Kernels with 3-digit versions are -stable kernels. They contain small(ish)
critical fixes for security problems or significant regressions discovered
in a given 4.x kernel.

This is the recommended branch for users who want the most recent stable
kernel and are not interested in helping test development/experimental
versions.

If no 4.x.y kernel is available, then the highest numbered 4.x kernel is
the current stable kernel.

\begin{notice}{note}{Note:}
The -stable team usually do make incremental patches available as well
as patches against the latest mainline release, but I only cover the
non-incremental ones below. The incremental ones can be found at
\href{https://www.kernel.org/pub/linux/kernel/v4.x/incr/}{https://www.kernel.org/pub/linux/kernel/v4.x/incr/}
\end{notice}

These patches are not incremental, meaning that for example the 4.7.3
patch does not apply on top of the 4.7.2 kernel source, but rather on top
of the base 4.7 kernel source.

So, in order to apply the 4.7.3 patch to your existing 4.7.2 kernel
source you have to first back out the 4.7.2 patch (so you are left with a
base 4.7 kernel source) and then apply the new 4.7.3 patch.

Here's a small example:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} cd \PYGZti{}/linux\PYGZhy{}4.7.2              \PYGZsh{} change to the kernel source dir
\PYGZdl{} patch \PYGZhy{}p1 \PYGZhy{}R \PYGZlt{} ../patch\PYGZhy{}4.7.2 \PYGZsh{} revert the 4.7.2 patch
\PYGZdl{} patch \PYGZhy{}p1 \PYGZlt{} ../patch\PYGZhy{}4.7.3    \PYGZsh{} apply the new 4.7.3 patch
\PYGZdl{} cd ..
\PYGZdl{} mv linux\PYGZhy{}4.7.2 linux\PYGZhy{}4.7.3    \PYGZsh{} rename the kernel source dir
\end{Verbatim}


\section{The -rc kernels}
\label{process/applying-patches:the-rc-kernels}
These are release-candidate kernels. These are development kernels released
by Linus whenever he deems the current git (the kernel's source management
tool) tree to be in a reasonably sane state adequate for testing.

These kernels are not stable and you should expect occasional breakage if
you intend to run them. This is however the most stable of the main
development branches and is also what will eventually turn into the next
stable kernel, so it is important that it be tested by as many people as
possible.

This is a good branch to run for people who want to help out testing
development kernels but do not want to run some of the really experimental
stuff (such people should see the sections about -next and -mm kernels below).

The -rc patches are not incremental, they apply to a base 4.x kernel, just
like the 4.x.y patches described above. The kernel version before the -rcN
suffix denotes the version of the kernel that this -rc kernel will eventually
turn into.

So, 4.8-rc5 means that this is the fifth release candidate for the 4.8
kernel and the patch should be applied on top of the 4.7 kernel source.

Here are 3 examples of how to apply these patches:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZsh{} first an example of moving from 4.7 to 4.8\PYGZhy{}rc3

\PYGZdl{} cd \PYGZti{}/linux\PYGZhy{}4.7                        \PYGZsh{} change to the 4.7 source dir
\PYGZdl{} patch \PYGZhy{}p1 \PYGZlt{} ../patch\PYGZhy{}4.8\PYGZhy{}rc3          \PYGZsh{} apply the 4.8\PYGZhy{}rc3 patch
\PYGZdl{} cd ..
\PYGZdl{} mv linux\PYGZhy{}4.7 linux\PYGZhy{}4.8\PYGZhy{}rc3            \PYGZsh{} rename the source dir

\PYGZsh{} now let\PYGZsq{}s move from 4.8\PYGZhy{}rc3 to 4.8\PYGZhy{}rc5

\PYGZdl{} cd \PYGZti{}/linux\PYGZhy{}4.8\PYGZhy{}rc3                    \PYGZsh{} change to the 4.8\PYGZhy{}rc3 dir
\PYGZdl{} patch \PYGZhy{}p1 \PYGZhy{}R \PYGZlt{} ../patch\PYGZhy{}4.8\PYGZhy{}rc3       \PYGZsh{} revert the 4.8\PYGZhy{}rc3 patch
\PYGZdl{} patch \PYGZhy{}p1 \PYGZlt{} ../patch\PYGZhy{}4.8\PYGZhy{}rc5          \PYGZsh{} apply the new 4.8\PYGZhy{}rc5 patch
\PYGZdl{} cd ..
\PYGZdl{} mv linux\PYGZhy{}4.8\PYGZhy{}rc3 linux\PYGZhy{}4.8\PYGZhy{}rc5        \PYGZsh{} rename the source dir

\PYGZsh{} finally let\PYGZsq{}s try and move from 4.7.3 to 4.8\PYGZhy{}rc5

\PYGZdl{} cd \PYGZti{}/linux\PYGZhy{}4.7.3                      \PYGZsh{} change to the kernel source dir
\PYGZdl{} patch \PYGZhy{}p1 \PYGZhy{}R \PYGZlt{} ../patch\PYGZhy{}4.7.3         \PYGZsh{} revert the 4.7.3 patch
\PYGZdl{} patch \PYGZhy{}p1 \PYGZlt{} ../patch\PYGZhy{}4.8\PYGZhy{}rc5          \PYGZsh{} apply new 4.8\PYGZhy{}rc5 patch
\PYGZdl{} cd ..
\PYGZdl{} mv linux\PYGZhy{}4.7.3 linux\PYGZhy{}4.8\PYGZhy{}rc5          \PYGZsh{} rename the kernel source dir
\end{Verbatim}


\section{The -mm patches and the linux-next tree}
\label{process/applying-patches:the-mm-patches-and-the-linux-next-tree}
The -mm patches are experimental patches released by Andrew Morton.

In the past, -mm tree were used to also test subsystem patches, but this
function is now done via the
\emph{linux-next \textless{}https://www.kernel.org/doc/man-pages/linux-next.html\textgreater{}}
tree. The Subsystem maintainers push their patches first to linux-next,
and, during the merge window, sends them directly to Linus.

The -mm patches serve as a sort of proving ground for new features and other
experimental patches that aren't merged via a subsystem tree.
Once such patches has proved its worth in -mm for a while Andrew pushes
it on to Linus for inclusion in mainline.

The linux-next tree is daily updated, and includes the -mm patches.
Both are in constant flux and contains many experimental features, a
lot of debugging patches not appropriate for mainline etc., and is the most
experimental of the branches described in this document.

These patches are not appropriate for use on systems that are supposed to be
stable and they are more risky to run than any of the other branches (make
sure you have up-to-date backups -- that goes for any experimental kernel but
even more so for -mm patches or using a Kernel from the linux-next tree).

Testing of -mm patches and linux-next is greatly appreciated since the whole
point of those are to weed out regressions, crashes, data corruption bugs,
build breakage (and any other bug in general) before changes are merged into
the more stable mainline Linus tree.

But testers of -mm and linux-next should be aware that breakages are
more common than in any other tree.

This concludes this list of explanations of the various kernel trees.
I hope you are now clear on how to apply the various patches and help testing
the kernel.

Thank you's to Randy Dunlap, Rolf Eike Beer, Linus Torvalds, Bodo Eggert,
Johannes Stezenbach, Grant Coady, Pavel Machek and others that I may have
forgotten for their reviews and contributions to this document.


\chapter{Adding a New System Call}
\label{process/adding-syscalls:adding-a-new-system-call}\label{process/adding-syscalls::doc}
This document describes what's involved in adding a new system call to the
Linux kernel, over and above the normal submission advice in
{\hyperref[process/submitting\string-patches:submittingpatches]{\emph{Documentation/process/submitting-patches.rst}}}.


\section{System Call Alternatives}
\label{process/adding-syscalls:system-call-alternatives}
The first thing to consider when adding a new system call is whether one of
the alternatives might be suitable instead.  Although system calls are the
most traditional and most obvious interaction points between userspace and the
kernel, there are other possibilities -- choose what fits best for your
interface.
\begin{itemize}
\item {} 
If the operations involved can be made to look like a filesystem-like
object, it may make more sense to create a new filesystem or device.  This
also makes it easier to encapsulate the new functionality in a kernel module
rather than requiring it to be built into the main kernel.
\begin{itemize}
\item {} 
If the new functionality involves operations where the kernel notifies
userspace that something has happened, then returning a new file
descriptor for the relevant object allows userspace to use
\code{poll}/\code{select}/\code{epoll} to receive that notification.

\item {} 
However, operations that don't map to
\emph{\texttt{read(2)}}/\emph{\texttt{write(2)}}-like operations
have to be implemented as \emph{\texttt{ioctl(2)}} requests, which can lead
to a somewhat opaque API.

\end{itemize}

\item {} 
If you're just exposing runtime system information, a new node in sysfs
(see \code{Documentation/filesystems/sysfs.txt}) or the \code{/proc} filesystem may
be more appropriate.  However, access to these mechanisms requires that the
relevant filesystem is mounted, which might not always be the case (e.g.
in a namespaced/sandboxed/chrooted environment).  Avoid adding any API to
debugfs, as this is not considered a `production' interface to userspace.

\item {} 
If the operation is specific to a particular file or file descriptor, then
an additional \emph{\texttt{fcntl(2)}} command option may be more appropriate.  However,
\emph{\texttt{fcntl(2)}} is a multiplexing system call that hides a lot of complexity, so
this option is best for when the new function is closely analogous to
existing \emph{\texttt{fcntl(2)}} functionality, or the new functionality is very simple
(for example, getting/setting a simple flag related to a file descriptor).

\item {} 
If the operation is specific to a particular task or process, then an
additional \emph{\texttt{prctl(2)}} command option may be more appropriate.  As
with \emph{\texttt{fcntl(2)}}, this system call is a complicated multiplexor so
is best reserved for near-analogs of existing \code{prctl()} commands or
getting/setting a simple flag related to a process.

\end{itemize}


\section{Designing the API: Planning for Extension}
\label{process/adding-syscalls:designing-the-api-planning-for-extension}
A new system call forms part of the API of the kernel, and has to be supported
indefinitely.  As such, it's a very good idea to explicitly discuss the
interface on the kernel mailing list, and it's important to plan for future
extensions of the interface.

(The syscall table is littered with historical examples where this wasn't done,
together with the corresponding follow-up system calls --
\code{eventfd}/\code{eventfd2}, \code{dup2}/\code{dup3}, \code{inotify\_init}/\code{inotify\_init1},
\code{pipe}/\code{pipe2}, \code{renameat}/\code{renameat2} -- so
learn from the history of the kernel and plan for extensions from the start.)

For simpler system calls that only take a couple of arguments, the preferred
way to allow for future extensibility is to include a flags argument to the
system call.  To make sure that userspace programs can safely use flags
between kernel versions, check whether the flags value holds any unknown
flags, and reject the system call (with \code{EINVAL}) if it does:

\begin{Verbatim}[commandchars=\\\{\}]
if (flags \PYGZam{} \PYGZti{}(THING\PYGZus{}FLAG1 \textbar{} THING\PYGZus{}FLAG2 \textbar{} THING\PYGZus{}FLAG3))
    return \PYGZhy{}EINVAL;
\end{Verbatim}

(If no flags values are used yet, check that the flags argument is zero.)

For more sophisticated system calls that involve a larger number of arguments,
it's preferred to encapsulate the majority of the arguments into a structure
that is passed in by pointer.  Such a structure can cope with future extension
by including a size argument in the structure:

\begin{Verbatim}[commandchars=\\\{\}]
struct xyzzy\PYGZus{}params \PYGZob{}
    u32 size; /* userspace sets p\PYGZhy{}\PYGZgt{}size = sizeof(struct xyzzy\PYGZus{}params) */
    u32 param\PYGZus{}1;
    u64 param\PYGZus{}2;
    u64 param\PYGZus{}3;
\PYGZcb{};
\end{Verbatim}

As long as any subsequently added field, say \code{param\_4}, is designed so that a
zero value gives the previous behaviour, then this allows both directions of
version mismatch:
\begin{itemize}
\item {} 
To cope with a later userspace program calling an older kernel, the kernel
code should check that any memory beyond the size of the structure that it
expects is zero (effectively checking that \code{param\_4 == 0}).

\item {} 
To cope with an older userspace program calling a newer kernel, the kernel
code can zero-extend a smaller instance of the structure (effectively
setting \code{param\_4 = 0}).

\end{itemize}

See \emph{\texttt{perf\_event\_open(2)}} and the \code{perf\_copy\_attr()} function (in
\code{kernel/events/core.c}) for an example of this approach.


\section{Designing the API: Other Considerations}
\label{process/adding-syscalls:designing-the-api-other-considerations}
If your new system call allows userspace to refer to a kernel object, it
should use a file descriptor as the handle for that object -- don't invent a
new type of userspace object handle when the kernel already has mechanisms and
well-defined semantics for using file descriptors.

If your new \emph{\texttt{xyzzy(2)}} system call does return a new file descriptor,
then the flags argument should include a value that is equivalent to setting
\code{O\_CLOEXEC} on the new FD.  This makes it possible for userspace to close
the timing window between \code{xyzzy()} and calling
\code{fcntl(fd, F\_SETFD, FD\_CLOEXEC)}, where an unexpected \code{fork()} and
\code{execve()} in another thread could leak a descriptor to
the exec'ed program. (However, resist the temptation to re-use the actual value
of the \code{O\_CLOEXEC} constant, as it is architecture-specific and is part of a
numbering space of \code{O\_*} flags that is fairly full.)

If your system call returns a new file descriptor, you should also consider
what it means to use the \emph{\texttt{poll(2)}} family of system calls on that file
descriptor. Making a file descriptor ready for reading or writing is the
normal way for the kernel to indicate to userspace that an event has
occurred on the corresponding kernel object.

If your new \emph{\texttt{xyzzy(2)}} system call involves a filename argument:

\begin{Verbatim}[commandchars=\\\{\}]
int sys\PYGZus{}xyzzy(const char \PYGZus{}\PYGZus{}user *path, ..., unsigned int flags);
\end{Verbatim}

you should also consider whether an \emph{\texttt{xyzzyat(2)}} version is more appropriate:

\begin{Verbatim}[commandchars=\\\{\}]
int sys\PYGZus{}xyzzyat(int dfd, const char \PYGZus{}\PYGZus{}user *path, ..., unsigned int flags);
\end{Verbatim}

This allows more flexibility for how userspace specifies the file in question;
in particular it allows userspace to request the functionality for an
already-opened file descriptor using the \code{AT\_EMPTY\_PATH} flag, effectively
giving an \emph{\texttt{fxyzzy(3)}} operation for free:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZhy{} xyzzyat(AT\PYGZus{}FDCWD, path, ..., 0) is equivalent to xyzzy(path,...)
\PYGZhy{} xyzzyat(fd, \PYGZdq{}\PYGZdq{}, ..., AT\PYGZus{}EMPTY\PYGZus{}PATH) is equivalent to fxyzzy(fd, ...)
\end{Verbatim}

(For more details on the rationale of the *at() calls, see the
\emph{\texttt{openat(2)}} man page; for an example of AT\_EMPTY\_PATH, see the
\emph{\texttt{fstatat(2)}} man page.)

If your new \emph{\texttt{xyzzy(2)}} system call involves a parameter describing an
offset within a file, make its type \code{loff\_t} so that 64-bit offsets can be
supported even on 32-bit architectures.

If your new \emph{\texttt{xyzzy(2)}} system call involves privileged functionality,
it needs to be governed by the appropriate Linux capability bit (checked with
a call to \code{capable()}), as described in the \emph{\texttt{capabilities(7)}} man
page.  Choose an existing capability bit that governs related functionality,
but try to avoid combining lots of only vaguely related functions together
under the same bit, as this goes against capabilities' purpose of splitting
the power of root.  In particular, avoid adding new uses of the already
overly-general \code{CAP\_SYS\_ADMIN} capability.

If your new \emph{\texttt{xyzzy(2)}} system call manipulates a process other than
the calling process, it should be restricted (using a call to
\code{ptrace\_may\_access()}) so that only a calling process with the same
permissions as the target process, or with the necessary capabilities, can
manipulate the target process.

Finally, be aware that some non-x86 architectures have an easier time if
system call parameters that are explicitly 64-bit fall on odd-numbered
arguments (i.e. parameter 1, 3, 5), to allow use of contiguous pairs of 32-bit
registers.  (This concern does not apply if the arguments are part of a
structure that's passed in by pointer.)


\section{Proposing the API}
\label{process/adding-syscalls:proposing-the-api}
To make new system calls easy to review, it's best to divide up the patchset
into separate chunks.  These should include at least the following items as
distinct commits (each of which is described further below):
\begin{itemize}
\item {} 
The core implementation of the system call, together with prototypes,
generic numbering, Kconfig changes and fallback stub implementation.

\item {} 
Wiring up of the new system call for one particular architecture, usually
x86 (including all of x86\_64, x86\_32 and x32).

\item {} 
A demonstration of the use of the new system call in userspace via a
selftest in \code{tools/testing/selftests/}.

\item {} 
A draft man-page for the new system call, either as plain text in the
cover letter, or as a patch to the (separate) man-pages repository.

\end{itemize}

New system call proposals, like any change to the kernel's API, should always
be cc'ed to \href{mailto:linux-api@vger.kernel.org}{linux-api@vger.kernel.org}.


\section{Generic System Call Implementation}
\label{process/adding-syscalls:generic-system-call-implementation}
The main entry point for your new \emph{\texttt{xyzzy(2)}} system call will be called
\code{sys\_xyzzy()}, but you add this entry point with the appropriate
\code{SYSCALL\_DEFINEn()} macro rather than explicitly.  The `n' indicates the
number of arguments to the system call, and the macro takes the system call name
followed by the (type, name) pairs for the parameters as arguments.  Using
this macro allows metadata about the new system call to be made available for
other tools.

The new entry point also needs a corresponding function prototype, in
\code{include/linux/syscalls.h}, marked as asmlinkage to match the way that system
calls are invoked:

\begin{Verbatim}[commandchars=\\\{\}]
asmlinkage long sys\PYGZus{}xyzzy(...);
\end{Verbatim}

Some architectures (e.g. x86) have their own architecture-specific syscall
tables, but several other architectures share a generic syscall table. Add your
new system call to the generic list by adding an entry to the list in
\code{include/uapi/asm-generic/unistd.h}:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZsh{}define \PYGZus{}\PYGZus{}NR\PYGZus{}xyzzy 292
\PYGZus{}\PYGZus{}SYSCALL(\PYGZus{}\PYGZus{}NR\PYGZus{}xyzzy, sys\PYGZus{}xyzzy)
\end{Verbatim}

Also update the \_\_NR\_syscalls count to reflect the additional system call, and
note that if multiple new system calls are added in the same merge window,
your new syscall number may get adjusted to resolve conflicts.

The file \code{kernel/sys\_ni.c} provides a fallback stub implementation of each
system call, returning \code{-ENOSYS}.  Add your new system call here too:

\begin{Verbatim}[commandchars=\\\{\}]
cond\PYGZus{}syscall(sys\PYGZus{}xyzzy);
\end{Verbatim}

Your new kernel functionality, and the system call that controls it, should
normally be optional, so add a \code{CONFIG} option (typically to
\code{init/Kconfig}) for it. As usual for new \code{CONFIG} options:
\begin{itemize}
\item {} 
Include a description of the new functionality and system call controlled
by the option.

\item {} 
Make the option depend on EXPERT if it should be hidden from normal users.

\item {} 
Make any new source files implementing the function dependent on the CONFIG
option in the Makefile (e.g. \code{obj-\$(CONFIG\_XYZZY\_SYSCALL) += xyzzy.c}).

\item {} 
Double check that the kernel still builds with the new CONFIG option turned
off.

\end{itemize}

To summarize, you need a commit that includes:
\begin{itemize}
\item {} 
\code{CONFIG} option for the new function, normally in \code{init/Kconfig}

\item {} 
\code{SYSCALL\_DEFINEn(xyzzy, ...)} for the entry point

\item {} 
corresponding prototype in \code{include/linux/syscalls.h}

\item {} 
generic table entry in \code{include/uapi/asm-generic/unistd.h}

\item {} 
fallback stub in \code{kernel/sys\_ni.c}

\end{itemize}


\section{x86 System Call Implementation}
\label{process/adding-syscalls:x86-system-call-implementation}
To wire up your new system call for x86 platforms, you need to update the
master syscall tables.  Assuming your new system call isn't special in some
way (see below), this involves a ``common'' entry (for x86\_64 and x32) in
arch/x86/entry/syscalls/syscall\_64.tbl:

\begin{Verbatim}[commandchars=\\\{\}]
333   common   xyzzy     sys\PYGZus{}xyzzy
\end{Verbatim}

and an ``i386'' entry in \code{arch/x86/entry/syscalls/syscall\_32.tbl}:

\begin{Verbatim}[commandchars=\\\{\}]
380   i386     xyzzy     sys\PYGZus{}xyzzy
\end{Verbatim}

Again, these numbers are liable to be changed if there are conflicts in the
relevant merge window.


\section{Compatibility System Calls (Generic)}
\label{process/adding-syscalls:compatibility-system-calls-generic}
For most system calls the same 64-bit implementation can be invoked even when
the userspace program is itself 32-bit; even if the system call's parameters
include an explicit pointer, this is handled transparently.

However, there are a couple of situations where a compatibility layer is
needed to cope with size differences between 32-bit and 64-bit.

The first is if the 64-bit kernel also supports 32-bit userspace programs, and
so needs to parse areas of (\code{\_\_user}) memory that could hold either 32-bit or
64-bit values.  In particular, this is needed whenever a system call argument
is:
\begin{itemize}
\item {} 
a pointer to a pointer

\item {} 
a pointer to a struct containing a pointer (e.g. \code{struct iovec \_\_user *})

\item {} 
a pointer to a varying sized integral type (\code{time\_t}, \code{off\_t},
\code{long}, ...)

\item {} 
a pointer to a struct containing a varying sized integral type.

\end{itemize}

The second situation that requires a compatibility layer is if one of the
system call's arguments has a type that is explicitly 64-bit even on a 32-bit
architecture, for example \code{loff\_t} or \code{\_\_u64}.  In this case, a value that
arrives at a 64-bit kernel from a 32-bit application will be split into two
32-bit values, which then need to be re-assembled in the compatibility layer.

(Note that a system call argument that's a pointer to an explicit 64-bit type
does \textbf{not} need a compatibility layer; for example, \emph{\texttt{splice(2)}}`s arguments of
type \code{loff\_t \_\_user *} do not trigger the need for a \code{compat\_} system call.)

The compatibility version of the system call is called \code{compat\_sys\_xyzzy()},
and is added with the \code{COMPAT\_SYSCALL\_DEFINEn()} macro, analogously to
SYSCALL\_DEFINEn.  This version of the implementation runs as part of a 64-bit
kernel, but expects to receive 32-bit parameter values and does whatever is
needed to deal with them.  (Typically, the \code{compat\_sys\_} version converts the
values to 64-bit versions and either calls on to the \code{sys\_} version, or both of
them call a common inner implementation function.)

The compat entry point also needs a corresponding function prototype, in
\code{include/linux/compat.h}, marked as asmlinkage to match the way that system
calls are invoked:

\begin{Verbatim}[commandchars=\\\{\}]
asmlinkage long compat\PYGZus{}sys\PYGZus{}xyzzy(...);
\end{Verbatim}

If the system call involves a structure that is laid out differently on 32-bit
and 64-bit systems, say \code{struct xyzzy\_args}, then the include/linux/compat.h
header file should also include a compat version of the structure (\code{struct
compat\_xyzzy\_args}) where each variable-size field has the appropriate
\code{compat\_} type that corresponds to the type in \code{struct xyzzy\_args}.  The
\code{compat\_sys\_xyzzy()} routine can then use this \code{compat\_} structure to
parse the arguments from a 32-bit invocation.

For example, if there are fields:

\begin{Verbatim}[commandchars=\\\{\}]
struct xyzzy\PYGZus{}args \PYGZob{}
    const char \PYGZus{}\PYGZus{}user *ptr;
    \PYGZus{}\PYGZus{}kernel\PYGZus{}long\PYGZus{}t varying\PYGZus{}val;
    u64 fixed\PYGZus{}val;
    /* ... */
\PYGZcb{};
\end{Verbatim}

in struct xyzzy\_args, then struct compat\_xyzzy\_args would have:

\begin{Verbatim}[commandchars=\\\{\}]
struct compat\PYGZus{}xyzzy\PYGZus{}args \PYGZob{}
    compat\PYGZus{}uptr\PYGZus{}t ptr;
    compat\PYGZus{}long\PYGZus{}t varying\PYGZus{}val;
    u64 fixed\PYGZus{}val;
    /* ... */
\PYGZcb{};
\end{Verbatim}

The generic system call list also needs adjusting to allow for the compat
version; the entry in \code{include/uapi/asm-generic/unistd.h} should use
\code{\_\_SC\_COMP} rather than \code{\_\_SYSCALL}:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZsh{}define \PYGZus{}\PYGZus{}NR\PYGZus{}xyzzy 292
\PYGZus{}\PYGZus{}SC\PYGZus{}COMP(\PYGZus{}\PYGZus{}NR\PYGZus{}xyzzy, sys\PYGZus{}xyzzy, compat\PYGZus{}sys\PYGZus{}xyzzy)
\end{Verbatim}

To summarize, you need:
\begin{itemize}
\item {} 
a \code{COMPAT\_SYSCALL\_DEFINEn(xyzzy, ...)} for the compat entry point

\item {} 
corresponding prototype in \code{include/linux/compat.h}

\item {} 
(if needed) 32-bit mapping struct in \code{include/linux/compat.h}

\item {} 
instance of \code{\_\_SC\_COMP} not \code{\_\_SYSCALL} in
\code{include/uapi/asm-generic/unistd.h}

\end{itemize}


\section{Compatibility System Calls (x86)}
\label{process/adding-syscalls:compatibility-system-calls-x86}
To wire up the x86 architecture of a system call with a compatibility version,
the entries in the syscall tables need to be adjusted.

First, the entry in \code{arch/x86/entry/syscalls/syscall\_32.tbl} gets an extra
column to indicate that a 32-bit userspace program running on a 64-bit kernel
should hit the compat entry point:

\begin{Verbatim}[commandchars=\\\{\}]
380   i386     xyzzy     sys\PYGZus{}xyzzy    compat\PYGZus{}sys\PYGZus{}xyzzy
\end{Verbatim}

Second, you need to figure out what should happen for the x32 ABI version of
the new system call.  There's a choice here: the layout of the arguments
should either match the 64-bit version or the 32-bit version.

If there's a pointer-to-a-pointer involved, the decision is easy: x32 is
ILP32, so the layout should match the 32-bit version, and the entry in
\code{arch/x86/entry/syscalls/syscall\_64.tbl} is split so that x32 programs hit
the compatibility wrapper:

\begin{Verbatim}[commandchars=\\\{\}]
333   64       xyzzy     sys\PYGZus{}xyzzy
...
555   x32      xyzzy     compat\PYGZus{}sys\PYGZus{}xyzzy
\end{Verbatim}

If no pointers are involved, then it is preferable to re-use the 64-bit system
call for the x32 ABI (and consequently the entry in
arch/x86/entry/syscalls/syscall\_64.tbl is unchanged).

In either case, you should check that the types involved in your argument
layout do indeed map exactly from x32 (-mx32) to either the 32-bit (-m32) or
64-bit (-m64) equivalents.


\section{System Calls Returning Elsewhere}
\label{process/adding-syscalls:system-calls-returning-elsewhere}
For most system calls, once the system call is complete the user program
continues exactly where it left off -- at the next instruction, with the
stack the same and most of the registers the same as before the system call,
and with the same virtual memory space.

However, a few system calls do things differently.  They might return to a
different location (\code{rt\_sigreturn}) or change the memory space
(\code{fork}/\code{vfork}/\code{clone}) or even architecture (\code{execve}/\code{execveat})
of the program.

To allow for this, the kernel implementation of the system call may need to
save and restore additional registers to the kernel stack, allowing complete
control of where and how execution continues after the system call.

This is arch-specific, but typically involves defining assembly entry points
that save/restore additional registers and invoke the real system call entry
point.

For x86\_64, this is implemented as a \code{stub\_xyzzy} entry point in
\code{arch/x86/entry/entry\_64.S}, and the entry in the syscall table
(\code{arch/x86/entry/syscalls/syscall\_64.tbl}) is adjusted to match:

\begin{Verbatim}[commandchars=\\\{\}]
333   common   xyzzy     stub\PYGZus{}xyzzy
\end{Verbatim}

The equivalent for 32-bit programs running on a 64-bit kernel is normally
called \code{stub32\_xyzzy} and implemented in \code{arch/x86/entry/entry\_64\_compat.S},
with the corresponding syscall table adjustment in
\code{arch/x86/entry/syscalls/syscall\_32.tbl}:

\begin{Verbatim}[commandchars=\\\{\}]
380   i386     xyzzy     sys\PYGZus{}xyzzy    stub32\PYGZus{}xyzzy
\end{Verbatim}

If the system call needs a compatibility layer (as in the previous section)
then the \code{stub32\_} version needs to call on to the \code{compat\_sys\_} version
of the system call rather than the native 64-bit version.  Also, if the x32 ABI
implementation is not common with the x86\_64 version, then its syscall
table will also need to invoke a stub that calls on to the \code{compat\_sys\_}
version.

For completeness, it's also nice to set up a mapping so that user-mode Linux
still works -- its syscall table will reference stub\_xyzzy, but the UML build
doesn't include \code{arch/x86/entry/entry\_64.S} implementation (because UML
simulates registers etc).  Fixing this is as simple as adding a \#define to
\code{arch/x86/um/sys\_call\_table\_64.c}:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZsh{}define stub\PYGZus{}xyzzy sys\PYGZus{}xyzzy
\end{Verbatim}


\section{Other Details}
\label{process/adding-syscalls:other-details}
Most of the kernel treats system calls in a generic way, but there is the
occasional exception that may need updating for your particular system call.

The audit subsystem is one such special case; it includes (arch-specific)
functions that classify some special types of system call -- specifically
file open (\code{open}/\code{openat}), program execution (\code{execve}/\code{exeveat}) or
socket multiplexor (\code{socketcall}) operations. If your new system call is
analogous to one of these, then the audit system should be updated.

More generally, if there is an existing system call that is analogous to your
new system call, it's worth doing a kernel-wide grep for the existing system
call to check there are no other special cases.


\section{Testing}
\label{process/adding-syscalls:testing}
A new system call should obviously be tested; it is also useful to provide
reviewers with a demonstration of how user space programs will use the system
call.  A good way to combine these aims is to include a simple self-test
program in a new directory under \code{tools/testing/selftests/}.

For a new system call, there will obviously be no libc wrapper function and so
the test will need to invoke it using \code{syscall()}; also, if the system call
involves a new userspace-visible structure, the corresponding header will need
to be installed to compile the test.

Make sure the selftest runs successfully on all supported architectures.  For
example, check that it works when compiled as an x86\_64 (-m64), x86\_32 (-m32)
and x32 (-mx32) ABI program.

For more extensive and thorough testing of new functionality, you should also
consider adding tests to the Linux Test Project, or to the xfstests project
for filesystem-related changes.
\begin{itemize}
\item {} 
\href{https://linux-test-project.github.io/}{https://linux-test-project.github.io/}

\item {} 
git://git.kernel.org/pub/scm/fs/xfs/xfstests-dev.git

\end{itemize}


\section{Man Page}
\label{process/adding-syscalls:man-page}
All new system calls should come with a complete man page, ideally using groff
markup, but plain text will do.  If groff is used, it's helpful to include a
pre-rendered ASCII version of the man page in the cover email for the
patchset, for the convenience of reviewers.

The man page should be cc'ed to \href{mailto:linux-man@vger.kernel.org}{linux-man@vger.kernel.org}
For more details, see \href{https://www.kernel.org/doc/man-pages/patches.html}{https://www.kernel.org/doc/man-pages/patches.html}


\section{References and Sources}
\label{process/adding-syscalls:references-and-sources}\begin{itemize}
\item {} 
LWN article from Michael Kerrisk on use of flags argument in system calls:
\href{https://lwn.net/Articles/585415/}{https://lwn.net/Articles/585415/}

\item {} 
LWN article from Michael Kerrisk on how to handle unknown flags in a system
call: \href{https://lwn.net/Articles/588444/}{https://lwn.net/Articles/588444/}

\item {} 
LWN article from Jake Edge describing constraints on 64-bit system call
arguments: \href{https://lwn.net/Articles/311630/}{https://lwn.net/Articles/311630/}

\item {} 
Pair of LWN articles from David Drysdale that describe the system call
implementation paths in detail for v3.14:
\begin{itemize}
\item {} 
\href{https://lwn.net/Articles/604287/}{https://lwn.net/Articles/604287/}

\item {} 
\href{https://lwn.net/Articles/604515/}{https://lwn.net/Articles/604515/}

\end{itemize}

\item {} 
Architecture-specific requirements for system calls are discussed in the
\emph{\texttt{syscall(2)}} man-page:
\href{http://man7.org/linux/man-pages/man2/syscall.2.html\#NOTES}{http://man7.org/linux/man-pages/man2/syscall.2.html\#NOTES}

\item {} 
Collated emails from Linus Torvalds discussing the problems with \code{ioctl()}:
\href{http://yarchive.net/comp/linux/ioctl.html}{http://yarchive.net/comp/linux/ioctl.html}

\item {} 
``How to not invent kernel interfaces'', Arnd Bergmann,
\href{http://www.ukuug.org/events/linux2007/2007/papers/Bergmann.pdf}{http://www.ukuug.org/events/linux2007/2007/papers/Bergmann.pdf}

\item {} 
LWN article from Michael Kerrisk on avoiding new uses of CAP\_SYS\_ADMIN:
\href{https://lwn.net/Articles/486306/}{https://lwn.net/Articles/486306/}

\item {} 
Recommendation from Andrew Morton that all related information for a new
system call should come in the same email thread:
\href{https://lkml.org/lkml/2014/7/24/641}{https://lkml.org/lkml/2014/7/24/641}

\item {} 
Recommendation from Michael Kerrisk that a new system call should come with
a man page: \href{https://lkml.org/lkml/2014/6/13/309}{https://lkml.org/lkml/2014/6/13/309}

\item {} 
Suggestion from Thomas Gleixner that x86 wire-up should be in a separate
commit: \href{https://lkml.org/lkml/2014/11/19/254}{https://lkml.org/lkml/2014/11/19/254}

\item {} 
Suggestion from Greg Kroah-Hartman that it's good for new system calls to
come with a man-page \& selftest: \href{https://lkml.org/lkml/2014/3/19/710}{https://lkml.org/lkml/2014/3/19/710}

\item {} 
Discussion from Michael Kerrisk of new system call vs. \emph{\texttt{prctl(2)}} extension:
\href{https://lkml.org/lkml/2014/6/3/411}{https://lkml.org/lkml/2014/6/3/411}

\item {} 
Suggestion from Ingo Molnar that system calls that involve multiple
arguments should encapsulate those arguments in a struct, which includes a
size field for future extensibility: \href{https://lkml.org/lkml/2015/7/30/117}{https://lkml.org/lkml/2015/7/30/117}

\item {} 
Numbering oddities arising from (re-)use of O\_* numbering space flags:
\begin{itemize}
\item {} 
commit 75069f2b5bfb (``vfs: renumber FMODE\_NONOTIFY and add to uniqueness
check'')

\item {} 
commit 12ed2e36c98a (``fanotify: FMODE\_NONOTIFY and \_\_O\_SYNC in sparc
conflict'')

\item {} 
commit bb458c644a59 (``Safer ABI for O\_TMPFILE'')

\end{itemize}

\item {} 
Discussion from Matthew Wilcox about restrictions on 64-bit arguments:
\href{https://lkml.org/lkml/2008/12/12/187}{https://lkml.org/lkml/2008/12/12/187}

\item {} 
Recommendation from Greg Kroah-Hartman that unknown flags should be
policed: \href{https://lkml.org/lkml/2014/7/17/577}{https://lkml.org/lkml/2014/7/17/577}

\item {} 
Recommendation from Linus Torvalds that x32 system calls should prefer
compatibility with 64-bit versions rather than 32-bit versions:
\href{https://lkml.org/lkml/2011/8/31/244}{https://lkml.org/lkml/2011/8/31/244}

\end{itemize}


\chapter{Linux magic numbers}
\label{process/magic-number::doc}\label{process/magic-number:linux-magic-numbers}
This file is a registry of magic numbers which are in use.  When you
add a magic number to a structure, you should also add it to this
file, since it is best if the magic numbers used by various structures
are unique.

It is a \textbf{very} good idea to protect kernel data structures with magic
numbers.  This allows you to check at run time whether (a) a structure
has been clobbered, or (b) you've passed the wrong structure to a
routine.  This last is especially useful --- particularly when you are
passing pointers to structures via a void * pointer.  The tty code,
for example, does this frequently to pass driver-specific and line
discipline-specific structures back and forth.

The way to use magic numbers is to declare then at the beginning of
the structure, like so:

\begin{Verbatim}[commandchars=\\\{\}]
struct tty\PYGZus{}ldisc \PYGZob{}
        int     magic;
        ...
\PYGZcb{};
\end{Verbatim}

Please follow this discipline when you are adding future enhancements
to the kernel!  It has saved me countless hours of debugging,
especially in the screwy cases where an array has been overrun and
structures following the array have been overwritten.  Using this
discipline, these cases get detected quickly and safely.

Changelog:

\begin{Verbatim}[commandchars=\\\{\}]
                                      Theodore Ts\PYGZsq{}o
                                      31 Mar 94

The magic table is current to Linux 2.1.55.

                                      Michael Chastain
                                      \PYGZlt{}mailto:mec@shout.net\PYGZgt{}
                                      22 Sep 1997

Now it should be up to date with Linux 2.1.112. Because
we are in feature freeze time it is very unlikely that
something will change before 2.2.x. The entries are
sorted by number field.

                                      Krzysztof G. Baranowski
                                      \PYGZlt{}mailto: kgb@knm.org.pl\PYGZgt{}
                                      29 Jul 1998

Updated the magic table to Linux 2.5.45. Right over the feature freeze,
but it is possible that some new magic numbers will sneak into the
kernel before 2.6.x yet.

                                      Petr Baudis
                                      \PYGZlt{}pasky@ucw.cz\PYGZgt{}
                                      03 Nov 2002

Updated the magic table to Linux 2.5.74.

                                      Fabian Frederick
                                      \PYGZlt{}ffrederick@users.sourceforge.net\PYGZgt{}
                                      09 Jul 2003
\end{Verbatim}

\begin{longtable}{|l|l|l|l|}
\hline
\textsf{\relax 
Magic Name
} & \textsf{\relax 
Number
} & \textsf{\relax 
Structure
} & \textsf{\relax 
File
}\\
\hline\endfirsthead

\multicolumn{4}{c}%
{{\textsf{\tablename\ \thetable{} -- continued from previous page}}} \\
\hline
\textsf{\relax 
Magic Name
} & \textsf{\relax 
Number
} & \textsf{\relax 
Structure
} & \textsf{\relax 
File
}\\
\hline\endhead

\hline \multicolumn{4}{|r|}{{\textsf{Continued on next page}}} \\ \hline
\endfoot

\endlastfoot


PG\_MAGIC
 & 
`P'
 & 
pg\_\{read,write\}\_hdr
 & 
\code{include/linux/pg.h}
\\
\hline
CMAGIC
 & 
0x0111
 & 
user
 & 
\code{include/linux/a.out.h}
\\
\hline
MKISS\_DRIVER\_MAGIC
 & 
0x04bf
 & 
mkiss\_channel
 & 
\code{drivers/net/mkiss.h}
\\
\hline
HDLC\_MAGIC
 & 
0x239e
 & 
n\_hdlc
 & 
\code{drivers/char/n\_hdlc.c}
\\
\hline
APM\_BIOS\_MAGIC
 & 
0x4101
 & 
apm\_user
 & 
\code{arch/x86/kernel/apm\_32.c}
\\
\hline
CYCLADES\_MAGIC
 & 
0x4359
 & 
cyclades\_port
 & 
\code{include/linux/cyclades.h}
\\
\hline
DB\_MAGIC
 & 
0x4442
 & 
fc\_info
 & 
\code{drivers/net/iph5526\_novram.c}
\\
\hline
DL\_MAGIC
 & 
0x444d
 & 
fc\_info
 & 
\code{drivers/net/iph5526\_novram.c}
\\
\hline
FASYNC\_MAGIC
 & 
0x4601
 & 
fasync\_struct
 & 
\code{include/linux/fs.h}
\\
\hline
FF\_MAGIC
 & 
0x4646
 & 
fc\_info
 & 
\code{drivers/net/iph5526\_novram.c}
\\
\hline
ISICOM\_MAGIC
 & 
0x4d54
 & 
isi\_port
 & 
\code{include/linux/isicom.h}
\\
\hline
PTY\_MAGIC
 & 
0x5001
 &  & 
\code{drivers/char/pty.c}
\\
\hline
PPP\_MAGIC
 & 
0x5002
 & 
ppp
 & 
\code{include/linux/if\_pppvar.h}
\\
\hline
SERIAL\_MAGIC
 & 
0x5301
 & 
async\_struct
 & 
\code{include/linux/serial.h}
\\
\hline
SSTATE\_MAGIC
 & 
0x5302
 & 
serial\_state
 & 
\code{include/linux/serial.h}
\\
\hline
SLIP\_MAGIC
 & 
0x5302
 & 
slip
 & 
\code{drivers/net/slip.h}
\\
\hline
STRIP\_MAGIC
 & 
0x5303
 & 
strip
 & 
\code{drivers/net/strip.c}
\\
\hline
X25\_ASY\_MAGIC
 & 
0x5303
 & 
x25\_asy
 & 
\code{drivers/net/x25\_asy.h}
\\
\hline
SIXPACK\_MAGIC
 & 
0x5304
 & 
sixpack
 & 
\code{drivers/net/hamradio/6pack.h}
\\
\hline
AX25\_MAGIC
 & 
0x5316
 & 
ax\_disp
 & 
\code{drivers/net/mkiss.h}
\\
\hline
TTY\_MAGIC
 & 
0x5401
 & 
tty\_struct
 & 
\code{include/linux/tty.h}
\\
\hline
MGSL\_MAGIC
 & 
0x5401
 & 
mgsl\_info
 & 
\code{drivers/char/synclink.c}
\\
\hline
TTY\_DRIVER\_MAGIC
 & 
0x5402
 & 
tty\_driver
 & 
\code{include/linux/tty\_driver.h}
\\
\hline
MGSLPC\_MAGIC
 & 
0x5402
 & 
mgslpc\_info
 & 
\code{drivers/char/pcmcia/synclink\_cs.c}
\\
\hline
TTY\_LDISC\_MAGIC
 & 
0x5403
 & 
tty\_ldisc
 & 
\code{include/linux/tty\_ldisc.h}
\\
\hline
USB\_SERIAL\_MAGIC
 & 
0x6702
 & 
usb\_serial
 & 
\code{drivers/usb/serial/usb-serial.h}
\\
\hline
FULL\_DUPLEX\_MAGIC
 & 
0x6969
 &  & 
\code{drivers/net/ethernet/dec/tulip/de2104x.c}
\\
\hline
USB\_BLUETOOTH\_MAGIC
 & 
0x6d02
 & 
usb\_bluetooth
 & 
\code{drivers/usb/class/bluetty.c}
\\
\hline
RFCOMM\_TTY\_MAGIC
 & 
0x6d02
 &  & 
\code{net/bluetooth/rfcomm/tty.c}
\\
\hline
USB\_SERIAL\_PORT\_MAGIC
 & 
0x7301
 & 
usb\_serial\_port
 & 
\code{drivers/usb/serial/usb-serial.h}
\\
\hline
CG\_MAGIC
 & 
0x00090255
 & 
ufs\_cylinder\_group
 & 
\code{include/linux/ufs\_fs.h}
\\
\hline
RPORT\_MAGIC
 & 
0x00525001
 & 
r\_port
 & 
\code{drivers/char/rocket\_int.h}
\\
\hline
LSEMAGIC
 & 
0x05091998
 & 
lse
 & 
\code{drivers/fc4/fc.c}
\\
\hline
GDTIOCTL\_MAGIC
 & 
0x06030f07
 & 
gdth\_iowr\_str
 & 
\code{drivers/scsi/gdth\_ioctl.h}
\\
\hline
RIEBL\_MAGIC
 & 
0x09051990
 &  & 
\code{drivers/net/atarilance.c}
\\
\hline
NBD\_REQUEST\_MAGIC
 & 
0x12560953
 & 
nbd\_request
 & 
\code{include/linux/nbd.h}
\\
\hline
RED\_MAGIC2
 & 
0x170fc2a5
 & 
(any)
 & 
\code{mm/slab.c}
\\
\hline
BAYCOM\_MAGIC
 & 
0x19730510
 & 
baycom\_state
 & 
\code{drivers/net/baycom\_epp.c}
\\
\hline
ISDN\_X25IFACE\_MAGIC
 & 
0x1e75a2b9
 & 
isdn\_x25iface\_proto\_data
 & 
\code{drivers/isdn/isdn\_x25iface.h}
\\
\hline
ECP\_MAGIC
 & 
0x21504345
 & 
cdkecpsig
 & 
\code{include/linux/cdk.h}
\\
\hline
LSOMAGIC
 & 
0x27091997
 & 
lso
 & 
\code{drivers/fc4/fc.c}
\\
\hline
LSMAGIC
 & 
0x2a3b4d2a
 & 
ls
 & 
\code{drivers/fc4/fc.c}
\\
\hline
WANPIPE\_MAGIC
 & 
0x414C4453
 & 
sdla\_\{dump,exec\}
 & 
\code{include/linux/wanpipe.h}
\\
\hline
CS\_CARD\_MAGIC
 & 
0x43525553
 & 
cs\_card
 & 
\code{sound/oss/cs46xx.c}
\\
\hline
LABELCL\_MAGIC
 & 
0x4857434c
 & 
labelcl\_info\_s
 & 
\code{include/asm/ia64/sn/labelcl.h}
\\
\hline
ISDN\_ASYNC\_MAGIC
 & 
0x49344C01
 & 
modem\_info
 & 
\code{include/linux/isdn.h}
\\
\hline
CTC\_ASYNC\_MAGIC
 & 
0x49344C01
 & 
ctc\_tty\_info
 & 
\code{drivers/s390/net/ctctty.c}
\\
\hline
ISDN\_NET\_MAGIC
 & 
0x49344C02
 & 
isdn\_net\_local\_s
 & 
\code{drivers/isdn/i4l/isdn\_net\_lib.h}
\\
\hline
SAVEKMSG\_MAGIC2
 & 
0x4B4D5347
 & 
savekmsg
 & 
\code{arch/*/amiga/config.c}
\\
\hline
CS\_STATE\_MAGIC
 & 
0x4c4f4749
 & 
cs\_state
 & 
\code{sound/oss/cs46xx.c}
\\
\hline
SLAB\_C\_MAGIC
 & 
0x4f17a36d
 & 
kmem\_cache
 & 
\code{mm/slab.c}
\\
\hline
COW\_MAGIC
 & 
0x4f4f4f4d
 & 
cow\_header\_v1
 & 
\code{arch/um/drivers/ubd\_user.c}
\\
\hline
I810\_CARD\_MAGIC
 & 
0x5072696E
 & 
i810\_card
 & 
\code{sound/oss/i810\_audio.c}
\\
\hline
TRIDENT\_CARD\_MAGIC
 & 
0x5072696E
 & 
trident\_card
 & 
\code{sound/oss/trident.c}
\\
\hline
ROUTER\_MAGIC
 & 
0x524d4157
 & 
wan\_device
 & 
{[}in \code{wanrouter.h} pre 3.9{]}
\\
\hline
SAVEKMSG\_MAGIC1
 & 
0x53415645
 & 
savekmsg
 & 
\code{arch/*/amiga/config.c}
\\
\hline
GDA\_MAGIC
 & 
0x58464552
 & 
gda
 & 
\code{arch/mips/include/asm/sn/gda.h}
\\
\hline
RED\_MAGIC1
 & 
0x5a2cf071
 & 
(any)
 & 
\code{mm/slab.c}
\\
\hline
EEPROM\_MAGIC\_VALUE
 & 
0x5ab478d2
 & 
lanai\_dev
 & 
\code{drivers/atm/lanai.c}
\\
\hline
HDLCDRV\_MAGIC
 & 
0x5ac6e778
 & 
hdlcdrv\_state
 & 
\code{include/linux/hdlcdrv.h}
\\
\hline
PCXX\_MAGIC
 & 
0x5c6df104
 & 
channel
 & 
\code{drivers/char/pcxx.h}
\\
\hline
KV\_MAGIC
 & 
0x5f4b565f
 & 
kernel\_vars\_s
 & 
\code{arch/mips/include/asm/sn/klkernvars.h}
\\
\hline
I810\_STATE\_MAGIC
 & 
0x63657373
 & 
i810\_state
 & 
\code{sound/oss/i810\_audio.c}
\\
\hline
TRIDENT\_STATE\_MAGIC
 & 
0x63657373
 & 
trient\_state
 & 
\code{sound/oss/trident.c}
\\
\hline
M3\_CARD\_MAGIC
 & 
0x646e6f50
 & 
m3\_card
 & 
\code{sound/oss/maestro3.c}
\\
\hline
FW\_HEADER\_MAGIC
 & 
0x65726F66
 & 
fw\_header
 & 
\code{drivers/atm/fore200e.h}
\\
\hline
SLOT\_MAGIC
 & 
0x67267321
 & 
slot
 & 
\code{drivers/hotplug/cpqphp.h}
\\
\hline
SLOT\_MAGIC
 & 
0x67267322
 & 
slot
 & 
\code{drivers/hotplug/acpiphp.h}
\\
\hline
LO\_MAGIC
 & 
0x68797548
 & 
nbd\_device
 & 
\code{include/linux/nbd.h}
\\
\hline
OPROFILE\_MAGIC
 & 
0x6f70726f
 & 
super\_block
 & 
\code{drivers/oprofile/oprofilefs.h}
\\
\hline
M3\_STATE\_MAGIC
 & 
0x734d724d
 & 
m3\_state
 & 
\code{sound/oss/maestro3.c}
\\
\hline
VMALLOC\_MAGIC
 & 
0x87654320
 & 
snd\_alloc\_track
 & 
\code{sound/core/memory.c}
\\
\hline
KMALLOC\_MAGIC
 & 
0x87654321
 & 
snd\_alloc\_track
 & 
\code{sound/core/memory.c}
\\
\hline
PWC\_MAGIC
 & 
0x89DC10AB
 & 
pwc\_device
 & 
\code{drivers/usb/media/pwc.h}
\\
\hline
NBD\_REPLY\_MAGIC
 & 
0x96744668
 & 
nbd\_reply
 & 
\code{include/linux/nbd.h}
\\
\hline
ENI155\_MAGIC
 & 
0xa54b872d
 & 
midway\_eprom
 & 
\code{drivers/atm/eni.h}
\\
\hline
CODA\_MAGIC
 & 
0xC0DAC0DA
 & 
coda\_file\_info
 & 
\code{fs/coda/coda\_fs\_i.h}
\\
\hline
DPMEM\_MAGIC
 & 
0xc0ffee11
 & 
gdt\_pci\_sram
 & 
\code{drivers/scsi/gdth.h}
\\
\hline
YAM\_MAGIC
 & 
0xF10A7654
 & 
yam\_port
 & 
\code{drivers/net/hamradio/yam.c}
\\
\hline
CCB\_MAGIC
 & 
0xf2691ad2
 & 
ccb
 & 
\code{drivers/scsi/ncr53c8xx.c}
\\
\hline
QUEUE\_MAGIC\_FREE
 & 
0xf7e1c9a3
 & 
queue\_entry
 & 
\code{drivers/scsi/arm/queue.c}
\\
\hline
QUEUE\_MAGIC\_USED
 & 
0xf7e1cc33
 & 
queue\_entry
 & 
\code{drivers/scsi/arm/queue.c}
\\
\hline
HTB\_CMAGIC
 & 
0xFEFAFEF1
 & 
htb\_class
 & 
\code{net/sched/sch\_htb.c}
\\
\hline
NMI\_MAGIC
 & 
0x48414d4d455201
 & 
nmi\_s
 & 
\code{arch/mips/include/asm/sn/nmi.h}
\\
\hline\end{longtable}


Note that there are also defined special per-driver magic numbers in sound
memory management. See \code{include/sound/sndmagic.h} for complete list of them. Many
OSS sound drivers have their magic numbers constructed from the soundcard PCI
ID - these are not listed here as well.

IrDA subsystem also uses large number of own magic numbers, see
\code{include/net/irda/irda.h} for a complete list of them.

HFS is another larger user of magic numbers - you can find them in
\code{fs/hfs/hfs.h}.


\chapter{Why the ``volatile'' type class should not be used}
\label{process/volatile-considered-harmful:volatile-considered-harmful}\label{process/volatile-considered-harmful::doc}\label{process/volatile-considered-harmful:why-the-volatile-type-class-should-not-be-used}
C programmers have often taken volatile to mean that the variable could be
changed outside of the current thread of execution; as a result, they are
sometimes tempted to use it in kernel code when shared data structures are
being used.  In other words, they have been known to treat volatile types
as a sort of easy atomic variable, which they are not.  The use of volatile in
kernel code is almost never correct; this document describes why.

The key point to understand with regard to volatile is that its purpose is
to suppress optimization, which is almost never what one really wants to
do.  In the kernel, one must protect shared data structures against
unwanted concurrent access, which is very much a different task.  The
process of protecting against unwanted concurrency will also avoid almost
all optimization-related problems in a more efficient way.

Like volatile, the kernel primitives which make concurrent access to data
safe (spinlocks, mutexes, memory barriers, etc.) are designed to prevent
unwanted optimization.  If they are being used properly, there will be no
need to use volatile as well.  If volatile is still necessary, there is
almost certainly a bug in the code somewhere.  In properly-written kernel
code, volatile can only serve to slow things down.

Consider a typical block of kernel code:

\begin{Verbatim}[commandchars=\\\{\}]
spin\PYGZus{}lock(\PYGZam{}the\PYGZus{}lock);
do\PYGZus{}something\PYGZus{}on(\PYGZam{}shared\PYGZus{}data);
do\PYGZus{}something\PYGZus{}else\PYGZus{}with(\PYGZam{}shared\PYGZus{}data);
spin\PYGZus{}unlock(\PYGZam{}the\PYGZus{}lock);
\end{Verbatim}

If all the code follows the locking rules, the value of shared\_data cannot
change unexpectedly while the\_lock is held.  Any other code which might
want to play with that data will be waiting on the lock.  The spinlock
primitives act as memory barriers - they are explicitly written to do so -
meaning that data accesses will not be optimized across them.  So the
compiler might think it knows what will be in shared\_data, but the
spin\_lock() call, since it acts as a memory barrier, will force it to
forget anything it knows.  There will be no optimization problems with
accesses to that data.

If shared\_data were declared volatile, the locking would still be
necessary.  But the compiler would also be prevented from optimizing access
to shared\_data \_within\_ the critical section, when we know that nobody else
can be working with it.  While the lock is held, shared\_data is not
volatile.  When dealing with shared data, proper locking makes volatile
unnecessary - and potentially harmful.

The volatile storage class was originally meant for memory-mapped I/O
registers.  Within the kernel, register accesses, too, should be protected
by locks, but one also does not want the compiler ``optimizing'' register
accesses within a critical section.  But, within the kernel, I/O memory
accesses are always done through accessor functions; accessing I/O memory
directly through pointers is frowned upon and does not work on all
architectures.  Those accessors are written to prevent unwanted
optimization, so, once again, volatile is unnecessary.

Another situation where one might be tempted to use volatile is
when the processor is busy-waiting on the value of a variable.  The right
way to perform a busy wait is:

\begin{Verbatim}[commandchars=\\\{\}]
while (my\PYGZus{}variable != what\PYGZus{}i\PYGZus{}want)
    cpu\PYGZus{}relax();
\end{Verbatim}

The cpu\_relax() call can lower CPU power consumption or yield to a
hyperthreaded twin processor; it also happens to serve as a compiler
barrier, so, once again, volatile is unnecessary.  Of course, busy-
waiting is generally an anti-social act to begin with.

There are still a few rare situations where volatile makes sense in the
kernel:
\begin{itemize}
\item {} 
The above-mentioned accessor functions might use volatile on
architectures where direct I/O memory access does work.  Essentially,
each accessor call becomes a little critical section on its own and
ensures that the access happens as expected by the programmer.

\item {} 
Inline assembly code which changes memory, but which has no other
visible side effects, risks being deleted by GCC.  Adding the volatile
keyword to asm statements will prevent this removal.

\item {} 
The jiffies variable is special in that it can have a different value
every time it is referenced, but it can be read without any special
locking.  So jiffies can be volatile, but the addition of other
variables of this type is strongly frowned upon.  Jiffies is considered
to be a ``stupid legacy'' issue (Linus's words) in this regard; fixing it
would be more trouble than it is worth.

\item {} 
Pointers to data structures in coherent memory which might be modified
by I/O devices can, sometimes, legitimately be volatile.  A ring buffer
used by a network adapter, where that adapter changes pointers to
indicate which descriptors have been processed, is an example of this
type of situation.

\end{itemize}

For most code, none of the above justifications for volatile apply.  As a
result, the use of volatile is likely to be seen as a bug and will bring
additional scrutiny to the code.  Developers who are tempted to use
volatile should take a step back and think about what they are truly trying
to accomplish.

Patches to remove volatile variables are generally welcome - as long as
they come with a justification which shows that the concurrency issues have
been properly thought through.


\section{References}
\label{process/volatile-considered-harmful:references}
{[}1{]} \href{http://lwn.net/Articles/233481/}{http://lwn.net/Articles/233481/}

{[}2{]} \href{http://lwn.net/Articles/233482/}{http://lwn.net/Articles/233482/}


\section{Credits}
\label{process/volatile-considered-harmful:credits}
Original impetus and research by Randy Dunlap

Written by Jonathan Corbet

Improvements via comments from Satyam Sharma, Johannes Stezenbach, Jesper
Juhl, Heikki Orsila, H. Peter Anvin, Philipp Hahn, and Stefan
Richter.



\renewcommand{\indexname}{Index}
\printindex
\end{document}
